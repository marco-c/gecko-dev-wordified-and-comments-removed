#
ifndef
FSTAR_UINT128_MSVC
#
define
FSTAR_UINT128_MSVC
#
include
"
FStar_UInt128
.
h
"
#
include
"
FStar_UInt_8_16_32_64
.
h
"
#
include
"
krml
/
internal
/
types
.
h
"
#
ifndef
_MSC_VER
#
error
This
file
only
works
with
the
MSVC
compiler
#
endif
#
if
defined
(
_M_X64
)
&
&
!
defined
(
KRML_VERIFIED_UINT128
)
#
define
HAS_OPTIMIZED
1
#
else
#
define
HAS_OPTIMIZED
0
#
endif
#
if
HAS_OPTIMIZED
#
include
<
immintrin
.
h
>
#
include
<
intrin
.
h
>
#
define
low
m128i_u64
[
0
]
#
define
high
m128i_u64
[
1
]
#
endif
inline
static
FStar_UInt128_uint128
load128_le
(
uint8_t
*
b
)
{
#
if
HAS_OPTIMIZED
return
_mm_loadu_si128
(
(
__m128i
*
)
b
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
load64_le
(
b
)
;
lit
.
high
=
load64_le
(
b
+
8
)
;
return
lit
;
#
endif
}
inline
static
void
store128_le
(
uint8_t
*
b
FStar_UInt128_uint128
n
)
{
store64_le
(
b
n
.
low
)
;
store64_le
(
b
+
8
n
.
high
)
;
}
inline
static
FStar_UInt128_uint128
load128_be
(
uint8_t
*
b
)
{
uint64_t
l
=
load64_be
(
b
+
8
)
;
uint64_t
h
=
load64_be
(
b
)
;
#
if
HAS_OPTIMIZED
return
_mm_set_epi64x
(
h
l
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
l
;
lit
.
high
=
h
;
return
lit
;
#
endif
}
inline
static
void
store128_be
(
uint8_t
*
b
uint128_t
n
)
{
store64_be
(
b
n
.
high
)
;
store64_be
(
b
+
8
n
.
low
)
;
}
inline
static
uint64_t
FStar_UInt128_constant_time_carry
(
uint64_t
a
uint64_t
b
)
{
return
(
a
^
(
a
^
b
|
a
-
b
^
b
)
)
>
>
(
uint32_t
)
63U
;
}
inline
static
uint64_t
FStar_UInt128_carry
(
uint64_t
a
uint64_t
b
)
{
return
FStar_UInt128_constant_time_carry
(
a
b
)
;
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_add
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
#
if
HAS_OPTIMIZED
uint64_t
l
h
;
unsigned
char
carry
=
_addcarry_u64
(
0
a
.
low
b
.
low
&
l
)
;
_addcarry_u64
(
carry
a
.
high
b
.
high
&
h
)
;
return
_mm_set_epi64x
(
h
l
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
a
.
low
+
b
.
low
;
lit
.
high
=
a
.
high
+
b
.
high
+
FStar_UInt128_carry
(
a
.
low
+
b
.
low
b
.
low
)
;
return
lit
;
#
endif
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_add_underspec
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
#
if
HAS_OPTIMIZED
return
FStar_UInt128_add
(
a
b
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
a
.
low
+
b
.
low
;
lit
.
high
=
a
.
high
+
b
.
high
+
FStar_UInt128_carry
(
a
.
low
+
b
.
low
b
.
low
;
return
lit
;
#
endif
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_add_mod
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
#
if
HAS_OPTIMIZED
return
FStar_UInt128_add
(
a
b
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
a
.
low
+
b
.
low
;
lit
.
high
=
a
.
high
+
b
.
high
+
FStar_UInt128_carry
(
a
.
low
+
b
.
low
b
.
low
)
;
return
lit
;
#
endif
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_sub
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
#
if
HAS_OPTIMIZED
uint64_t
l
h
;
unsigned
char
borrow
=
_subborrow_u64
(
0
a
.
low
b
.
low
&
l
)
;
_subborrow_u64
(
borrow
a
.
high
b
.
high
&
h
)
;
return
_mm_set_epi64x
(
h
l
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
a
.
low
-
b
.
low
;
lit
.
high
=
a
.
high
-
b
.
high
-
FStar_UInt128_carry
(
a
.
low
a
.
low
-
b
.
low
)
;
return
lit
;
#
endif
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_sub_underspec
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
#
if
HAS_OPTIMIZED
return
FStar_UInt128_sub
(
a
b
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
a
.
low
-
b
.
low
;
lit
.
high
=
a
.
high
-
b
.
high
-
FStar_UInt128_carry
(
a
.
low
a
.
low
-
b
.
low
)
;
return
lit
;
#
endif
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_sub_mod_impl
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
FStar_UInt128_uint128
lit
;
lit
.
low
=
a
.
low
-
b
.
low
;
lit
.
high
=
a
.
high
-
b
.
high
-
FStar_UInt128_carry
(
a
.
low
a
.
low
-
b
.
low
)
;
return
lit
;
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_sub_mod
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
#
if
HAS_OPTIMIZED
return
FStar_UInt128_sub
(
a
b
)
;
#
else
return
FStar_UInt128_sub_mod_impl
(
a
b
)
;
#
endif
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_logand
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
#
if
HAS_OPTIMIZED
return
_mm_and_si128
(
a
b
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
a
.
low
&
b
.
low
;
lit
.
high
=
a
.
high
&
b
.
high
;
return
lit
;
#
endif
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_logxor
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
#
if
HAS_OPTIMIZED
return
_mm_xor_si128
(
a
b
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
a
.
low
^
b
.
low
;
lit
.
high
=
a
.
high
^
b
.
high
;
return
lit
;
#
endif
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_logor
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
#
if
HAS_OPTIMIZED
return
_mm_or_si128
(
a
b
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
a
.
low
|
b
.
low
;
lit
.
high
=
a
.
high
|
b
.
high
;
return
lit
;
#
endif
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_lognot
(
FStar_UInt128_uint128
a
)
{
#
if
HAS_OPTIMIZED
return
_mm_andnot_si128
(
a
a
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
~
a
.
low
;
lit
.
high
=
~
a
.
high
;
return
lit
;
#
endif
}
static
const
uint32_t
FStar_UInt128_u32_64
=
(
uint32_t
)
64U
;
inline
static
uint64_t
FStar_UInt128_add_u64_shift_left
(
uint64_t
hi
uint64_t
lo
uint32_t
s
)
{
return
(
hi
<
<
s
)
+
(
lo
>
>
(
FStar_UInt128_u32_64
-
s
)
)
;
}
inline
static
uint64_t
FStar_UInt128_add_u64_shift_left_respec
(
uint64_t
hi
uint64_t
lo
uint32_t
s
)
{
return
FStar_UInt128_add_u64_shift_left
(
hi
lo
s
)
;
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_shift_left_small
(
FStar_UInt128_uint128
a
uint32_t
s
)
{
if
(
s
=
=
(
uint32_t
)
0U
)
return
a
;
else
{
FStar_UInt128_uint128
lit
;
lit
.
low
=
a
.
low
<
<
s
;
lit
.
high
=
FStar_UInt128_add_u64_shift_left_respec
(
a
.
high
a
.
low
s
)
;
return
lit
;
}
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_shift_left_large
(
FStar_UInt128_uint128
a
uint32_t
s
)
{
FStar_UInt128_uint128
lit
;
lit
.
low
=
(
uint64_t
)
0U
;
lit
.
high
=
a
.
low
<
<
(
s
-
FStar_UInt128_u32_64
)
;
return
lit
;
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_shift_left
(
FStar_UInt128_uint128
a
uint32_t
s
)
{
#
if
HAS_OPTIMIZED
if
(
s
=
=
0
)
{
return
a
;
}
else
if
(
s
<
FStar_UInt128_u32_64
)
{
uint64_t
l
=
a
.
low
<
<
s
;
uint64_t
h
=
__shiftleft128
(
a
.
low
a
.
high
(
unsigned
char
)
s
)
;
return
_mm_set_epi64x
(
h
l
)
;
}
else
{
return
_mm_set_epi64x
(
a
.
low
<
<
(
s
-
FStar_UInt128_u32_64
)
0
)
;
}
#
else
if
(
s
<
FStar_UInt128_u32_64
)
return
FStar_UInt128_shift_left_small
(
a
s
)
;
else
return
FStar_UInt128_shift_left_large
(
a
s
)
;
#
endif
}
inline
static
uint64_t
FStar_UInt128_add_u64_shift_right
(
uint64_t
hi
uint64_t
lo
uint32_t
s
)
{
return
(
lo
>
>
s
)
+
(
hi
<
<
(
FStar_UInt128_u32_64
-
s
)
)
;
}
inline
static
uint64_t
FStar_UInt128_add_u64_shift_right_respec
(
uint64_t
hi
uint64_t
lo
uint32_t
s
)
{
return
FStar_UInt128_add_u64_shift_right
(
hi
lo
s
)
;
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_shift_right_small
(
FStar_UInt128_uint128
a
uint32_t
s
)
{
if
(
s
=
=
(
uint32_t
)
0U
)
return
a
;
else
{
FStar_UInt128_uint128
lit
;
lit
.
low
=
FStar_UInt128_add_u64_shift_right_respec
(
a
.
high
a
.
low
s
)
;
lit
.
high
=
a
.
high
>
>
s
;
return
lit
;
}
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_shift_right_large
(
FStar_UInt128_uint128
a
uint32_t
s
)
{
FStar_UInt128_uint128
lit
;
lit
.
low
=
a
.
high
>
>
(
s
-
FStar_UInt128_u32_64
)
;
lit
.
high
=
(
uint64_t
)
0U
;
return
lit
;
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_shift_right
(
FStar_UInt128_uint128
a
uint32_t
s
)
{
#
if
HAS_OPTIMIZED
if
(
s
=
=
0
)
{
return
a
;
}
else
if
(
s
<
FStar_UInt128_u32_64
)
{
uint64_t
l
=
__shiftright128
(
a
.
low
a
.
high
(
unsigned
char
)
s
)
;
uint64_t
h
=
a
.
high
>
>
s
;
return
_mm_set_epi64x
(
h
l
)
;
}
else
{
return
_mm_set_epi64x
(
0
a
.
high
>
>
(
s
-
FStar_UInt128_u32_64
)
)
;
}
#
else
if
(
s
<
FStar_UInt128_u32_64
)
return
FStar_UInt128_shift_right_small
(
a
s
)
;
else
return
FStar_UInt128_shift_right_large
(
a
s
)
;
#
endif
}
inline
static
bool
FStar_UInt128_eq
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
return
a
.
low
=
=
b
.
low
&
&
a
.
high
=
=
b
.
high
;
}
inline
static
bool
FStar_UInt128_gt
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
return
a
.
high
>
b
.
high
|
|
a
.
high
=
=
b
.
high
&
&
a
.
low
>
b
.
low
;
}
inline
static
bool
FStar_UInt128_lt
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
return
a
.
high
<
b
.
high
|
|
a
.
high
=
=
b
.
high
&
&
a
.
low
<
b
.
low
;
}
inline
static
bool
FStar_UInt128_gte
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
return
a
.
high
>
b
.
high
|
|
a
.
high
=
=
b
.
high
&
&
a
.
low
>
=
b
.
low
;
}
inline
static
bool
FStar_UInt128_lte
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
return
a
.
high
<
b
.
high
|
|
a
.
high
=
=
b
.
high
&
&
a
.
low
<
=
b
.
low
;
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_eq_mask
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
#
if
HAS_OPTIMIZED
__m128i
r32
=
_mm_cmpeq_epi32
(
a
b
)
;
__m128i
s32
=
_mm_shuffle_epi32
(
r32
_MM_SHUFFLE
(
2
3
0
1
)
)
;
__m128i
ret64
=
_mm_and_si128
(
r32
s32
)
;
__m128i
s64
=
_mm_shuffle_epi32
(
ret64
_MM_SHUFFLE
(
1
0
3
2
)
)
;
return
_mm_and_si128
(
ret64
s64
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
FStar_UInt64_eq_mask
(
a
.
low
b
.
low
)
&
FStar_UInt64_eq_mask
(
a
.
high
b
.
high
)
;
lit
.
high
=
FStar_UInt64_eq_mask
(
a
.
low
b
.
low
)
&
FStar_UInt64_eq_mask
(
a
.
high
b
.
high
)
;
return
lit
;
#
endif
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_gte_mask
(
FStar_UInt128_uint128
a
FStar_UInt128_uint128
b
)
{
#
if
HAS_OPTIMIZED
&
&
0
__m128i
gt
=
_mm_cmpgt_epi32
(
a
b
)
;
__m128i
eq
=
_mm_cmpeq_epi32
(
a
b
)
;
__m128i
ge
=
_mm_or_si128
(
gt
eq
)
;
__m128i
ge0
=
ge
;
__m128i
eq0
=
eq
;
__m128i
ge1
=
_mm_srli_si128
(
ge
4
)
;
__m128i
t1
=
_mm_and_si128
(
eq0
ge1
)
;
__m128i
ret
=
_mm_or_si128
(
ge
t1
)
;
__m128i
eq1
=
_mm_srli_si128
(
eq
4
)
;
__m128i
ge2
=
_mm_srli_si128
(
ge1
4
)
;
__m128i
t2
=
_mm_and_si128
(
eq0
_mm_and_si128
(
eq1
ge2
)
)
;
ret
=
_mm_or_si128
(
ret
t2
)
;
__m128i
eq2
=
_mm_srli_si128
(
eq1
4
)
;
__m128i
ge3
=
_mm_srli_si128
(
ge2
4
)
;
__m128i
t3
=
_mm_and_si128
(
eq0
_mm_and_si128
(
eq1
_mm_and_si128
(
eq2
ge3
)
)
)
;
ret
=
_mm_or_si128
(
ret
t3
)
;
return
_mm_shuffle_epi32
(
ret
_MM_SHUFFLE
(
0
0
0
0
)
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
FStar_UInt64_gte_mask
(
a
.
high
b
.
high
)
&
~
FStar_UInt64_eq_mask
(
a
.
high
b
.
high
)
|
FStar_UInt64_eq_mask
(
a
.
high
b
.
high
)
&
FStar_UInt64_gte_mask
(
a
.
low
b
.
low
)
;
lit
.
high
=
FStar_UInt64_gte_mask
(
a
.
high
b
.
high
)
&
~
FStar_UInt64_eq_mask
(
a
.
high
b
.
high
)
|
FStar_UInt64_eq_mask
(
a
.
high
b
.
high
)
&
FStar_UInt64_gte_mask
(
a
.
low
b
.
low
)
;
return
lit
;
#
endif
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_uint64_to_uint128
(
uint64_t
a
)
{
#
if
HAS_OPTIMIZED
return
_mm_set_epi64x
(
0
a
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
a
;
lit
.
high
=
(
uint64_t
)
0U
;
return
lit
;
#
endif
}
inline
static
uint64_t
FStar_UInt128_uint128_to_uint64
(
FStar_UInt128_uint128
a
)
{
return
a
.
low
;
}
inline
static
uint64_t
FStar_UInt128_u64_mod_32
(
uint64_t
a
)
{
return
a
&
(
uint64_t
)
0xffffffffU
;
}
static
uint32_t
FStar_UInt128_u32_32
=
(
uint32_t
)
32U
;
inline
static
uint64_t
FStar_UInt128_u32_combine
(
uint64_t
hi
uint64_t
lo
)
{
return
lo
+
(
hi
<
<
FStar_UInt128_u32_32
)
;
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_mul32
(
uint64_t
x
uint32_t
y
)
{
#
if
HAS_OPTIMIZED
uint64_t
l
h
;
l
=
_umul128
(
x
(
uint64_t
)
y
&
h
)
;
return
_mm_set_epi64x
(
h
l
)
;
#
else
FStar_UInt128_uint128
lit
;
lit
.
low
=
FStar_UInt128_u32_combine
(
(
x
>
>
FStar_UInt128_u32_32
)
*
(
uint64_t
)
y
+
(
FStar_UInt128_u64_mod_32
(
x
)
*
(
uint64_t
)
y
>
>
FStar_UInt128_u32_32
)
FStar_UInt128_u64_mod_32
(
FStar_UInt128_u64_mod_32
(
x
)
*
(
uint64_t
)
y
)
)
;
lit
.
high
=
(
x
>
>
FStar_UInt128_u32_32
)
*
(
uint64_t
)
y
+
(
FStar_UInt128_u64_mod_32
(
x
)
*
(
uint64_t
)
y
>
>
FStar_UInt128_u32_32
)
>
>
FStar_UInt128_u32_32
;
return
lit
;
#
endif
}
typedef
struct
K_quad_s
{
uint64_t
fst
;
uint64_t
snd
;
uint64_t
thd
;
uint64_t
f3
;
}
K_quad
;
inline
static
K_quad
FStar_UInt128_mul_wide_impl_t_
(
uint64_t
x
uint64_t
y
)
{
K_quad
tmp
;
tmp
.
fst
=
FStar_UInt128_u64_mod_32
(
x
)
;
tmp
.
snd
=
FStar_UInt128_u64_mod_32
(
FStar_UInt128_u64_mod_32
(
x
)
*
FStar_UInt128_u64_mod_32
(
y
)
)
;
tmp
.
thd
=
x
>
>
FStar_UInt128_u32_32
;
tmp
.
f3
=
(
x
>
>
FStar_UInt128_u32_32
)
*
FStar_UInt128_u64_mod_32
(
y
)
+
(
FStar_UInt128_u64_mod_32
(
x
)
*
FStar_UInt128_u64_mod_32
(
y
)
>
>
FStar_UInt128_u32_32
)
;
return
tmp
;
}
static
uint64_t
FStar_UInt128_u32_combine_
(
uint64_t
hi
uint64_t
lo
)
{
return
lo
+
(
hi
<
<
FStar_UInt128_u32_32
)
;
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_mul_wide_impl
(
uint64_t
x
uint64_t
y
)
{
K_quad
scrut
=
FStar_UInt128_mul_wide_impl_t_
(
x
y
)
;
uint64_t
u1
=
scrut
.
fst
;
uint64_t
w3
=
scrut
.
snd
;
uint64_t
x_
=
scrut
.
thd
;
uint64_t
t_
=
scrut
.
f3
;
FStar_UInt128_uint128
lit
;
lit
.
low
=
FStar_UInt128_u32_combine_
(
u1
*
(
y
>
>
FStar_UInt128_u32_32
)
+
FStar_UInt128_u64_mod_32
(
t_
)
w3
)
;
lit
.
high
=
x_
*
(
y
>
>
FStar_UInt128_u32_32
)
+
(
t_
>
>
FStar_UInt128_u32_32
)
+
(
(
u1
*
(
y
>
>
FStar_UInt128_u32_32
)
+
FStar_UInt128_u64_mod_32
(
t_
)
)
>
>
FStar_UInt128_u32_32
)
;
return
lit
;
}
inline
static
FStar_UInt128_uint128
FStar_UInt128_mul_wide
(
uint64_t
x
uint64_t
y
)
{
#
if
HAS_OPTIMIZED
uint64_t
l
h
;
l
=
_umul128
(
x
y
&
h
)
;
return
_mm_set_epi64x
(
h
l
)
;
#
else
return
FStar_UInt128_mul_wide_impl
(
x
y
)
;
#
endif
}
#
undef
low
#
undef
high
#
endif
