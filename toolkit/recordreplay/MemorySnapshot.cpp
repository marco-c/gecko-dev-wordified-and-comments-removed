#
include
"
MemorySnapshot
.
h
"
#
include
"
ipc
/
ChildInternal
.
h
"
#
include
"
mozilla
/
Maybe
.
h
"
#
include
"
DirtyMemoryHandler
.
h
"
#
include
"
InfallibleVector
.
h
"
#
include
"
ProcessRecordReplay
.
h
"
#
include
"
ProcessRewind
.
h
"
#
include
"
SpinLock
.
h
"
#
include
"
SplayTree
.
h
"
#
include
"
Thread
.
h
"
#
include
<
algorithm
>
#
include
<
mach
/
mach
.
h
>
#
include
<
mach
/
mach_vm
.
h
>
namespace
mozilla
{
namespace
recordreplay
{
struct
AllocatedMemoryRegion
{
uint8_t
*
mBase
;
size_t
mSize
;
bool
mExecutable
;
AllocatedMemoryRegion
(
)
:
mBase
(
nullptr
)
mSize
(
0
)
mExecutable
(
false
)
{
}
AllocatedMemoryRegion
(
uint8_t
*
aBase
size_t
aSize
bool
aExecutable
)
:
mBase
(
aBase
)
mSize
(
aSize
)
mExecutable
(
aExecutable
)
{
}
struct
AddressSort
{
typedef
void
*
Lookup
;
static
void
*
getLookup
(
const
AllocatedMemoryRegion
&
aRegion
)
{
return
aRegion
.
mBase
;
}
static
ssize_t
compare
(
void
*
aAddress
const
AllocatedMemoryRegion
&
aRegion
)
{
return
(
uint8_t
*
)
aAddress
-
aRegion
.
mBase
;
}
}
;
struct
SizeReverseSort
{
typedef
size_t
Lookup
;
static
size_t
getLookup
(
const
AllocatedMemoryRegion
&
aRegion
)
{
return
aRegion
.
mSize
;
}
static
ssize_t
compare
(
size_t
aSize
const
AllocatedMemoryRegion
&
aRegion
)
{
return
aRegion
.
mSize
-
aSize
;
}
}
;
}
;
struct
DirtyPage
{
uint8_t
*
mBase
;
uint8_t
*
mOriginal
;
bool
mExecutable
;
DirtyPage
(
uint8_t
*
aBase
uint8_t
*
aOriginal
bool
aExecutable
)
:
mBase
(
aBase
)
mOriginal
(
aOriginal
)
mExecutable
(
aExecutable
)
{
}
struct
AddressSort
{
typedef
uint8_t
*
Lookup
;
static
uint8_t
*
getLookup
(
const
DirtyPage
&
aPage
)
{
return
aPage
.
mBase
;
}
static
ssize_t
compare
(
uint8_t
*
aBase
const
DirtyPage
&
aPage
)
{
return
aBase
-
aPage
.
mBase
;
}
}
;
}
;
typedef
SplayTree
<
DirtyPage
DirtyPage
:
:
AddressSort
AllocPolicy
<
MemoryKind
:
:
SortedDirtyPageSet
>
4
>
SortedDirtyPageSet
;
struct
DirtyPageSet
{
CheckpointId
mCheckpoint
;
InfallibleVector
<
DirtyPage
256
AllocPolicy
<
MemoryKind
:
:
DirtyPageSet
>
>
mPages
;
explicit
DirtyPageSet
(
const
CheckpointId
&
aCheckpoint
)
:
mCheckpoint
(
aCheckpoint
)
{
}
}
;
struct
SnapshotThreadWorklist
{
size_t
mThreadIndex
;
size_t
mThreadId
;
InfallibleVector
<
DirtyPageSet
256
AllocPolicy
<
MemoryKind
:
:
Generic
>
>
mSets
;
}
;
class
SnapshotThreadCondition
{
Atomic
<
bool
SequentiallyConsistent
Behavior
:
:
DontPreserve
>
mActive
;
Atomic
<
int32_t
SequentiallyConsistent
Behavior
:
:
DontPreserve
>
mCount
;
public
:
void
ActivateBegin
(
)
;
void
ActivateEnd
(
)
;
bool
IsActive
(
)
;
void
WaitUntilNoLongerActive
(
)
;
}
;
static
const
size_t
NumSnapshotThreads
=
8
;
class
FreeRegionSet
{
MemoryKind
mKind
;
SpinLock
mLock
;
static
const
size_t
ChunkPages
=
4
;
void
*
mNextChunk
;
void
MaybeRefillNextChunk
(
AutoSpinLock
&
aLockHeld
)
;
void
*
TakeNextChunk
(
)
;
struct
MyAllocPolicy
{
FreeRegionSet
&
mSet
;
template
<
typename
T
>
void
free_
(
T
*
aPtr
size_t
aSize
)
{
MOZ_CRASH
(
)
;
}
template
<
typename
T
>
T
*
pod_malloc
(
size_t
aNumElems
)
{
MOZ_RELEASE_ASSERT
(
sizeof
(
T
)
*
aNumElems
<
=
ChunkPages
*
PageSize
)
;
return
(
T
*
)
mSet
.
TakeNextChunk
(
)
;
}
explicit
MyAllocPolicy
(
FreeRegionSet
&
aSet
)
:
mSet
(
aSet
)
{
}
}
;
typedef
SplayTree
<
AllocatedMemoryRegion
AllocatedMemoryRegion
:
:
SizeReverseSort
MyAllocPolicy
ChunkPages
>
Tree
;
Tree
mRegions
;
void
InsertLockHeld
(
void
*
aAddress
size_t
aSize
AutoSpinLock
&
aLockHeld
)
;
void
*
ExtractLockHeld
(
size_t
aSize
AutoSpinLock
&
aLockHeld
)
;
public
:
explicit
FreeRegionSet
(
MemoryKind
aKind
)
:
mKind
(
aKind
)
mRegions
(
MyAllocPolicy
(
*
this
)
)
{
}
static
FreeRegionSet
&
Get
(
MemoryKind
aKind
)
;
void
Insert
(
void
*
aAddress
size_t
aSize
)
;
void
*
Extract
(
void
*
aAddress
size_t
aSize
)
;
bool
Intersects
(
void
*
aAddress
size_t
aSize
)
;
}
;
struct
MemoryInfo
{
bool
mMemoryChangesAllowed
;
static
const
size_t
MaxInitialUntrackedRegions
=
256
;
AllocatedMemoryRegion
mInitialUntrackedRegions
[
MaxInitialUntrackedRegions
]
;
SpinLock
mInitialUntrackedRegionsLock
;
SplayTree
<
AllocatedMemoryRegion
AllocatedMemoryRegion
:
:
AddressSort
AllocPolicy
<
MemoryKind
:
:
TrackedRegions
>
4
>
mTrackedRegions
;
InfallibleVector
<
AllocatedMemoryRegion
512
AllocPolicy
<
MemoryKind
:
:
TrackedRegions
>
>
mTrackedRegionsByAllocationOrder
;
SpinLock
mTrackedRegionsLock
;
SortedDirtyPageSet
mActiveDirty
;
SpinLock
mActiveDirtyLock
;
FreeRegionSet
mFreeUntrackedRegions
;
SnapshotThreadWorklist
mSnapshotWorklists
[
NumSnapshotThreads
]
;
SnapshotThreadCondition
mSnapshotThreadsShouldRestore
;
SnapshotThreadCondition
mSnapshotThreadsShouldIdle
;
Atomic
<
size_t
SequentiallyConsistent
Behavior
:
:
DontPreserve
>
mCountdown
;
double
mStartTime
;
uint32_t
mTimeHits
[
(
size_t
)
TimerKind
:
:
Count
]
;
double
mTimeTotals
[
(
size_t
)
TimerKind
:
:
Count
]
;
Atomic
<
ssize_t
Relaxed
Behavior
:
:
DontPreserve
>
mMemoryBalance
[
(
size_t
)
MemoryKind
:
:
Count
]
;
void
*
mDirtyMemoryFaults
[
50
]
;
bool
mIntentionalCrashesAllowed
;
bool
mCrashSoon
;
MemoryInfo
(
)
:
mMemoryChangesAllowed
(
true
)
mFreeUntrackedRegions
(
MemoryKind
:
:
FreeRegions
)
mStartTime
(
CurrentTime
(
)
)
mIntentionalCrashesAllowed
(
true
)
{
}
}
;
static
MemoryInfo
*
gMemoryInfo
=
nullptr
;
void
SetMemoryChangesAllowed
(
bool
aAllowed
)
{
MOZ_RELEASE_ASSERT
(
gMemoryInfo
-
>
mMemoryChangesAllowed
=
=
!
aAllowed
)
;
gMemoryInfo
-
>
mMemoryChangesAllowed
=
aAllowed
;
}
static
void
EnsureMemoryChangesAllowed
(
)
{
while
(
!
gMemoryInfo
-
>
mMemoryChangesAllowed
)
{
ThreadYield
(
)
;
}
}
void
StartCountdown
(
size_t
aCount
)
{
gMemoryInfo
-
>
mCountdown
=
aCount
;
}
AutoCountdown
:
:
AutoCountdown
(
size_t
aCount
)
{
StartCountdown
(
aCount
)
;
}
AutoCountdown
:
:
~
AutoCountdown
(
)
{
StartCountdown
(
0
)
;
}
#
ifdef
WANT_COUNTDOWN_THREAD
static
void
CountdownThreadMain
(
void
*
)
{
while
(
true
)
{
if
(
gMemoryInfo
-
>
mCountdown
&
&
-
-
gMemoryInfo
-
>
mCountdown
=
=
0
)
{
child
:
:
ReportFatalError
(
"
CountdownThread
activated
"
)
;
}
ThreadYield
(
)
;
}
}
#
endif
AutoTimer
:
:
AutoTimer
(
TimerKind
aKind
)
:
mKind
(
aKind
)
mStart
(
CurrentTime
(
)
)
{
}
AutoTimer
:
:
~
AutoTimer
(
)
{
if
(
gMemoryInfo
)
{
gMemoryInfo
-
>
mTimeHits
[
(
size_t
)
mKind
]
+
+
;
gMemoryInfo
-
>
mTimeTotals
[
(
size_t
)
mKind
]
+
=
CurrentTime
(
)
-
mStart
;
}
}
static
const
char
*
gTimerKindNames
[
]
=
{
#
define
DefineTimerKindName
(
aKind
)
#
aKind
ForEachTimerKind
(
DefineTimerKindName
)
#
undef
DefineTimerKindName
}
;
void
DumpTimers
(
)
{
if
(
!
gMemoryInfo
)
{
return
;
}
Print
(
"
Times
%
.
2fs
\
n
"
(
CurrentTime
(
)
-
gMemoryInfo
-
>
mStartTime
)
/
1000000
.
0
)
;
for
(
size_t
i
=
0
;
i
<
(
size_t
)
TimerKind
:
:
Count
;
i
+
+
)
{
uint32_t
hits
=
gMemoryInfo
-
>
mTimeHits
[
i
]
;
double
time
=
gMemoryInfo
-
>
mTimeTotals
[
i
]
;
Print
(
"
%
s
:
%
d
hits
%
.
2fs
\
n
"
gTimerKindNames
[
i
]
(
int
)
hits
time
/
1000000
.
0
)
;
}
}
void
SetAllowIntentionalCrashes
(
bool
aAllowed
)
{
gMemoryInfo
-
>
mIntentionalCrashesAllowed
=
aAllowed
;
}
extern
"
C
"
{
MOZ_EXPORT
void
RecordReplayInterface_InternalRecordReplayDirective
(
long
aDirective
)
{
switch
(
(
Directive
)
aDirective
)
{
case
Directive
:
:
CrashSoon
:
gMemoryInfo
-
>
mCrashSoon
=
true
;
break
;
case
Directive
:
:
MaybeCrash
:
if
(
gMemoryInfo
-
>
mIntentionalCrashesAllowed
&
&
gMemoryInfo
-
>
mCrashSoon
)
{
PrintSpew
(
"
Intentionally
Crashing
!
\
n
"
)
;
MOZ_CRASH
(
"
RecordReplayDirective
intentional
crash
"
)
;
}
gMemoryInfo
-
>
mCrashSoon
=
false
;
break
;
case
Directive
:
:
AlwaysSaveTemporaryCheckpoints
:
navigation
:
:
AlwaysSaveTemporaryCheckpoints
(
)
;
break
;
case
Directive
:
:
AlwaysMarkMajorCheckpoints
:
child
:
:
NotifyAlwaysMarkMajorCheckpoints
(
)
;
break
;
default
:
MOZ_CRASH
(
"
Unknown
directive
"
)
;
}
}
}
void
SnapshotThreadCondition
:
:
ActivateBegin
(
)
{
MOZ_RELEASE_ASSERT
(
Thread
:
:
CurrentIsMainThread
(
)
)
;
MOZ_RELEASE_ASSERT
(
!
mActive
)
;
mActive
=
true
;
for
(
size_t
i
=
0
;
i
<
NumSnapshotThreads
;
i
+
+
)
{
Thread
:
:
Notify
(
gMemoryInfo
-
>
mSnapshotWorklists
[
i
]
.
mThreadId
)
;
}
while
(
mCount
!
=
NumSnapshotThreads
)
{
Thread
:
:
WaitNoIdle
(
)
;
}
}
void
SnapshotThreadCondition
:
:
ActivateEnd
(
)
{
MOZ_RELEASE_ASSERT
(
Thread
:
:
CurrentIsMainThread
(
)
)
;
MOZ_RELEASE_ASSERT
(
mActive
)
;
mActive
=
false
;
for
(
size_t
i
=
0
;
i
<
NumSnapshotThreads
;
i
+
+
)
{
Thread
:
:
Notify
(
gMemoryInfo
-
>
mSnapshotWorklists
[
i
]
.
mThreadId
)
;
}
while
(
mCount
)
{
Thread
:
:
WaitNoIdle
(
)
;
}
}
bool
SnapshotThreadCondition
:
:
IsActive
(
)
{
MOZ_RELEASE_ASSERT
(
!
Thread
:
:
CurrentIsMainThread
(
)
)
;
return
mActive
;
}
void
SnapshotThreadCondition
:
:
WaitUntilNoLongerActive
(
)
{
MOZ_RELEASE_ASSERT
(
!
Thread
:
:
CurrentIsMainThread
(
)
)
;
MOZ_RELEASE_ASSERT
(
mActive
)
;
if
(
NumSnapshotThreads
=
=
+
+
mCount
)
{
Thread
:
:
Notify
(
MainThreadId
)
;
}
while
(
mActive
)
{
Thread
:
:
WaitNoIdle
(
)
;
}
if
(
0
=
=
-
-
mCount
)
{
Thread
:
:
Notify
(
MainThreadId
)
;
}
}
static
uint8_t
*
AllocatePageCopy
(
)
{
return
(
uint8_t
*
)
AllocateMemory
(
PageSize
MemoryKind
:
:
PageCopy
)
;
}
static
void
FreePageCopy
(
uint8_t
*
aPage
)
{
DeallocateMemory
(
aPage
PageSize
MemoryKind
:
:
PageCopy
)
;
}
void
MemoryMove
(
void
*
aDst
const
void
*
aSrc
size_t
aSize
)
{
MOZ_RELEASE_ASSERT
(
(
size_t
)
aDst
%
sizeof
(
uint32_t
)
=
=
0
)
;
MOZ_RELEASE_ASSERT
(
(
size_t
)
aSrc
%
sizeof
(
uint32_t
)
=
=
0
)
;
MOZ_RELEASE_ASSERT
(
aSize
%
sizeof
(
uint32_t
)
=
=
0
)
;
MOZ_RELEASE_ASSERT
(
(
size_t
)
aDst
<
=
(
size_t
)
aSrc
|
|
(
size_t
)
aDst
>
=
(
size_t
)
aSrc
+
aSize
)
;
uint32_t
*
ndst
=
(
uint32_t
*
)
aDst
;
const
uint32_t
*
nsrc
=
(
const
uint32_t
*
)
aSrc
;
for
(
size_t
i
=
0
;
i
<
aSize
/
sizeof
(
uint32_t
)
;
i
+
+
)
{
ndst
[
i
]
=
nsrc
[
i
]
;
}
}
void
MemoryZero
(
void
*
aDst
size_t
aSize
)
{
MOZ_RELEASE_ASSERT
(
(
size_t
)
aDst
%
sizeof
(
uint32_t
)
=
=
0
)
;
MOZ_RELEASE_ASSERT
(
aSize
%
sizeof
(
uint32_t
)
=
=
0
)
;
volatile
uint32_t
*
ndst
=
(
uint32_t
*
)
aDst
;
for
(
size_t
i
=
0
;
i
<
aSize
/
sizeof
(
uint32_t
)
;
i
+
+
)
{
ndst
[
i
]
=
0
;
}
}
static
bool
IsTrackedAddress
(
void
*
aAddress
bool
*
aExecutable
)
{
AutoSpinLock
lock
(
gMemoryInfo
-
>
mTrackedRegionsLock
)
;
Maybe
<
AllocatedMemoryRegion
>
region
=
gMemoryInfo
-
>
mTrackedRegions
.
lookupClosestLessOrEqual
(
aAddress
)
;
if
(
region
.
isSome
(
)
&
&
MemoryContains
(
region
.
ref
(
)
.
mBase
region
.
ref
(
)
.
mSize
aAddress
)
)
{
if
(
aExecutable
)
{
*
aExecutable
=
region
.
ref
(
)
.
mExecutable
;
}
return
true
;
}
return
false
;
}
bool
HandleDirtyMemoryFault
(
uint8_t
*
aAddress
)
{
EnsureMemoryChangesAllowed
(
)
;
bool
different
=
false
;
for
(
size_t
i
=
ArrayLength
(
gMemoryInfo
-
>
mDirtyMemoryFaults
)
-
1
;
i
;
i
-
-
)
{
gMemoryInfo
-
>
mDirtyMemoryFaults
[
i
]
=
gMemoryInfo
-
>
mDirtyMemoryFaults
[
i
-
1
]
;
if
(
gMemoryInfo
-
>
mDirtyMemoryFaults
[
i
]
!
=
aAddress
)
{
different
=
true
;
}
}
gMemoryInfo
-
>
mDirtyMemoryFaults
[
0
]
=
aAddress
;
if
(
!
different
)
{
Print
(
"
WARNING
:
Repeated
accesses
to
the
same
dirty
address
%
p
\
n
"
aAddress
)
;
}
aAddress
=
PageBase
(
aAddress
)
;
AutoSpinLock
lock
(
gMemoryInfo
-
>
mActiveDirtyLock
)
;
if
(
gMemoryInfo
-
>
mActiveDirty
.
maybeLookup
(
aAddress
)
)
{
return
true
;
}
bool
executable
;
if
(
!
IsTrackedAddress
(
aAddress
&
executable
)
)
{
return
false
;
}
uint8_t
*
original
=
AllocatePageCopy
(
)
;
MemoryMove
(
original
aAddress
PageSize
)
;
gMemoryInfo
-
>
mActiveDirty
.
insert
(
aAddress
DirtyPage
(
aAddress
original
executable
)
)
;
DirectUnprotectMemory
(
aAddress
PageSize
executable
)
;
return
true
;
}
void
UnrecoverableSnapshotFailure
(
)
{
AutoSpinLock
lock
(
gMemoryInfo
-
>
mTrackedRegionsLock
)
;
DirectUnprotectMemory
(
PageBase
(
&
errno
)
PageSize
false
)
;
for
(
auto
region
:
gMemoryInfo
-
>
mTrackedRegionsByAllocationOrder
)
{
DirectUnprotectMemory
(
region
.
mBase
region
.
mSize
region
.
mExecutable
true
)
;
}
}
void
AddInitialUntrackedMemoryRegion
(
uint8_t
*
aBase
size_t
aSize
)
{
MOZ_RELEASE_ASSERT
(
!
HasSavedCheckpoint
(
)
)
;
if
(
gInitializationFailureMessage
)
{
return
;
}
static
void
*
gSkippedRegion
;
if
(
!
gSkippedRegion
)
{
gSkippedRegion
=
aBase
;
return
;
}
MOZ_RELEASE_ASSERT
(
gSkippedRegion
=
=
gMemoryInfo
)
;
AutoSpinLock
lock
(
gMemoryInfo
-
>
mInitialUntrackedRegionsLock
)
;
for
(
AllocatedMemoryRegion
&
region
:
gMemoryInfo
-
>
mInitialUntrackedRegions
)
{
if
(
!
region
.
mBase
)
{
region
.
mBase
=
aBase
;
region
.
mSize
=
aSize
;
return
;
}
}
MOZ_CRASH
(
)
;
}
static
void
RemoveInitialUntrackedRegion
(
uint8_t
*
aBase
size_t
aSize
)
{
MOZ_RELEASE_ASSERT
(
!
HasSavedCheckpoint
(
)
)
;
AutoSpinLock
lock
(
gMemoryInfo
-
>
mInitialUntrackedRegionsLock
)
;
for
(
AllocatedMemoryRegion
&
region
:
gMemoryInfo
-
>
mInitialUntrackedRegions
)
{
if
(
region
.
mBase
=
=
aBase
)
{
MOZ_RELEASE_ASSERT
(
region
.
mSize
=
=
aSize
)
;
region
.
mBase
=
nullptr
;
region
.
mSize
=
0
;
return
;
}
}
MOZ_CRASH
(
)
;
}
static
void
MarkThreadStacksAsUntracked
(
)
{
for
(
size_t
i
=
MainThreadId
;
i
<
=
MaxThreadId
;
i
+
+
)
{
Thread
*
thread
=
Thread
:
:
GetById
(
i
)
;
AddInitialUntrackedMemoryRegion
(
thread
-
>
StackBase
(
)
thread
-
>
StackSize
(
)
)
;
}
}
static
bool
MaybeExtractMemoryRegion
(
uint8_t
*
aAddress
size_t
*
aSize
uint8_t
*
*
aRemaining
size_t
*
aRemainingSize
uint8_t
*
aExclude
size_t
aExcludeSize
)
{
uint8_t
*
addrLimit
=
aAddress
+
*
aSize
;
MOZ_RELEASE_ASSERT
(
(
size_t
)
aExclude
%
PageSize
=
=
0
)
;
aExcludeSize
=
RoundupSizeToPageBoundary
(
aExcludeSize
)
;
uint8_t
*
excludeLimit
=
aExclude
+
aExcludeSize
;
if
(
excludeLimit
<
=
aAddress
|
|
addrLimit
<
=
aExclude
)
{
return
false
;
}
*
aSize
=
std
:
:
max
<
ssize_t
>
(
aExclude
-
aAddress
0
)
;
if
(
aRemaining
)
{
*
aRemaining
=
excludeLimit
;
*
aRemainingSize
=
std
:
:
max
<
ssize_t
>
(
addrLimit
-
*
aRemaining
0
)
;
}
return
true
;
}
static
void
ExtractTrackedInitialMemoryRegion
(
uint8_t
*
aAddress
size_t
*
aSize
uint8_t
*
*
aRemaining
size_t
*
aRemainingSize
)
{
const
AllocatedMemoryRegion
*
earliestIntersect
=
nullptr
;
for
(
const
AllocatedMemoryRegion
&
region
:
gMemoryInfo
-
>
mInitialUntrackedRegions
)
{
size_t
size
=
*
aSize
;
if
(
MaybeExtractMemoryRegion
(
aAddress
&
size
nullptr
nullptr
region
.
mBase
region
.
mSize
)
)
{
if
(
!
earliestIntersect
|
|
region
.
mBase
<
earliestIntersect
-
>
mBase
)
{
earliestIntersect
=
&
region
;
}
}
}
if
(
earliestIntersect
)
{
if
(
!
MaybeExtractMemoryRegion
(
aAddress
aSize
aRemaining
aRemainingSize
earliestIntersect
-
>
mBase
earliestIntersect
-
>
mSize
)
)
{
MOZ_CRASH
(
)
;
}
}
else
{
*
aRemaining
=
aAddress
+
*
aSize
;
*
aRemainingSize
=
0
;
}
}
static
void
AddTrackedRegion
(
uint8_t
*
aAddress
size_t
aSize
bool
aExecutable
)
{
if
(
aSize
)
{
AutoSpinLock
lock
(
gMemoryInfo
-
>
mTrackedRegionsLock
)
;
gMemoryInfo
-
>
mTrackedRegions
.
insert
(
aAddress
AllocatedMemoryRegion
(
aAddress
aSize
aExecutable
)
)
;
gMemoryInfo
-
>
mTrackedRegionsByAllocationOrder
.
emplaceBack
(
aAddress
aSize
aExecutable
)
;
}
}
void
AddInitialTrackedMemoryRegions
(
uint8_t
*
aAddress
size_t
aSize
bool
aExecutable
)
{
while
(
aSize
)
{
uint8_t
*
remaining
;
size_t
remainingSize
;
ExtractTrackedInitialMemoryRegion
(
aAddress
&
aSize
&
remaining
&
remainingSize
)
;
AddTrackedRegion
(
aAddress
aSize
aExecutable
)
;
aAddress
=
remaining
;
aSize
=
remainingSize
;
}
}
static
void
UpdateNumTrackedRegionsForSnapshot
(
)
;
static
void
ProcessAllInitialMemoryRegions
(
)
{
MOZ_ASSERT
(
!
AreThreadEventsPassedThrough
(
)
)
;
{
AutoPassThroughThreadEvents
pt
;
for
(
mach_vm_address_t
addr
=
0
;
;
)
{
mach_vm_size_t
nbytes
;
vm_region_basic_info_64
info
;
mach_msg_type_number_t
info_count
=
sizeof
(
vm_region_basic_info_64
)
;
mach_port_t
some_port
;
kern_return_t
rv
=
mach_vm_region
(
mach_task_self
(
)
&
addr
&
nbytes
VM_REGION_BASIC_INFO
(
vm_region_info_t
)
&
info
&
info_count
&
some_port
)
;
if
(
rv
=
=
KERN_INVALID_ADDRESS
)
{
break
;
}
MOZ_RELEASE_ASSERT
(
rv
=
=
KERN_SUCCESS
)
;
if
(
info
.
max_protection
&
VM_PROT_WRITE
)
{
MOZ_RELEASE_ASSERT
(
info
.
max_protection
&
VM_PROT_READ
)
;
AddInitialTrackedMemoryRegions
(
reinterpret_cast
<
uint8_t
*
>
(
addr
)
nbytes
info
.
max_protection
&
VM_PROT_EXECUTE
)
;
}
addr
+
=
nbytes
;
}
}
UpdateNumTrackedRegionsForSnapshot
(
)
;
AutoDisallowMemoryChanges
disallow
;
for
(
const
AllocatedMemoryRegion
&
region
:
gMemoryInfo
-
>
mTrackedRegionsByAllocationOrder
)
{
DirectWriteProtectMemory
(
region
.
mBase
region
.
mSize
region
.
mExecutable
)
;
}
}
static
FreeRegionSet
gFreeRegions
(
MemoryKind
:
:
Tracked
)
;
static
size_t
gNumTrackedRegions
;
static
void
UpdateNumTrackedRegionsForSnapshot
(
)
{
MOZ_ASSERT
(
Thread
:
:
CurrentIsMainThread
(
)
)
;
gNumTrackedRegions
=
gMemoryInfo
-
>
mTrackedRegionsByAllocationOrder
.
length
(
)
;
}
void
FixupFreeRegionsAfterRewind
(
)
{
size_t
newTrackedRegions
=
gMemoryInfo
-
>
mTrackedRegionsByAllocationOrder
.
length
(
)
;
for
(
size_t
i
=
gNumTrackedRegions
;
i
<
newTrackedRegions
;
i
+
+
)
{
const
AllocatedMemoryRegion
&
region
=
gMemoryInfo
-
>
mTrackedRegionsByAllocationOrder
[
i
]
;
gFreeRegions
.
Insert
(
region
.
mBase
region
.
mSize
)
;
}
gNumTrackedRegions
=
newTrackedRegions
;
}
FreeRegionSet
&
FreeRegionSet
:
:
Get
(
MemoryKind
aKind
)
{
return
(
aKind
=
=
MemoryKind
:
:
Tracked
)
?
gFreeRegions
:
gMemoryInfo
-
>
mFreeUntrackedRegions
;
}
void
*
FreeRegionSet
:
:
TakeNextChunk
(
)
{
MOZ_RELEASE_ASSERT
(
mNextChunk
)
;
void
*
res
=
mNextChunk
;
mNextChunk
=
nullptr
;
return
res
;
}
void
FreeRegionSet
:
:
InsertLockHeld
(
void
*
aAddress
size_t
aSize
AutoSpinLock
&
aLockHeld
)
{
mRegions
.
insert
(
aSize
AllocatedMemoryRegion
(
(
uint8_t
*
)
aAddress
aSize
true
)
)
;
}
void
FreeRegionSet
:
:
MaybeRefillNextChunk
(
AutoSpinLock
&
aLockHeld
)
{
if
(
mNextChunk
)
{
return
;
}
size_t
size
=
ChunkPages
*
PageSize
;
gMemoryInfo
-
>
mMemoryBalance
[
(
size_t
)
mKind
]
+
=
size
;
mNextChunk
=
ExtractLockHeld
(
size
aLockHeld
)
;
if
(
!
mNextChunk
)
{
mNextChunk
=
DirectAllocateMemory
(
nullptr
size
)
;
RegisterAllocatedMemory
(
mNextChunk
size
mKind
)
;
}
}
void
FreeRegionSet
:
:
Insert
(
void
*
aAddress
size_t
aSize
)
{
MOZ_RELEASE_ASSERT
(
aAddress
&
&
aAddress
=
=
PageBase
(
aAddress
)
)
;
MOZ_RELEASE_ASSERT
(
aSize
&
&
aSize
=
=
RoundupSizeToPageBoundary
(
aSize
)
)
;
AutoSpinLock
lock
(
mLock
)
;
MaybeRefillNextChunk
(
lock
)
;
InsertLockHeld
(
aAddress
aSize
lock
)
;
}
void
*
FreeRegionSet
:
:
ExtractLockHeld
(
size_t
aSize
AutoSpinLock
&
aLockHeld
)
{
Maybe
<
AllocatedMemoryRegion
>
best
=
mRegions
.
lookupClosestLessOrEqual
(
aSize
true
)
;
if
(
best
.
isSome
(
)
)
{
MOZ_RELEASE_ASSERT
(
best
.
ref
(
)
.
mSize
>
=
aSize
)
;
uint8_t
*
res
=
best
.
ref
(
)
.
mBase
;
if
(
best
.
ref
(
)
.
mSize
>
aSize
)
{
InsertLockHeld
(
res
+
aSize
best
.
ref
(
)
.
mSize
-
aSize
aLockHeld
)
;
}
MemoryZero
(
res
aSize
)
;
return
res
;
}
return
nullptr
;
}
void
*
FreeRegionSet
:
:
Extract
(
void
*
aAddress
size_t
aSize
)
{
MOZ_RELEASE_ASSERT
(
aAddress
=
=
PageBase
(
aAddress
)
)
;
MOZ_RELEASE_ASSERT
(
aSize
&
&
aSize
=
=
RoundupSizeToPageBoundary
(
aSize
)
)
;
AutoSpinLock
lock
(
mLock
)
;
if
(
aAddress
)
{
MaybeRefillNextChunk
(
lock
)
;
for
(
typename
Tree
:
:
Iter
iter
=
mRegions
.
begin
(
)
;
!
iter
.
done
(
)
;
+
+
iter
)
{
uint8_t
*
regionBase
=
iter
.
ref
(
)
.
mBase
;
uint8_t
*
regionExtent
=
regionBase
+
iter
.
ref
(
)
.
mSize
;
uint8_t
*
addrBase
=
(
uint8_t
*
)
aAddress
;
uint8_t
*
addrExtent
=
addrBase
+
aSize
;
if
(
regionBase
<
=
addrBase
&
&
regionExtent
>
=
addrExtent
)
{
iter
.
removeEntry
(
)
;
if
(
regionBase
<
addrBase
)
{
InsertLockHeld
(
regionBase
addrBase
-
regionBase
lock
)
;
}
if
(
regionExtent
>
addrExtent
)
{
InsertLockHeld
(
addrExtent
regionExtent
-
addrExtent
lock
)
;
}
MemoryZero
(
aAddress
aSize
)
;
return
aAddress
;
}
}
}
return
ExtractLockHeld
(
aSize
lock
)
;
}
bool
FreeRegionSet
:
:
Intersects
(
void
*
aAddress
size_t
aSize
)
{
AutoSpinLock
lock
(
mLock
)
;
for
(
typename
Tree
:
:
Iter
iter
=
mRegions
.
begin
(
)
;
!
iter
.
done
(
)
;
+
+
iter
)
{
if
(
MemoryIntersects
(
iter
.
ref
(
)
.
mBase
iter
.
ref
(
)
.
mSize
aAddress
aSize
)
)
{
return
true
;
}
}
return
false
;
}
void
RegisterAllocatedMemory
(
void
*
aBaseAddress
size_t
aSize
MemoryKind
aKind
)
{
MOZ_RELEASE_ASSERT
(
aBaseAddress
=
=
PageBase
(
aBaseAddress
)
)
;
MOZ_RELEASE_ASSERT
(
aSize
=
=
RoundupSizeToPageBoundary
(
aSize
)
)
;
uint8_t
*
aAddress
=
reinterpret_cast
<
uint8_t
*
>
(
aBaseAddress
)
;
if
(
aKind
!
=
MemoryKind
:
:
Tracked
)
{
if
(
!
HasSavedCheckpoint
(
)
)
{
AddInitialUntrackedMemoryRegion
(
aAddress
aSize
)
;
}
}
else
if
(
HasSavedCheckpoint
(
)
)
{
EnsureMemoryChangesAllowed
(
)
;
DirectWriteProtectMemory
(
aAddress
aSize
true
)
;
AddTrackedRegion
(
aAddress
aSize
true
)
;
}
}
void
CheckFixedMemory
(
void
*
aAddress
size_t
aSize
)
{
MOZ_RELEASE_ASSERT
(
aAddress
=
=
PageBase
(
aAddress
)
)
;
MOZ_RELEASE_ASSERT
(
aSize
=
=
RoundupSizeToPageBoundary
(
aSize
)
)
;
if
(
!
HasSavedCheckpoint
(
)
)
{
return
;
}
{
AutoSpinLock
lock
(
gMemoryInfo
-
>
mTrackedRegionsLock
)
;
for
(
size_t
offset
=
0
;
offset
<
aSize
;
offset
+
=
PageSize
)
{
uint8_t
*
page
=
(
uint8_t
*
)
aAddress
+
offset
;
Maybe
<
AllocatedMemoryRegion
>
region
=
gMemoryInfo
-
>
mTrackedRegions
.
lookupClosestLessOrEqual
(
page
)
;
if
(
!
region
.
isSome
(
)
|
|
!
MemoryContains
(
region
.
ref
(
)
.
mBase
region
.
ref
(
)
.
mSize
page
PageSize
)
)
{
child
:
:
ReportFatalError
(
"
Fixed
memory
is
not
tracked
!
"
)
;
}
}
}
if
(
gFreeRegions
.
Intersects
(
aAddress
aSize
)
)
{
child
:
:
ReportFatalError
(
"
Fixed
memory
is
currently
free
!
"
)
;
}
}
void
RestoreWritableFixedMemory
(
void
*
aAddress
size_t
aSize
)
{
MOZ_RELEASE_ASSERT
(
aAddress
=
=
PageBase
(
aAddress
)
)
;
MOZ_RELEASE_ASSERT
(
aSize
=
=
RoundupSizeToPageBoundary
(
aSize
)
)
;
if
(
!
HasSavedCheckpoint
(
)
)
{
return
;
}
AutoSpinLock
lock
(
gMemoryInfo
-
>
mActiveDirtyLock
)
;
for
(
size_t
offset
=
0
;
offset
<
aSize
;
offset
+
=
PageSize
)
{
uint8_t
*
page
=
(
uint8_t
*
)
aAddress
+
offset
;
if
(
gMemoryInfo
-
>
mActiveDirty
.
maybeLookup
(
page
)
)
{
DirectUnprotectMemory
(
page
PageSize
true
)
;
}
}
}
void
*
AllocateMemoryTryAddress
(
void
*
aAddress
size_t
aSize
MemoryKind
aKind
)
{
MOZ_RELEASE_ASSERT
(
aAddress
=
=
PageBase
(
aAddress
)
)
;
aSize
=
RoundupSizeToPageBoundary
(
aSize
)
;
if
(
gMemoryInfo
)
{
gMemoryInfo
-
>
mMemoryBalance
[
(
size_t
)
aKind
]
+
=
aSize
;
}
if
(
HasSavedCheckpoint
(
)
)
{
if
(
void
*
res
=
FreeRegionSet
:
:
Get
(
aKind
)
.
Extract
(
aAddress
aSize
)
)
{
return
res
;
}
}
void
*
res
=
DirectAllocateMemory
(
aAddress
aSize
)
;
RegisterAllocatedMemory
(
res
aSize
aKind
)
;
return
res
;
}
void
*
AllocateMemory
(
size_t
aSize
MemoryKind
aKind
)
{
if
(
!
IsReplaying
(
)
)
{
return
DirectAllocateMemory
(
nullptr
aSize
)
;
}
return
AllocateMemoryTryAddress
(
nullptr
aSize
aKind
)
;
}
void
DeallocateMemory
(
void
*
aAddress
size_t
aSize
MemoryKind
aKind
)
{
aSize
+
=
(
uint8_t
*
)
aAddress
-
PageBase
(
aAddress
)
;
aAddress
=
PageBase
(
aAddress
)
;
aSize
=
RoundupSizeToPageBoundary
(
aSize
)
;
if
(
!
aAddress
|
|
!
aSize
)
{
return
;
}
if
(
gMemoryInfo
)
{
gMemoryInfo
-
>
mMemoryBalance
[
(
size_t
)
aKind
]
-
=
aSize
;
}
if
(
!
HasSavedCheckpoint
(
)
)
{
if
(
IsReplaying
(
)
&
&
aKind
!
=
MemoryKind
:
:
Tracked
)
{
RemoveInitialUntrackedRegion
(
(
uint8_t
*
)
aAddress
aSize
)
;
}
DirectDeallocateMemory
(
aAddress
aSize
)
;
return
;
}
if
(
aKind
=
=
MemoryKind
:
:
Tracked
)
{
bool
executable
;
if
(
!
IsTrackedAddress
(
aAddress
&
executable
)
|
|
!
executable
)
{
return
;
}
}
FreeRegionSet
:
:
Get
(
aKind
)
.
Insert
(
aAddress
aSize
)
;
}
static
void
SnapshotThreadRestoreLastDiffSnapshot
(
SnapshotThreadWorklist
*
aWorklist
)
{
CheckpointId
checkpoint
=
GetLastSavedCheckpoint
(
)
;
DirtyPageSet
&
set
=
aWorklist
-
>
mSets
.
back
(
)
;
MOZ_RELEASE_ASSERT
(
set
.
mCheckpoint
=
=
checkpoint
)
;
for
(
size_t
index
=
0
;
index
<
set
.
mPages
.
length
(
)
;
index
+
+
)
{
const
DirtyPage
&
page
=
set
.
mPages
[
index
]
;
MOZ_RELEASE_ASSERT
(
page
.
mOriginal
)
;
DirectUnprotectMemory
(
page
.
mBase
PageSize
page
.
mExecutable
)
;
MemoryMove
(
page
.
mBase
page
.
mOriginal
PageSize
)
;
DirectWriteProtectMemory
(
page
.
mBase
PageSize
page
.
mExecutable
)
;
FreePageCopy
(
page
.
mOriginal
)
;
}
if
(
!
aWorklist
-
>
mSets
.
empty
(
)
)
{
MOZ_RELEASE_ASSERT
(
&
set
=
=
&
aWorklist
-
>
mSets
.
back
(
)
)
;
aWorklist
-
>
mSets
.
popBack
(
)
;
}
}
void
SnapshotThreadMain
(
void
*
aArgument
)
{
size_t
threadIndex
=
(
size_t
)
aArgument
;
SnapshotThreadWorklist
*
worklist
=
&
gMemoryInfo
-
>
mSnapshotWorklists
[
threadIndex
]
;
worklist
-
>
mThreadIndex
=
threadIndex
;
while
(
true
)
{
if
(
gMemoryInfo
-
>
mSnapshotThreadsShouldRestore
.
IsActive
(
)
)
{
SnapshotThreadRestoreLastDiffSnapshot
(
worklist
)
;
gMemoryInfo
-
>
mSnapshotThreadsShouldRestore
.
WaitUntilNoLongerActive
(
)
;
}
if
(
gMemoryInfo
-
>
mSnapshotThreadsShouldIdle
.
IsActive
(
)
)
{
gMemoryInfo
-
>
mSnapshotThreadsShouldIdle
.
WaitUntilNoLongerActive
(
)
;
}
Thread
:
:
WaitNoIdle
(
)
;
}
}
static
bool
MemoryEquals
(
void
*
aDst
void
*
aSrc
size_t
aSize
)
{
MOZ_ASSERT
(
(
size_t
)
aDst
%
sizeof
(
size_t
)
=
=
0
)
;
MOZ_ASSERT
(
(
size_t
)
aSrc
%
sizeof
(
size_t
)
=
=
0
)
;
MOZ_ASSERT
(
aSize
%
sizeof
(
size_t
)
=
=
0
)
;
size_t
*
ndst
=
(
size_t
*
)
aDst
;
size_t
*
nsrc
=
(
size_t
*
)
aSrc
;
for
(
size_t
i
=
0
;
i
<
aSize
/
sizeof
(
size_t
)
;
i
+
+
)
{
if
(
ndst
[
i
]
!
=
nsrc
[
i
]
)
{
return
false
;
}
}
return
true
;
}
static
void
AddDirtyPageToWorklist
(
uint8_t
*
aAddress
uint8_t
*
aOriginal
bool
aExecutable
)
{
MOZ_ASSERT
(
(
size_t
)
aAddress
%
PageSize
=
=
0
)
;
if
(
MemoryEquals
(
aAddress
aOriginal
PageSize
)
)
{
FreePageCopy
(
aOriginal
)
;
}
else
{
size_t
pageIndex
=
(
(
size_t
)
aAddress
/
PageSize
)
%
NumSnapshotThreads
;
SnapshotThreadWorklist
*
worklist
=
&
gMemoryInfo
-
>
mSnapshotWorklists
[
pageIndex
]
;
MOZ_RELEASE_ASSERT
(
!
worklist
-
>
mSets
.
empty
(
)
)
;
DirtyPageSet
&
set
=
worklist
-
>
mSets
.
back
(
)
;
MOZ_RELEASE_ASSERT
(
set
.
mCheckpoint
=
=
GetLastSavedCheckpoint
(
)
)
;
set
.
mPages
.
emplaceBack
(
aAddress
aOriginal
aExecutable
)
;
}
}
void
InitializeMemorySnapshots
(
)
{
MOZ_RELEASE_ASSERT
(
gMemoryInfo
=
=
nullptr
)
;
void
*
memory
=
AllocateMemory
(
sizeof
(
MemoryInfo
)
MemoryKind
:
:
Generic
)
;
gMemoryInfo
=
new
(
memory
)
MemoryInfo
(
)
;
AddInitialUntrackedMemoryRegion
(
reinterpret_cast
<
uint8_t
*
>
(
memory
)
sizeof
(
MemoryInfo
)
)
;
}
void
InitializeCountdownThread
(
)
{
#
ifdef
WANT_COUNTDOWN_THREAD
Thread
:
:
SpawnNonRecordedThread
(
CountdownThreadMain
nullptr
)
;
#
endif
}
void
TakeFirstMemorySnapshot
(
)
{
MOZ_RELEASE_ASSERT
(
Thread
:
:
CurrentIsMainThread
(
)
)
;
MOZ_RELEASE_ASSERT
(
gMemoryInfo
-
>
mTrackedRegions
.
empty
(
)
)
;
{
AutoPassThroughThreadEvents
pt
;
for
(
size_t
i
=
0
;
i
<
NumSnapshotThreads
;
i
+
+
)
{
Thread
*
thread
=
Thread
:
:
SpawnNonRecordedThread
(
SnapshotThreadMain
(
void
*
)
i
)
;
gMemoryInfo
-
>
mSnapshotWorklists
[
i
]
.
mThreadId
=
thread
-
>
Id
(
)
;
}
}
MarkThreadStacksAsUntracked
(
)
;
ProcessAllInitialMemoryRegions
(
)
;
}
void
TakeDiffMemorySnapshot
(
)
{
MOZ_RELEASE_ASSERT
(
Thread
:
:
CurrentIsMainThread
(
)
)
;
UpdateNumTrackedRegionsForSnapshot
(
)
;
AutoDisallowMemoryChanges
disallow
;
gMemoryInfo
-
>
mSnapshotThreadsShouldIdle
.
ActivateBegin
(
)
;
for
(
size_t
i
=
0
;
i
<
NumSnapshotThreads
;
i
+
+
)
{
SnapshotThreadWorklist
*
worklist
=
&
gMemoryInfo
-
>
mSnapshotWorklists
[
i
]
;
worklist
-
>
mSets
.
emplaceBack
(
GetLastSavedCheckpoint
(
)
)
;
}
for
(
SortedDirtyPageSet
:
:
Iter
iter
=
gMemoryInfo
-
>
mActiveDirty
.
begin
(
)
;
!
iter
.
done
(
)
;
+
+
iter
)
{
AddDirtyPageToWorklist
(
iter
.
ref
(
)
.
mBase
iter
.
ref
(
)
.
mOriginal
iter
.
ref
(
)
.
mExecutable
)
;
DirectWriteProtectMemory
(
iter
.
ref
(
)
.
mBase
PageSize
iter
.
ref
(
)
.
mExecutable
)
;
}
gMemoryInfo
-
>
mActiveDirty
.
clear
(
)
;
gMemoryInfo
-
>
mSnapshotThreadsShouldIdle
.
ActivateEnd
(
)
;
}
void
RestoreMemoryToLastSavedCheckpoint
(
)
{
MOZ_RELEASE_ASSERT
(
Thread
:
:
CurrentIsMainThread
(
)
)
;
AutoDisallowMemoryChanges
disallow
;
for
(
SortedDirtyPageSet
:
:
Iter
iter
=
gMemoryInfo
-
>
mActiveDirty
.
begin
(
)
;
!
iter
.
done
(
)
;
+
+
iter
)
{
MemoryMove
(
iter
.
ref
(
)
.
mBase
iter
.
ref
(
)
.
mOriginal
PageSize
)
;
FreePageCopy
(
iter
.
ref
(
)
.
mOriginal
)
;
DirectWriteProtectMemory
(
iter
.
ref
(
)
.
mBase
PageSize
iter
.
ref
(
)
.
mExecutable
)
;
}
gMemoryInfo
-
>
mActiveDirty
.
clear
(
)
;
}
void
RestoreMemoryToLastSavedDiffCheckpoint
(
)
{
MOZ_RELEASE_ASSERT
(
Thread
:
:
CurrentIsMainThread
(
)
)
;
MOZ_RELEASE_ASSERT
(
gMemoryInfo
-
>
mActiveDirty
.
empty
(
)
)
;
AutoDisallowMemoryChanges
disallow
;
gMemoryInfo
-
>
mSnapshotThreadsShouldRestore
.
ActivateBegin
(
)
;
gMemoryInfo
-
>
mSnapshotThreadsShouldRestore
.
ActivateEnd
(
)
;
}
}
}
