#
ifndef
GOOGLE_PROTOBUF_ARENA_IMPL_H__
#
define
GOOGLE_PROTOBUF_ARENA_IMPL_H__
#
include
<
limits
>
#
include
<
google
/
protobuf
/
stubs
/
atomic_sequence_num
.
h
>
#
include
<
google
/
protobuf
/
stubs
/
atomicops
.
h
>
#
include
<
google
/
protobuf
/
stubs
/
common
.
h
>
#
include
<
google
/
protobuf
/
stubs
/
logging
.
h
>
#
include
<
google
/
protobuf
/
stubs
/
mutex
.
h
>
#
include
<
google
/
protobuf
/
stubs
/
type_traits
.
h
>
namespace
google
{
namespace
protobuf
{
namespace
internal
{
inline
size_t
AlignUpTo8
(
size_t
n
)
{
return
(
n
+
7
)
&
-
8
;
}
class
LIBPROTOBUF_EXPORT
ArenaImpl
{
public
:
struct
Options
{
size_t
start_block_size
;
size_t
max_block_size
;
char
*
initial_block
;
size_t
initial_block_size
;
void
*
(
*
block_alloc
)
(
size_t
)
;
void
(
*
block_dealloc
)
(
void
*
size_t
)
;
template
<
typename
O
>
explicit
Options
(
const
O
&
options
)
:
start_block_size
(
options
.
start_block_size
)
max_block_size
(
options
.
max_block_size
)
initial_block
(
options
.
initial_block
)
initial_block_size
(
options
.
initial_block_size
)
block_alloc
(
options
.
block_alloc
)
block_dealloc
(
options
.
block_dealloc
)
{
}
}
;
template
<
typename
O
>
explicit
ArenaImpl
(
const
O
&
options
)
:
options_
(
options
)
{
Init
(
)
;
}
~
ArenaImpl
(
)
;
uint64
Reset
(
)
;
uint64
SpaceAllocated
(
)
const
;
uint64
SpaceUsed
(
)
const
;
void
*
AllocateAligned
(
size_t
n
)
;
void
*
AllocateAlignedAndAddCleanup
(
size_t
n
void
(
*
cleanup
)
(
void
*
)
)
;
void
AddCleanup
(
void
*
elem
void
(
*
cleanup
)
(
void
*
)
)
;
private
:
struct
CleanupNode
{
void
*
elem
;
void
(
*
cleanup
)
(
void
*
)
;
}
;
struct
CleanupChunk
{
static
size_t
SizeOf
(
size_t
i
)
{
return
sizeof
(
CleanupChunk
)
+
(
sizeof
(
CleanupNode
)
*
(
i
-
1
)
)
;
}
size_t
len
;
size_t
size
;
CleanupChunk
*
next
;
CleanupNode
nodes
[
1
]
;
}
;
struct
Block
{
void
*
owner
;
Block
*
next
;
CleanupChunk
*
cleanup
;
size_t
pos
;
size_t
size
;
GOOGLE_ATTRIBUTE_ALWAYS_INLINE
size_t
avail
(
)
const
{
return
size
-
pos
;
}
}
;
struct
ThreadCache
{
int64
last_lifecycle_id_seen
;
Block
*
last_block_used_
;
}
;
static
const
size_t
kHeaderSize
=
(
sizeof
(
Block
)
+
7
)
&
-
8
;
#
if
LANG_CXX11
static_assert
(
kHeaderSize
%
8
=
=
0
"
kHeaderSize
must
be
a
multiple
of
8
.
"
)
;
#
endif
static
google
:
:
protobuf
:
:
internal
:
:
SequenceNumber
lifecycle_id_generator_
;
#
if
defined
(
GOOGLE_PROTOBUF_NO_THREADLOCAL
)
static
ThreadCache
&
thread_cache
(
)
;
#
elif
defined
(
PROTOBUF_USE_DLLS
)
static
ThreadCache
&
thread_cache
(
)
;
#
else
static
GOOGLE_THREAD_LOCAL
ThreadCache
thread_cache_
;
static
ThreadCache
&
thread_cache
(
)
{
return
thread_cache_
;
}
#
endif
void
Init
(
)
;
uint64
FreeBlocks
(
Block
*
head
)
;
void
AddCleanupInBlock
(
Block
*
b
void
*
elem
void
(
*
cleanup
)
(
void
*
)
)
;
Block
*
ExpandCleanupList
(
Block
*
b
)
;
void
CleanupList
(
Block
*
head
)
;
uint64
ResetInternal
(
)
;
inline
void
CacheBlock
(
Block
*
block
)
{
thread_cache
(
)
.
last_block_used_
=
block
;
thread_cache
(
)
.
last_lifecycle_id_seen
=
lifecycle_id_
;
google
:
:
protobuf
:
:
internal
:
:
Release_Store
(
&
hint_
reinterpret_cast
<
google
:
:
protobuf
:
:
internal
:
:
AtomicWord
>
(
block
)
)
;
}
google
:
:
protobuf
:
:
internal
:
:
AtomicWord
blocks_
;
google
:
:
protobuf
:
:
internal
:
:
AtomicWord
hint_
;
uint64
space_allocated_
;
bool
owns_first_block_
;
mutable
Mutex
blocks_lock_
;
void
AddBlock
(
Block
*
b
)
;
void
AddBlockInternal
(
Block
*
b
)
;
Block
*
GetBlock
(
size_t
n
)
;
Block
*
GetBlockSlow
(
void
*
me
Block
*
my_full_block
size_t
n
)
;
Block
*
FindBlock
(
void
*
me
)
;
Block
*
NewBlock
(
void
*
me
Block
*
my_last_block
size_t
min_bytes
size_t
start_block_size
size_t
max_block_size
)
;
static
void
*
AllocFromBlock
(
Block
*
b
size_t
n
)
;
int64
lifecycle_id_
;
Options
options_
;
GOOGLE_DISALLOW_EVIL_CONSTRUCTORS
(
ArenaImpl
)
;
}
;
}
}
}
#
endif
