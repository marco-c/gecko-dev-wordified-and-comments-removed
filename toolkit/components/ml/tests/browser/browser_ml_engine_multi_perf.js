"
use
strict
"
;
const
ENGINES
=
{
intent
:
{
engineId
:
"
intent
"
taskName
:
"
text
-
classification
"
modelId
:
"
Mozilla
/
mobilebert
-
uncased
-
finetuned
-
LoRA
-
intent
-
classifier
"
modelRevision
:
"
main
"
modelHubUrlTemplate
:
"
{
model
}
/
{
revision
}
"
dtype
:
"
q8
"
device
:
"
wasm
"
request
:
{
args
:
[
[
"
restaurants
in
seattle
wa
"
]
]
}
}
suggest
:
{
engineId
:
"
suggest
"
taskName
:
"
token
-
classification
"
modelId
:
"
Mozilla
/
distilbert
-
uncased
-
NER
-
LoRA
"
modelRevision
:
"
main
"
dtype
:
"
q8
"
device
:
"
wasm
"
request
:
{
args
:
[
[
"
restaurants
in
seattle
wa
"
]
]
}
}
engine3
:
{
engineId
:
"
engine3
"
taskName
:
"
feature
-
extraction
"
modelId
:
"
Xenova
/
all
-
MiniLM
-
L6
-
v2
"
modelRevision
:
"
main
"
dtype
:
"
q8
"
device
:
"
wasm
"
request
:
{
args
:
[
[
"
Yet
another
example
sentence
"
"
Checking
sentence
handling
"
]
]
options
:
{
pooling
:
"
mean
"
normalize
:
true
}
}
}
engine4
:
{
engineId
:
"
engine4
"
taskName
:
"
feature
-
extraction
"
modelId
:
"
Xenova
/
all
-
MiniLM
-
L6
-
v2
"
modelRevision
:
"
main
"
dtype
:
"
q8
"
device
:
"
wasm
"
request
:
{
args
:
[
[
"
Final
example
sentence
"
"
Ensuring
unique
inputs
"
]
]
options
:
{
pooling
:
"
mean
"
normalize
:
true
}
}
}
}
;
const
BASE_METRICS
=
[
PIPELINE_READY_LATENCY
INITIALIZATION_LATENCY
MODEL_RUN_LATENCY
]
;
const
METRICS
=
[
]
;
for
(
let
engineKey
of
Object
.
keys
(
ENGINES
)
)
{
for
(
let
metric
of
BASE_METRICS
)
{
METRICS
.
push
(
{
engineKey
}
-
{
metric
}
)
;
}
}
METRICS
.
push
(
TOTAL_MEMORY_USAGE
)
;
const
journal
=
{
}
;
for
(
let
metric
of
METRICS
)
{
journal
[
metric
]
=
[
]
;
}
const
perfMetadata
=
{
owner
:
"
GenAI
Team
"
name
:
"
ML
Test
Multi
Model
"
description
:
"
Testing
model
execution
concurrenty
"
options
:
{
default
:
{
perfherder
:
true
perfherder_metrics
:
[
{
name
:
"
latency
"
unit
:
"
ms
"
shouldAlert
:
true
}
{
name
:
"
memory
"
unit
:
"
MB
"
shouldAlert
:
true
}
]
verbose
:
true
manifest
:
"
perftest
.
toml
"
manifest_flavor
:
"
browser
-
chrome
"
try_platform
:
[
"
linux
"
"
mac
"
"
win
"
]
}
}
}
;
for
(
let
metric
of
METRICS
)
{
perfMetadata
.
options
.
default
.
perfherder_metrics
.
push
(
{
name
:
metric
unit
:
metric
.
includes
(
"
latency
"
)
?
"
ms
"
:
"
MB
"
shouldAlert
:
true
}
)
;
}
requestLongerTimeout
(
120
)
;
async
function
runEngineWithMetrics
(
engineInstance
engineConfig
iterations
=
1
)
{
const
journal
=
{
}
;
const
engine
=
engineInstance
.
engine
;
for
(
let
i
=
0
;
i
<
iterations
;
i
+
+
)
{
const
res
=
await
engine
.
run
(
engineConfig
.
request
)
;
let
metrics
=
fetchMetrics
(
res
.
metrics
)
;
for
(
const
[
metricName
metricVal
]
of
Object
.
entries
(
metrics
)
)
{
const
prefixedMetricName
=
{
engineConfig
.
engineId
}
-
{
metricName
}
;
if
(
!
journal
[
prefixedMetricName
]
)
{
journal
[
prefixedMetricName
]
=
[
]
;
}
journal
[
prefixedMetricName
]
.
push
(
metricVal
)
;
}
}
return
journal
;
}
add_task
(
async
function
test_ml_generic_pipeline_concurrent_separate_phases
(
)
{
const
engineInstances
=
await
Promise
.
all
(
Object
.
values
(
ENGINES
)
.
map
(
async
engineConfig
=
>
{
const
{
cleanup
engine
}
=
await
initializeEngine
(
new
PipelineOptions
(
engineConfig
)
)
;
return
{
cleanup
engine
}
;
}
)
)
;
info
(
"
All
engines
initialized
successfully
"
)
;
const
allJournals
=
await
Promise
.
all
(
engineInstances
.
map
(
(
engineInstance
index
)
=
>
runEngineWithMetrics
(
engineInstance
Object
.
values
(
ENGINES
)
[
index
]
ITERATIONS
)
)
)
;
const
combinedJournal
=
allJournals
.
reduce
(
(
acc
journal
)
=
>
{
Object
.
entries
(
journal
)
.
forEach
(
(
[
key
values
]
)
=
>
{
if
(
!
acc
[
key
]
)
{
acc
[
key
]
=
[
]
;
}
acc
[
key
]
.
push
(
.
.
.
values
)
;
}
)
;
return
acc
;
}
{
}
)
;
Assert
.
ok
(
true
)
;
const
memUsage
=
await
getTotalMemoryUsage
(
)
;
(
combinedJournal
[
"
total
-
memory
-
usage
"
]
=
combinedJournal
[
"
total
-
memory
-
usage
"
]
|
|
[
]
)
.
push
(
memUsage
)
;
reportMetrics
(
combinedJournal
)
;
await
EngineProcess
.
destroyMLEngine
(
)
;
for
(
const
instance
of
engineInstances
)
{
await
instance
.
cleanup
(
)
;
}
}
)
;
