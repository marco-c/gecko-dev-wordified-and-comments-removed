import
*
as
__WEBPACK_EXTERNAL_MODULE_chrome_global_content_ml_ort_dev_js_1378f1ef__
from
"
chrome
:
/
/
global
/
content
/
ml
/
ort
-
dev
.
js
"
;
var
__webpack_modules__
=
(
{
"
onnxruntime
-
web
"
:
(
(
module
)
=
>
{
var
x
=
y
=
>
{
var
x
=
{
}
;
__webpack_require__
.
d
(
x
y
)
;
return
x
;
}
var
y
=
x
=
>
(
)
=
>
x
module
.
exports
=
__WEBPACK_EXTERNAL_MODULE_chrome_global_content_ml_ort_dev_js_1378f1ef__
;
}
)
"
?
7a2c
"
:
(
(
)
=
>
{
}
)
"
?
a42a
"
:
(
(
)
=
>
{
}
)
"
?
2b25
"
:
(
(
)
=
>
{
}
)
"
?
e65c
"
:
(
(
)
=
>
{
}
)
"
?
569f
"
:
(
(
)
=
>
{
}
)
"
?
3f59
"
:
(
(
)
=
>
{
}
)
"
?
154a
"
:
(
(
)
=
>
{
}
)
"
.
/
node_modules
/
huggingface
/
jinja
/
dist
/
index
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
Environment
"
:
(
)
=
>
(
Environment
)
"
Interpreter
"
:
(
)
=
>
(
Interpreter
)
"
Template
"
:
(
)
=
>
(
Template
)
"
parse
"
:
(
)
=
>
(
parse
)
"
tokenize
"
:
(
)
=
>
(
tokenize
)
}
)
;
var
TOKEN_TYPES
=
Object
.
freeze
(
{
Text
:
"
Text
"
NumericLiteral
:
"
NumericLiteral
"
BooleanLiteral
:
"
BooleanLiteral
"
StringLiteral
:
"
StringLiteral
"
Identifier
:
"
Identifier
"
Equals
:
"
Equals
"
OpenParen
:
"
OpenParen
"
CloseParen
:
"
CloseParen
"
OpenStatement
:
"
OpenStatement
"
CloseStatement
:
"
CloseStatement
"
OpenExpression
:
"
OpenExpression
"
CloseExpression
:
"
CloseExpression
"
OpenSquareBracket
:
"
OpenSquareBracket
"
CloseSquareBracket
:
"
CloseSquareBracket
"
OpenCurlyBracket
:
"
OpenCurlyBracket
"
CloseCurlyBracket
:
"
CloseCurlyBracket
"
Comma
:
"
Comma
"
Dot
:
"
Dot
"
Colon
:
"
Colon
"
Pipe
:
"
Pipe
"
CallOperator
:
"
CallOperator
"
AdditiveBinaryOperator
:
"
AdditiveBinaryOperator
"
MultiplicativeBinaryOperator
:
"
MultiplicativeBinaryOperator
"
ComparisonBinaryOperator
:
"
ComparisonBinaryOperator
"
UnaryOperator
:
"
UnaryOperator
"
Set
:
"
Set
"
If
:
"
If
"
For
:
"
For
"
In
:
"
In
"
Is
:
"
Is
"
NotIn
:
"
NotIn
"
Else
:
"
Else
"
EndIf
:
"
EndIf
"
ElseIf
:
"
ElseIf
"
EndFor
:
"
EndFor
"
And
:
"
And
"
Or
:
"
Or
"
Not
:
"
UnaryOperator
"
}
)
;
var
KEYWORDS
=
Object
.
freeze
(
{
set
:
TOKEN_TYPES
.
Set
for
:
TOKEN_TYPES
.
For
in
:
TOKEN_TYPES
.
In
is
:
TOKEN_TYPES
.
Is
if
:
TOKEN_TYPES
.
If
else
:
TOKEN_TYPES
.
Else
endif
:
TOKEN_TYPES
.
EndIf
elif
:
TOKEN_TYPES
.
ElseIf
endfor
:
TOKEN_TYPES
.
EndFor
and
:
TOKEN_TYPES
.
And
or
:
TOKEN_TYPES
.
Or
not
:
TOKEN_TYPES
.
Not
"
not
in
"
:
TOKEN_TYPES
.
NotIn
true
:
TOKEN_TYPES
.
BooleanLiteral
false
:
TOKEN_TYPES
.
BooleanLiteral
}
)
;
var
Token
=
class
{
constructor
(
value
type
)
{
this
.
value
=
value
;
this
.
type
=
type
;
}
}
;
function
isWord
(
char
)
{
return
/
\
w
/
.
test
(
char
)
;
}
function
isInteger
(
char
)
{
return
/
[
0
-
9
]
/
.
test
(
char
)
;
}
var
ORDERED_MAPPING_TABLE
=
[
[
"
{
%
"
TOKEN_TYPES
.
OpenStatement
]
[
"
%
}
"
TOKEN_TYPES
.
CloseStatement
]
[
"
{
{
"
TOKEN_TYPES
.
OpenExpression
]
[
"
}
}
"
TOKEN_TYPES
.
CloseExpression
]
[
"
(
"
TOKEN_TYPES
.
OpenParen
]
[
"
)
"
TOKEN_TYPES
.
CloseParen
]
[
"
{
"
TOKEN_TYPES
.
OpenCurlyBracket
]
[
"
}
"
TOKEN_TYPES
.
CloseCurlyBracket
]
[
"
[
"
TOKEN_TYPES
.
OpenSquareBracket
]
[
"
]
"
TOKEN_TYPES
.
CloseSquareBracket
]
[
"
"
TOKEN_TYPES
.
Comma
]
[
"
.
"
TOKEN_TYPES
.
Dot
]
[
"
:
"
TOKEN_TYPES
.
Colon
]
[
"
|
"
TOKEN_TYPES
.
Pipe
]
[
"
<
=
"
TOKEN_TYPES
.
ComparisonBinaryOperator
]
[
"
>
=
"
TOKEN_TYPES
.
ComparisonBinaryOperator
]
[
"
=
=
"
TOKEN_TYPES
.
ComparisonBinaryOperator
]
[
"
!
=
"
TOKEN_TYPES
.
ComparisonBinaryOperator
]
[
"
<
"
TOKEN_TYPES
.
ComparisonBinaryOperator
]
[
"
>
"
TOKEN_TYPES
.
ComparisonBinaryOperator
]
[
"
+
"
TOKEN_TYPES
.
AdditiveBinaryOperator
]
[
"
-
"
TOKEN_TYPES
.
AdditiveBinaryOperator
]
[
"
*
"
TOKEN_TYPES
.
MultiplicativeBinaryOperator
]
[
"
/
"
TOKEN_TYPES
.
MultiplicativeBinaryOperator
]
[
"
%
"
TOKEN_TYPES
.
MultiplicativeBinaryOperator
]
[
"
=
"
TOKEN_TYPES
.
Equals
]
]
;
var
ESCAPE_CHARACTERS
=
new
Map
(
[
[
"
n
"
"
\
n
"
]
[
"
t
"
"
"
]
[
"
r
"
"
\
r
"
]
[
"
b
"
"
\
b
"
]
[
"
f
"
"
\
f
"
]
[
"
v
"
"
\
v
"
]
[
"
'
"
"
'
"
]
[
'
"
'
'
"
'
]
[
"
\
\
"
"
\
\
"
]
]
)
;
function
preprocess
(
template
options
=
{
}
)
{
if
(
template
.
endsWith
(
"
\
n
"
)
)
{
template
=
template
.
slice
(
0
-
1
)
;
}
template
=
template
.
replace
(
/
{
#
.
*
?
#
}
/
gs
"
{
#
#
}
"
)
;
if
(
options
.
lstrip_blocks
)
{
template
=
template
.
replace
(
/
^
[
\
t
]
*
(
{
[
#
%
]
)
/
gm
"
1
"
)
;
}
if
(
options
.
trim_blocks
)
{
template
=
template
.
replace
(
/
(
[
#
%
]
}
)
\
n
/
g
"
1
"
)
;
}
return
template
.
replace
(
/
{
#
#
}
/
g
"
"
)
.
replace
(
/
-
%
}
\
s
*
/
g
"
%
}
"
)
.
replace
(
/
\
s
*
{
%
-
/
g
"
{
%
"
)
.
replace
(
/
-
}
}
\
s
*
/
g
"
}
}
"
)
.
replace
(
/
\
s
*
{
{
-
/
g
"
{
{
"
)
;
}
function
tokenize
(
source
options
=
{
}
)
{
const
tokens
=
[
]
;
const
src
=
preprocess
(
source
options
)
;
let
cursorPosition
=
0
;
const
consumeWhile
=
(
predicate
)
=
>
{
let
str
=
"
"
;
while
(
predicate
(
src
[
cursorPosition
]
)
)
{
if
(
src
[
cursorPosition
]
=
=
=
"
\
\
"
)
{
+
+
cursorPosition
;
if
(
cursorPosition
>
=
src
.
length
)
throw
new
SyntaxError
(
"
Unexpected
end
of
input
"
)
;
const
escaped
=
src
[
cursorPosition
+
+
]
;
const
unescaped
=
ESCAPE_CHARACTERS
.
get
(
escaped
)
;
if
(
unescaped
=
=
=
void
0
)
{
throw
new
SyntaxError
(
Unexpected
escaped
character
:
{
escaped
}
)
;
}
str
+
=
unescaped
;
continue
;
}
str
+
=
src
[
cursorPosition
+
+
]
;
if
(
cursorPosition
>
=
src
.
length
)
throw
new
SyntaxError
(
"
Unexpected
end
of
input
"
)
;
}
return
str
;
}
;
main
:
while
(
cursorPosition
<
src
.
length
)
{
const
lastTokenType
=
tokens
.
at
(
-
1
)
?
.
type
;
if
(
lastTokenType
=
=
=
void
0
|
|
lastTokenType
=
=
=
TOKEN_TYPES
.
CloseStatement
|
|
lastTokenType
=
=
=
TOKEN_TYPES
.
CloseExpression
)
{
let
text
=
"
"
;
while
(
cursorPosition
<
src
.
length
&
&
!
(
src
[
cursorPosition
]
=
=
=
"
{
"
&
&
(
src
[
cursorPosition
+
1
]
=
=
=
"
%
"
|
|
src
[
cursorPosition
+
1
]
=
=
=
"
{
"
)
)
)
{
text
+
=
src
[
cursorPosition
+
+
]
;
}
if
(
text
.
length
>
0
)
{
tokens
.
push
(
new
Token
(
text
TOKEN_TYPES
.
Text
)
)
;
continue
;
}
}
consumeWhile
(
(
char2
)
=
>
/
\
s
/
.
test
(
char2
)
)
;
const
char
=
src
[
cursorPosition
]
;
if
(
char
=
=
=
"
-
"
|
|
char
=
=
=
"
+
"
)
{
const
lastTokenType2
=
tokens
.
at
(
-
1
)
?
.
type
;
if
(
lastTokenType2
=
=
=
TOKEN_TYPES
.
Text
|
|
lastTokenType2
=
=
=
void
0
)
{
throw
new
SyntaxError
(
Unexpected
character
:
{
char
}
)
;
}
switch
(
lastTokenType2
)
{
case
TOKEN_TYPES
.
Identifier
:
case
TOKEN_TYPES
.
NumericLiteral
:
case
TOKEN_TYPES
.
BooleanLiteral
:
case
TOKEN_TYPES
.
StringLiteral
:
case
TOKEN_TYPES
.
CloseParen
:
case
TOKEN_TYPES
.
CloseSquareBracket
:
break
;
default
:
{
+
+
cursorPosition
;
const
num
=
consumeWhile
(
isInteger
)
;
tokens
.
push
(
new
Token
(
{
char
}
{
num
}
num
.
length
>
0
?
TOKEN_TYPES
.
NumericLiteral
:
TOKEN_TYPES
.
UnaryOperator
)
)
;
continue
;
}
}
}
for
(
const
[
char2
token
]
of
ORDERED_MAPPING_TABLE
)
{
const
slice2
=
src
.
slice
(
cursorPosition
cursorPosition
+
char2
.
length
)
;
if
(
slice2
=
=
=
char2
)
{
tokens
.
push
(
new
Token
(
char2
token
)
)
;
cursorPosition
+
=
char2
.
length
;
continue
main
;
}
}
if
(
char
=
=
=
"
'
"
|
|
char
=
=
=
'
"
'
)
{
+
+
cursorPosition
;
const
str
=
consumeWhile
(
(
c
)
=
>
c
!
=
=
char
)
;
tokens
.
push
(
new
Token
(
str
TOKEN_TYPES
.
StringLiteral
)
)
;
+
+
cursorPosition
;
continue
;
}
if
(
isInteger
(
char
)
)
{
const
num
=
consumeWhile
(
isInteger
)
;
tokens
.
push
(
new
Token
(
num
TOKEN_TYPES
.
NumericLiteral
)
)
;
continue
;
}
if
(
isWord
(
char
)
)
{
const
word
=
consumeWhile
(
isWord
)
;
const
type
=
Object
.
hasOwn
(
KEYWORDS
word
)
?
KEYWORDS
[
word
]
:
TOKEN_TYPES
.
Identifier
;
if
(
type
=
=
=
TOKEN_TYPES
.
In
&
&
tokens
.
at
(
-
1
)
?
.
type
=
=
=
TOKEN_TYPES
.
Not
)
{
tokens
.
pop
(
)
;
tokens
.
push
(
new
Token
(
"
not
in
"
TOKEN_TYPES
.
NotIn
)
)
;
}
else
{
tokens
.
push
(
new
Token
(
word
type
)
)
;
}
continue
;
}
throw
new
SyntaxError
(
Unexpected
character
:
{
char
}
)
;
}
return
tokens
;
}
var
Statement
=
class
{
type
=
"
Statement
"
;
}
;
var
Program
=
class
extends
Statement
{
constructor
(
body
)
{
super
(
)
;
this
.
body
=
body
;
}
type
=
"
Program
"
;
}
;
var
If
=
class
extends
Statement
{
constructor
(
test
body
alternate
)
{
super
(
)
;
this
.
test
=
test
;
this
.
body
=
body
;
this
.
alternate
=
alternate
;
}
type
=
"
If
"
;
}
;
var
For
=
class
extends
Statement
{
constructor
(
loopvar
iterable
body
)
{
super
(
)
;
this
.
loopvar
=
loopvar
;
this
.
iterable
=
iterable
;
this
.
body
=
body
;
}
type
=
"
For
"
;
}
;
var
SetStatement
=
class
extends
Statement
{
constructor
(
assignee
value
)
{
super
(
)
;
this
.
assignee
=
assignee
;
this
.
value
=
value
;
}
type
=
"
Set
"
;
}
;
var
Expression
=
class
extends
Statement
{
type
=
"
Expression
"
;
}
;
var
MemberExpression
=
class
extends
Expression
{
constructor
(
object
property
computed
)
{
super
(
)
;
this
.
object
=
object
;
this
.
property
=
property
;
this
.
computed
=
computed
;
}
type
=
"
MemberExpression
"
;
}
;
var
CallExpression
=
class
extends
Expression
{
constructor
(
callee
args
)
{
super
(
)
;
this
.
callee
=
callee
;
this
.
args
=
args
;
}
type
=
"
CallExpression
"
;
}
;
var
Identifier
=
class
extends
Expression
{
constructor
(
value
)
{
super
(
)
;
this
.
value
=
value
;
}
type
=
"
Identifier
"
;
}
;
var
Literal
=
class
extends
Expression
{
constructor
(
value
)
{
super
(
)
;
this
.
value
=
value
;
}
type
=
"
Literal
"
;
}
;
var
NumericLiteral
=
class
extends
Literal
{
type
=
"
NumericLiteral
"
;
}
;
var
StringLiteral
=
class
extends
Literal
{
type
=
"
StringLiteral
"
;
}
;
var
BooleanLiteral
=
class
extends
Literal
{
type
=
"
BooleanLiteral
"
;
}
;
var
ArrayLiteral
=
class
extends
Literal
{
type
=
"
ArrayLiteral
"
;
}
;
var
TupleLiteral
=
class
extends
Literal
{
type
=
"
TupleLiteral
"
;
}
;
var
ObjectLiteral
=
class
extends
Literal
{
type
=
"
ObjectLiteral
"
;
}
;
var
BinaryExpression
=
class
extends
Expression
{
constructor
(
operator
left
right
)
{
super
(
)
;
this
.
operator
=
operator
;
this
.
left
=
left
;
this
.
right
=
right
;
}
type
=
"
BinaryExpression
"
;
}
;
var
FilterExpression
=
class
extends
Expression
{
constructor
(
operand
filter
)
{
super
(
)
;
this
.
operand
=
operand
;
this
.
filter
=
filter
;
}
type
=
"
FilterExpression
"
;
}
;
var
TestExpression
=
class
extends
Expression
{
constructor
(
operand
negate
test
)
{
super
(
)
;
this
.
operand
=
operand
;
this
.
negate
=
negate
;
this
.
test
=
test
;
}
type
=
"
TestExpression
"
;
}
;
var
UnaryExpression
=
class
extends
Expression
{
constructor
(
operator
argument
)
{
super
(
)
;
this
.
operator
=
operator
;
this
.
argument
=
argument
;
}
type
=
"
UnaryExpression
"
;
}
;
var
SliceExpression
=
class
extends
Expression
{
constructor
(
start
=
void
0
stop
=
void
0
step
=
void
0
)
{
super
(
)
;
this
.
start
=
start
;
this
.
stop
=
stop
;
this
.
step
=
step
;
}
type
=
"
SliceExpression
"
;
}
;
var
KeywordArgumentExpression
=
class
extends
Expression
{
constructor
(
key
value
)
{
super
(
)
;
this
.
key
=
key
;
this
.
value
=
value
;
}
type
=
"
KeywordArgumentExpression
"
;
}
;
function
parse
(
tokens
)
{
const
program
=
new
Program
(
[
]
)
;
let
current
=
0
;
function
expect
(
type
error
)
{
const
prev
=
tokens
[
current
+
+
]
;
if
(
!
prev
|
|
prev
.
type
!
=
=
type
)
{
throw
new
Error
(
Parser
Error
:
{
error
}
.
{
prev
.
type
}
!
=
=
{
type
}
.
)
;
}
return
prev
;
}
function
parseAny
(
)
{
switch
(
tokens
[
current
]
.
type
)
{
case
TOKEN_TYPES
.
Text
:
return
parseText
(
)
;
case
TOKEN_TYPES
.
OpenStatement
:
return
parseJinjaStatement
(
)
;
case
TOKEN_TYPES
.
OpenExpression
:
return
parseJinjaExpression
(
)
;
default
:
throw
new
SyntaxError
(
Unexpected
token
type
:
{
tokens
[
current
]
.
type
}
)
;
}
}
function
not
(
.
.
.
types
)
{
return
current
+
types
.
length
<
=
tokens
.
length
&
&
types
.
some
(
(
type
i
)
=
>
type
!
=
=
tokens
[
current
+
i
]
.
type
)
;
}
function
is
(
.
.
.
types
)
{
return
current
+
types
.
length
<
=
tokens
.
length
&
&
types
.
every
(
(
type
i
)
=
>
type
=
=
=
tokens
[
current
+
i
]
.
type
)
;
}
function
parseText
(
)
{
return
new
StringLiteral
(
expect
(
TOKEN_TYPES
.
Text
"
Expected
text
token
"
)
.
value
)
;
}
function
parseJinjaStatement
(
)
{
expect
(
TOKEN_TYPES
.
OpenStatement
"
Expected
opening
statement
token
"
)
;
let
result
;
switch
(
tokens
[
current
]
.
type
)
{
case
TOKEN_TYPES
.
Set
:
+
+
current
;
result
=
parseSetStatement
(
)
;
expect
(
TOKEN_TYPES
.
CloseStatement
"
Expected
closing
statement
token
"
)
;
break
;
case
TOKEN_TYPES
.
If
:
+
+
current
;
result
=
parseIfStatement
(
)
;
expect
(
TOKEN_TYPES
.
OpenStatement
"
Expected
{
%
token
"
)
;
expect
(
TOKEN_TYPES
.
EndIf
"
Expected
endif
token
"
)
;
expect
(
TOKEN_TYPES
.
CloseStatement
"
Expected
%
}
token
"
)
;
break
;
case
TOKEN_TYPES
.
For
:
+
+
current
;
result
=
parseForStatement
(
)
;
expect
(
TOKEN_TYPES
.
OpenStatement
"
Expected
{
%
token
"
)
;
expect
(
TOKEN_TYPES
.
EndFor
"
Expected
endfor
token
"
)
;
expect
(
TOKEN_TYPES
.
CloseStatement
"
Expected
%
}
token
"
)
;
break
;
default
:
throw
new
SyntaxError
(
Unknown
statement
type
:
{
tokens
[
current
]
.
type
}
)
;
}
return
result
;
}
function
parseJinjaExpression
(
)
{
expect
(
TOKEN_TYPES
.
OpenExpression
"
Expected
opening
expression
token
"
)
;
const
result
=
parseExpression
(
)
;
expect
(
TOKEN_TYPES
.
CloseExpression
"
Expected
closing
expression
token
"
)
;
return
result
;
}
function
parseSetStatement
(
)
{
const
left
=
parseExpression
(
)
;
if
(
is
(
TOKEN_TYPES
.
Equals
)
)
{
+
+
current
;
const
value
=
parseSetStatement
(
)
;
return
new
SetStatement
(
left
value
)
;
}
return
left
;
}
function
parseIfStatement
(
)
{
const
test
=
parseExpression
(
)
;
expect
(
TOKEN_TYPES
.
CloseStatement
"
Expected
closing
statement
token
"
)
;
const
body
=
[
]
;
const
alternate
=
[
]
;
while
(
!
(
tokens
[
current
]
?
.
type
=
=
=
TOKEN_TYPES
.
OpenStatement
&
&
(
tokens
[
current
+
1
]
?
.
type
=
=
=
TOKEN_TYPES
.
ElseIf
|
|
tokens
[
current
+
1
]
?
.
type
=
=
=
TOKEN_TYPES
.
Else
|
|
tokens
[
current
+
1
]
?
.
type
=
=
=
TOKEN_TYPES
.
EndIf
)
)
)
{
body
.
push
(
parseAny
(
)
)
;
}
if
(
tokens
[
current
]
?
.
type
=
=
=
TOKEN_TYPES
.
OpenStatement
&
&
tokens
[
current
+
1
]
?
.
type
!
=
=
TOKEN_TYPES
.
EndIf
)
{
+
+
current
;
if
(
is
(
TOKEN_TYPES
.
ElseIf
)
)
{
expect
(
TOKEN_TYPES
.
ElseIf
"
Expected
elseif
token
"
)
;
alternate
.
push
(
parseIfStatement
(
)
)
;
}
else
{
expect
(
TOKEN_TYPES
.
Else
"
Expected
else
token
"
)
;
expect
(
TOKEN_TYPES
.
CloseStatement
"
Expected
closing
statement
token
"
)
;
while
(
!
(
tokens
[
current
]
?
.
type
=
=
=
TOKEN_TYPES
.
OpenStatement
&
&
tokens
[
current
+
1
]
?
.
type
=
=
=
TOKEN_TYPES
.
EndIf
)
)
{
alternate
.
push
(
parseAny
(
)
)
;
}
}
}
return
new
If
(
test
body
alternate
)
;
}
function
parseExpressionSequence
(
primary
=
false
)
{
const
fn
=
primary
?
parsePrimaryExpression
:
parseExpression
;
const
expressions
=
[
fn
(
)
]
;
const
isTuple
=
is
(
TOKEN_TYPES
.
Comma
)
;
while
(
isTuple
)
{
+
+
current
;
expressions
.
push
(
fn
(
)
)
;
if
(
!
is
(
TOKEN_TYPES
.
Comma
)
)
{
break
;
}
}
return
isTuple
?
new
TupleLiteral
(
expressions
)
:
expressions
[
0
]
;
}
function
parseForStatement
(
)
{
const
loopVariable
=
parseExpressionSequence
(
true
)
;
if
(
!
(
loopVariable
instanceof
Identifier
|
|
loopVariable
instanceof
TupleLiteral
)
)
{
throw
new
SyntaxError
(
Expected
identifier
/
tuple
for
the
loop
variable
got
{
loopVariable
.
type
}
instead
)
;
}
expect
(
TOKEN_TYPES
.
In
"
Expected
in
keyword
following
loop
variable
"
)
;
const
iterable
=
parseExpression
(
)
;
expect
(
TOKEN_TYPES
.
CloseStatement
"
Expected
closing
statement
token
"
)
;
const
body
=
[
]
;
while
(
not
(
TOKEN_TYPES
.
OpenStatement
TOKEN_TYPES
.
EndFor
)
)
{
body
.
push
(
parseAny
(
)
)
;
}
return
new
For
(
loopVariable
iterable
body
)
;
}
function
parseExpression
(
)
{
return
parseTernaryExpression
(
)
;
}
function
parseTernaryExpression
(
)
{
const
a
=
parseLogicalOrExpression
(
)
;
if
(
is
(
TOKEN_TYPES
.
If
)
)
{
+
+
current
;
const
predicate
=
parseLogicalOrExpression
(
)
;
expect
(
TOKEN_TYPES
.
Else
"
Expected
else
token
"
)
;
const
b
=
parseLogicalOrExpression
(
)
;
return
new
If
(
predicate
[
a
]
[
b
]
)
;
}
return
a
;
}
function
parseLogicalOrExpression
(
)
{
let
left
=
parseLogicalAndExpression
(
)
;
while
(
is
(
TOKEN_TYPES
.
Or
)
)
{
const
operator
=
tokens
[
current
]
;
+
+
current
;
const
right
=
parseLogicalAndExpression
(
)
;
left
=
new
BinaryExpression
(
operator
left
right
)
;
}
return
left
;
}
function
parseLogicalAndExpression
(
)
{
let
left
=
parseLogicalNegationExpression
(
)
;
while
(
is
(
TOKEN_TYPES
.
And
)
)
{
const
operator
=
tokens
[
current
]
;
+
+
current
;
const
right
=
parseLogicalNegationExpression
(
)
;
left
=
new
BinaryExpression
(
operator
left
right
)
;
}
return
left
;
}
function
parseLogicalNegationExpression
(
)
{
let
right
;
while
(
is
(
TOKEN_TYPES
.
Not
)
)
{
const
operator
=
tokens
[
current
]
;
+
+
current
;
const
arg
=
parseLogicalNegationExpression
(
)
;
right
=
new
UnaryExpression
(
operator
arg
)
;
}
return
right
?
?
parseComparisonExpression
(
)
;
}
function
parseComparisonExpression
(
)
{
let
left
=
parseAdditiveExpression
(
)
;
while
(
is
(
TOKEN_TYPES
.
ComparisonBinaryOperator
)
|
|
is
(
TOKEN_TYPES
.
In
)
|
|
is
(
TOKEN_TYPES
.
NotIn
)
)
{
const
operator
=
tokens
[
current
]
;
+
+
current
;
const
right
=
parseAdditiveExpression
(
)
;
left
=
new
BinaryExpression
(
operator
left
right
)
;
}
return
left
;
}
function
parseAdditiveExpression
(
)
{
let
left
=
parseMultiplicativeExpression
(
)
;
while
(
is
(
TOKEN_TYPES
.
AdditiveBinaryOperator
)
)
{
const
operator
=
tokens
[
current
]
;
+
+
current
;
const
right
=
parseMultiplicativeExpression
(
)
;
left
=
new
BinaryExpression
(
operator
left
right
)
;
}
return
left
;
}
function
parseCallMemberExpression
(
)
{
const
member
=
parseMemberExpression
(
)
;
if
(
is
(
TOKEN_TYPES
.
OpenParen
)
)
{
return
parseCallExpression
(
member
)
;
}
return
member
;
}
function
parseCallExpression
(
callee
)
{
let
callExpression
=
new
CallExpression
(
callee
parseArgs
(
)
)
;
if
(
is
(
TOKEN_TYPES
.
OpenParen
)
)
{
callExpression
=
parseCallExpression
(
callExpression
)
;
}
return
callExpression
;
}
function
parseArgs
(
)
{
expect
(
TOKEN_TYPES
.
OpenParen
"
Expected
opening
parenthesis
for
arguments
list
"
)
;
const
args
=
parseArgumentsList
(
)
;
expect
(
TOKEN_TYPES
.
CloseParen
"
Expected
closing
parenthesis
for
arguments
list
"
)
;
return
args
;
}
function
parseArgumentsList
(
)
{
const
args
=
[
]
;
while
(
!
is
(
TOKEN_TYPES
.
CloseParen
)
)
{
let
argument
=
parseExpression
(
)
;
if
(
is
(
TOKEN_TYPES
.
Equals
)
)
{
+
+
current
;
if
(
!
(
argument
instanceof
Identifier
)
)
{
throw
new
SyntaxError
(
Expected
identifier
for
keyword
argument
)
;
}
const
value
=
parseExpression
(
)
;
argument
=
new
KeywordArgumentExpression
(
argument
value
)
;
}
args
.
push
(
argument
)
;
if
(
is
(
TOKEN_TYPES
.
Comma
)
)
{
+
+
current
;
}
}
return
args
;
}
function
parseMemberExpressionArgumentsList
(
)
{
const
slices
=
[
]
;
let
isSlice
=
false
;
while
(
!
is
(
TOKEN_TYPES
.
CloseSquareBracket
)
)
{
if
(
is
(
TOKEN_TYPES
.
Colon
)
)
{
slices
.
push
(
void
0
)
;
+
+
current
;
isSlice
=
true
;
}
else
{
slices
.
push
(
parseExpression
(
)
)
;
if
(
is
(
TOKEN_TYPES
.
Colon
)
)
{
+
+
current
;
isSlice
=
true
;
}
}
}
if
(
slices
.
length
=
=
=
0
)
{
throw
new
SyntaxError
(
Expected
at
least
one
argument
for
member
/
slice
expression
)
;
}
if
(
isSlice
)
{
if
(
slices
.
length
>
3
)
{
throw
new
SyntaxError
(
Expected
0
-
3
arguments
for
slice
expression
)
;
}
return
new
SliceExpression
(
.
.
.
slices
)
;
}
return
slices
[
0
]
;
}
function
parseMemberExpression
(
)
{
let
object
=
parsePrimaryExpression
(
)
;
while
(
is
(
TOKEN_TYPES
.
Dot
)
|
|
is
(
TOKEN_TYPES
.
OpenSquareBracket
)
)
{
const
operator
=
tokens
[
current
]
;
+
+
current
;
let
property
;
const
computed
=
operator
.
type
!
=
=
TOKEN_TYPES
.
Dot
;
if
(
computed
)
{
property
=
parseMemberExpressionArgumentsList
(
)
;
expect
(
TOKEN_TYPES
.
CloseSquareBracket
"
Expected
closing
square
bracket
"
)
;
}
else
{
property
=
parsePrimaryExpression
(
)
;
if
(
property
.
type
!
=
=
"
Identifier
"
)
{
throw
new
SyntaxError
(
Expected
identifier
following
dot
operator
)
;
}
}
object
=
new
MemberExpression
(
object
property
computed
)
;
}
return
object
;
}
function
parseMultiplicativeExpression
(
)
{
let
left
=
parseTestExpression
(
)
;
while
(
is
(
TOKEN_TYPES
.
MultiplicativeBinaryOperator
)
)
{
const
operator
=
tokens
[
current
]
;
+
+
current
;
const
right
=
parseTestExpression
(
)
;
left
=
new
BinaryExpression
(
operator
left
right
)
;
}
return
left
;
}
function
parseTestExpression
(
)
{
let
operand
=
parseFilterExpression
(
)
;
while
(
is
(
TOKEN_TYPES
.
Is
)
)
{
+
+
current
;
const
negate
=
is
(
TOKEN_TYPES
.
Not
)
;
if
(
negate
)
{
+
+
current
;
}
let
filter
=
parsePrimaryExpression
(
)
;
if
(
filter
instanceof
BooleanLiteral
)
{
filter
=
new
Identifier
(
filter
.
value
.
toString
(
)
)
;
}
if
(
!
(
filter
instanceof
Identifier
)
)
{
throw
new
SyntaxError
(
Expected
identifier
for
the
test
)
;
}
operand
=
new
TestExpression
(
operand
negate
filter
)
;
}
return
operand
;
}
function
parseFilterExpression
(
)
{
let
operand
=
parseCallMemberExpression
(
)
;
while
(
is
(
TOKEN_TYPES
.
Pipe
)
)
{
+
+
current
;
let
filter
=
parsePrimaryExpression
(
)
;
if
(
!
(
filter
instanceof
Identifier
)
)
{
throw
new
SyntaxError
(
Expected
identifier
for
the
filter
)
;
}
if
(
is
(
TOKEN_TYPES
.
OpenParen
)
)
{
filter
=
parseCallExpression
(
filter
)
;
}
operand
=
new
FilterExpression
(
operand
filter
)
;
}
return
operand
;
}
function
parsePrimaryExpression
(
)
{
const
token
=
tokens
[
current
]
;
switch
(
token
.
type
)
{
case
TOKEN_TYPES
.
NumericLiteral
:
+
+
current
;
return
new
NumericLiteral
(
Number
(
token
.
value
)
)
;
case
TOKEN_TYPES
.
StringLiteral
:
+
+
current
;
return
new
StringLiteral
(
token
.
value
)
;
case
TOKEN_TYPES
.
BooleanLiteral
:
+
+
current
;
return
new
BooleanLiteral
(
token
.
value
=
=
=
"
true
"
)
;
case
TOKEN_TYPES
.
Identifier
:
+
+
current
;
return
new
Identifier
(
token
.
value
)
;
case
TOKEN_TYPES
.
OpenParen
:
{
+
+
current
;
const
expression
=
parseExpressionSequence
(
)
;
if
(
tokens
[
current
]
.
type
!
=
=
TOKEN_TYPES
.
CloseParen
)
{
throw
new
SyntaxError
(
Expected
closing
parenthesis
got
{
tokens
[
current
]
.
type
}
instead
)
;
}
+
+
current
;
return
expression
;
}
case
TOKEN_TYPES
.
OpenSquareBracket
:
{
+
+
current
;
const
values
=
[
]
;
while
(
!
is
(
TOKEN_TYPES
.
CloseSquareBracket
)
)
{
values
.
push
(
parseExpression
(
)
)
;
if
(
is
(
TOKEN_TYPES
.
Comma
)
)
{
+
+
current
;
}
}
+
+
current
;
return
new
ArrayLiteral
(
values
)
;
}
case
TOKEN_TYPES
.
OpenCurlyBracket
:
{
+
+
current
;
const
values
=
new
Map
(
)
;
while
(
!
is
(
TOKEN_TYPES
.
CloseCurlyBracket
)
)
{
const
key
=
parseExpression
(
)
;
expect
(
TOKEN_TYPES
.
Colon
"
Expected
colon
between
key
and
value
in
object
literal
"
)
;
const
value
=
parseExpression
(
)
;
values
.
set
(
key
value
)
;
if
(
is
(
TOKEN_TYPES
.
Comma
)
)
{
+
+
current
;
}
}
+
+
current
;
return
new
ObjectLiteral
(
values
)
;
}
default
:
throw
new
SyntaxError
(
Unexpected
token
:
{
token
.
type
}
)
;
}
}
while
(
current
<
tokens
.
length
)
{
program
.
body
.
push
(
parseAny
(
)
)
;
}
return
program
;
}
function
range
(
start
stop
step
=
1
)
{
if
(
stop
=
=
=
void
0
)
{
stop
=
start
;
start
=
0
;
}
const
result
=
[
]
;
for
(
let
i
=
start
;
i
<
stop
;
i
+
=
step
)
{
result
.
push
(
i
)
;
}
return
result
;
}
function
slice
(
array
start
stop
step
=
1
)
{
const
direction
=
Math
.
sign
(
step
)
;
if
(
direction
>
=
0
)
{
start
=
(
start
?
?
=
0
)
<
0
?
Math
.
max
(
array
.
length
+
start
0
)
:
Math
.
min
(
start
array
.
length
)
;
stop
=
(
stop
?
?
=
array
.
length
)
<
0
?
Math
.
max
(
array
.
length
+
stop
0
)
:
Math
.
min
(
stop
array
.
length
)
;
}
else
{
start
=
(
start
?
?
=
array
.
length
-
1
)
<
0
?
Math
.
max
(
array
.
length
+
start
-
1
)
:
Math
.
min
(
start
array
.
length
-
1
)
;
stop
=
(
stop
?
?
=
-
1
)
<
-
1
?
Math
.
max
(
array
.
length
+
stop
-
1
)
:
Math
.
min
(
stop
array
.
length
-
1
)
;
}
const
result
=
[
]
;
for
(
let
i
=
start
;
direction
*
i
<
direction
*
stop
;
i
+
=
step
)
{
result
.
push
(
array
[
i
]
)
;
}
return
result
;
}
function
titleCase
(
value
)
{
return
value
.
replace
(
/
\
b
\
w
/
g
(
c
)
=
>
c
.
toUpperCase
(
)
)
;
}
var
RuntimeValue
=
class
{
type
=
"
RuntimeValue
"
;
value
;
builtins
=
new
Map
(
)
;
constructor
(
value
=
void
0
)
{
this
.
value
=
value
;
}
__bool__
(
)
{
return
new
BooleanValue
(
!
!
this
.
value
)
;
}
}
;
var
NumericValue
=
class
extends
RuntimeValue
{
type
=
"
NumericValue
"
;
}
;
var
StringValue
=
class
extends
RuntimeValue
{
type
=
"
StringValue
"
;
builtins
=
new
Map
(
[
[
"
upper
"
new
FunctionValue
(
(
)
=
>
{
return
new
StringValue
(
this
.
value
.
toUpperCase
(
)
)
;
}
)
]
[
"
lower
"
new
FunctionValue
(
(
)
=
>
{
return
new
StringValue
(
this
.
value
.
toLowerCase
(
)
)
;
}
)
]
[
"
strip
"
new
FunctionValue
(
(
)
=
>
{
return
new
StringValue
(
this
.
value
.
trim
(
)
)
;
}
)
]
[
"
title
"
new
FunctionValue
(
(
)
=
>
{
return
new
StringValue
(
titleCase
(
this
.
value
)
)
;
}
)
]
[
"
length
"
new
NumericValue
(
this
.
value
.
length
)
]
]
)
;
}
;
var
BooleanValue
=
class
extends
RuntimeValue
{
type
=
"
BooleanValue
"
;
}
;
var
ObjectValue
=
class
extends
RuntimeValue
{
type
=
"
ObjectValue
"
;
__bool__
(
)
{
return
new
BooleanValue
(
this
.
value
.
size
>
0
)
;
}
builtins
=
new
Map
(
[
[
"
get
"
new
FunctionValue
(
(
[
key
defaultValue
]
)
=
>
{
if
(
!
(
key
instanceof
StringValue
)
)
{
throw
new
Error
(
Object
key
must
be
a
string
:
got
{
key
.
type
}
)
;
}
return
this
.
value
.
get
(
key
.
value
)
?
?
defaultValue
?
?
new
NullValue
(
)
;
}
)
]
[
"
items
"
new
FunctionValue
(
(
)
=
>
{
return
new
ArrayValue
(
Array
.
from
(
this
.
value
.
entries
(
)
)
.
map
(
(
[
key
value
]
)
=
>
new
ArrayValue
(
[
new
StringValue
(
key
)
value
]
)
)
)
;
}
)
]
]
)
;
}
;
var
ArrayValue
=
class
extends
RuntimeValue
{
type
=
"
ArrayValue
"
;
builtins
=
new
Map
(
[
[
"
length
"
new
NumericValue
(
this
.
value
.
length
)
]
]
)
;
__bool__
(
)
{
return
new
BooleanValue
(
this
.
value
.
length
>
0
)
;
}
}
;
var
TupleValue
=
class
extends
ArrayValue
{
type
=
"
TupleValue
"
;
}
;
var
FunctionValue
=
class
extends
RuntimeValue
{
type
=
"
FunctionValue
"
;
}
;
var
NullValue
=
class
extends
RuntimeValue
{
type
=
"
NullValue
"
;
}
;
var
UndefinedValue
=
class
extends
RuntimeValue
{
type
=
"
UndefinedValue
"
;
}
;
var
Environment
=
class
{
constructor
(
parent
)
{
this
.
parent
=
parent
;
}
variables
=
new
Map
(
[
[
"
namespace
"
new
FunctionValue
(
(
args
)
=
>
{
if
(
args
.
length
=
=
=
0
)
{
return
new
ObjectValue
(
new
Map
(
)
)
;
}
if
(
args
.
length
!
=
=
1
|
|
!
(
args
[
0
]
instanceof
ObjectValue
)
)
{
throw
new
Error
(
"
namespace
expects
either
zero
arguments
or
a
single
object
argument
"
)
;
}
return
args
[
0
]
;
}
)
]
]
)
;
tests
=
new
Map
(
[
[
"
boolean
"
(
operand
)
=
>
operand
.
type
=
=
=
"
BooleanValue
"
]
[
"
callable
"
(
operand
)
=
>
operand
instanceof
FunctionValue
]
[
"
odd
"
(
operand
)
=
>
{
if
(
operand
.
type
!
=
=
"
NumericValue
"
)
{
throw
new
Error
(
Cannot
apply
test
"
odd
"
to
type
:
{
operand
.
type
}
)
;
}
return
operand
.
value
%
2
!
=
=
0
;
}
]
[
"
even
"
(
operand
)
=
>
{
if
(
operand
.
type
!
=
=
"
NumericValue
"
)
{
throw
new
Error
(
Cannot
apply
test
"
even
"
to
type
:
{
operand
.
type
}
)
;
}
return
operand
.
value
%
2
=
=
=
0
;
}
]
[
"
false
"
(
operand
)
=
>
operand
.
type
=
=
=
"
BooleanValue
"
&
&
!
operand
.
value
]
[
"
true
"
(
operand
)
=
>
operand
.
type
=
=
=
"
BooleanValue
"
&
&
operand
.
value
]
[
"
number
"
(
operand
)
=
>
operand
.
type
=
=
=
"
NumericValue
"
]
[
"
integer
"
(
operand
)
=
>
operand
.
type
=
=
=
"
NumericValue
"
&
&
Number
.
isInteger
(
operand
.
value
)
]
[
"
iterable
"
(
operand
)
=
>
operand
instanceof
ArrayValue
|
|
operand
instanceof
StringValue
]
[
"
lower
"
(
operand
)
=
>
{
const
str
=
operand
.
value
;
return
operand
.
type
=
=
=
"
StringValue
"
&
&
str
=
=
=
str
.
toLowerCase
(
)
;
}
]
[
"
upper
"
(
operand
)
=
>
{
const
str
=
operand
.
value
;
return
operand
.
type
=
=
=
"
StringValue
"
&
&
str
=
=
=
str
.
toUpperCase
(
)
;
}
]
[
"
none
"
(
operand
)
=
>
operand
.
type
=
=
=
"
NullValue
"
]
[
"
defined
"
(
operand
)
=
>
operand
.
type
!
=
=
"
UndefinedValue
"
]
[
"
undefined
"
(
operand
)
=
>
operand
.
type
=
=
=
"
UndefinedValue
"
]
[
"
equalto
"
(
a
b
)
=
>
a
.
value
=
=
=
b
.
value
]
]
)
;
set
(
name
value
)
{
return
this
.
declareVariable
(
name
convertToRuntimeValues
(
value
)
)
;
}
declareVariable
(
name
value
)
{
if
(
this
.
variables
.
has
(
name
)
)
{
throw
new
SyntaxError
(
Variable
already
declared
:
{
name
}
)
;
}
this
.
variables
.
set
(
name
value
)
;
return
value
;
}
setVariable
(
name
value
)
{
this
.
variables
.
set
(
name
value
)
;
return
value
;
}
resolve
(
name
)
{
if
(
this
.
variables
.
has
(
name
)
)
{
return
this
;
}
if
(
this
.
parent
)
{
return
this
.
parent
.
resolve
(
name
)
;
}
throw
new
Error
(
Unknown
variable
:
{
name
}
)
;
}
lookupVariable
(
name
)
{
try
{
return
this
.
resolve
(
name
)
.
variables
.
get
(
name
)
?
?
new
UndefinedValue
(
)
;
}
catch
{
return
new
UndefinedValue
(
)
;
}
}
}
;
var
Interpreter
=
class
{
global
;
constructor
(
env
)
{
this
.
global
=
env
?
?
new
Environment
(
)
;
}
run
(
program
)
{
return
this
.
evaluate
(
program
this
.
global
)
;
}
evaluateBinaryExpression
(
node
environment
)
{
const
left
=
this
.
evaluate
(
node
.
left
environment
)
;
switch
(
node
.
operator
.
value
)
{
case
"
and
"
:
return
left
.
__bool__
(
)
.
value
?
this
.
evaluate
(
node
.
right
environment
)
:
left
;
case
"
or
"
:
return
left
.
__bool__
(
)
.
value
?
left
:
this
.
evaluate
(
node
.
right
environment
)
;
}
const
right
=
this
.
evaluate
(
node
.
right
environment
)
;
switch
(
node
.
operator
.
value
)
{
case
"
=
=
"
:
return
new
BooleanValue
(
left
.
value
=
=
right
.
value
)
;
case
"
!
=
"
:
return
new
BooleanValue
(
left
.
value
!
=
right
.
value
)
;
}
if
(
left
instanceof
UndefinedValue
|
|
right
instanceof
UndefinedValue
)
{
throw
new
Error
(
"
Cannot
perform
operation
on
undefined
values
"
)
;
}
else
if
(
left
instanceof
NullValue
|
|
right
instanceof
NullValue
)
{
throw
new
Error
(
"
Cannot
perform
operation
on
null
values
"
)
;
}
else
if
(
left
instanceof
NumericValue
&
&
right
instanceof
NumericValue
)
{
switch
(
node
.
operator
.
value
)
{
case
"
+
"
:
return
new
NumericValue
(
left
.
value
+
right
.
value
)
;
case
"
-
"
:
return
new
NumericValue
(
left
.
value
-
right
.
value
)
;
case
"
*
"
:
return
new
NumericValue
(
left
.
value
*
right
.
value
)
;
case
"
/
"
:
return
new
NumericValue
(
left
.
value
/
right
.
value
)
;
case
"
%
"
:
return
new
NumericValue
(
left
.
value
%
right
.
value
)
;
case
"
<
"
:
return
new
BooleanValue
(
left
.
value
<
right
.
value
)
;
case
"
>
"
:
return
new
BooleanValue
(
left
.
value
>
right
.
value
)
;
case
"
>
=
"
:
return
new
BooleanValue
(
left
.
value
>
=
right
.
value
)
;
case
"
<
=
"
:
return
new
BooleanValue
(
left
.
value
<
=
right
.
value
)
;
}
}
else
if
(
left
instanceof
ArrayValue
&
&
right
instanceof
ArrayValue
)
{
switch
(
node
.
operator
.
value
)
{
case
"
+
"
:
return
new
ArrayValue
(
left
.
value
.
concat
(
right
.
value
)
)
;
}
}
else
if
(
right
instanceof
ArrayValue
)
{
const
member
=
right
.
value
.
find
(
(
x
)
=
>
x
.
value
=
=
=
left
.
value
)
!
=
=
void
0
;
switch
(
node
.
operator
.
value
)
{
case
"
in
"
:
return
new
BooleanValue
(
member
)
;
case
"
not
in
"
:
return
new
BooleanValue
(
!
member
)
;
}
}
if
(
left
instanceof
StringValue
|
|
right
instanceof
StringValue
)
{
switch
(
node
.
operator
.
value
)
{
case
"
+
"
:
return
new
StringValue
(
left
.
value
.
toString
(
)
+
right
.
value
.
toString
(
)
)
;
}
}
if
(
left
instanceof
StringValue
&
&
right
instanceof
StringValue
)
{
switch
(
node
.
operator
.
value
)
{
case
"
in
"
:
return
new
BooleanValue
(
right
.
value
.
includes
(
left
.
value
)
)
;
case
"
not
in
"
:
return
new
BooleanValue
(
!
right
.
value
.
includes
(
left
.
value
)
)
;
}
}
if
(
left
instanceof
StringValue
&
&
right
instanceof
ObjectValue
)
{
switch
(
node
.
operator
.
value
)
{
case
"
in
"
:
return
new
BooleanValue
(
right
.
value
.
has
(
left
.
value
)
)
;
case
"
not
in
"
:
return
new
BooleanValue
(
!
right
.
value
.
has
(
left
.
value
)
)
;
}
}
throw
new
SyntaxError
(
Unknown
operator
"
{
node
.
operator
.
value
}
"
between
{
left
.
type
}
and
{
right
.
type
}
)
;
}
evaluateFilterExpression
(
node
environment
)
{
const
operand
=
this
.
evaluate
(
node
.
operand
environment
)
;
if
(
node
.
filter
.
type
=
=
=
"
Identifier
"
)
{
const
filter
=
node
.
filter
;
if
(
operand
instanceof
ArrayValue
)
{
switch
(
filter
.
value
)
{
case
"
list
"
:
return
operand
;
case
"
first
"
:
return
operand
.
value
[
0
]
;
case
"
last
"
:
return
operand
.
value
[
operand
.
value
.
length
-
1
]
;
case
"
length
"
:
return
new
NumericValue
(
operand
.
value
.
length
)
;
case
"
reverse
"
:
return
new
ArrayValue
(
operand
.
value
.
reverse
(
)
)
;
case
"
sort
"
:
return
new
ArrayValue
(
operand
.
value
.
sort
(
(
a
b
)
=
>
{
if
(
a
.
type
!
=
=
b
.
type
)
{
throw
new
Error
(
Cannot
compare
different
types
:
{
a
.
type
}
and
{
b
.
type
}
)
;
}
switch
(
a
.
type
)
{
case
"
NumericValue
"
:
return
a
.
value
-
b
.
value
;
case
"
StringValue
"
:
return
a
.
value
.
localeCompare
(
b
.
value
)
;
default
:
throw
new
Error
(
Cannot
compare
type
:
{
a
.
type
}
)
;
}
}
)
)
;
default
:
throw
new
Error
(
Unknown
ArrayValue
filter
:
{
filter
.
value
}
)
;
}
}
else
if
(
operand
instanceof
StringValue
)
{
switch
(
filter
.
value
)
{
case
"
length
"
:
return
new
NumericValue
(
operand
.
value
.
length
)
;
case
"
upper
"
:
return
new
StringValue
(
operand
.
value
.
toUpperCase
(
)
)
;
case
"
lower
"
:
return
new
StringValue
(
operand
.
value
.
toLowerCase
(
)
)
;
case
"
title
"
:
return
new
StringValue
(
titleCase
(
operand
.
value
)
)
;
case
"
capitalize
"
:
return
new
StringValue
(
operand
.
value
.
charAt
(
0
)
.
toUpperCase
(
)
+
operand
.
value
.
slice
(
1
)
)
;
case
"
trim
"
:
return
new
StringValue
(
operand
.
value
.
trim
(
)
)
;
default
:
throw
new
Error
(
Unknown
StringValue
filter
:
{
filter
.
value
}
)
;
}
}
else
if
(
operand
instanceof
NumericValue
)
{
switch
(
filter
.
value
)
{
case
"
abs
"
:
return
new
NumericValue
(
Math
.
abs
(
operand
.
value
)
)
;
default
:
throw
new
Error
(
Unknown
NumericValue
filter
:
{
filter
.
value
}
)
;
}
}
else
if
(
operand
instanceof
ObjectValue
)
{
switch
(
filter
.
value
)
{
case
"
items
"
:
return
new
ArrayValue
(
Array
.
from
(
operand
.
value
.
entries
(
)
)
.
map
(
(
[
key
value
]
)
=
>
new
ArrayValue
(
[
new
StringValue
(
key
)
value
]
)
)
)
;
case
"
length
"
:
return
new
NumericValue
(
operand
.
value
.
size
)
;
default
:
throw
new
Error
(
Unknown
ObjectValue
filter
:
{
filter
.
value
}
)
;
}
}
throw
new
Error
(
Cannot
apply
filter
"
{
filter
.
value
}
"
to
type
:
{
operand
.
type
}
)
;
}
else
if
(
node
.
filter
.
type
=
=
=
"
CallExpression
"
)
{
const
filter
=
node
.
filter
;
if
(
filter
.
callee
.
type
!
=
=
"
Identifier
"
)
{
throw
new
Error
(
Unknown
filter
:
{
filter
.
callee
.
type
}
)
;
}
const
filterName
=
filter
.
callee
.
value
;
if
(
operand
instanceof
ArrayValue
)
{
switch
(
filterName
)
{
case
"
selectattr
"
:
{
if
(
operand
.
value
.
some
(
(
x
)
=
>
!
(
x
instanceof
ObjectValue
)
)
)
{
throw
new
Error
(
"
selectattr
can
only
be
applied
to
array
of
objects
"
)
;
}
if
(
filter
.
args
.
some
(
(
x
)
=
>
x
.
type
!
=
=
"
StringLiteral
"
)
)
{
throw
new
Error
(
"
arguments
of
selectattr
must
be
strings
"
)
;
}
const
[
attr
testName
value
]
=
filter
.
args
.
map
(
(
x
)
=
>
this
.
evaluate
(
x
environment
)
)
;
let
testFunction
;
if
(
testName
)
{
const
test
=
environment
.
tests
.
get
(
testName
.
value
)
;
if
(
!
test
)
{
throw
new
Error
(
Unknown
test
:
{
testName
.
value
}
)
;
}
testFunction
=
test
;
}
else
{
testFunction
=
(
.
.
.
x
)
=
>
x
[
0
]
.
__bool__
(
)
.
value
;
}
const
filtered
=
operand
.
value
.
filter
(
(
item
)
=
>
{
const
a
=
item
.
value
.
get
(
attr
.
value
)
;
if
(
a
)
{
return
testFunction
(
a
value
)
;
}
return
false
;
}
)
;
return
new
ArrayValue
(
filtered
)
;
}
}
throw
new
Error
(
Unknown
ArrayValue
filter
:
{
filterName
}
)
;
}
else
{
throw
new
Error
(
Cannot
apply
filter
"
{
filterName
}
"
to
type
:
{
operand
.
type
}
)
;
}
}
throw
new
Error
(
Unknown
filter
:
{
node
.
filter
.
type
}
)
;
}
evaluateTestExpression
(
node
environment
)
{
const
operand
=
this
.
evaluate
(
node
.
operand
environment
)
;
const
test
=
environment
.
tests
.
get
(
node
.
test
.
value
)
;
if
(
!
test
)
{
throw
new
Error
(
Unknown
test
:
{
node
.
test
.
value
}
)
;
}
const
result
=
test
(
operand
)
;
return
new
BooleanValue
(
node
.
negate
?
!
result
:
result
)
;
}
evaluateUnaryExpression
(
node
environment
)
{
const
argument
=
this
.
evaluate
(
node
.
argument
environment
)
;
switch
(
node
.
operator
.
value
)
{
case
"
not
"
:
return
new
BooleanValue
(
!
argument
.
value
)
;
default
:
throw
new
SyntaxError
(
Unknown
operator
:
{
node
.
operator
.
value
}
)
;
}
}
evalProgram
(
program
environment
)
{
return
this
.
evaluateBlock
(
program
.
body
environment
)
;
}
evaluateBlock
(
statements
environment
)
{
let
result
=
"
"
;
for
(
const
statement
of
statements
)
{
const
lastEvaluated
=
this
.
evaluate
(
statement
environment
)
;
if
(
lastEvaluated
.
type
!
=
=
"
NullValue
"
&
&
lastEvaluated
.
type
!
=
=
"
UndefinedValue
"
)
{
result
+
=
lastEvaluated
.
value
;
}
}
return
new
StringValue
(
result
)
;
}
evaluateIdentifier
(
node
environment
)
{
return
environment
.
lookupVariable
(
node
.
value
)
;
}
evaluateCallExpression
(
expr
environment
)
{
const
args
=
[
]
;
const
kwargs
=
new
Map
(
)
;
for
(
const
argument
of
expr
.
args
)
{
if
(
argument
.
type
=
=
=
"
KeywordArgumentExpression
"
)
{
const
kwarg
=
argument
;
kwargs
.
set
(
kwarg
.
key
.
value
this
.
evaluate
(
kwarg
.
value
environment
)
)
;
}
else
{
args
.
push
(
this
.
evaluate
(
argument
environment
)
)
;
}
}
if
(
kwargs
.
size
>
0
)
{
args
.
push
(
new
ObjectValue
(
kwargs
)
)
;
}
const
fn
=
this
.
evaluate
(
expr
.
callee
environment
)
;
if
(
fn
.
type
!
=
=
"
FunctionValue
"
)
{
throw
new
Error
(
Cannot
call
something
that
is
not
a
function
:
got
{
fn
.
type
}
)
;
}
return
fn
.
value
(
args
environment
)
;
}
evaluateSliceExpression
(
object
expr
environment
)
{
if
(
!
(
object
instanceof
ArrayValue
|
|
object
instanceof
StringValue
)
)
{
throw
new
Error
(
"
Slice
object
must
be
an
array
or
string
"
)
;
}
const
start
=
this
.
evaluate
(
expr
.
start
environment
)
;
const
stop
=
this
.
evaluate
(
expr
.
stop
environment
)
;
const
step
=
this
.
evaluate
(
expr
.
step
environment
)
;
if
(
!
(
start
instanceof
NumericValue
|
|
start
instanceof
UndefinedValue
)
)
{
throw
new
Error
(
"
Slice
start
must
be
numeric
or
undefined
"
)
;
}
if
(
!
(
stop
instanceof
NumericValue
|
|
stop
instanceof
UndefinedValue
)
)
{
throw
new
Error
(
"
Slice
stop
must
be
numeric
or
undefined
"
)
;
}
if
(
!
(
step
instanceof
NumericValue
|
|
step
instanceof
UndefinedValue
)
)
{
throw
new
Error
(
"
Slice
step
must
be
numeric
or
undefined
"
)
;
}
if
(
object
instanceof
ArrayValue
)
{
return
new
ArrayValue
(
slice
(
object
.
value
start
.
value
stop
.
value
step
.
value
)
)
;
}
else
{
return
new
StringValue
(
slice
(
Array
.
from
(
object
.
value
)
start
.
value
stop
.
value
step
.
value
)
.
join
(
"
"
)
)
;
}
}
evaluateMemberExpression
(
expr
environment
)
{
const
object
=
this
.
evaluate
(
expr
.
object
environment
)
;
let
property
;
if
(
expr
.
computed
)
{
if
(
expr
.
property
.
type
=
=
=
"
SliceExpression
"
)
{
return
this
.
evaluateSliceExpression
(
object
expr
.
property
environment
)
;
}
else
{
property
=
this
.
evaluate
(
expr
.
property
environment
)
;
}
}
else
{
property
=
new
StringValue
(
expr
.
property
.
value
)
;
}
let
value
;
if
(
object
instanceof
ObjectValue
)
{
if
(
!
(
property
instanceof
StringValue
)
)
{
throw
new
Error
(
Cannot
access
property
with
non
-
string
:
got
{
property
.
type
}
)
;
}
value
=
object
.
value
.
get
(
property
.
value
)
?
?
object
.
builtins
.
get
(
property
.
value
)
;
}
else
if
(
object
instanceof
ArrayValue
|
|
object
instanceof
StringValue
)
{
if
(
property
instanceof
NumericValue
)
{
value
=
object
.
value
.
at
(
property
.
value
)
;
if
(
object
instanceof
StringValue
)
{
value
=
new
StringValue
(
object
.
value
.
at
(
property
.
value
)
)
;
}
}
else
if
(
property
instanceof
StringValue
)
{
value
=
object
.
builtins
.
get
(
property
.
value
)
;
}
else
{
throw
new
Error
(
Cannot
access
property
with
non
-
string
/
non
-
number
:
got
{
property
.
type
}
)
;
}
}
else
{
if
(
!
(
property
instanceof
StringValue
)
)
{
throw
new
Error
(
Cannot
access
property
with
non
-
string
:
got
{
property
.
type
}
)
;
}
value
=
object
.
builtins
.
get
(
property
.
value
)
;
}
return
value
instanceof
RuntimeValue
?
value
:
new
UndefinedValue
(
)
;
}
evaluateSet
(
node
environment
)
{
const
rhs
=
this
.
evaluate
(
node
.
value
environment
)
;
if
(
node
.
assignee
.
type
=
=
=
"
Identifier
"
)
{
const
variableName
=
node
.
assignee
.
value
;
environment
.
setVariable
(
variableName
rhs
)
;
}
else
if
(
node
.
assignee
.
type
=
=
=
"
MemberExpression
"
)
{
const
member
=
node
.
assignee
;
const
object
=
this
.
evaluate
(
member
.
object
environment
)
;
if
(
!
(
object
instanceof
ObjectValue
)
)
{
throw
new
Error
(
"
Cannot
assign
to
member
of
non
-
object
"
)
;
}
if
(
member
.
property
.
type
!
=
=
"
Identifier
"
)
{
throw
new
Error
(
"
Cannot
assign
to
member
with
non
-
identifier
property
"
)
;
}
object
.
value
.
set
(
member
.
property
.
value
rhs
)
;
}
else
{
throw
new
Error
(
Invalid
LHS
inside
assignment
expression
:
{
JSON
.
stringify
(
node
.
assignee
)
}
)
;
}
return
new
NullValue
(
)
;
}
evaluateIf
(
node
environment
)
{
const
test
=
this
.
evaluate
(
node
.
test
environment
)
;
return
this
.
evaluateBlock
(
test
.
__bool__
(
)
.
value
?
node
.
body
:
node
.
alternate
environment
)
;
}
evaluateFor
(
node
environment
)
{
const
scope
=
new
Environment
(
environment
)
;
const
iterable
=
this
.
evaluate
(
node
.
iterable
scope
)
;
if
(
!
(
iterable
instanceof
ArrayValue
)
)
{
throw
new
Error
(
Expected
iterable
type
in
for
loop
:
got
{
iterable
.
type
}
)
;
}
let
result
=
"
"
;
for
(
let
i
=
0
;
i
<
iterable
.
value
.
length
;
+
+
i
)
{
const
loop
=
new
Map
(
[
[
"
index
"
new
NumericValue
(
i
+
1
)
]
[
"
index0
"
new
NumericValue
(
i
)
]
[
"
revindex
"
new
NumericValue
(
iterable
.
value
.
length
-
i
)
]
[
"
revindex0
"
new
NumericValue
(
iterable
.
value
.
length
-
i
-
1
)
]
[
"
first
"
new
BooleanValue
(
i
=
=
=
0
)
]
[
"
last
"
new
BooleanValue
(
i
=
=
=
iterable
.
value
.
length
-
1
)
]
[
"
length
"
new
NumericValue
(
iterable
.
value
.
length
)
]
[
"
previtem
"
i
>
0
?
iterable
.
value
[
i
-
1
]
:
new
UndefinedValue
(
)
]
[
"
nextitem
"
i
<
iterable
.
value
.
length
-
1
?
iterable
.
value
[
i
+
1
]
:
new
UndefinedValue
(
)
]
]
)
;
scope
.
setVariable
(
"
loop
"
new
ObjectValue
(
loop
)
)
;
const
current
=
iterable
.
value
[
i
]
;
if
(
node
.
loopvar
.
type
=
=
=
"
Identifier
"
)
{
scope
.
setVariable
(
node
.
loopvar
.
value
current
)
;
}
else
if
(
node
.
loopvar
.
type
=
=
=
"
TupleLiteral
"
)
{
const
loopvar
=
node
.
loopvar
;
if
(
current
.
type
!
=
=
"
ArrayValue
"
)
{
throw
new
Error
(
Cannot
unpack
non
-
iterable
type
:
{
current
.
type
}
)
;
}
const
c
=
current
;
if
(
loopvar
.
value
.
length
!
=
=
c
.
value
.
length
)
{
throw
new
Error
(
Too
{
loopvar
.
value
.
length
>
c
.
value
.
length
?
"
few
"
:
"
many
"
}
items
to
unpack
)
;
}
for
(
let
j
=
0
;
j
<
loopvar
.
value
.
length
;
+
+
j
)
{
if
(
loopvar
.
value
[
j
]
.
type
!
=
=
"
Identifier
"
)
{
throw
new
Error
(
Cannot
unpack
non
-
identifier
type
:
{
loopvar
.
value
[
j
]
.
type
}
)
;
}
scope
.
setVariable
(
loopvar
.
value
[
j
]
.
value
c
.
value
[
j
]
)
;
}
}
const
evaluated
=
this
.
evaluateBlock
(
node
.
body
scope
)
;
result
+
=
evaluated
.
value
;
}
return
new
StringValue
(
result
)
;
}
evaluate
(
statement
environment
)
{
if
(
statement
=
=
=
void
0
)
return
new
UndefinedValue
(
)
;
switch
(
statement
.
type
)
{
case
"
Program
"
:
return
this
.
evalProgram
(
statement
environment
)
;
case
"
Set
"
:
return
this
.
evaluateSet
(
statement
environment
)
;
case
"
If
"
:
return
this
.
evaluateIf
(
statement
environment
)
;
case
"
For
"
:
return
this
.
evaluateFor
(
statement
environment
)
;
case
"
NumericLiteral
"
:
return
new
NumericValue
(
Number
(
statement
.
value
)
)
;
case
"
StringLiteral
"
:
return
new
StringValue
(
statement
.
value
)
;
case
"
BooleanLiteral
"
:
return
new
BooleanValue
(
statement
.
value
)
;
case
"
ArrayLiteral
"
:
return
new
ArrayValue
(
statement
.
value
.
map
(
(
x
)
=
>
this
.
evaluate
(
x
environment
)
)
)
;
case
"
TupleLiteral
"
:
return
new
TupleValue
(
statement
.
value
.
map
(
(
x
)
=
>
this
.
evaluate
(
x
environment
)
)
)
;
case
"
ObjectLiteral
"
:
{
const
mapping
=
new
Map
(
)
;
for
(
const
[
key
value
]
of
statement
.
value
)
{
const
evaluatedKey
=
this
.
evaluate
(
key
environment
)
;
if
(
!
(
evaluatedKey
instanceof
StringValue
)
)
{
throw
new
Error
(
Object
keys
must
be
strings
:
got
{
evaluatedKey
.
type
}
)
;
}
mapping
.
set
(
evaluatedKey
.
value
this
.
evaluate
(
value
environment
)
)
;
}
return
new
ObjectValue
(
mapping
)
;
}
case
"
Identifier
"
:
return
this
.
evaluateIdentifier
(
statement
environment
)
;
case
"
CallExpression
"
:
return
this
.
evaluateCallExpression
(
statement
environment
)
;
case
"
MemberExpression
"
:
return
this
.
evaluateMemberExpression
(
statement
environment
)
;
case
"
UnaryExpression
"
:
return
this
.
evaluateUnaryExpression
(
statement
environment
)
;
case
"
BinaryExpression
"
:
return
this
.
evaluateBinaryExpression
(
statement
environment
)
;
case
"
FilterExpression
"
:
return
this
.
evaluateFilterExpression
(
statement
environment
)
;
case
"
TestExpression
"
:
return
this
.
evaluateTestExpression
(
statement
environment
)
;
default
:
throw
new
SyntaxError
(
Unknown
node
type
:
{
statement
.
type
}
)
;
}
}
}
;
function
convertToRuntimeValues
(
input
)
{
switch
(
typeof
input
)
{
case
"
number
"
:
return
new
NumericValue
(
input
)
;
case
"
string
"
:
return
new
StringValue
(
input
)
;
case
"
boolean
"
:
return
new
BooleanValue
(
input
)
;
case
"
object
"
:
if
(
input
=
=
=
null
)
{
return
new
NullValue
(
)
;
}
else
if
(
Array
.
isArray
(
input
)
)
{
return
new
ArrayValue
(
input
.
map
(
convertToRuntimeValues
)
)
;
}
else
{
return
new
ObjectValue
(
new
Map
(
Object
.
entries
(
input
)
.
map
(
(
[
key
value
]
)
=
>
[
key
convertToRuntimeValues
(
value
)
]
)
)
)
;
}
case
"
function
"
:
return
new
FunctionValue
(
(
args
_scope
)
=
>
{
const
result
=
input
(
.
.
.
args
.
map
(
(
x
)
=
>
x
.
value
)
)
?
?
null
;
return
convertToRuntimeValues
(
result
)
;
}
)
;
default
:
throw
new
Error
(
Cannot
convert
to
runtime
value
:
{
input
}
)
;
}
}
var
Template
=
class
{
parsed
;
constructor
(
template
)
{
const
tokens
=
tokenize
(
template
{
lstrip_blocks
:
true
trim_blocks
:
true
}
)
;
this
.
parsed
=
parse
(
tokens
)
;
}
render
(
items
)
{
const
env
=
new
Environment
(
)
;
env
.
set
(
"
false
"
false
)
;
env
.
set
(
"
true
"
true
)
;
env
.
set
(
"
raise_exception
"
(
args
)
=
>
{
throw
new
Error
(
args
)
;
}
)
;
env
.
set
(
"
range
"
range
)
;
for
(
const
[
key
value
]
of
Object
.
entries
(
items
)
)
{
env
.
set
(
key
value
)
;
}
const
interpreter
=
new
Interpreter
(
env
)
;
const
result
=
interpreter
.
run
(
this
.
parsed
)
;
return
result
.
value
;
}
}
;
}
)
"
.
/
src
/
backends
/
onnx
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
ONNX
"
:
(
)
=
>
(
ONNX
)
"
executionProviders
"
:
(
)
=
>
(
executionProviders
)
}
)
;
var
onnxruntime_web__WEBPACK_IMPORTED_MODULE_0__
=
__webpack_require__
(
"
onnxruntime
-
web
"
)
;
const
ONNX_WEB
=
ort
;
const
ONNX_NODE
=
null
;
let
ONNX
;
const
executionProviders
=
[
'
wasm
'
]
;
if
(
typeof
process
!
=
=
'
undefined
'
&
&
process
?
.
release
?
.
name
=
=
=
'
node
'
)
{
ONNX
=
ONNX_NODE
.
default
?
?
ONNX_NODE
;
executionProviders
.
unshift
(
'
cpu
'
)
;
}
else
{
ONNX
=
ONNX_WEB
.
default
?
?
ONNX_WEB
;
const
isIOS
=
typeof
navigator
!
=
=
'
undefined
'
&
&
/
iP
(
hone
|
od
|
ad
)
.
+
16_4
.
+
AppleWebKit
/
.
test
(
navigator
.
userAgent
)
;
if
(
isIOS
)
{
ONNX
.
env
.
wasm
.
simd
=
false
;
}
}
}
)
"
.
/
src
/
configs
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
AutoConfig
"
:
(
)
=
>
(
AutoConfig
)
"
PretrainedConfig
"
:
(
)
=
>
(
PretrainedConfig
)
}
)
;
var
_utils_hub_js__WEBPACK_IMPORTED_MODULE_0__
=
__webpack_require__
(
"
.
/
src
/
utils
/
hub
.
js
"
)
;
async
function
loadConfig
(
pretrained_model_name_or_path
options
)
{
let
info
=
await
(
0
_utils_hub_js__WEBPACK_IMPORTED_MODULE_0__
.
getModelJSON
)
(
pretrained_model_name_or_path
'
config
.
json
'
true
options
)
;
return
info
;
}
class
PretrainedConfig
{
constructor
(
configJSON
)
{
this
.
model_type
=
null
;
this
.
is_encoder_decoder
=
false
;
Object
.
assign
(
this
configJSON
)
;
}
static
async
from_pretrained
(
pretrained_model_name_or_path
{
progress_callback
=
null
config
=
null
cache_dir
=
null
local_files_only
=
false
revision
=
'
main
'
}
=
{
}
)
{
let
data
=
config
?
?
await
loadConfig
(
pretrained_model_name_or_path
{
progress_callback
config
cache_dir
local_files_only
revision
}
)
return
new
this
(
data
)
;
}
}
class
AutoConfig
{
static
async
from_pretrained
(
.
.
.
args
)
{
return
PretrainedConfig
.
from_pretrained
(
.
.
.
args
)
;
}
}
}
)
"
.
/
src
/
env
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
env
"
:
(
)
=
>
(
env
)
}
)
;
var
fs__WEBPACK_IMPORTED_MODULE_0__
=
__webpack_require__
(
"
?
569f
"
)
;
var
path__WEBPACK_IMPORTED_MODULE_1__
=
__webpack_require__
(
"
?
3f59
"
)
;
var
url__WEBPACK_IMPORTED_MODULE_2__
=
__webpack_require__
(
"
?
154a
"
)
;
var
_backends_onnx_js__WEBPACK_IMPORTED_MODULE_3__
=
__webpack_require__
(
"
.
/
src
/
backends
/
onnx
.
js
"
)
;
const
{
env
:
onnx_env
}
=
_backends_onnx_js__WEBPACK_IMPORTED_MODULE_3__
.
ONNX
;
const
VERSION
=
'
2
.
16
.
1
'
;
const
WEB_CACHE_AVAILABLE
=
typeof
self
!
=
=
'
undefined
'
&
&
'
caches
'
in
self
;
const
FS_AVAILABLE
=
!
isEmpty
(
fs__WEBPACK_IMPORTED_MODULE_0__
)
;
const
PATH_AVAILABLE
=
!
isEmpty
(
path__WEBPACK_IMPORTED_MODULE_1__
)
;
const
RUNNING_LOCALLY
=
FS_AVAILABLE
&
&
PATH_AVAILABLE
;
const
__dirname
=
RUNNING_LOCALLY
?
path__WEBPACK_IMPORTED_MODULE_1__
.
dirname
(
path__WEBPACK_IMPORTED_MODULE_1__
.
dirname
(
url__WEBPACK_IMPORTED_MODULE_2__
.
fileURLToPath
(
"
file
:
/
/
/
Users
/
tarekziade
/
Dev
/
mozilla
-
central
/
toolkit
/
components
/
ml
/
vendor
/
tmp
/
transformers
.
js
/
src
/
env
.
js
"
)
)
)
:
'
.
/
'
;
const
DEFAULT_CACHE_DIR
=
RUNNING_LOCALLY
?
path__WEBPACK_IMPORTED_MODULE_1__
.
join
(
__dirname
'
/
.
cache
/
'
)
:
null
;
const
DEFAULT_LOCAL_MODEL_PATH
=
'
/
models
/
'
;
const
localModelPath
=
RUNNING_LOCALLY
?
path__WEBPACK_IMPORTED_MODULE_1__
.
join
(
__dirname
DEFAULT_LOCAL_MODEL_PATH
)
:
DEFAULT_LOCAL_MODEL_PATH
;
if
(
onnx_env
?
.
wasm
)
{
onnx_env
.
wasm
.
wasmPaths
=
RUNNING_LOCALLY
?
path__WEBPACK_IMPORTED_MODULE_1__
.
join
(
__dirname
'
/
dist
/
'
)
:
https
:
/
/
cdn
.
jsdelivr
.
net
/
npm
/
xenova
/
transformers
{
VERSION
}
/
dist
/
;
}
const
env
=
{
backends
:
{
onnx
:
onnx_env
tfjs
:
{
}
}
__dirname
version
:
VERSION
allowRemoteModels
:
true
remoteHost
:
'
https
:
/
/
huggingface
.
co
/
'
remotePathTemplate
:
'
{
model
}
/
resolve
/
{
revision
}
/
'
allowLocalModels
:
true
localModelPath
:
localModelPath
useFS
:
FS_AVAILABLE
useBrowserCache
:
WEB_CACHE_AVAILABLE
useFSCache
:
FS_AVAILABLE
cacheDir
:
DEFAULT_CACHE_DIR
useCustomCache
:
false
customCache
:
null
}
function
isEmpty
(
obj
)
{
return
Object
.
keys
(
obj
)
.
length
=
=
=
0
;
}
}
)
"
.
/
src
/
models
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
ASTForAudioClassification
"
:
(
)
=
>
(
ASTForAudioClassification
)
"
ASTModel
"
:
(
)
=
>
(
ASTModel
)
"
ASTPreTrainedModel
"
:
(
)
=
>
(
ASTPreTrainedModel
)
"
AlbertForMaskedLM
"
:
(
)
=
>
(
AlbertForMaskedLM
)
"
AlbertForQuestionAnswering
"
:
(
)
=
>
(
AlbertForQuestionAnswering
)
"
AlbertForSequenceClassification
"
:
(
)
=
>
(
AlbertForSequenceClassification
)
"
AlbertModel
"
:
(
)
=
>
(
AlbertModel
)
"
AlbertPreTrainedModel
"
:
(
)
=
>
(
AlbertPreTrainedModel
)
"
AutoModel
"
:
(
)
=
>
(
AutoModel
)
"
AutoModelForAudioClassification
"
:
(
)
=
>
(
AutoModelForAudioClassification
)
"
AutoModelForAudioFrameClassification
"
:
(
)
=
>
(
AutoModelForAudioFrameClassification
)
"
AutoModelForCTC
"
:
(
)
=
>
(
AutoModelForCTC
)
"
AutoModelForCausalLM
"
:
(
)
=
>
(
AutoModelForCausalLM
)
"
AutoModelForDepthEstimation
"
:
(
)
=
>
(
AutoModelForDepthEstimation
)
"
AutoModelForDocumentQuestionAnswering
"
:
(
)
=
>
(
AutoModelForDocumentQuestionAnswering
)
"
AutoModelForImageClassification
"
:
(
)
=
>
(
AutoModelForImageClassification
)
"
AutoModelForImageFeatureExtraction
"
:
(
)
=
>
(
AutoModelForImageFeatureExtraction
)
"
AutoModelForImageMatting
"
:
(
)
=
>
(
AutoModelForImageMatting
)
"
AutoModelForImageSegmentation
"
:
(
)
=
>
(
AutoModelForImageSegmentation
)
"
AutoModelForImageToImage
"
:
(
)
=
>
(
AutoModelForImageToImage
)
"
AutoModelForMaskGeneration
"
:
(
)
=
>
(
AutoModelForMaskGeneration
)
"
AutoModelForMaskedLM
"
:
(
)
=
>
(
AutoModelForMaskedLM
)
"
AutoModelForObjectDetection
"
:
(
)
=
>
(
AutoModelForObjectDetection
)
"
AutoModelForQuestionAnswering
"
:
(
)
=
>
(
AutoModelForQuestionAnswering
)
"
AutoModelForSemanticSegmentation
"
:
(
)
=
>
(
AutoModelForSemanticSegmentation
)
"
AutoModelForSeq2SeqLM
"
:
(
)
=
>
(
AutoModelForSeq2SeqLM
)
"
AutoModelForSequenceClassification
"
:
(
)
=
>
(
AutoModelForSequenceClassification
)
"
AutoModelForSpeechSeq2Seq
"
:
(
)
=
>
(
AutoModelForSpeechSeq2Seq
)
"
AutoModelForTextToSpectrogram
"
:
(
)
=
>
(
AutoModelForTextToSpectrogram
)
"
AutoModelForTextToWaveform
"
:
(
)
=
>
(
AutoModelForTextToWaveform
)
"
AutoModelForTokenClassification
"
:
(
)
=
>
(
AutoModelForTokenClassification
)
"
AutoModelForVision2Seq
"
:
(
)
=
>
(
AutoModelForVision2Seq
)
"
AutoModelForXVector
"
:
(
)
=
>
(
AutoModelForXVector
)
"
AutoModelForZeroShotObjectDetection
"
:
(
)
=
>
(
AutoModelForZeroShotObjectDetection
)
"
BartForConditionalGeneration
"
:
(
)
=
>
(
BartForConditionalGeneration
)
"
BartForSequenceClassification
"
:
(
)
=
>
(
BartForSequenceClassification
)
"
BartModel
"
:
(
)
=
>
(
BartModel
)
"
BartPretrainedModel
"
:
(
)
=
>
(
BartPretrainedModel
)
"
BaseModelOutput
"
:
(
)
=
>
(
BaseModelOutput
)
"
BeitForImageClassification
"
:
(
)
=
>
(
BeitForImageClassification
)
"
BeitModel
"
:
(
)
=
>
(
BeitModel
)
"
BeitPreTrainedModel
"
:
(
)
=
>
(
BeitPreTrainedModel
)
"
BertForMaskedLM
"
:
(
)
=
>
(
BertForMaskedLM
)
"
BertForQuestionAnswering
"
:
(
)
=
>
(
BertForQuestionAnswering
)
"
BertForSequenceClassification
"
:
(
)
=
>
(
BertForSequenceClassification
)
"
BertForTokenClassification
"
:
(
)
=
>
(
BertForTokenClassification
)
"
BertModel
"
:
(
)
=
>
(
BertModel
)
"
BertPreTrainedModel
"
:
(
)
=
>
(
BertPreTrainedModel
)
"
BlenderbotForConditionalGeneration
"
:
(
)
=
>
(
BlenderbotForConditionalGeneration
)
"
BlenderbotModel
"
:
(
)
=
>
(
BlenderbotModel
)
"
BlenderbotPreTrainedModel
"
:
(
)
=
>
(
BlenderbotPreTrainedModel
)
"
BlenderbotSmallForConditionalGeneration
"
:
(
)
=
>
(
BlenderbotSmallForConditionalGeneration
)
"
BlenderbotSmallModel
"
:
(
)
=
>
(
BlenderbotSmallModel
)
"
BlenderbotSmallPreTrainedModel
"
:
(
)
=
>
(
BlenderbotSmallPreTrainedModel
)
"
BloomForCausalLM
"
:
(
)
=
>
(
BloomForCausalLM
)
"
BloomModel
"
:
(
)
=
>
(
BloomModel
)
"
BloomPreTrainedModel
"
:
(
)
=
>
(
BloomPreTrainedModel
)
"
CLIPModel
"
:
(
)
=
>
(
CLIPModel
)
"
CLIPPreTrainedModel
"
:
(
)
=
>
(
CLIPPreTrainedModel
)
"
CLIPSegForImageSegmentation
"
:
(
)
=
>
(
CLIPSegForImageSegmentation
)
"
CLIPSegModel
"
:
(
)
=
>
(
CLIPSegModel
)
"
CLIPSegPreTrainedModel
"
:
(
)
=
>
(
CLIPSegPreTrainedModel
)
"
CLIPTextModelWithProjection
"
:
(
)
=
>
(
CLIPTextModelWithProjection
)
"
CLIPVisionModelWithProjection
"
:
(
)
=
>
(
CLIPVisionModelWithProjection
)
"
CamembertForMaskedLM
"
:
(
)
=
>
(
CamembertForMaskedLM
)
"
CamembertForQuestionAnswering
"
:
(
)
=
>
(
CamembertForQuestionAnswering
)
"
CamembertForSequenceClassification
"
:
(
)
=
>
(
CamembertForSequenceClassification
)
"
CamembertForTokenClassification
"
:
(
)
=
>
(
CamembertForTokenClassification
)
"
CamembertModel
"
:
(
)
=
>
(
CamembertModel
)
"
CamembertPreTrainedModel
"
:
(
)
=
>
(
CamembertPreTrainedModel
)
"
CausalLMOutput
"
:
(
)
=
>
(
CausalLMOutput
)
"
CausalLMOutputWithPast
"
:
(
)
=
>
(
CausalLMOutputWithPast
)
"
ChineseCLIPModel
"
:
(
)
=
>
(
ChineseCLIPModel
)
"
ChineseCLIPPreTrainedModel
"
:
(
)
=
>
(
ChineseCLIPPreTrainedModel
)
"
ClapAudioModelWithProjection
"
:
(
)
=
>
(
ClapAudioModelWithProjection
)
"
ClapModel
"
:
(
)
=
>
(
ClapModel
)
"
ClapPreTrainedModel
"
:
(
)
=
>
(
ClapPreTrainedModel
)
"
ClapTextModelWithProjection
"
:
(
)
=
>
(
ClapTextModelWithProjection
)
"
CodeGenForCausalLM
"
:
(
)
=
>
(
CodeGenForCausalLM
)
"
CodeGenModel
"
:
(
)
=
>
(
CodeGenModel
)
"
CodeGenPreTrainedModel
"
:
(
)
=
>
(
CodeGenPreTrainedModel
)
"
ConvBertForMaskedLM
"
:
(
)
=
>
(
ConvBertForMaskedLM
)
"
ConvBertForQuestionAnswering
"
:
(
)
=
>
(
ConvBertForQuestionAnswering
)
"
ConvBertForSequenceClassification
"
:
(
)
=
>
(
ConvBertForSequenceClassification
)
"
ConvBertForTokenClassification
"
:
(
)
=
>
(
ConvBertForTokenClassification
)
"
ConvBertModel
"
:
(
)
=
>
(
ConvBertModel
)
"
ConvBertPreTrainedModel
"
:
(
)
=
>
(
ConvBertPreTrainedModel
)
"
ConvNextForImageClassification
"
:
(
)
=
>
(
ConvNextForImageClassification
)
"
ConvNextModel
"
:
(
)
=
>
(
ConvNextModel
)
"
ConvNextPreTrainedModel
"
:
(
)
=
>
(
ConvNextPreTrainedModel
)
"
ConvNextV2ForImageClassification
"
:
(
)
=
>
(
ConvNextV2ForImageClassification
)
"
ConvNextV2Model
"
:
(
)
=
>
(
ConvNextV2Model
)
"
ConvNextV2PreTrainedModel
"
:
(
)
=
>
(
ConvNextV2PreTrainedModel
)
"
DPTForDepthEstimation
"
:
(
)
=
>
(
DPTForDepthEstimation
)
"
DPTModel
"
:
(
)
=
>
(
DPTModel
)
"
DPTPreTrainedModel
"
:
(
)
=
>
(
DPTPreTrainedModel
)
"
DebertaForMaskedLM
"
:
(
)
=
>
(
DebertaForMaskedLM
)
"
DebertaForQuestionAnswering
"
:
(
)
=
>
(
DebertaForQuestionAnswering
)
"
DebertaForSequenceClassification
"
:
(
)
=
>
(
DebertaForSequenceClassification
)
"
DebertaForTokenClassification
"
:
(
)
=
>
(
DebertaForTokenClassification
)
"
DebertaModel
"
:
(
)
=
>
(
DebertaModel
)
"
DebertaPreTrainedModel
"
:
(
)
=
>
(
DebertaPreTrainedModel
)
"
DebertaV2ForMaskedLM
"
:
(
)
=
>
(
DebertaV2ForMaskedLM
)
"
DebertaV2ForQuestionAnswering
"
:
(
)
=
>
(
DebertaV2ForQuestionAnswering
)
"
DebertaV2ForSequenceClassification
"
:
(
)
=
>
(
DebertaV2ForSequenceClassification
)
"
DebertaV2ForTokenClassification
"
:
(
)
=
>
(
DebertaV2ForTokenClassification
)
"
DebertaV2Model
"
:
(
)
=
>
(
DebertaV2Model
)
"
DebertaV2PreTrainedModel
"
:
(
)
=
>
(
DebertaV2PreTrainedModel
)
"
DeiTForImageClassification
"
:
(
)
=
>
(
DeiTForImageClassification
)
"
DeiTModel
"
:
(
)
=
>
(
DeiTModel
)
"
DeiTPreTrainedModel
"
:
(
)
=
>
(
DeiTPreTrainedModel
)
"
DepthAnythingForDepthEstimation
"
:
(
)
=
>
(
DepthAnythingForDepthEstimation
)
"
DepthAnythingPreTrainedModel
"
:
(
)
=
>
(
DepthAnythingPreTrainedModel
)
"
DetrForObjectDetection
"
:
(
)
=
>
(
DetrForObjectDetection
)
"
DetrForSegmentation
"
:
(
)
=
>
(
DetrForSegmentation
)
"
DetrModel
"
:
(
)
=
>
(
DetrModel
)
"
DetrObjectDetectionOutput
"
:
(
)
=
>
(
DetrObjectDetectionOutput
)
"
DetrPreTrainedModel
"
:
(
)
=
>
(
DetrPreTrainedModel
)
"
DetrSegmentationOutput
"
:
(
)
=
>
(
DetrSegmentationOutput
)
"
Dinov2ForImageClassification
"
:
(
)
=
>
(
Dinov2ForImageClassification
)
"
Dinov2Model
"
:
(
)
=
>
(
Dinov2Model
)
"
Dinov2PreTrainedModel
"
:
(
)
=
>
(
Dinov2PreTrainedModel
)
"
DistilBertForMaskedLM
"
:
(
)
=
>
(
DistilBertForMaskedLM
)
"
DistilBertForQuestionAnswering
"
:
(
)
=
>
(
DistilBertForQuestionAnswering
)
"
DistilBertForSequenceClassification
"
:
(
)
=
>
(
DistilBertForSequenceClassification
)
"
DistilBertForTokenClassification
"
:
(
)
=
>
(
DistilBertForTokenClassification
)
"
DistilBertModel
"
:
(
)
=
>
(
DistilBertModel
)
"
DistilBertPreTrainedModel
"
:
(
)
=
>
(
DistilBertPreTrainedModel
)
"
DonutSwinModel
"
:
(
)
=
>
(
DonutSwinModel
)
"
DonutSwinPreTrainedModel
"
:
(
)
=
>
(
DonutSwinPreTrainedModel
)
"
EfficientNetForImageClassification
"
:
(
)
=
>
(
EfficientNetForImageClassification
)
"
EfficientNetModel
"
:
(
)
=
>
(
EfficientNetModel
)
"
EfficientNetPreTrainedModel
"
:
(
)
=
>
(
EfficientNetPreTrainedModel
)
"
ElectraForMaskedLM
"
:
(
)
=
>
(
ElectraForMaskedLM
)
"
ElectraForQuestionAnswering
"
:
(
)
=
>
(
ElectraForQuestionAnswering
)
"
ElectraForSequenceClassification
"
:
(
)
=
>
(
ElectraForSequenceClassification
)
"
ElectraForTokenClassification
"
:
(
)
=
>
(
ElectraForTokenClassification
)
"
ElectraModel
"
:
(
)
=
>
(
ElectraModel
)
"
ElectraPreTrainedModel
"
:
(
)
=
>
(
ElectraPreTrainedModel
)
"
EsmForMaskedLM
"
:
(
)
=
>
(
EsmForMaskedLM
)
"
EsmForSequenceClassification
"
:
(
)
=
>
(
EsmForSequenceClassification
)
"
EsmForTokenClassification
"
:
(
)
=
>
(
EsmForTokenClassification
)
"
EsmModel
"
:
(
)
=
>
(
EsmModel
)
"
EsmPreTrainedModel
"
:
(
)
=
>
(
EsmPreTrainedModel
)
"
FalconForCausalLM
"
:
(
)
=
>
(
FalconForCausalLM
)
"
FalconModel
"
:
(
)
=
>
(
FalconModel
)
"
FalconPreTrainedModel
"
:
(
)
=
>
(
FalconPreTrainedModel
)
"
GLPNForDepthEstimation
"
:
(
)
=
>
(
GLPNForDepthEstimation
)
"
GLPNModel
"
:
(
)
=
>
(
GLPNModel
)
"
GLPNPreTrainedModel
"
:
(
)
=
>
(
GLPNPreTrainedModel
)
"
GPT2LMHeadModel
"
:
(
)
=
>
(
GPT2LMHeadModel
)
"
GPT2Model
"
:
(
)
=
>
(
GPT2Model
)
"
GPT2PreTrainedModel
"
:
(
)
=
>
(
GPT2PreTrainedModel
)
"
GPTBigCodeForCausalLM
"
:
(
)
=
>
(
GPTBigCodeForCausalLM
)
"
GPTBigCodeModel
"
:
(
)
=
>
(
GPTBigCodeModel
)
"
GPTBigCodePreTrainedModel
"
:
(
)
=
>
(
GPTBigCodePreTrainedModel
)
"
GPTJForCausalLM
"
:
(
)
=
>
(
GPTJForCausalLM
)
"
GPTJModel
"
:
(
)
=
>
(
GPTJModel
)
"
GPTJPreTrainedModel
"
:
(
)
=
>
(
GPTJPreTrainedModel
)
"
GPTNeoForCausalLM
"
:
(
)
=
>
(
GPTNeoForCausalLM
)
"
GPTNeoModel
"
:
(
)
=
>
(
GPTNeoModel
)
"
GPTNeoPreTrainedModel
"
:
(
)
=
>
(
GPTNeoPreTrainedModel
)
"
GPTNeoXForCausalLM
"
:
(
)
=
>
(
GPTNeoXForCausalLM
)
"
GPTNeoXModel
"
:
(
)
=
>
(
GPTNeoXModel
)
"
GPTNeoXPreTrainedModel
"
:
(
)
=
>
(
GPTNeoXPreTrainedModel
)
"
HubertForCTC
"
:
(
)
=
>
(
HubertForCTC
)
"
HubertForSequenceClassification
"
:
(
)
=
>
(
HubertForSequenceClassification
)
"
HubertModel
"
:
(
)
=
>
(
HubertModel
)
"
HubertPreTrainedModel
"
:
(
)
=
>
(
HubertPreTrainedModel
)
"
ImageMattingOutput
"
:
(
)
=
>
(
ImageMattingOutput
)
"
LlamaForCausalLM
"
:
(
)
=
>
(
LlamaForCausalLM
)
"
LlamaModel
"
:
(
)
=
>
(
LlamaModel
)
"
LlamaPreTrainedModel
"
:
(
)
=
>
(
LlamaPreTrainedModel
)
"
LongT5ForConditionalGeneration
"
:
(
)
=
>
(
LongT5ForConditionalGeneration
)
"
LongT5Model
"
:
(
)
=
>
(
LongT5Model
)
"
LongT5PreTrainedModel
"
:
(
)
=
>
(
LongT5PreTrainedModel
)
"
M2M100ForConditionalGeneration
"
:
(
)
=
>
(
M2M100ForConditionalGeneration
)
"
M2M100Model
"
:
(
)
=
>
(
M2M100Model
)
"
M2M100PreTrainedModel
"
:
(
)
=
>
(
M2M100PreTrainedModel
)
"
MBartForCausalLM
"
:
(
)
=
>
(
MBartForCausalLM
)
"
MBartForConditionalGeneration
"
:
(
)
=
>
(
MBartForConditionalGeneration
)
"
MBartForSequenceClassification
"
:
(
)
=
>
(
MBartForSequenceClassification
)
"
MBartModel
"
:
(
)
=
>
(
MBartModel
)
"
MBartPreTrainedModel
"
:
(
)
=
>
(
MBartPreTrainedModel
)
"
MPNetForMaskedLM
"
:
(
)
=
>
(
MPNetForMaskedLM
)
"
MPNetForQuestionAnswering
"
:
(
)
=
>
(
MPNetForQuestionAnswering
)
"
MPNetForSequenceClassification
"
:
(
)
=
>
(
MPNetForSequenceClassification
)
"
MPNetForTokenClassification
"
:
(
)
=
>
(
MPNetForTokenClassification
)
"
MPNetModel
"
:
(
)
=
>
(
MPNetModel
)
"
MPNetPreTrainedModel
"
:
(
)
=
>
(
MPNetPreTrainedModel
)
"
MT5ForConditionalGeneration
"
:
(
)
=
>
(
MT5ForConditionalGeneration
)
"
MT5Model
"
:
(
)
=
>
(
MT5Model
)
"
MT5PreTrainedModel
"
:
(
)
=
>
(
MT5PreTrainedModel
)
"
MarianMTModel
"
:
(
)
=
>
(
MarianMTModel
)
"
MarianModel
"
:
(
)
=
>
(
MarianModel
)
"
MarianPreTrainedModel
"
:
(
)
=
>
(
MarianPreTrainedModel
)
"
MaskedLMOutput
"
:
(
)
=
>
(
MaskedLMOutput
)
"
MistralForCausalLM
"
:
(
)
=
>
(
MistralForCausalLM
)
"
MistralModel
"
:
(
)
=
>
(
MistralModel
)
"
MistralPreTrainedModel
"
:
(
)
=
>
(
MistralPreTrainedModel
)
"
MobileBertForMaskedLM
"
:
(
)
=
>
(
MobileBertForMaskedLM
)
"
MobileBertForQuestionAnswering
"
:
(
)
=
>
(
MobileBertForQuestionAnswering
)
"
MobileBertForSequenceClassification
"
:
(
)
=
>
(
MobileBertForSequenceClassification
)
"
MobileBertModel
"
:
(
)
=
>
(
MobileBertModel
)
"
MobileBertPreTrainedModel
"
:
(
)
=
>
(
MobileBertPreTrainedModel
)
"
MobileViTForImageClassification
"
:
(
)
=
>
(
MobileViTForImageClassification
)
"
MobileViTModel
"
:
(
)
=
>
(
MobileViTModel
)
"
MobileViTPreTrainedModel
"
:
(
)
=
>
(
MobileViTPreTrainedModel
)
"
ModelOutput
"
:
(
)
=
>
(
ModelOutput
)
"
MptForCausalLM
"
:
(
)
=
>
(
MptForCausalLM
)
"
MptModel
"
:
(
)
=
>
(
MptModel
)
"
MptPreTrainedModel
"
:
(
)
=
>
(
MptPreTrainedModel
)
"
NomicBertModel
"
:
(
)
=
>
(
NomicBertModel
)
"
NomicBertPreTrainedModel
"
:
(
)
=
>
(
NomicBertPreTrainedModel
)
"
OPTForCausalLM
"
:
(
)
=
>
(
OPTForCausalLM
)
"
OPTModel
"
:
(
)
=
>
(
OPTModel
)
"
OPTPreTrainedModel
"
:
(
)
=
>
(
OPTPreTrainedModel
)
"
OwlViTForObjectDetection
"
:
(
)
=
>
(
OwlViTForObjectDetection
)
"
OwlViTModel
"
:
(
)
=
>
(
OwlViTModel
)
"
OwlViTPreTrainedModel
"
:
(
)
=
>
(
OwlViTPreTrainedModel
)
"
Owlv2ForObjectDetection
"
:
(
)
=
>
(
Owlv2ForObjectDetection
)
"
Owlv2Model
"
:
(
)
=
>
(
Owlv2Model
)
"
Owlv2PreTrainedModel
"
:
(
)
=
>
(
Owlv2PreTrainedModel
)
"
PhiForCausalLM
"
:
(
)
=
>
(
PhiForCausalLM
)
"
PhiModel
"
:
(
)
=
>
(
PhiModel
)
"
PhiPreTrainedModel
"
:
(
)
=
>
(
PhiPreTrainedModel
)
"
PreTrainedModel
"
:
(
)
=
>
(
PreTrainedModel
)
"
PretrainedMixin
"
:
(
)
=
>
(
PretrainedMixin
)
"
QuestionAnsweringModelOutput
"
:
(
)
=
>
(
QuestionAnsweringModelOutput
)
"
Qwen2ForCausalLM
"
:
(
)
=
>
(
Qwen2ForCausalLM
)
"
Qwen2Model
"
:
(
)
=
>
(
Qwen2Model
)
"
Qwen2PreTrainedModel
"
:
(
)
=
>
(
Qwen2PreTrainedModel
)
"
ResNetForImageClassification
"
:
(
)
=
>
(
ResNetForImageClassification
)
"
ResNetModel
"
:
(
)
=
>
(
ResNetModel
)
"
ResNetPreTrainedModel
"
:
(
)
=
>
(
ResNetPreTrainedModel
)
"
RoFormerForMaskedLM
"
:
(
)
=
>
(
RoFormerForMaskedLM
)
"
RoFormerForQuestionAnswering
"
:
(
)
=
>
(
RoFormerForQuestionAnswering
)
"
RoFormerForSequenceClassification
"
:
(
)
=
>
(
RoFormerForSequenceClassification
)
"
RoFormerForTokenClassification
"
:
(
)
=
>
(
RoFormerForTokenClassification
)
"
RoFormerModel
"
:
(
)
=
>
(
RoFormerModel
)
"
RoFormerPreTrainedModel
"
:
(
)
=
>
(
RoFormerPreTrainedModel
)
"
RobertaForMaskedLM
"
:
(
)
=
>
(
RobertaForMaskedLM
)
"
RobertaForQuestionAnswering
"
:
(
)
=
>
(
RobertaForQuestionAnswering
)
"
RobertaForSequenceClassification
"
:
(
)
=
>
(
RobertaForSequenceClassification
)
"
RobertaForTokenClassification
"
:
(
)
=
>
(
RobertaForTokenClassification
)
"
RobertaModel
"
:
(
)
=
>
(
RobertaModel
)
"
RobertaPreTrainedModel
"
:
(
)
=
>
(
RobertaPreTrainedModel
)
"
SamImageSegmentationOutput
"
:
(
)
=
>
(
SamImageSegmentationOutput
)
"
SamModel
"
:
(
)
=
>
(
SamModel
)
"
SamPreTrainedModel
"
:
(
)
=
>
(
SamPreTrainedModel
)
"
SegformerForImageClassification
"
:
(
)
=
>
(
SegformerForImageClassification
)
"
SegformerForSemanticSegmentation
"
:
(
)
=
>
(
SegformerForSemanticSegmentation
)
"
SegformerModel
"
:
(
)
=
>
(
SegformerModel
)
"
SegformerPreTrainedModel
"
:
(
)
=
>
(
SegformerPreTrainedModel
)
"
Seq2SeqLMOutput
"
:
(
)
=
>
(
Seq2SeqLMOutput
)
"
SequenceClassifierOutput
"
:
(
)
=
>
(
SequenceClassifierOutput
)
"
SiglipModel
"
:
(
)
=
>
(
SiglipModel
)
"
SiglipPreTrainedModel
"
:
(
)
=
>
(
SiglipPreTrainedModel
)
"
SiglipTextModel
"
:
(
)
=
>
(
SiglipTextModel
)
"
SiglipVisionModel
"
:
(
)
=
>
(
SiglipVisionModel
)
"
SpeechT5ForSpeechToText
"
:
(
)
=
>
(
SpeechT5ForSpeechToText
)
"
SpeechT5ForTextToSpeech
"
:
(
)
=
>
(
SpeechT5ForTextToSpeech
)
"
SpeechT5HifiGan
"
:
(
)
=
>
(
SpeechT5HifiGan
)
"
SpeechT5Model
"
:
(
)
=
>
(
SpeechT5Model
)
"
SpeechT5PreTrainedModel
"
:
(
)
=
>
(
SpeechT5PreTrainedModel
)
"
SqueezeBertForMaskedLM
"
:
(
)
=
>
(
SqueezeBertForMaskedLM
)
"
SqueezeBertForQuestionAnswering
"
:
(
)
=
>
(
SqueezeBertForQuestionAnswering
)
"
SqueezeBertForSequenceClassification
"
:
(
)
=
>
(
SqueezeBertForSequenceClassification
)
"
SqueezeBertModel
"
:
(
)
=
>
(
SqueezeBertModel
)
"
SqueezeBertPreTrainedModel
"
:
(
)
=
>
(
SqueezeBertPreTrainedModel
)
"
StableLmForCausalLM
"
:
(
)
=
>
(
StableLmForCausalLM
)
"
StableLmModel
"
:
(
)
=
>
(
StableLmModel
)
"
StableLmPreTrainedModel
"
:
(
)
=
>
(
StableLmPreTrainedModel
)
"
Starcoder2ForCausalLM
"
:
(
)
=
>
(
Starcoder2ForCausalLM
)
"
Starcoder2Model
"
:
(
)
=
>
(
Starcoder2Model
)
"
Starcoder2PreTrainedModel
"
:
(
)
=
>
(
Starcoder2PreTrainedModel
)
"
Swin2SRForImageSuperResolution
"
:
(
)
=
>
(
Swin2SRForImageSuperResolution
)
"
Swin2SRModel
"
:
(
)
=
>
(
Swin2SRModel
)
"
Swin2SRPreTrainedModel
"
:
(
)
=
>
(
Swin2SRPreTrainedModel
)
"
SwinForImageClassification
"
:
(
)
=
>
(
SwinForImageClassification
)
"
SwinModel
"
:
(
)
=
>
(
SwinModel
)
"
SwinPreTrainedModel
"
:
(
)
=
>
(
SwinPreTrainedModel
)
"
T5ForConditionalGeneration
"
:
(
)
=
>
(
T5ForConditionalGeneration
)
"
T5Model
"
:
(
)
=
>
(
T5Model
)
"
T5PreTrainedModel
"
:
(
)
=
>
(
T5PreTrainedModel
)
"
TableTransformerForObjectDetection
"
:
(
)
=
>
(
TableTransformerForObjectDetection
)
"
TableTransformerModel
"
:
(
)
=
>
(
TableTransformerModel
)
"
TableTransformerObjectDetectionOutput
"
:
(
)
=
>
(
TableTransformerObjectDetectionOutput
)
"
TableTransformerPreTrainedModel
"
:
(
)
=
>
(
TableTransformerPreTrainedModel
)
"
TokenClassifierOutput
"
:
(
)
=
>
(
TokenClassifierOutput
)
"
TrOCRForCausalLM
"
:
(
)
=
>
(
TrOCRForCausalLM
)
"
TrOCRPreTrainedModel
"
:
(
)
=
>
(
TrOCRPreTrainedModel
)
"
UniSpeechForCTC
"
:
(
)
=
>
(
UniSpeechForCTC
)
"
UniSpeechForSequenceClassification
"
:
(
)
=
>
(
UniSpeechForSequenceClassification
)
"
UniSpeechModel
"
:
(
)
=
>
(
UniSpeechModel
)
"
UniSpeechPreTrainedModel
"
:
(
)
=
>
(
UniSpeechPreTrainedModel
)
"
UniSpeechSatForAudioFrameClassification
"
:
(
)
=
>
(
UniSpeechSatForAudioFrameClassification
)
"
UniSpeechSatForCTC
"
:
(
)
=
>
(
UniSpeechSatForCTC
)
"
UniSpeechSatForSequenceClassification
"
:
(
)
=
>
(
UniSpeechSatForSequenceClassification
)
"
UniSpeechSatModel
"
:
(
)
=
>
(
UniSpeechSatModel
)
"
UniSpeechSatPreTrainedModel
"
:
(
)
=
>
(
UniSpeechSatPreTrainedModel
)
"
ViTForImageClassification
"
:
(
)
=
>
(
ViTForImageClassification
)
"
ViTModel
"
:
(
)
=
>
(
ViTModel
)
"
ViTPreTrainedModel
"
:
(
)
=
>
(
ViTPreTrainedModel
)
"
VisionEncoderDecoderModel
"
:
(
)
=
>
(
VisionEncoderDecoderModel
)
"
VitMatteForImageMatting
"
:
(
)
=
>
(
VitMatteForImageMatting
)
"
VitMattePreTrainedModel
"
:
(
)
=
>
(
VitMattePreTrainedModel
)
"
VitsModel
"
:
(
)
=
>
(
VitsModel
)
"
VitsModelOutput
"
:
(
)
=
>
(
VitsModelOutput
)
"
VitsPreTrainedModel
"
:
(
)
=
>
(
VitsPreTrainedModel
)
"
Wav2Vec2BertForCTC
"
:
(
)
=
>
(
Wav2Vec2BertForCTC
)
"
Wav2Vec2BertForSequenceClassification
"
:
(
)
=
>
(
Wav2Vec2BertForSequenceClassification
)
"
Wav2Vec2BertModel
"
:
(
)
=
>
(
Wav2Vec2BertModel
)
"
Wav2Vec2BertPreTrainedModel
"
:
(
)
=
>
(
Wav2Vec2BertPreTrainedModel
)
"
Wav2Vec2ForAudioFrameClassification
"
:
(
)
=
>
(
Wav2Vec2ForAudioFrameClassification
)
"
Wav2Vec2ForCTC
"
:
(
)
=
>
(
Wav2Vec2ForCTC
)
"
Wav2Vec2ForSequenceClassification
"
:
(
)
=
>
(
Wav2Vec2ForSequenceClassification
)
"
Wav2Vec2Model
"
:
(
)
=
>
(
Wav2Vec2Model
)
"
Wav2Vec2PreTrainedModel
"
:
(
)
=
>
(
Wav2Vec2PreTrainedModel
)
"
WavLMForAudioFrameClassification
"
:
(
)
=
>
(
WavLMForAudioFrameClassification
)
"
WavLMForCTC
"
:
(
)
=
>
(
WavLMForCTC
)
"
WavLMForSequenceClassification
"
:
(
)
=
>
(
WavLMForSequenceClassification
)
"
WavLMForXVector
"
:
(
)
=
>
(
WavLMForXVector
)
"
WavLMModel
"
:
(
)
=
>
(
WavLMModel
)
"
WavLMPreTrainedModel
"
:
(
)
=
>
(
WavLMPreTrainedModel
)
"
WhisperForConditionalGeneration
"
:
(
)
=
>
(
WhisperForConditionalGeneration
)
"
WhisperModel
"
:
(
)
=
>
(
WhisperModel
)
"
WhisperPreTrainedModel
"
:
(
)
=
>
(
WhisperPreTrainedModel
)
"
XLMForQuestionAnswering
"
:
(
)
=
>
(
XLMForQuestionAnswering
)
"
XLMForSequenceClassification
"
:
(
)
=
>
(
XLMForSequenceClassification
)
"
XLMForTokenClassification
"
:
(
)
=
>
(
XLMForTokenClassification
)
"
XLMModel
"
:
(
)
=
>
(
XLMModel
)
"
XLMPreTrainedModel
"
:
(
)
=
>
(
XLMPreTrainedModel
)
"
XLMRobertaForMaskedLM
"
:
(
)
=
>
(
XLMRobertaForMaskedLM
)
"
XLMRobertaForQuestionAnswering
"
:
(
)
=
>
(
XLMRobertaForQuestionAnswering
)
"
XLMRobertaForSequenceClassification
"
:
(
)
=
>
(
XLMRobertaForSequenceClassification
)
"
XLMRobertaForTokenClassification
"
:
(
)
=
>
(
XLMRobertaForTokenClassification
)
"
XLMRobertaModel
"
:
(
)
=
>
(
XLMRobertaModel
)
"
XLMRobertaPreTrainedModel
"
:
(
)
=
>
(
XLMRobertaPreTrainedModel
)
"
XLMWithLMHeadModel
"
:
(
)
=
>
(
XLMWithLMHeadModel
)
"
XVectorOutput
"
:
(
)
=
>
(
XVectorOutput
)
"
YolosForObjectDetection
"
:
(
)
=
>
(
YolosForObjectDetection
)
"
YolosModel
"
:
(
)
=
>
(
YolosModel
)
"
YolosObjectDetectionOutput
"
:
(
)
=
>
(
YolosObjectDetectionOutput
)
"
YolosPreTrainedModel
"
:
(
)
=
>
(
YolosPreTrainedModel
)
}
)
;
var
_configs_js__WEBPACK_IMPORTED_MODULE_0__
=
__webpack_require__
(
"
.
/
src
/
configs
.
js
"
)
;
var
_utils_core_js__WEBPACK_IMPORTED_MODULE_1__
=
__webpack_require__
(
"
.
/
src
/
utils
/
core
.
js
"
)
;
var
_utils_hub_js__WEBPACK_IMPORTED_MODULE_2__
=
__webpack_require__
(
"
.
/
src
/
utils
/
hub
.
js
"
)
;
var
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
=
__webpack_require__
(
"
.
/
src
/
utils
/
generation
.
js
"
)
;
var
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
=
__webpack_require__
(
"
.
/
src
/
utils
/
tensor
.
js
"
)
;
var
_backends_onnx_js__WEBPACK_IMPORTED_MODULE_5__
=
__webpack_require__
(
"
.
/
src
/
backends
/
onnx
.
js
"
)
;
var
_transformers_js__WEBPACK_IMPORTED_MODULE_6__
=
__webpack_require__
(
"
.
/
src
/
transformers
.
js
"
)
;
const
{
InferenceSession
Tensor
:
ONNXTensor
env
}
=
_backends_onnx_js__WEBPACK_IMPORTED_MODULE_5__
.
ONNX
;
const
MODEL_TYPES
=
{
EncoderOnly
:
0
EncoderDecoder
:
1
Seq2Seq
:
2
Vision2Seq
:
3
DecoderOnly
:
4
MaskGeneration
:
5
}
const
MODEL_TYPE_MAPPING
=
new
Map
(
)
;
const
MODEL_NAME_TO_CLASS_MAPPING
=
new
Map
(
)
;
const
MODEL_CLASS_TO_NAME_MAPPING
=
new
Map
(
)
;
async
function
constructSession
(
pretrained_model_name_or_path
fileName
options
)
{
let
modelFileName
=
onnx
/
{
fileName
}
{
options
.
quantized
?
'
_quantized
'
:
'
'
}
.
onnx
;
let
buffer
=
await
(
0
_utils_hub_js__WEBPACK_IMPORTED_MODULE_2__
.
getModelFile
)
(
pretrained_model_name_or_path
modelFileName
true
options
)
;
try
{
return
await
InferenceSession
.
create
(
buffer
{
executionProviders
:
_backends_onnx_js__WEBPACK_IMPORTED_MODULE_5__
.
executionProviders
}
)
;
}
catch
(
err
)
{
if
(
_backends_onnx_js__WEBPACK_IMPORTED_MODULE_5__
.
executionProviders
.
length
=
=
=
1
&
&
_backends_onnx_js__WEBPACK_IMPORTED_MODULE_5__
.
executionProviders
[
0
]
=
=
=
'
wasm
'
)
{
throw
err
;
}
console
.
warn
(
err
)
;
console
.
warn
(
'
Something
went
wrong
during
model
construction
(
most
likely
a
missing
operation
)
.
'
+
'
Using
wasm
as
a
fallback
.
'
)
return
await
InferenceSession
.
create
(
buffer
{
executionProviders
:
[
'
wasm
'
]
}
)
;
}
}
function
validateInputs
(
session
inputs
)
{
const
checkedInputs
=
Object
.
create
(
null
)
;
const
missingInputs
=
[
]
;
for
(
const
inputName
of
session
.
inputNames
)
{
const
tensor
=
inputs
[
inputName
]
;
if
(
!
(
tensor
instanceof
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
)
)
{
missingInputs
.
push
(
inputName
)
;
continue
;
}
checkedInputs
[
inputName
]
=
env
.
wasm
.
proxy
?
tensor
.
clone
(
)
:
tensor
;
}
if
(
missingInputs
.
length
>
0
)
{
throw
new
Error
(
An
error
occurred
during
model
execution
:
"
Missing
the
following
inputs
:
{
missingInputs
.
join
(
'
'
)
}
.
)
;
}
const
numInputsProvided
=
Object
.
keys
(
inputs
)
.
length
;
const
numInputsNeeded
=
session
.
inputNames
.
length
;
if
(
numInputsProvided
>
numInputsNeeded
)
{
let
ignored
=
Object
.
keys
(
inputs
)
.
filter
(
inputName
=
>
!
session
.
inputNames
.
includes
(
inputName
)
)
;
console
.
warn
(
WARNING
:
Too
many
inputs
were
provided
(
{
numInputsProvided
}
>
{
numInputsNeeded
}
)
.
The
following
inputs
will
be
ignored
:
"
{
ignored
.
join
(
'
'
)
}
"
.
)
;
}
return
checkedInputs
;
}
async
function
sessionRun
(
session
inputs
)
{
const
checkedInputs
=
validateInputs
(
session
inputs
)
;
try
{
let
output
=
await
session
.
run
(
checkedInputs
)
;
output
=
replaceTensors
(
output
)
;
return
output
;
}
catch
(
e
)
{
console
.
error
(
An
error
occurred
during
model
execution
:
"
{
e
}
"
.
)
;
console
.
error
(
'
Inputs
given
to
model
:
'
checkedInputs
)
;
throw
e
;
}
}
function
replaceTensors
(
obj
)
{
for
(
let
prop
in
obj
)
{
if
(
obj
[
prop
]
instanceof
ONNXTensor
)
{
obj
[
prop
]
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
obj
[
prop
]
)
;
}
else
if
(
typeof
obj
[
prop
]
=
=
=
'
object
'
)
{
replaceTensors
(
obj
[
prop
]
)
;
}
}
return
obj
;
}
function
toI64Tensor
(
items
)
{
if
(
items
instanceof
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
)
{
return
items
;
}
if
(
items
.
length
=
=
=
0
)
{
throw
Error
(
"
items
must
be
non
-
empty
"
)
;
}
if
(
Array
.
isArray
(
items
[
0
]
)
)
{
if
(
items
.
some
(
x
=
>
x
.
length
!
=
=
items
[
0
]
.
length
)
)
{
throw
Error
(
"
Unable
to
create
tensor
you
should
probably
activate
truncation
and
/
or
padding
with
'
padding
=
True
'
and
/
or
'
truncation
=
True
'
to
have
batched
tensors
with
the
same
length
.
"
)
}
return
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
int64
'
BigInt64Array
.
from
(
items
.
flat
(
)
.
map
(
x
=
>
BigInt
(
x
)
)
)
[
items
.
length
items
[
0
]
.
length
]
)
;
}
else
{
return
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
int64
'
BigInt64Array
.
from
(
items
.
map
(
x
=
>
BigInt
(
x
)
)
)
[
1
items
.
length
]
)
;
}
}
function
prepareAttentionMask
(
self
tokens
)
{
let
pad_token_id
=
self
.
config
.
pad_token_id
?
?
null
;
let
eos_token_id
=
self
.
config
.
eos_token_id
?
?
null
;
if
(
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_1__
.
isIntegralNumber
)
(
eos_token_id
)
)
{
eos_token_id
=
[
eos_token_id
]
;
}
let
is_pad_token_in_inputs
=
tokens
.
indexOf
(
pad_token_id
)
!
=
=
-
1
;
let
is_pad_token_not_equal_to_eos_token_id
=
(
eos_token_id
=
=
=
null
)
|
|
!
eos_token_id
.
includes
(
pad_token_id
)
if
(
is_pad_token_in_inputs
&
&
is_pad_token_not_equal_to_eos_token_id
)
{
let
data
=
BigInt64Array
.
from
(
tokens
.
data
.
map
(
x
=
>
x
!
=
pad_token_id
)
)
return
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
int64
'
data
tokens
.
dims
)
}
else
{
return
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
ones_like
)
(
tokens
)
;
}
}
function
preparePositionIds
(
session
feeds
use_cache_branch
)
{
if
(
!
session
.
inputNames
.
includes
(
'
position_ids
'
)
)
return
;
const
data
=
new
BigInt64Array
(
feeds
.
attention_mask
.
data
.
length
)
;
for
(
let
i
=
0
;
i
<
feeds
.
attention_mask
.
dims
[
0
]
;
+
+
i
)
{
let
start
=
i
*
feeds
.
attention_mask
.
dims
[
1
]
;
let
sum
=
BigInt
(
0
)
;
for
(
let
j
=
0
;
j
<
feeds
.
attention_mask
.
dims
[
1
]
;
+
+
j
)
{
const
index
=
start
+
j
;
if
(
feeds
.
attention_mask
.
data
[
index
]
=
=
=
0n
)
{
data
[
index
]
=
BigInt
(
1
)
;
}
else
{
data
[
index
]
=
sum
;
sum
+
=
feeds
.
attention_mask
.
data
[
index
]
;
}
}
}
feeds
.
position_ids
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
int64
'
data
feeds
.
attention_mask
.
dims
)
;
if
(
use_cache_branch
)
{
feeds
.
position_ids
=
feeds
.
position_ids
.
slice
(
null
-
1
)
.
unsqueeze_
(
-
1
)
;
}
}
function
boolTensor
(
value
)
{
return
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
bool
'
[
value
]
[
1
]
)
;
}
async
function
seq2seqForward
(
self
model_inputs
)
{
let
{
encoder_outputs
past_key_values
}
=
model_inputs
;
if
(
!
encoder_outputs
)
{
encoder_outputs
=
(
await
encoderForward
(
self
model_inputs
)
)
.
last_hidden_state
;
}
let
decoderFeeds
=
{
input_ids
:
model_inputs
.
decoder_input_ids
encoder_hidden_states
:
encoder_outputs
}
;
const
use_cache_branch
=
!
!
past_key_values
;
if
(
self
.
decoder_merged_session
.
inputNames
.
includes
(
'
use_cache_branch
'
)
)
{
decoderFeeds
.
use_cache_branch
=
boolTensor
(
use_cache_branch
)
;
}
if
(
self
.
decoder_merged_session
.
inputNames
.
includes
(
'
encoder_attention_mask
'
)
)
{
decoderFeeds
.
encoder_attention_mask
=
model_inputs
.
attention_mask
}
preparePositionIds
(
self
.
decoder_merged_session
decoderFeeds
use_cache_branch
)
;
self
.
addPastKeyValues
(
decoderFeeds
past_key_values
)
;
const
decoderResults
=
await
sessionRun
(
self
.
decoder_merged_session
decoderFeeds
)
;
let
logits
=
decoderResults
.
logits
;
past_key_values
=
self
.
getPastKeyValues
(
decoderResults
past_key_values
)
;
const
attns
=
self
.
getAttentions
(
decoderResults
)
;
return
new
Seq2SeqLMOutput
(
{
logits
past_key_values
encoder_outputs
.
.
.
attns
}
)
;
}
function
seq2seqStartBeams
(
self
inputTokenIds
generation_config
numOutputTokens
)
{
let
beams
=
[
]
;
let
beamId
=
0
;
const
requires_attention_mask
=
self
.
requires_attention_mask
?
?
true
;
let
decoder_input_ids
=
generation_config
.
decoder_input_ids
?
?
generation_config
.
decoder_start_token_id
?
?
generation_config
.
bos_token_id
?
?
generation_config
.
eos_token_id
;
if
(
decoder_input_ids
instanceof
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
)
{
decoder_input_ids
=
decoder_input_ids
.
tolist
(
)
.
flat
(
)
;
}
else
if
(
!
Array
.
isArray
(
decoder_input_ids
)
)
{
decoder_input_ids
=
[
decoder_input_ids
]
;
}
for
(
let
tokens
of
inputTokenIds
)
{
tokens
.
dims
=
[
1
.
.
.
tokens
.
dims
]
let
start
=
{
inputs
:
tokens
encoder_outputs
:
null
prev_model_outputs
:
null
output_token_ids
:
decoder_input_ids
done
:
false
score
:
0
id
:
beamId
+
+
}
if
(
requires_attention_mask
)
{
start
.
attention_mask
=
prepareAttentionMask
(
self
tokens
)
;
}
beams
.
push
(
start
)
;
}
return
beams
;
}
async
function
seq2seqRunBeam
(
self
beam
)
{
const
input_name
=
self
.
main_input_name
;
let
decoder_input_ids
=
beam
.
output_token_ids
;
if
(
beam
.
prev_model_outputs
)
{
decoder_input_ids
=
decoder_input_ids
.
slice
(
-
1
)
;
}
let
model_inputs
=
{
[
input_name
]
:
beam
.
inputs
decoder_input_ids
:
toI64Tensor
(
decoder_input_ids
)
encoder_outputs
:
beam
.
encoder_outputs
past_key_values
:
beam
.
prev_model_outputs
?
.
past_key_values
}
if
(
beam
.
attention_mask
)
{
model_inputs
.
attention_mask
=
beam
.
attention_mask
}
let
output
=
await
self
.
forward
(
model_inputs
)
;
beam
.
prev_model_outputs
=
output
;
beam
.
encoder_outputs
=
output
.
encoder_outputs
;
return
output
;
}
function
seq2seqUpdatebeam
(
beam
newTokenId
)
{
beam
.
output_token_ids
=
[
.
.
.
beam
.
output_token_ids
newTokenId
]
;
}
async
function
encoderForward
(
self
model_inputs
)
{
const
encoderFeeds
=
Object
.
create
(
null
)
;
for
(
const
key
of
self
.
session
.
inputNames
)
{
encoderFeeds
[
key
]
=
model_inputs
[
key
]
;
}
if
(
self
.
session
.
inputNames
.
includes
(
'
token_type_ids
'
)
&
&
!
encoderFeeds
.
token_type_ids
)
{
encoderFeeds
.
token_type_ids
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
int64
'
new
BigInt64Array
(
encoderFeeds
.
input_ids
.
data
.
length
)
encoderFeeds
.
input_ids
.
dims
)
}
return
await
sessionRun
(
self
.
session
encoderFeeds
)
;
}
async
function
decoderForward
(
self
model_inputs
)
{
let
{
input_ids
past_key_values
attention_mask
}
=
model_inputs
;
let
decoderFeeds
=
{
input_ids
:
input_ids
attention_mask
:
attention_mask
?
?
prepareAttentionMask
(
self
input_ids
)
}
const
use_cache_branch
=
!
!
past_key_values
;
if
(
self
.
session
.
inputNames
.
includes
(
'
use_cache_branch
'
)
)
{
decoderFeeds
.
use_cache_branch
=
boolTensor
(
use_cache_branch
)
;
}
preparePositionIds
(
self
.
session
decoderFeeds
use_cache_branch
)
;
self
.
addPastKeyValues
(
decoderFeeds
past_key_values
)
;
let
decoderResults
=
await
sessionRun
(
self
.
session
decoderFeeds
)
;
let
logits
=
decoderResults
.
logits
;
past_key_values
=
self
.
getPastKeyValues
(
decoderResults
past_key_values
)
;
return
{
logits
past_key_values
}
;
}
function
decoderStartBeams
(
self
inputTokenIds
generation_config
numOutputTokens
inputs_attention_mask
)
{
let
beams
=
[
]
;
let
beamId
=
0
;
for
(
let
tokens
of
inputTokenIds
)
{
let
output_token_ids
=
tokens
.
tolist
(
)
.
map
(
Number
)
;
tokens
.
dims
=
[
1
.
.
.
tokens
.
dims
]
let
attn_mask
;
if
(
inputs_attention_mask
)
{
attn_mask
=
inputs_attention_mask
[
beamId
]
;
attn_mask
.
dims
=
[
1
.
.
.
attn_mask
.
dims
]
}
else
{
attn_mask
=
prepareAttentionMask
(
self
tokens
)
}
let
start
=
{
input
:
tokens
model_input_ids
:
tokens
attention_mask
:
attn_mask
prev_model_outputs
:
null
output_token_ids
:
output_token_ids
num_output_tokens
:
numOutputTokens
done
:
false
score
:
0
id
:
beamId
+
+
}
beams
.
push
(
start
)
;
}
return
beams
;
}
async
function
decoderRunBeam
(
self
beam
)
{
let
attnMaskData
=
new
BigInt64Array
(
beam
.
output_token_ids
.
length
)
.
fill
(
1n
)
let
model_inputs
=
{
input_ids
:
beam
.
model_input_ids
attention_mask
:
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
int64
'
attnMaskData
[
1
attnMaskData
.
length
]
)
past_key_values
:
beam
.
prev_model_outputs
?
.
past_key_values
}
let
output
=
await
self
.
forward
(
model_inputs
)
;
beam
.
prev_model_outputs
=
output
;
return
output
;
}
function
decoderUpdatebeam
(
beam
newTokenId
)
{
beam
.
output_token_ids
=
[
.
.
.
beam
.
output_token_ids
newTokenId
]
;
beam
.
model_input_ids
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
int64
'
[
BigInt
(
newTokenId
)
]
[
1
1
]
)
;
}
class
PreTrainedModel
extends
_utils_core_js__WEBPACK_IMPORTED_MODULE_1__
.
Callable
{
main_input_name
=
'
input_ids
'
;
constructor
(
config
session
)
{
super
(
)
;
this
.
config
=
config
;
this
.
session
=
session
;
const
modelName
=
MODEL_CLASS_TO_NAME_MAPPING
.
get
(
this
.
constructor
)
;
const
modelType
=
MODEL_TYPE_MAPPING
.
get
(
modelName
)
;
this
.
can_generate
=
false
;
this
.
_runBeam
=
null
;
this
.
_getStartBeams
=
null
;
this
.
_updateBeam
=
null
;
this
.
_forward
=
null
;
if
(
modelType
=
=
=
MODEL_TYPES
.
DecoderOnly
)
{
this
.
can_generate
=
true
;
this
.
_runBeam
=
decoderRunBeam
;
this
.
_getStartBeams
=
decoderStartBeams
;
this
.
_updateBeam
=
decoderUpdatebeam
;
this
.
_forward
=
decoderForward
;
}
else
if
(
modelType
=
=
=
MODEL_TYPES
.
Seq2Seq
|
|
modelType
=
=
=
MODEL_TYPES
.
Vision2Seq
)
{
this
.
can_generate
=
true
;
this
.
_runBeam
=
seq2seqRunBeam
;
this
.
_getStartBeams
=
seq2seqStartBeams
;
this
.
_updateBeam
=
seq2seqUpdatebeam
;
this
.
_forward
=
seq2seqForward
;
}
else
if
(
modelType
=
=
=
MODEL_TYPES
.
EncoderDecoder
)
{
this
.
_forward
=
encoderForward
;
}
else
{
this
.
_forward
=
encoderForward
;
}
}
async
dispose
(
)
{
const
promises
=
[
]
;
for
(
let
key
of
Object
.
keys
(
this
)
)
{
const
item
=
this
[
key
]
;
if
(
item
instanceof
InferenceSession
)
{
promises
.
push
(
item
.
handler
.
dispose
(
)
)
}
}
return
await
Promise
.
all
(
promises
)
;
}
static
async
from_pretrained
(
pretrained_model_name_or_path
{
quantized
=
true
progress_callback
=
null
config
=
null
cache_dir
=
null
local_files_only
=
false
revision
=
'
main
'
model_file_name
=
null
}
=
{
}
)
{
let
options
=
{
quantized
progress_callback
config
cache_dir
local_files_only
revision
model_file_name
}
const
modelName
=
MODEL_CLASS_TO_NAME_MAPPING
.
get
(
this
)
;
const
modelType
=
MODEL_TYPE_MAPPING
.
get
(
modelName
)
;
let
info
;
if
(
modelType
=
=
=
MODEL_TYPES
.
DecoderOnly
)
{
info
=
await
Promise
.
all
(
[
_configs_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoConfig
.
from_pretrained
(
pretrained_model_name_or_path
options
)
constructSession
(
pretrained_model_name_or_path
options
.
model_file_name
?
?
'
decoder_model_merged
'
options
)
(
0
_utils_hub_js__WEBPACK_IMPORTED_MODULE_2__
.
getModelJSON
)
(
pretrained_model_name_or_path
'
generation_config
.
json
'
false
options
)
]
)
;
}
else
if
(
modelType
=
=
=
MODEL_TYPES
.
Seq2Seq
|
|
modelType
=
=
=
MODEL_TYPES
.
Vision2Seq
)
{
info
=
await
Promise
.
all
(
[
_configs_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoConfig
.
from_pretrained
(
pretrained_model_name_or_path
options
)
constructSession
(
pretrained_model_name_or_path
'
encoder_model
'
options
)
constructSession
(
pretrained_model_name_or_path
'
decoder_model_merged
'
options
)
(
0
_utils_hub_js__WEBPACK_IMPORTED_MODULE_2__
.
getModelJSON
)
(
pretrained_model_name_or_path
'
generation_config
.
json
'
false
options
)
]
)
;
}
else
if
(
modelType
=
=
=
MODEL_TYPES
.
MaskGeneration
)
{
info
=
await
Promise
.
all
(
[
_configs_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoConfig
.
from_pretrained
(
pretrained_model_name_or_path
options
)
constructSession
(
pretrained_model_name_or_path
'
vision_encoder
'
options
)
constructSession
(
pretrained_model_name_or_path
'
prompt_encoder_mask_decoder
'
options
)
]
)
;
}
else
if
(
modelType
=
=
=
MODEL_TYPES
.
EncoderDecoder
)
{
info
=
await
Promise
.
all
(
[
_configs_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoConfig
.
from_pretrained
(
pretrained_model_name_or_path
options
)
constructSession
(
pretrained_model_name_or_path
'
encoder_model
'
options
)
constructSession
(
pretrained_model_name_or_path
'
decoder_model_merged
'
options
)
]
)
;
}
else
{
if
(
modelType
!
=
=
MODEL_TYPES
.
EncoderOnly
)
{
console
.
warn
(
Model
type
for
'
{
modelName
?
?
config
?
.
model_type
}
'
not
found
assuming
encoder
-
only
architecture
.
Please
report
this
at
https
:
/
/
github
.
com
/
xenova
/
transformers
.
js
/
issues
/
new
/
choose
.
)
}
info
=
await
Promise
.
all
(
[
_configs_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoConfig
.
from_pretrained
(
pretrained_model_name_or_path
options
)
constructSession
(
pretrained_model_name_or_path
options
.
model_file_name
?
?
'
model
'
options
)
]
)
;
}
return
new
this
(
.
.
.
info
)
;
}
async
_call
(
model_inputs
)
{
return
await
this
.
forward
(
model_inputs
)
;
}
async
forward
(
model_inputs
)
{
return
await
this
.
_forward
(
this
model_inputs
)
;
}
_get_logits_processor
(
generation_config
input_ids_seq_length
logits_processor
=
null
)
{
const
processors
=
new
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
.
LogitsProcessorList
(
)
;
if
(
generation_config
.
repetition_penalty
!
=
=
null
&
&
generation_config
.
repetition_penalty
!
=
=
1
.
0
)
{
processors
.
push
(
new
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
.
RepetitionPenaltyLogitsProcessor
(
generation_config
.
repetition_penalty
)
)
;
}
if
(
generation_config
.
no_repeat_ngram_size
!
=
=
null
&
&
generation_config
.
no_repeat_ngram_size
>
0
)
{
processors
.
push
(
new
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
.
NoRepeatNGramLogitsProcessor
(
generation_config
.
no_repeat_ngram_size
)
)
;
}
if
(
generation_config
.
bad_words_ids
!
=
=
null
)
{
processors
.
push
(
new
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
.
NoBadWordsLogitsProcessor
(
generation_config
.
bad_words_ids
generation_config
.
eos_token_id
)
)
;
}
if
(
generation_config
.
min_length
!
=
=
null
&
&
generation_config
.
eos_token_id
!
=
=
null
&
&
generation_config
.
min_length
>
0
)
{
processors
.
push
(
new
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
.
MinLengthLogitsProcessor
(
generation_config
.
min_length
generation_config
.
eos_token_id
)
)
;
}
if
(
generation_config
.
min_new_tokens
!
=
=
null
&
&
generation_config
.
eos_token_id
!
=
=
null
&
&
generation_config
.
min_new_tokens
>
0
)
{
processors
.
push
(
new
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
.
MinNewTokensLengthLogitsProcessor
(
input_ids_seq_length
generation_config
.
min_new_tokens
generation_config
.
eos_token_id
)
)
;
}
if
(
generation_config
.
forced_bos_token_id
!
=
=
null
)
{
processors
.
push
(
new
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
.
ForcedBOSTokenLogitsProcessor
(
generation_config
.
forced_bos_token_id
)
)
;
}
if
(
generation_config
.
forced_eos_token_id
!
=
=
null
)
{
processors
.
push
(
new
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
.
ForcedEOSTokenLogitsProcessor
(
generation_config
.
max_length
generation_config
.
forced_eos_token_id
)
)
;
}
if
(
generation_config
.
begin_suppress_tokens
!
=
=
null
)
{
let
begin_index
=
(
input_ids_seq_length
>
1
|
|
generation_config
.
forced_bos_token_id
=
=
=
null
)
?
input_ids_seq_length
:
input_ids_seq_length
+
1
;
if
(
generation_config
.
forced_decoder_ids
!
=
=
null
)
{
begin_index
+
=
generation_config
.
forced_decoder_ids
[
generation_config
.
forced_decoder_ids
.
length
-
1
]
[
0
]
;
}
processors
.
push
(
new
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
.
SuppressTokensAtBeginLogitsProcessor
(
generation_config
.
begin_suppress_tokens
begin_index
)
)
;
}
if
(
generation_config
.
forced_decoder_ids
!
=
=
null
)
{
processors
.
push
(
new
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
.
ForceTokensLogitsProcessor
(
generation_config
.
forced_decoder_ids
)
)
;
}
if
(
logits_processor
!
=
=
null
)
{
processors
.
extend
(
logits_processor
)
}
return
processors
;
}
_get_generation_config
(
generation_config
)
{
let
gen_config
=
new
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
.
GenerationConfig
(
this
.
config
)
;
if
(
'
generation_config
'
in
this
)
{
Object
.
assign
(
gen_config
this
.
generation_config
)
;
}
if
(
generation_config
!
=
=
null
)
{
Object
.
assign
(
gen_config
generation_config
)
;
}
return
gen_config
;
}
async
generate
(
inputs
generation_config
=
null
logits_processor
=
null
{
inputs_attention_mask
=
null
}
=
{
}
)
{
if
(
!
this
.
can_generate
)
{
const
modelName
=
MODEL_CLASS_TO_NAME_MAPPING
.
get
(
this
.
constructor
)
;
let
errorMessage
=
The
current
model
class
(
{
modelName
}
)
is
not
compatible
with
\
.
generate
(
)
\
as
it
doesn
'
t
have
a
language
model
head
.
const
modelType
=
this
.
config
.
model_type
;
const
possibleInfo
=
MODEL_WITH_LM_HEAD_MAPPING_NAMES
.
get
(
modelType
)
?
?
MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES
.
get
(
modelType
)
?
?
MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES
.
get
(
modelType
)
?
?
MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES
.
get
(
modelType
)
;
if
(
possibleInfo
)
{
errorMessage
+
=
Please
use
the
following
class
instead
:
'
{
possibleInfo
[
0
]
}
'
;
}
throw
Error
(
errorMessage
)
;
}
if
(
!
(
inputs
instanceof
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
)
&
&
!
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_1__
.
isTypedArray
)
(
inputs
)
&
&
!
Array
.
isArray
(
inputs
)
)
{
throw
Error
(
\
inputs
\
must
be
a
Tensor
TypedArray
or
Array
but
is
"
{
inputs
.
constructor
.
name
}
"
.
)
;
}
let
input_ids_seq_length
;
if
(
this
.
config
.
is_encoder_decoder
)
{
input_ids_seq_length
=
0
;
}
else
{
input_ids_seq_length
=
inputs
instanceof
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
?
inputs
.
dims
.
at
(
-
1
)
:
inputs
.
length
;
if
(
input_ids_seq_length
=
=
=
0
)
{
throw
Error
(
"
Must
supply
a
non
-
empty
array
of
input
token
ids
.
"
)
}
}
generation_config
=
this
.
_get_generation_config
(
generation_config
)
;
logits_processor
=
logits_processor
?
?
new
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
.
LogitsProcessorList
(
)
logits_processor
=
this
.
_get_logits_processor
(
generation_config
input_ids_seq_length
logits_processor
)
let
eos_token_ids
=
generation_config
.
eos_token_id
;
if
(
eos_token_ids
!
=
=
null
&
&
!
Array
.
isArray
(
eos_token_ids
)
)
{
eos_token_ids
=
[
eos_token_ids
]
;
}
let
numOutputTokens
=
1
;
const
maxOutputTokens
=
numOutputTokens
+
(
generation_config
.
max_new_tokens
?
?
Infinity
)
;
const
useMaxLength
=
Number
.
isInteger
(
generation_config
.
max_length
)
&
&
(
generation_config
.
max_new_tokens
?
?
null
)
=
=
=
null
;
let
sampler
=
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
.
Sampler
.
getSampler
(
generation_config
)
;
let
beams
=
this
.
getStartBeams
(
inputs
generation_config
numOutputTokens
inputs_attention_mask
)
;
while
(
beams
.
some
(
x
=
>
!
x
.
done
)
&
&
numOutputTokens
<
maxOutputTokens
)
{
let
newest_beams
=
[
]
;
for
(
let
beam
of
beams
)
{
if
(
beam
.
done
)
{
newest_beams
.
push
(
beam
)
;
continue
}
if
(
useMaxLength
&
&
beam
.
output_token_ids
.
length
>
=
generation_config
.
max_length
)
{
beam
.
done
=
true
;
newest_beams
.
push
(
beam
)
;
continue
}
let
output
=
await
this
.
runBeam
(
beam
)
;
if
(
generation_config
.
output_attentions
)
{
this
.
addAttentionsToBeam
(
beam
output
)
;
}
if
(
generation_config
.
output_scores
)
{
}
let
logits
=
output
.
logits
.
slice
(
null
-
1
null
)
;
logits_processor
(
beam
.
output_token_ids
logits
)
;
let
sampledTokens
=
sampler
(
logits
)
;
for
(
let
[
newTokenId
logProb
]
of
sampledTokens
)
{
let
newBeam
=
{
.
.
.
beam
}
;
this
.
updateBeam
(
newBeam
newTokenId
)
;
newBeam
.
score
+
=
logProb
;
if
(
eos_token_ids
&
&
eos_token_ids
.
includes
(
newTokenId
)
)
{
newBeam
.
done
=
true
;
}
newest_beams
.
push
(
newBeam
)
;
}
}
+
+
numOutputTokens
;
newest_beams
=
this
.
groupBeams
(
newest_beams
)
.
map
(
group
=
>
group
.
sort
(
(
a
b
)
=
>
b
.
score
-
a
.
score
)
.
slice
(
0
generation_config
.
num_beams
)
)
;
beams
=
newest_beams
.
flat
(
)
;
if
(
generation_config
.
callback_function
)
{
generation_config
.
callback_function
(
beams
)
;
}
}
const
groupedBeams
=
this
.
groupBeams
(
beams
)
;
const
getFlattened
=
(
key
)
=
>
groupedBeams
.
map
(
batch
=
>
{
if
(
generation_config
.
num_return_sequences
>
1
)
{
return
batch
.
slice
(
0
generation_config
.
num_return_sequences
)
.
map
(
x
=
>
x
[
key
]
)
;
}
else
{
return
[
batch
[
0
]
[
key
]
]
;
}
}
)
.
flat
(
)
;
const
sequences
=
getFlattened
(
'
output_token_ids
'
)
;
if
(
generation_config
.
return_dict_in_generate
)
{
const
decoder_attentions
=
getFlattened
(
'
decoder_attentions
'
)
;
const
cross_attentions
=
getFlattened
(
'
cross_attentions
'
)
;
return
{
sequences
decoder_attentions
cross_attentions
}
}
else
{
return
sequences
;
}
}
addAttentionsToBeam
(
beam
output
)
{
if
(
this
.
config
.
is_encoder_decoder
)
{
if
(
!
output
.
cross_attentions
|
|
output
.
cross_attentions
.
length
=
=
=
0
)
{
throw
Error
(
"
output_attentions
is
true
but
the
model
did
not
produce
cross
-
attentions
.
"
+
"
This
is
most
likely
because
the
model
was
not
exported
with
output_attentions
=
True
.
"
)
}
if
(
!
beam
.
cross_attentions
)
{
beam
.
cross_attentions
=
[
]
;
}
beam
.
cross_attentions
.
push
(
output
.
cross_attentions
)
;
}
if
(
!
output
.
decoder_attentions
|
|
output
.
decoder_attentions
.
length
=
=
=
0
)
{
throw
Error
(
"
output_attentions
is
true
but
the
model
did
not
produce
decoder
-
attentions
.
"
+
"
This
is
most
likely
because
the
model
was
not
exported
with
output_attentions
=
True
.
"
)
}
if
(
!
beam
.
decoder_attentions
)
{
beam
.
decoder_attentions
=
[
]
;
}
beam
.
decoder_attentions
.
push
(
output
.
decoder_attentions
)
;
}
groupBeams
(
beams
)
{
const
groups
=
Object
.
create
(
null
)
;
for
(
const
obj
of
beams
)
{
if
(
groups
[
obj
.
id
]
=
=
=
undefined
)
{
groups
[
obj
.
id
]
=
[
obj
]
;
}
else
{
groups
[
obj
.
id
]
.
push
(
obj
)
;
}
}
return
Object
.
values
(
groups
)
;
}
getPastKeyValues
(
decoderResults
pastKeyValues
)
{
const
pkvs
=
Object
.
create
(
null
)
;
for
(
const
name
in
decoderResults
)
{
if
(
name
.
startsWith
(
'
present
'
)
)
{
let
newName
=
name
.
replace
(
'
present
'
'
past_key_values
'
)
;
if
(
pastKeyValues
&
&
name
.
includes
(
'
encoder
'
)
)
{
pkvs
[
newName
]
=
pastKeyValues
[
newName
]
;
}
else
{
pkvs
[
newName
]
=
decoderResults
[
name
]
;
}
}
}
return
pkvs
;
}
getAttentions
(
decoderResults
)
{
const
attns
=
Object
.
create
(
null
)
;
for
(
const
attnName
of
[
'
cross_attentions
'
'
decoder_attentions
'
]
)
{
const
result
=
[
]
;
for
(
const
name
in
decoderResults
)
{
if
(
name
.
startsWith
(
attnName
)
)
{
const
index
=
name
.
split
(
'
.
'
)
.
pop
(
)
result
[
index
]
=
decoderResults
[
name
]
;
}
}
attns
[
attnName
]
=
result
;
}
return
attns
;
}
addPastKeyValues
(
decoderFeeds
pastKeyValues
)
{
if
(
pastKeyValues
)
{
Object
.
assign
(
decoderFeeds
pastKeyValues
)
}
else
{
const
batch_size
=
1
;
if
(
this
.
config
.
is_encoder_decoder
&
&
(
this
.
add_encoder_pkv
?
?
true
)
)
{
let
encoder_dims
=
[
batch_size
this
.
num_encoder_heads
0
this
.
encoder_dim_kv
]
;
let
decoder_dims
=
[
batch_size
this
.
num_decoder_heads
0
this
.
decoder_dim_kv
]
;
for
(
let
i
=
0
;
i
<
this
.
num_decoder_layers
;
+
+
i
)
{
decoderFeeds
[
past_key_values
.
{
i
}
.
encoder
.
key
]
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
float32
'
[
]
encoder_dims
)
decoderFeeds
[
past_key_values
.
{
i
}
.
encoder
.
value
]
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
float32
'
[
]
encoder_dims
)
decoderFeeds
[
past_key_values
.
{
i
}
.
decoder
.
key
]
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
float32
'
[
]
decoder_dims
)
decoderFeeds
[
past_key_values
.
{
i
}
.
decoder
.
value
]
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
float32
'
[
]
decoder_dims
)
}
}
else
if
(
this
.
config
.
model_type
=
=
=
'
falcon
'
)
{
let
dims
=
[
batch_size
*
this
.
num_heads
0
this
.
dim_kv
]
for
(
let
i
=
0
;
i
<
this
.
num_layers
;
+
+
i
)
{
decoderFeeds
[
past_key_values
.
{
i
}
.
key
]
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
float32
'
[
]
dims
)
decoderFeeds
[
past_key_values
.
{
i
}
.
value
]
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
float32
'
[
]
dims
)
}
}
else
if
(
this
.
config
.
multi_query
)
{
let
dims
=
[
batch_size
*
this
.
num_heads
0
2
*
this
.
dim_kv
]
for
(
let
i
=
0
;
i
<
this
.
num_layers
;
+
+
i
)
{
decoderFeeds
[
past_key_values
.
{
i
}
.
key_value
]
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
float32
'
[
]
dims
)
}
}
else
if
(
this
.
config
.
model_type
=
=
=
'
bloom
'
)
{
let
keyDims
=
[
batch_size
*
this
.
num_heads
this
.
dim_kv
0
]
let
valueDims
=
[
batch_size
*
this
.
num_heads
0
this
.
dim_kv
]
for
(
let
i
=
0
;
i
<
this
.
num_layers
;
+
+
i
)
{
decoderFeeds
[
past_key_values
.
{
i
}
.
key
]
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
float32
'
[
]
keyDims
)
decoderFeeds
[
past_key_values
.
{
i
}
.
value
]
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
float32
'
[
]
valueDims
)
}
}
else
{
let
dims
=
[
batch_size
this
.
num_heads
0
this
.
dim_kv
]
for
(
let
i
=
0
;
i
<
this
.
num_layers
;
+
+
i
)
{
decoderFeeds
[
past_key_values
.
{
i
}
.
key
]
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
float32
'
[
]
dims
)
decoderFeeds
[
past_key_values
.
{
i
}
.
value
]
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
float32
'
[
]
dims
)
}
}
}
}
getStartBeams
(
inputTokenIds
generation_config
numOutputTokens
inputs_attention_mask
)
{
return
this
.
_getStartBeams
(
this
inputTokenIds
generation_config
numOutputTokens
inputs_attention_mask
)
}
async
runBeam
(
beam
)
{
return
await
this
.
_runBeam
(
this
beam
)
;
}
updateBeam
(
beam
newTokenId
)
{
return
this
.
_updateBeam
(
beam
newTokenId
)
;
}
}
class
ModelOutput
{
}
class
BaseModelOutput
extends
ModelOutput
{
constructor
(
{
last_hidden_state
hidden_states
=
null
attentions
=
null
}
)
{
super
(
)
;
this
.
last_hidden_state
=
last_hidden_state
;
this
.
hidden_states
=
hidden_states
;
this
.
attentions
=
attentions
;
}
}
class
BertPreTrainedModel
extends
PreTrainedModel
{
}
class
BertModel
extends
BertPreTrainedModel
{
}
class
BertForMaskedLM
extends
BertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
BertForSequenceClassification
extends
BertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
BertForTokenClassification
extends
BertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
BertForQuestionAnswering
extends
BertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
NomicBertPreTrainedModel
extends
PreTrainedModel
{
}
class
NomicBertModel
extends
NomicBertPreTrainedModel
{
}
class
RoFormerPreTrainedModel
extends
PreTrainedModel
{
}
class
RoFormerModel
extends
RoFormerPreTrainedModel
{
}
class
RoFormerForMaskedLM
extends
RoFormerPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
RoFormerForSequenceClassification
extends
RoFormerPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
RoFormerForTokenClassification
extends
RoFormerPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
RoFormerForQuestionAnswering
extends
RoFormerPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
ConvBertPreTrainedModel
extends
PreTrainedModel
{
}
class
ConvBertModel
extends
ConvBertPreTrainedModel
{
}
class
ConvBertForMaskedLM
extends
ConvBertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
ConvBertForSequenceClassification
extends
ConvBertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
ConvBertForTokenClassification
extends
ConvBertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
ConvBertForQuestionAnswering
extends
ConvBertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
ElectraPreTrainedModel
extends
PreTrainedModel
{
}
class
ElectraModel
extends
ElectraPreTrainedModel
{
}
class
ElectraForMaskedLM
extends
ElectraPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
ElectraForSequenceClassification
extends
ElectraPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
ElectraForTokenClassification
extends
ElectraPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
ElectraForQuestionAnswering
extends
ElectraPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
CamembertPreTrainedModel
extends
PreTrainedModel
{
}
class
CamembertModel
extends
CamembertPreTrainedModel
{
}
class
CamembertForMaskedLM
extends
CamembertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
CamembertForSequenceClassification
extends
CamembertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
CamembertForTokenClassification
extends
CamembertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
CamembertForQuestionAnswering
extends
CamembertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DebertaPreTrainedModel
extends
PreTrainedModel
{
}
class
DebertaModel
extends
DebertaPreTrainedModel
{
}
class
DebertaForMaskedLM
extends
DebertaPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DebertaForSequenceClassification
extends
DebertaPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DebertaForTokenClassification
extends
DebertaPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DebertaForQuestionAnswering
extends
DebertaPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DebertaV2PreTrainedModel
extends
PreTrainedModel
{
}
class
DebertaV2Model
extends
DebertaV2PreTrainedModel
{
}
class
DebertaV2ForMaskedLM
extends
DebertaV2PreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DebertaV2ForSequenceClassification
extends
DebertaV2PreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DebertaV2ForTokenClassification
extends
DebertaV2PreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DebertaV2ForQuestionAnswering
extends
DebertaV2PreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DistilBertPreTrainedModel
extends
PreTrainedModel
{
}
class
DistilBertModel
extends
DistilBertPreTrainedModel
{
}
class
DistilBertForSequenceClassification
extends
DistilBertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DistilBertForTokenClassification
extends
DistilBertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DistilBertForQuestionAnswering
extends
DistilBertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DistilBertForMaskedLM
extends
DistilBertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
EsmPreTrainedModel
extends
PreTrainedModel
{
}
class
EsmModel
extends
EsmPreTrainedModel
{
}
class
EsmForMaskedLM
extends
EsmPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
EsmForSequenceClassification
extends
EsmPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
EsmForTokenClassification
extends
EsmPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
MobileBertPreTrainedModel
extends
PreTrainedModel
{
}
class
MobileBertModel
extends
MobileBertPreTrainedModel
{
}
class
MobileBertForMaskedLM
extends
MobileBertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
MobileBertForSequenceClassification
extends
MobileBertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
MobileBertForQuestionAnswering
extends
MobileBertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
MPNetPreTrainedModel
extends
PreTrainedModel
{
}
class
MPNetModel
extends
MPNetPreTrainedModel
{
}
class
MPNetForMaskedLM
extends
MPNetPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
MPNetForSequenceClassification
extends
MPNetPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
MPNetForTokenClassification
extends
MPNetPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
MPNetForQuestionAnswering
extends
MPNetPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
SqueezeBertPreTrainedModel
extends
PreTrainedModel
{
}
class
SqueezeBertModel
extends
SqueezeBertPreTrainedModel
{
}
class
SqueezeBertForMaskedLM
extends
SqueezeBertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
SqueezeBertForSequenceClassification
extends
SqueezeBertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
SqueezeBertForQuestionAnswering
extends
SqueezeBertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
AlbertPreTrainedModel
extends
PreTrainedModel
{
}
class
AlbertModel
extends
AlbertPreTrainedModel
{
}
class
AlbertForSequenceClassification
extends
AlbertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
AlbertForQuestionAnswering
extends
AlbertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
AlbertForMaskedLM
extends
AlbertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
T5PreTrainedModel
extends
PreTrainedModel
{
}
;
class
T5Model
extends
T5PreTrainedModel
{
}
class
T5ForConditionalGeneration
extends
T5PreTrainedModel
{
constructor
(
config
session
decoder_merged_session
generation_config
)
{
super
(
config
session
)
;
this
.
decoder_merged_session
=
decoder_merged_session
;
this
.
generation_config
=
generation_config
;
this
.
num_decoder_layers
=
this
.
config
.
num_decoder_layers
;
this
.
num_decoder_heads
=
this
.
config
.
num_heads
;
this
.
decoder_dim_kv
=
this
.
config
.
d_kv
;
this
.
num_encoder_layers
=
this
.
config
.
num_layers
;
this
.
num_encoder_heads
=
this
.
config
.
num_heads
;
this
.
encoder_dim_kv
=
this
.
config
.
d_kv
;
}
}
class
LongT5PreTrainedModel
extends
PreTrainedModel
{
}
;
class
LongT5Model
extends
LongT5PreTrainedModel
{
}
class
LongT5ForConditionalGeneration
extends
LongT5PreTrainedModel
{
constructor
(
config
session
decoder_merged_session
generation_config
)
{
super
(
config
session
)
;
this
.
decoder_merged_session
=
decoder_merged_session
;
this
.
generation_config
=
generation_config
;
this
.
num_decoder_layers
=
this
.
config
.
num_decoder_layers
;
this
.
num_decoder_heads
=
this
.
config
.
num_heads
;
this
.
decoder_dim_kv
=
this
.
config
.
d_kv
;
this
.
num_encoder_layers
=
this
.
config
.
num_layers
;
this
.
num_encoder_heads
=
this
.
config
.
num_heads
;
this
.
encoder_dim_kv
=
this
.
config
.
d_kv
;
}
}
class
MT5PreTrainedModel
extends
PreTrainedModel
{
}
;
class
MT5Model
extends
MT5PreTrainedModel
{
}
class
MT5ForConditionalGeneration
extends
MT5PreTrainedModel
{
constructor
(
config
session
decoder_merged_session
generation_config
)
{
super
(
config
session
)
;
this
.
decoder_merged_session
=
decoder_merged_session
;
this
.
generation_config
=
generation_config
;
this
.
num_decoder_layers
=
this
.
config
.
num_decoder_layers
;
this
.
num_decoder_heads
=
this
.
config
.
num_heads
;
this
.
decoder_dim_kv
=
this
.
config
.
d_kv
;
this
.
num_encoder_layers
=
this
.
config
.
num_layers
;
this
.
num_encoder_heads
=
this
.
config
.
num_heads
;
this
.
encoder_dim_kv
=
this
.
config
.
d_kv
;
}
}
class
BartPretrainedModel
extends
PreTrainedModel
{
}
;
class
BartModel
extends
BartPretrainedModel
{
}
class
BartForConditionalGeneration
extends
BartPretrainedModel
{
constructor
(
config
session
decoder_merged_session
generation_config
)
{
super
(
config
session
)
;
this
.
decoder_merged_session
=
decoder_merged_session
;
this
.
generation_config
=
generation_config
;
this
.
num_decoder_layers
=
this
.
config
.
decoder_layers
;
this
.
num_decoder_heads
=
this
.
config
.
decoder_attention_heads
;
this
.
decoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_decoder_heads
;
this
.
num_encoder_layers
=
this
.
config
.
encoder_layers
;
this
.
num_encoder_heads
=
this
.
config
.
encoder_attention_heads
;
this
.
encoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_encoder_heads
;
}
}
class
BartForSequenceClassification
extends
BartPretrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
MBartPreTrainedModel
extends
PreTrainedModel
{
}
;
class
MBartModel
extends
MBartPreTrainedModel
{
}
class
MBartForConditionalGeneration
extends
MBartPreTrainedModel
{
constructor
(
config
session
decoder_merged_session
generation_config
)
{
super
(
config
session
)
;
this
.
decoder_merged_session
=
decoder_merged_session
;
this
.
generation_config
=
generation_config
;
this
.
num_decoder_layers
=
this
.
config
.
decoder_layers
;
this
.
num_decoder_heads
=
this
.
config
.
decoder_attention_heads
;
this
.
decoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_decoder_heads
;
this
.
num_encoder_layers
=
this
.
config
.
encoder_layers
;
this
.
num_encoder_heads
=
this
.
config
.
encoder_attention_heads
;
this
.
encoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_encoder_heads
;
}
}
class
MBartForSequenceClassification
extends
MBartPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
MBartForCausalLM
extends
MBartPreTrainedModel
{
constructor
(
config
decoder_merged_session
generation_config
)
{
super
(
config
decoder_merged_session
)
;
this
.
generation_config
=
generation_config
;
this
.
num_decoder_layers
=
this
.
config
.
decoder_layers
;
this
.
num_decoder_heads
=
this
.
config
.
decoder_attention_heads
;
this
.
decoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_decoder_heads
;
this
.
num_encoder_layers
=
this
.
config
.
encoder_layers
;
this
.
num_encoder_heads
=
this
.
config
.
encoder_attention_heads
;
this
.
encoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_encoder_heads
;
}
}
class
BlenderbotPreTrainedModel
extends
PreTrainedModel
{
}
;
class
BlenderbotModel
extends
BlenderbotPreTrainedModel
{
}
class
BlenderbotForConditionalGeneration
extends
BlenderbotPreTrainedModel
{
constructor
(
config
session
decoder_merged_session
generation_config
)
{
super
(
config
session
)
;
this
.
decoder_merged_session
=
decoder_merged_session
;
this
.
generation_config
=
generation_config
;
this
.
num_decoder_layers
=
this
.
config
.
decoder_layers
;
this
.
num_decoder_heads
=
this
.
config
.
decoder_attention_heads
;
this
.
decoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_decoder_heads
;
this
.
num_encoder_layers
=
this
.
config
.
encoder_layers
;
this
.
num_encoder_heads
=
this
.
config
.
encoder_attention_heads
;
this
.
encoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_encoder_heads
;
}
}
class
BlenderbotSmallPreTrainedModel
extends
PreTrainedModel
{
}
;
class
BlenderbotSmallModel
extends
BlenderbotSmallPreTrainedModel
{
}
class
BlenderbotSmallForConditionalGeneration
extends
BlenderbotSmallPreTrainedModel
{
constructor
(
config
session
decoder_merged_session
generation_config
)
{
super
(
config
session
)
;
this
.
decoder_merged_session
=
decoder_merged_session
;
this
.
generation_config
=
generation_config
;
this
.
num_decoder_layers
=
this
.
config
.
decoder_layers
;
this
.
num_decoder_heads
=
this
.
config
.
decoder_attention_heads
;
this
.
decoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_decoder_heads
;
this
.
num_encoder_layers
=
this
.
config
.
encoder_layers
;
this
.
num_encoder_heads
=
this
.
config
.
encoder_attention_heads
;
this
.
encoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_encoder_heads
;
}
}
class
RobertaPreTrainedModel
extends
PreTrainedModel
{
}
class
RobertaModel
extends
RobertaPreTrainedModel
{
}
class
RobertaForMaskedLM
extends
RobertaPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
RobertaForSequenceClassification
extends
RobertaPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
RobertaForTokenClassification
extends
RobertaPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
RobertaForQuestionAnswering
extends
RobertaPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
XLMPreTrainedModel
extends
PreTrainedModel
{
}
class
XLMModel
extends
XLMPreTrainedModel
{
}
class
XLMWithLMHeadModel
extends
XLMPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
XLMForSequenceClassification
extends
XLMPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
XLMForTokenClassification
extends
XLMPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
XLMForQuestionAnswering
extends
XLMPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
XLMRobertaPreTrainedModel
extends
PreTrainedModel
{
}
class
XLMRobertaModel
extends
XLMRobertaPreTrainedModel
{
}
class
XLMRobertaForMaskedLM
extends
XLMRobertaPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
MaskedLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
XLMRobertaForSequenceClassification
extends
XLMRobertaPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
XLMRobertaForTokenClassification
extends
XLMRobertaPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
XLMRobertaForQuestionAnswering
extends
XLMRobertaPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
QuestionAnsweringModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
ASTPreTrainedModel
extends
PreTrainedModel
{
}
;
class
ASTModel
extends
ASTPreTrainedModel
{
}
class
ASTForAudioClassification
extends
ASTPreTrainedModel
{
}
class
WhisperPreTrainedModel
extends
PreTrainedModel
{
}
;
class
WhisperModel
extends
WhisperPreTrainedModel
{
}
class
WhisperForConditionalGeneration
extends
WhisperPreTrainedModel
{
requires_attention_mask
=
false
;
main_input_name
=
'
input_features
'
;
constructor
(
config
session
decoder_merged_session
generation_config
)
{
super
(
config
session
)
;
this
.
decoder_merged_session
=
decoder_merged_session
;
this
.
generation_config
=
generation_config
;
this
.
num_decoder_layers
=
this
.
config
.
decoder_layers
;
this
.
num_decoder_heads
=
this
.
config
.
decoder_attention_heads
;
this
.
decoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_decoder_heads
;
this
.
num_encoder_layers
=
this
.
config
.
encoder_layers
;
this
.
num_encoder_heads
=
this
.
config
.
encoder_attention_heads
;
this
.
encoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_encoder_heads
;
}
async
generate
(
inputs
generation_config
=
null
logits_processor
=
null
)
{
generation_config
=
this
.
_get_generation_config
(
generation_config
)
;
generation_config
.
return_timestamps
?
?
=
false
;
if
(
generation_config
.
return_timestamps
)
{
logits_processor
=
[
new
_utils_generation_js__WEBPACK_IMPORTED_MODULE_3__
.
WhisperTimeStampLogitsProcessor
(
generation_config
)
]
}
if
(
generation_config
.
return_token_timestamps
)
{
generation_config
.
output_attentions
=
true
;
generation_config
.
return_dict_in_generate
=
true
;
if
(
generation_config
.
task
=
=
=
'
translate
'
)
{
console
.
warn
(
"
Token
-
level
timestamps
may
not
be
reliable
for
task
'
translate
'
.
"
)
}
if
(
!
generation_config
.
alignment_heads
)
{
throw
new
Error
(
"
Model
generation
config
has
no
alignment_heads
token
-
level
timestamps
not
available
.
"
+
"
See
https
:
/
/
gist
.
github
.
com
/
hollance
/
42e32852f24243b748ae6bc1f985b13a
on
how
to
add
this
property
to
the
generation
config
.
"
)
}
}
const
outputs
=
await
super
.
generate
(
inputs
generation_config
logits_processor
)
;
if
(
generation_config
.
return_token_timestamps
&
&
generation_config
.
alignment_heads
)
{
outputs
[
"
token_timestamps
"
]
=
this
.
_extract_token_timestamps
(
outputs
generation_config
.
alignment_heads
generation_config
.
num_frames
)
}
return
outputs
}
_extract_token_timestamps
(
generate_outputs
alignment_heads
num_frames
=
null
time_precision
=
0
.
02
)
{
if
(
!
generate_outputs
.
cross_attentions
)
{
throw
new
Error
(
"
Model
outputs
must
contain
cross
attentions
to
extract
timestamps
.
"
+
"
This
is
most
likely
because
the
model
was
not
exported
with
output_attentions
=
True
.
"
)
}
let
median_filter_width
=
this
.
config
.
median_filter_width
;
if
(
median_filter_width
=
=
=
undefined
)
{
console
.
warn
(
"
Model
config
has
no
median_filter_width
using
default
value
of
7
.
"
)
median_filter_width
=
7
;
}
const
batchedMatrices
=
generate_outputs
.
cross_attentions
.
map
(
batch
=
>
{
let
cross_attentions
=
Array
.
from
(
{
length
:
this
.
config
.
decoder_layers
}
(
_
i
)
=
>
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
cat
)
(
batch
.
map
(
x
=
>
x
[
i
]
)
2
)
)
;
let
weights
=
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
stack
)
(
alignment_heads
.
map
(
(
[
l
h
]
)
=
>
{
return
num_frames
?
cross_attentions
[
l
]
.
slice
(
null
h
null
[
0
num_frames
]
)
:
cross_attentions
[
l
]
.
slice
(
null
h
)
;
}
)
)
;
weights
=
weights
.
transpose
(
1
0
2
3
)
let
[
std
calculatedMean
]
=
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
std_mean
)
(
weights
-
2
0
true
)
;
let
smoothedWeights
=
weights
.
clone
(
)
;
for
(
let
a
=
0
;
a
<
smoothedWeights
.
dims
[
0
]
;
+
+
a
)
{
let
aTensor
=
smoothedWeights
[
a
]
;
for
(
let
b
=
0
;
b
<
aTensor
.
dims
[
0
]
;
+
+
b
)
{
let
bTensor
=
aTensor
[
b
]
;
const
stdTensor
=
std
[
a
]
[
b
]
[
0
]
;
const
meanTensor
=
calculatedMean
[
a
]
[
b
]
[
0
]
;
for
(
let
c
=
0
;
c
<
bTensor
.
dims
[
0
]
;
+
+
c
)
{
let
cTensor
=
bTensor
[
c
]
;
for
(
let
d
=
0
;
d
<
cTensor
.
data
.
length
;
+
+
d
)
{
cTensor
.
data
[
d
]
=
(
cTensor
.
data
[
d
]
-
meanTensor
.
data
[
d
]
)
/
stdTensor
.
data
[
d
]
}
cTensor
.
data
.
set
(
(
0
_transformers_js__WEBPACK_IMPORTED_MODULE_6__
.
medianFilter
)
(
cTensor
.
data
median_filter_width
)
)
}
}
}
const
matrix
=
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
mean
)
(
smoothedWeights
1
)
;
return
matrix
;
}
)
;
const
timestampsShape
=
[
generate_outputs
.
sequences
.
length
generate_outputs
.
sequences
[
0
]
.
length
]
;
const
timestamps
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
float32
'
new
Float32Array
(
timestampsShape
[
0
]
*
timestampsShape
[
1
]
)
timestampsShape
)
;
for
(
let
batch_idx
=
0
;
batch_idx
<
timestampsShape
[
0
]
;
+
+
batch_idx
)
{
const
matrix
=
batchedMatrices
[
batch_idx
]
.
neg
(
)
.
squeeze_
(
0
)
;
let
[
text_indices
time_indices
]
=
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
dynamicTimeWarping
)
(
matrix
)
;
let
diffs
=
Array
.
from
(
{
length
:
text_indices
.
length
-
1
}
(
v
i
)
=
>
text_indices
[
i
+
1
]
-
text_indices
[
i
]
)
;
let
jumps
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_1__
.
mergeArrays
)
(
[
1
]
diffs
)
.
map
(
x
=
>
!
!
x
)
;
let
jump_times
=
[
]
;
for
(
let
i
=
0
;
i
<
jumps
.
length
;
+
+
i
)
{
if
(
jumps
[
i
]
)
{
jump_times
.
push
(
time_indices
[
i
]
*
time_precision
)
;
}
}
timestamps
[
batch_idx
]
.
data
.
set
(
jump_times
1
)
}
return
timestamps
;
}
}
class
VisionEncoderDecoderModel
extends
PreTrainedModel
{
main_input_name
=
'
pixel_values
'
;
constructor
(
config
session
decoder_merged_session
generation_config
)
{
super
(
config
session
)
;
this
.
decoder_merged_session
=
decoder_merged_session
;
this
.
generation_config
=
generation_config
;
const
encoderConfig
=
this
.
config
.
encoder
;
const
decoderConfig
=
this
.
config
.
decoder
;
const
encoderModelType
=
encoderConfig
.
model_type
;
const
encoderModel
=
MODEL_MAPPING_NAMES_ENCODER_ONLY
.
get
(
encoderModelType
)
?
?
MODEL_MAPPING_NAMES_ENCODER_DECODER
.
get
(
encoderModelType
)
;
if
(
!
encoderModel
)
{
console
.
warn
(
Model
type
for
encoder
'
{
encoderModelType
}
'
not
found
assuming
encoder
-
only
architecture
.
Please
report
this
at
https
:
/
/
github
.
com
/
xenova
/
transformers
.
js
/
issues
/
new
/
choose
.
)
;
}
const
decoderModel
=
MODEL_WITH_LM_HEAD_MAPPING_NAMES
.
get
(
decoderConfig
.
model_type
)
;
if
(
!
decoderModel
)
{
throw
new
Error
(
Unable
to
construct
\
VisionEncoderDecoder
\
due
to
unsupported
decoder
:
"
{
this
.
config
.
decoder
.
model_type
}
"
)
;
}
const
decoderModelClass
=
decoderModel
[
1
]
;
const
decoder
=
new
decoderModelClass
(
decoderConfig
decoder_merged_session
generation_config
)
;
this
.
add_encoder_pkv
=
'
num_decoder_layers
'
in
decoder
;
if
(
this
.
add_encoder_pkv
)
{
this
.
num_decoder_layers
=
decoder
.
num_decoder_layers
;
this
.
num_decoder_heads
=
decoder
.
num_decoder_heads
;
this
.
decoder_dim_kv
=
decoder
.
decoder_dim_kv
;
this
.
num_encoder_layers
=
decoder
.
num_encoder_layers
;
this
.
num_encoder_heads
=
decoder
.
num_encoder_heads
;
this
.
encoder_dim_kv
=
decoder
.
encoder_dim_kv
;
}
else
{
this
.
num_layers
=
decoder
.
num_layers
;
this
.
num_heads
=
decoder
.
num_heads
;
this
.
dim_kv
=
decoder
.
dim_kv
;
}
}
}
class
CLIPPreTrainedModel
extends
PreTrainedModel
{
}
class
CLIPModel
extends
CLIPPreTrainedModel
{
}
class
CLIPTextModelWithProjection
extends
CLIPPreTrainedModel
{
static
async
from_pretrained
(
pretrained_model_name_or_path
options
=
{
}
)
{
options
.
model_file_name
?
?
=
'
text_model
'
;
return
super
.
from_pretrained
(
pretrained_model_name_or_path
options
)
;
}
}
class
CLIPVisionModelWithProjection
extends
CLIPPreTrainedModel
{
static
async
from_pretrained
(
pretrained_model_name_or_path
options
=
{
}
)
{
options
.
model_file_name
?
?
=
'
vision_model
'
;
return
super
.
from_pretrained
(
pretrained_model_name_or_path
options
)
;
}
}
class
SiglipPreTrainedModel
extends
PreTrainedModel
{
}
class
SiglipModel
extends
SiglipPreTrainedModel
{
}
class
SiglipTextModel
extends
SiglipPreTrainedModel
{
static
async
from_pretrained
(
pretrained_model_name_or_path
options
=
{
}
)
{
options
.
model_file_name
?
?
=
'
text_model
'
;
return
super
.
from_pretrained
(
pretrained_model_name_or_path
options
)
;
}
}
class
SiglipVisionModel
extends
CLIPPreTrainedModel
{
static
async
from_pretrained
(
pretrained_model_name_or_path
options
=
{
}
)
{
options
.
model_file_name
?
?
=
'
vision_model
'
;
return
super
.
from_pretrained
(
pretrained_model_name_or_path
options
)
;
}
}
class
ChineseCLIPPreTrainedModel
extends
PreTrainedModel
{
}
class
ChineseCLIPModel
extends
ChineseCLIPPreTrainedModel
{
}
class
CLIPSegPreTrainedModel
extends
PreTrainedModel
{
}
class
CLIPSegModel
extends
CLIPSegPreTrainedModel
{
}
class
CLIPSegForImageSegmentation
extends
CLIPSegPreTrainedModel
{
}
class
GPT2PreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
n_head
this
.
num_layers
=
this
.
config
.
n_layer
this
.
dim_kv
=
this
.
config
.
n_embd
/
this
.
num_heads
;
}
}
class
GPT2Model
extends
GPT2PreTrainedModel
{
}
class
GPT2LMHeadModel
extends
GPT2PreTrainedModel
{
}
class
GPTNeoPreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
num_heads
;
this
.
num_layers
=
this
.
config
.
num_layers
;
this
.
dim_kv
=
this
.
config
.
hidden_size
/
this
.
num_heads
;
}
}
class
GPTNeoModel
extends
GPTNeoPreTrainedModel
{
}
class
GPTNeoForCausalLM
extends
GPTNeoPreTrainedModel
{
}
class
GPTNeoXPreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
num_attention_heads
;
this
.
num_layers
=
this
.
config
.
num_hidden_layers
;
this
.
dim_kv
=
this
.
config
.
hidden_size
/
this
.
num_heads
;
}
}
class
GPTNeoXModel
extends
GPTNeoXPreTrainedModel
{
}
class
GPTNeoXForCausalLM
extends
GPTNeoXPreTrainedModel
{
}
class
GPTJPreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
n_head
this
.
num_layers
=
this
.
config
.
n_layer
this
.
dim_kv
=
this
.
config
.
n_embd
/
this
.
num_heads
;
}
}
class
GPTJModel
extends
GPTJPreTrainedModel
{
}
class
GPTJForCausalLM
extends
GPTJPreTrainedModel
{
}
class
GPTBigCodePreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
n_head
this
.
num_layers
=
this
.
config
.
n_layer
this
.
dim_kv
=
this
.
config
.
n_embd
/
this
.
num_heads
;
}
}
class
GPTBigCodeModel
extends
GPTBigCodePreTrainedModel
{
}
class
GPTBigCodeForCausalLM
extends
GPTBigCodePreTrainedModel
{
}
class
CodeGenPreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
n_head
this
.
num_layers
=
this
.
config
.
n_layer
this
.
dim_kv
=
this
.
config
.
n_embd
/
this
.
num_heads
;
}
}
class
CodeGenModel
extends
CodeGenPreTrainedModel
{
}
class
CodeGenForCausalLM
extends
CodeGenPreTrainedModel
{
}
class
LlamaPreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
num_key_value_heads
?
?
this
.
config
.
num_attention_heads
this
.
num_layers
=
this
.
config
.
num_hidden_layers
this
.
dim_kv
=
this
.
config
.
hidden_size
/
this
.
config
.
num_attention_heads
}
}
class
LlamaModel
extends
LlamaPreTrainedModel
{
}
class
LlamaForCausalLM
extends
LlamaPreTrainedModel
{
}
class
Qwen2PreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
num_key_value_heads
?
?
this
.
config
.
num_attention_heads
this
.
num_layers
=
this
.
config
.
num_hidden_layers
this
.
dim_kv
=
this
.
config
.
hidden_size
/
this
.
config
.
num_attention_heads
}
}
class
Qwen2Model
extends
Qwen2PreTrainedModel
{
}
class
Qwen2ForCausalLM
extends
Qwen2PreTrainedModel
{
}
class
PhiPreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
;
this
.
num_heads
=
this
.
config
.
num_attention_heads
;
this
.
num_layers
=
this
.
config
.
num_hidden_layers
;
this
.
dim_kv
=
this
.
config
.
hidden_size
/
this
.
num_heads
;
}
}
class
PhiModel
extends
PhiPreTrainedModel
{
}
class
PhiForCausalLM
extends
PhiPreTrainedModel
{
}
class
BloomPreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
n_head
this
.
num_layers
=
this
.
config
.
n_layer
this
.
dim_kv
=
this
.
config
.
hidden_size
/
this
.
num_heads
;
}
}
class
BloomModel
extends
BloomPreTrainedModel
{
}
class
BloomForCausalLM
extends
BloomPreTrainedModel
{
}
class
MptPreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
n_heads
this
.
num_layers
=
this
.
config
.
n_layers
this
.
dim_kv
=
this
.
config
.
d_model
/
this
.
num_heads
;
}
}
class
MptModel
extends
MptPreTrainedModel
{
}
class
MptForCausalLM
extends
MptPreTrainedModel
{
}
class
OPTPreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
num_attention_heads
;
this
.
num_layers
=
this
.
config
.
num_hidden_layers
;
this
.
dim_kv
=
this
.
config
.
hidden_size
/
this
.
num_heads
;
}
}
class
OPTModel
extends
OPTPreTrainedModel
{
}
class
OPTForCausalLM
extends
OPTPreTrainedModel
{
}
class
ViTPreTrainedModel
extends
PreTrainedModel
{
}
class
ViTModel
extends
ViTPreTrainedModel
{
}
class
ViTForImageClassification
extends
ViTPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
VitMattePreTrainedModel
extends
PreTrainedModel
{
}
class
VitMatteForImageMatting
extends
VitMattePreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
ImageMattingOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
MobileViTPreTrainedModel
extends
PreTrainedModel
{
}
class
MobileViTModel
extends
MobileViTPreTrainedModel
{
}
class
MobileViTForImageClassification
extends
MobileViTPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
OwlViTPreTrainedModel
extends
PreTrainedModel
{
}
class
OwlViTModel
extends
OwlViTPreTrainedModel
{
}
class
OwlViTForObjectDetection
extends
OwlViTPreTrainedModel
{
}
class
Owlv2PreTrainedModel
extends
PreTrainedModel
{
}
class
Owlv2Model
extends
Owlv2PreTrainedModel
{
}
class
Owlv2ForObjectDetection
extends
Owlv2PreTrainedModel
{
}
class
BeitPreTrainedModel
extends
PreTrainedModel
{
}
class
BeitModel
extends
BeitPreTrainedModel
{
}
class
BeitForImageClassification
extends
BeitPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DetrPreTrainedModel
extends
PreTrainedModel
{
}
class
DetrModel
extends
DetrPreTrainedModel
{
}
class
DetrForObjectDetection
extends
DetrPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
DetrObjectDetectionOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DetrForSegmentation
extends
DetrPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
DetrSegmentationOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
DetrObjectDetectionOutput
extends
ModelOutput
{
constructor
(
{
logits
pred_boxes
}
)
{
super
(
)
;
this
.
logits
=
logits
;
this
.
pred_boxes
=
pred_boxes
;
}
}
class
DetrSegmentationOutput
extends
ModelOutput
{
constructor
(
{
logits
pred_boxes
pred_masks
}
)
{
super
(
)
;
this
.
logits
=
logits
;
this
.
pred_boxes
=
pred_boxes
;
this
.
pred_masks
=
pred_masks
;
}
}
class
TableTransformerPreTrainedModel
extends
PreTrainedModel
{
}
class
TableTransformerModel
extends
TableTransformerPreTrainedModel
{
}
class
TableTransformerForObjectDetection
extends
TableTransformerPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TableTransformerObjectDetectionOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
TableTransformerObjectDetectionOutput
extends
DetrObjectDetectionOutput
{
}
class
DeiTPreTrainedModel
extends
PreTrainedModel
{
}
class
DeiTModel
extends
DeiTPreTrainedModel
{
}
class
DeiTForImageClassification
extends
DeiTPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
ResNetPreTrainedModel
extends
PreTrainedModel
{
}
class
ResNetModel
extends
ResNetPreTrainedModel
{
}
class
ResNetForImageClassification
extends
ResNetPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
SwinPreTrainedModel
extends
PreTrainedModel
{
}
class
SwinModel
extends
SwinPreTrainedModel
{
}
class
SwinForImageClassification
extends
SwinPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
Swin2SRPreTrainedModel
extends
PreTrainedModel
{
}
class
Swin2SRModel
extends
Swin2SRPreTrainedModel
{
}
class
Swin2SRForImageSuperResolution
extends
Swin2SRPreTrainedModel
{
}
class
DPTPreTrainedModel
extends
PreTrainedModel
{
}
class
DPTModel
extends
DPTPreTrainedModel
{
}
class
DPTForDepthEstimation
extends
DPTPreTrainedModel
{
}
class
DepthAnythingPreTrainedModel
extends
PreTrainedModel
{
}
class
DepthAnythingForDepthEstimation
extends
DepthAnythingPreTrainedModel
{
}
class
GLPNPreTrainedModel
extends
PreTrainedModel
{
}
class
GLPNModel
extends
GLPNPreTrainedModel
{
}
class
GLPNForDepthEstimation
extends
GLPNPreTrainedModel
{
}
class
DonutSwinPreTrainedModel
extends
PreTrainedModel
{
}
class
DonutSwinModel
extends
DonutSwinPreTrainedModel
{
}
class
ConvNextPreTrainedModel
extends
PreTrainedModel
{
}
class
ConvNextModel
extends
ConvNextPreTrainedModel
{
}
class
ConvNextForImageClassification
extends
ConvNextPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
ConvNextV2PreTrainedModel
extends
PreTrainedModel
{
}
class
ConvNextV2Model
extends
ConvNextV2PreTrainedModel
{
}
class
ConvNextV2ForImageClassification
extends
ConvNextV2PreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
Dinov2PreTrainedModel
extends
PreTrainedModel
{
}
class
Dinov2Model
extends
Dinov2PreTrainedModel
{
}
class
Dinov2ForImageClassification
extends
Dinov2PreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
YolosPreTrainedModel
extends
PreTrainedModel
{
}
class
YolosModel
extends
YolosPreTrainedModel
{
}
class
YolosForObjectDetection
extends
YolosPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
YolosObjectDetectionOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
YolosObjectDetectionOutput
extends
ModelOutput
{
constructor
(
{
logits
pred_boxes
}
)
{
super
(
)
;
this
.
logits
=
logits
;
this
.
pred_boxes
=
pred_boxes
;
}
}
class
SamPreTrainedModel
extends
PreTrainedModel
{
}
class
SamModel
extends
SamPreTrainedModel
{
constructor
(
config
vision_encoder
prompt_encoder_mask_decoder
)
{
super
(
config
vision_encoder
)
;
this
.
prompt_encoder_mask_decoder
=
prompt_encoder_mask_decoder
;
}
async
get_image_embeddings
(
{
pixel_values
}
)
{
return
await
encoderForward
(
this
{
pixel_values
}
)
}
async
forward
(
model_inputs
)
{
if
(
!
model_inputs
.
image_embeddings
|
|
!
model_inputs
.
image_positional_embeddings
)
{
model_inputs
=
{
.
.
.
model_inputs
.
.
.
(
await
this
.
get_image_embeddings
(
model_inputs
)
)
}
}
if
(
!
model_inputs
.
input_labels
)
{
const
shape
=
model_inputs
.
input_points
.
dims
.
slice
(
0
-
1
)
;
const
numElements
=
shape
.
reduce
(
(
a
b
)
=
>
a
*
b
1
)
;
model_inputs
.
input_labels
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
int64
'
new
BigInt64Array
(
numElements
)
.
fill
(
1n
)
shape
)
;
}
return
await
sessionRun
(
this
.
prompt_encoder_mask_decoder
{
input_points
:
model_inputs
.
input_points
input_labels
:
model_inputs
.
input_labels
image_embeddings
:
model_inputs
.
image_embeddings
image_positional_embeddings
:
model_inputs
.
image_positional_embeddings
}
)
;
}
async
_call
(
model_inputs
)
{
return
new
SamImageSegmentationOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
SamImageSegmentationOutput
extends
ModelOutput
{
constructor
(
{
iou_scores
pred_masks
}
)
{
super
(
)
;
this
.
iou_scores
=
iou_scores
;
this
.
pred_masks
=
pred_masks
;
}
}
class
MarianPreTrainedModel
extends
PreTrainedModel
{
}
;
class
MarianModel
extends
MarianPreTrainedModel
{
}
class
MarianMTModel
extends
MarianPreTrainedModel
{
constructor
(
config
session
decoder_merged_session
generation_config
)
{
super
(
config
session
)
;
this
.
decoder_merged_session
=
decoder_merged_session
;
this
.
generation_config
=
generation_config
;
this
.
num_decoder_layers
=
this
.
config
.
decoder_layers
;
this
.
num_decoder_heads
=
this
.
config
.
decoder_attention_heads
;
this
.
decoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_decoder_heads
;
this
.
num_encoder_layers
=
this
.
config
.
encoder_layers
;
this
.
num_encoder_heads
=
this
.
config
.
encoder_attention_heads
;
this
.
encoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_encoder_heads
;
}
}
class
M2M100PreTrainedModel
extends
PreTrainedModel
{
}
;
class
M2M100Model
extends
M2M100PreTrainedModel
{
}
class
M2M100ForConditionalGeneration
extends
M2M100PreTrainedModel
{
constructor
(
config
session
decoder_merged_session
generation_config
)
{
super
(
config
session
)
;
this
.
decoder_merged_session
=
decoder_merged_session
;
this
.
generation_config
=
generation_config
;
this
.
num_decoder_layers
=
this
.
config
.
decoder_layers
;
this
.
num_decoder_heads
=
this
.
config
.
decoder_attention_heads
;
this
.
decoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_decoder_heads
;
this
.
num_encoder_layers
=
this
.
config
.
encoder_layers
;
this
.
num_encoder_heads
=
this
.
config
.
encoder_attention_heads
;
this
.
encoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_encoder_heads
;
}
}
class
Wav2Vec2PreTrainedModel
extends
PreTrainedModel
{
}
;
class
Wav2Vec2Model
extends
Wav2Vec2PreTrainedModel
{
}
class
Wav2Vec2ForCTC
extends
Wav2Vec2PreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
CausalLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
Wav2Vec2ForSequenceClassification
extends
Wav2Vec2PreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
Wav2Vec2ForAudioFrameClassification
extends
Wav2Vec2PreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
UniSpeechPreTrainedModel
extends
PreTrainedModel
{
}
;
class
UniSpeechModel
extends
UniSpeechPreTrainedModel
{
}
class
UniSpeechForCTC
extends
UniSpeechPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
CausalLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
UniSpeechForSequenceClassification
extends
UniSpeechPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
UniSpeechSatPreTrainedModel
extends
PreTrainedModel
{
}
;
class
UniSpeechSatModel
extends
UniSpeechSatPreTrainedModel
{
}
class
UniSpeechSatForCTC
extends
UniSpeechSatPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
CausalLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
UniSpeechSatForSequenceClassification
extends
UniSpeechSatPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
UniSpeechSatForAudioFrameClassification
extends
UniSpeechSatPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
Wav2Vec2BertPreTrainedModel
extends
PreTrainedModel
{
}
;
class
Wav2Vec2BertModel
extends
Wav2Vec2BertPreTrainedModel
{
}
class
Wav2Vec2BertForCTC
extends
Wav2Vec2BertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
CausalLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
Wav2Vec2BertForSequenceClassification
extends
Wav2Vec2BertPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
HubertPreTrainedModel
extends
PreTrainedModel
{
}
class
HubertModel
extends
Wav2Vec2PreTrainedModel
{
}
class
HubertForCTC
extends
Wav2Vec2PreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
CausalLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
HubertForSequenceClassification
extends
Wav2Vec2PreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
WavLMPreTrainedModel
extends
PreTrainedModel
{
}
;
class
WavLMModel
extends
WavLMPreTrainedModel
{
}
class
WavLMForCTC
extends
WavLMPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
CausalLMOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
WavLMForSequenceClassification
extends
WavLMPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
WavLMForXVector
extends
WavLMPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
XVectorOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
WavLMForAudioFrameClassification
extends
WavLMPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
TokenClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
SpeechT5PreTrainedModel
extends
PreTrainedModel
{
}
;
class
SpeechT5Model
extends
SpeechT5PreTrainedModel
{
}
;
class
SpeechT5ForSpeechToText
extends
SpeechT5PreTrainedModel
{
}
class
SpeechT5ForTextToSpeech
extends
SpeechT5PreTrainedModel
{
constructor
(
config
session
decoder_merged_session
generation_config
)
{
super
(
config
session
)
;
this
.
decoder_merged_session
=
decoder_merged_session
;
this
.
generation_config
=
generation_config
;
this
.
num_decoder_layers
=
this
.
config
.
decoder_layers
;
this
.
num_decoder_heads
=
this
.
config
.
decoder_attention_heads
;
this
.
decoder_dim_kv
=
this
.
config
.
hidden_size
/
this
.
num_decoder_heads
;
this
.
num_encoder_layers
=
this
.
config
.
encoder_layers
;
this
.
num_encoder_heads
=
this
.
config
.
encoder_attention_heads
;
this
.
encoder_dim_kv
=
this
.
config
.
hidden_size
/
this
.
num_encoder_heads
;
}
async
generate_speech
(
input_values
speaker_embeddings
{
threshold
=
0
.
5
minlenratio
=
0
.
0
maxlenratio
=
20
.
0
vocoder
=
null
}
=
{
}
)
{
const
model_inputs
=
{
input_ids
:
input_values
}
const
{
encoder_outputs
encoder_attention_mask
}
=
await
encoderForward
(
this
model_inputs
)
;
const
r
=
encoder_outputs
.
dims
[
1
]
/
this
.
config
.
reduction_factor
;
const
maxlen
=
Math
.
floor
(
r
*
maxlenratio
)
;
const
minlen
=
Math
.
floor
(
r
*
minlenratio
)
;
const
num_mel_bins
=
this
.
config
.
num_mel_bins
;
let
spectrogramParts
=
[
]
;
let
past_key_values
=
null
;
let
decoder_outputs
=
null
;
let
idx
=
0
;
while
(
true
)
{
+
+
idx
;
const
use_cache_branch
=
boolTensor
(
!
!
decoder_outputs
)
;
let
output_sequence
;
if
(
decoder_outputs
)
{
output_sequence
=
decoder_outputs
.
output_sequence_out
;
}
else
{
output_sequence
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
Tensor
(
'
float32
'
new
Float32Array
(
num_mel_bins
)
[
1
1
num_mel_bins
]
)
}
let
decoderFeeds
=
{
use_cache_branch
output_sequence
encoder_attention_mask
:
encoder_attention_mask
speaker_embeddings
:
speaker_embeddings
encoder_hidden_states
:
encoder_outputs
}
;
this
.
addPastKeyValues
(
decoderFeeds
past_key_values
)
;
decoder_outputs
=
await
sessionRun
(
this
.
decoder_merged_session
decoderFeeds
)
;
past_key_values
=
this
.
getPastKeyValues
(
decoder_outputs
past_key_values
)
;
const
{
prob
spectrum
}
=
decoder_outputs
;
spectrogramParts
.
push
(
spectrum
)
;
if
(
idx
>
=
minlen
&
&
(
Array
.
from
(
prob
.
data
)
.
filter
(
p
=
>
p
>
=
threshold
)
.
length
>
0
|
|
idx
>
=
maxlen
)
)
{
break
;
}
}
const
spectrogram
=
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_4__
.
cat
)
(
spectrogramParts
)
;
const
{
waveform
}
=
await
sessionRun
(
vocoder
.
session
{
spectrogram
}
)
;
return
{
spectrogram
waveform
}
}
}
class
SpeechT5HifiGan
extends
PreTrainedModel
{
main_input_name
=
'
spectrogram
'
;
}
class
TrOCRPreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
;
this
.
num_encoder_layers
=
this
.
num_decoder_layers
=
this
.
config
.
decoder_layers
;
this
.
num_encoder_heads
=
this
.
num_decoder_heads
=
this
.
config
.
decoder_attention_heads
;
this
.
encoder_dim_kv
=
this
.
decoder_dim_kv
=
this
.
config
.
d_model
/
this
.
num_decoder_heads
;
}
}
class
TrOCRForCausalLM
extends
TrOCRPreTrainedModel
{
}
class
MistralPreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
num_key_value_heads
;
this
.
num_layers
=
this
.
config
.
num_hidden_layers
;
this
.
dim_kv
=
this
.
config
.
hidden_size
/
this
.
config
.
num_attention_heads
;
}
}
class
MistralModel
extends
MistralPreTrainedModel
{
}
class
MistralForCausalLM
extends
MistralPreTrainedModel
{
}
class
Starcoder2PreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
num_key_value_heads
;
this
.
num_layers
=
this
.
config
.
num_hidden_layers
;
this
.
dim_kv
=
this
.
config
.
hidden_size
/
this
.
config
.
num_attention_heads
;
}
}
class
Starcoder2Model
extends
Starcoder2PreTrainedModel
{
}
class
Starcoder2ForCausalLM
extends
Starcoder2PreTrainedModel
{
}
class
FalconPreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
num_attention_heads
;
this
.
num_layers
=
this
.
config
.
num_hidden_layers
;
this
.
dim_kv
=
this
.
config
.
hidden_size
/
this
.
config
.
num_attention_heads
;
}
}
class
FalconModel
extends
FalconPreTrainedModel
{
}
class
FalconForCausalLM
extends
FalconPreTrainedModel
{
}
class
ClapPreTrainedModel
extends
PreTrainedModel
{
}
class
ClapModel
extends
ClapPreTrainedModel
{
}
class
ClapTextModelWithProjection
extends
ClapPreTrainedModel
{
static
async
from_pretrained
(
pretrained_model_name_or_path
options
=
{
}
)
{
options
.
model_file_name
?
?
=
'
text_model
'
;
return
super
.
from_pretrained
(
pretrained_model_name_or_path
options
)
;
}
}
class
ClapAudioModelWithProjection
extends
ClapPreTrainedModel
{
static
async
from_pretrained
(
pretrained_model_name_or_path
options
=
{
}
)
{
options
.
model_file_name
?
?
=
'
audio_model
'
;
return
super
.
from_pretrained
(
pretrained_model_name_or_path
options
)
;
}
}
class
VitsPreTrainedModel
extends
PreTrainedModel
{
}
class
VitsModel
extends
VitsPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
VitsModelOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
SegformerPreTrainedModel
extends
PreTrainedModel
{
}
class
SegformerModel
extends
SegformerPreTrainedModel
{
}
class
SegformerForImageClassification
extends
SegformerPreTrainedModel
{
}
class
SegformerForSemanticSegmentation
extends
SegformerPreTrainedModel
{
}
class
StableLmPreTrainedModel
extends
PreTrainedModel
{
constructor
(
config
session
generation_config
)
{
super
(
config
session
)
;
this
.
generation_config
=
generation_config
;
this
.
config
.
pad_token_id
=
this
.
config
.
eos_token_id
this
.
num_heads
=
this
.
config
.
num_attention_heads
;
this
.
num_layers
=
this
.
config
.
num_hidden_layers
;
this
.
dim_kv
=
this
.
config
.
hidden_size
/
this
.
num_heads
;
}
}
class
StableLmModel
extends
StableLmPreTrainedModel
{
}
class
StableLmForCausalLM
extends
StableLmPreTrainedModel
{
}
class
EfficientNetPreTrainedModel
extends
PreTrainedModel
{
}
class
EfficientNetModel
extends
EfficientNetPreTrainedModel
{
}
class
EfficientNetForImageClassification
extends
EfficientNetPreTrainedModel
{
async
_call
(
model_inputs
)
{
return
new
SequenceClassifierOutput
(
await
super
.
_call
(
model_inputs
)
)
;
}
}
class
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
null
;
static
BASE_IF_FAIL
=
false
;
static
async
from_pretrained
(
pretrained_model_name_or_path
{
quantized
=
true
progress_callback
=
null
config
=
null
cache_dir
=
null
local_files_only
=
false
revision
=
'
main
'
model_file_name
=
null
}
=
{
}
)
{
let
options
=
{
quantized
progress_callback
config
cache_dir
local_files_only
revision
model_file_name
}
config
=
await
_configs_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoConfig
.
from_pretrained
(
pretrained_model_name_or_path
options
)
;
if
(
!
options
.
config
)
{
options
.
config
=
config
;
}
if
(
!
this
.
MODEL_CLASS_MAPPINGS
)
{
throw
new
Error
(
"
MODEL_CLASS_MAPPINGS
not
implemented
for
this
type
of
AutoClass
:
"
+
this
.
name
)
;
}
for
(
let
MODEL_CLASS_MAPPING
of
this
.
MODEL_CLASS_MAPPINGS
)
{
const
modelInfo
=
MODEL_CLASS_MAPPING
.
get
(
config
.
model_type
)
;
if
(
!
modelInfo
)
{
continue
;
}
return
await
modelInfo
[
1
]
.
from_pretrained
(
pretrained_model_name_or_path
options
)
;
}
if
(
this
.
BASE_IF_FAIL
)
{
console
.
warn
(
Unknown
model
class
"
{
config
.
model_type
}
"
attempting
to
construct
from
base
class
.
)
;
return
await
PreTrainedModel
.
from_pretrained
(
pretrained_model_name_or_path
options
)
;
}
else
{
throw
Error
(
Unsupported
model
type
:
{
config
.
model_type
}
)
}
}
}
const
MODEL_MAPPING_NAMES_ENCODER_ONLY
=
new
Map
(
[
[
'
bert
'
[
'
BertModel
'
BertModel
]
]
[
'
nomic_bert
'
[
'
NomicBertModel
'
NomicBertModel
]
]
[
'
roformer
'
[
'
RoFormerModel
'
RoFormerModel
]
]
[
'
electra
'
[
'
ElectraModel
'
ElectraModel
]
]
[
'
esm
'
[
'
EsmModel
'
EsmModel
]
]
[
'
convbert
'
[
'
ConvBertModel
'
ConvBertModel
]
]
[
'
camembert
'
[
'
CamembertModel
'
CamembertModel
]
]
[
'
deberta
'
[
'
DebertaModel
'
DebertaModel
]
]
[
'
deberta
-
v2
'
[
'
DebertaV2Model
'
DebertaV2Model
]
]
[
'
mpnet
'
[
'
MPNetModel
'
MPNetModel
]
]
[
'
albert
'
[
'
AlbertModel
'
AlbertModel
]
]
[
'
distilbert
'
[
'
DistilBertModel
'
DistilBertModel
]
]
[
'
roberta
'
[
'
RobertaModel
'
RobertaModel
]
]
[
'
xlm
'
[
'
XLMModel
'
XLMModel
]
]
[
'
xlm
-
roberta
'
[
'
XLMRobertaModel
'
XLMRobertaModel
]
]
[
'
clap
'
[
'
ClapModel
'
ClapModel
]
]
[
'
clip
'
[
'
CLIPModel
'
CLIPModel
]
]
[
'
clipseg
'
[
'
CLIPSegModel
'
CLIPSegModel
]
]
[
'
chinese_clip
'
[
'
ChineseCLIPModel
'
ChineseCLIPModel
]
]
[
'
siglip
'
[
'
SiglipModel
'
SiglipModel
]
]
[
'
mobilebert
'
[
'
MobileBertModel
'
MobileBertModel
]
]
[
'
squeezebert
'
[
'
SqueezeBertModel
'
SqueezeBertModel
]
]
[
'
wav2vec2
'
[
'
Wav2Vec2Model
'
Wav2Vec2Model
]
]
[
'
wav2vec2
-
bert
'
[
'
Wav2Vec2BertModel
'
Wav2Vec2BertModel
]
]
[
'
unispeech
'
[
'
UniSpeechModel
'
UniSpeechModel
]
]
[
'
unispeech
-
sat
'
[
'
UniSpeechSatModel
'
UniSpeechSatModel
]
]
[
'
hubert
'
[
'
HubertModel
'
HubertModel
]
]
[
'
wavlm
'
[
'
WavLMModel
'
WavLMModel
]
]
[
'
audio
-
spectrogram
-
transformer
'
[
'
ASTModel
'
ASTModel
]
]
[
'
vits
'
[
'
VitsModel
'
VitsModel
]
]
[
'
detr
'
[
'
DetrModel
'
DetrModel
]
]
[
'
table
-
transformer
'
[
'
TableTransformerModel
'
TableTransformerModel
]
]
[
'
vit
'
[
'
ViTModel
'
ViTModel
]
]
[
'
mobilevit
'
[
'
MobileViTModel
'
MobileViTModel
]
]
[
'
owlvit
'
[
'
OwlViTModel
'
OwlViTModel
]
]
[
'
owlv2
'
[
'
Owlv2Model
'
Owlv2Model
]
]
[
'
beit
'
[
'
BeitModel
'
BeitModel
]
]
[
'
deit
'
[
'
DeiTModel
'
DeiTModel
]
]
[
'
convnext
'
[
'
ConvNextModel
'
ConvNextModel
]
]
[
'
convnextv2
'
[
'
ConvNextV2Model
'
ConvNextV2Model
]
]
[
'
dinov2
'
[
'
Dinov2Model
'
Dinov2Model
]
]
[
'
resnet
'
[
'
ResNetModel
'
ResNetModel
]
]
[
'
swin
'
[
'
SwinModel
'
SwinModel
]
]
[
'
swin2sr
'
[
'
Swin2SRModel
'
Swin2SRModel
]
]
[
'
donut
-
swin
'
[
'
DonutSwinModel
'
DonutSwinModel
]
]
[
'
yolos
'
[
'
YolosModel
'
YolosModel
]
]
[
'
dpt
'
[
'
DPTModel
'
DPTModel
]
]
[
'
glpn
'
[
'
GLPNModel
'
GLPNModel
]
]
[
'
hifigan
'
[
'
SpeechT5HifiGan
'
SpeechT5HifiGan
]
]
[
'
efficientnet
'
[
'
EfficientNetModel
'
EfficientNetModel
]
]
]
)
;
const
MODEL_MAPPING_NAMES_ENCODER_DECODER
=
new
Map
(
[
[
'
t5
'
[
'
T5Model
'
T5Model
]
]
[
'
longt5
'
[
'
LongT5Model
'
LongT5Model
]
]
[
'
mt5
'
[
'
MT5Model
'
MT5Model
]
]
[
'
bart
'
[
'
BartModel
'
BartModel
]
]
[
'
mbart
'
[
'
MBartModel
'
MBartModel
]
]
[
'
marian
'
[
'
MarianModel
'
MarianModel
]
]
[
'
whisper
'
[
'
WhisperModel
'
WhisperModel
]
]
[
'
m2m_100
'
[
'
M2M100Model
'
M2M100Model
]
]
[
'
blenderbot
'
[
'
BlenderbotModel
'
BlenderbotModel
]
]
[
'
blenderbot
-
small
'
[
'
BlenderbotSmallModel
'
BlenderbotSmallModel
]
]
]
)
;
const
MODEL_MAPPING_NAMES_DECODER_ONLY
=
new
Map
(
[
[
'
bloom
'
[
'
BloomModel
'
BloomModel
]
]
[
'
gpt2
'
[
'
GPT2Model
'
GPT2Model
]
]
[
'
gptj
'
[
'
GPTJModel
'
GPTJModel
]
]
[
'
gpt_bigcode
'
[
'
GPTBigCodeModel
'
GPTBigCodeModel
]
]
[
'
gpt_neo
'
[
'
GPTNeoModel
'
GPTNeoModel
]
]
[
'
gpt_neox
'
[
'
GPTNeoXModel
'
GPTNeoXModel
]
]
[
'
codegen
'
[
'
CodeGenModel
'
CodeGenModel
]
]
[
'
llama
'
[
'
LlamaModel
'
LlamaModel
]
]
[
'
qwen2
'
[
'
Qwen2Model
'
Qwen2Model
]
]
[
'
phi
'
[
'
PhiModel
'
PhiModel
]
]
[
'
mpt
'
[
'
MptModel
'
MptModel
]
]
[
'
opt
'
[
'
OPTModel
'
OPTModel
]
]
[
'
mistral
'
[
'
MistralModel
'
MistralModel
]
]
[
'
starcoder2
'
[
'
Starcoder2Model
'
Starcoder2Model
]
]
[
'
falcon
'
[
'
FalconModel
'
FalconModel
]
]
]
)
;
const
MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES
=
new
Map
(
[
[
'
speecht5
'
[
'
SpeechT5ForSpeechToText
'
SpeechT5ForSpeechToText
]
]
[
'
whisper
'
[
'
WhisperForConditionalGeneration
'
WhisperForConditionalGeneration
]
]
]
)
;
const
MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES
=
new
Map
(
[
[
'
speecht5
'
[
'
SpeechT5ForTextToSpeech
'
SpeechT5ForTextToSpeech
]
]
]
)
;
const
MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING_NAMES
=
new
Map
(
[
[
'
vits
'
[
'
VitsModel
'
VitsModel
]
]
]
)
;
const
MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES
=
new
Map
(
[
[
'
bert
'
[
'
BertForSequenceClassification
'
BertForSequenceClassification
]
]
[
'
roformer
'
[
'
RoFormerForSequenceClassification
'
RoFormerForSequenceClassification
]
]
[
'
electra
'
[
'
ElectraForSequenceClassification
'
ElectraForSequenceClassification
]
]
[
'
esm
'
[
'
EsmForSequenceClassification
'
EsmForSequenceClassification
]
]
[
'
convbert
'
[
'
ConvBertForSequenceClassification
'
ConvBertForSequenceClassification
]
]
[
'
camembert
'
[
'
CamembertForSequenceClassification
'
CamembertForSequenceClassification
]
]
[
'
deberta
'
[
'
DebertaForSequenceClassification
'
DebertaForSequenceClassification
]
]
[
'
deberta
-
v2
'
[
'
DebertaV2ForSequenceClassification
'
DebertaV2ForSequenceClassification
]
]
[
'
mpnet
'
[
'
MPNetForSequenceClassification
'
MPNetForSequenceClassification
]
]
[
'
albert
'
[
'
AlbertForSequenceClassification
'
AlbertForSequenceClassification
]
]
[
'
distilbert
'
[
'
DistilBertForSequenceClassification
'
DistilBertForSequenceClassification
]
]
[
'
roberta
'
[
'
RobertaForSequenceClassification
'
RobertaForSequenceClassification
]
]
[
'
xlm
'
[
'
XLMForSequenceClassification
'
XLMForSequenceClassification
]
]
[
'
xlm
-
roberta
'
[
'
XLMRobertaForSequenceClassification
'
XLMRobertaForSequenceClassification
]
]
[
'
bart
'
[
'
BartForSequenceClassification
'
BartForSequenceClassification
]
]
[
'
mbart
'
[
'
MBartForSequenceClassification
'
MBartForSequenceClassification
]
]
[
'
mobilebert
'
[
'
MobileBertForSequenceClassification
'
MobileBertForSequenceClassification
]
]
[
'
squeezebert
'
[
'
SqueezeBertForSequenceClassification
'
SqueezeBertForSequenceClassification
]
]
]
)
;
const
MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES
=
new
Map
(
[
[
'
bert
'
[
'
BertForTokenClassification
'
BertForTokenClassification
]
]
[
'
roformer
'
[
'
RoFormerForTokenClassification
'
RoFormerForTokenClassification
]
]
[
'
electra
'
[
'
ElectraForTokenClassification
'
ElectraForTokenClassification
]
]
[
'
esm
'
[
'
EsmForTokenClassification
'
EsmForTokenClassification
]
]
[
'
convbert
'
[
'
ConvBertForTokenClassification
'
ConvBertForTokenClassification
]
]
[
'
camembert
'
[
'
CamembertForTokenClassification
'
CamembertForTokenClassification
]
]
[
'
deberta
'
[
'
DebertaForTokenClassification
'
DebertaForTokenClassification
]
]
[
'
deberta
-
v2
'
[
'
DebertaV2ForTokenClassification
'
DebertaV2ForTokenClassification
]
]
[
'
mpnet
'
[
'
MPNetForTokenClassification
'
MPNetForTokenClassification
]
]
[
'
distilbert
'
[
'
DistilBertForTokenClassification
'
DistilBertForTokenClassification
]
]
[
'
roberta
'
[
'
RobertaForTokenClassification
'
RobertaForTokenClassification
]
]
[
'
xlm
'
[
'
XLMForTokenClassification
'
XLMForTokenClassification
]
]
[
'
xlm
-
roberta
'
[
'
XLMRobertaForTokenClassification
'
XLMRobertaForTokenClassification
]
]
]
)
;
const
MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES
=
new
Map
(
[
[
'
t5
'
[
'
T5ForConditionalGeneration
'
T5ForConditionalGeneration
]
]
[
'
longt5
'
[
'
LongT5ForConditionalGeneration
'
LongT5ForConditionalGeneration
]
]
[
'
mt5
'
[
'
MT5ForConditionalGeneration
'
MT5ForConditionalGeneration
]
]
[
'
bart
'
[
'
BartForConditionalGeneration
'
BartForConditionalGeneration
]
]
[
'
mbart
'
[
'
MBartForConditionalGeneration
'
MBartForConditionalGeneration
]
]
[
'
marian
'
[
'
MarianMTModel
'
MarianMTModel
]
]
[
'
m2m_100
'
[
'
M2M100ForConditionalGeneration
'
M2M100ForConditionalGeneration
]
]
[
'
blenderbot
'
[
'
BlenderbotForConditionalGeneration
'
BlenderbotForConditionalGeneration
]
]
[
'
blenderbot
-
small
'
[
'
BlenderbotSmallForConditionalGeneration
'
BlenderbotSmallForConditionalGeneration
]
]
]
)
;
const
MODEL_WITH_LM_HEAD_MAPPING_NAMES
=
new
Map
(
[
[
'
bloom
'
[
'
BloomForCausalLM
'
BloomForCausalLM
]
]
[
'
gpt2
'
[
'
GPT2LMHeadModel
'
GPT2LMHeadModel
]
]
[
'
gptj
'
[
'
GPTJForCausalLM
'
GPTJForCausalLM
]
]
[
'
gpt_bigcode
'
[
'
GPTBigCodeForCausalLM
'
GPTBigCodeForCausalLM
]
]
[
'
gpt_neo
'
[
'
GPTNeoForCausalLM
'
GPTNeoForCausalLM
]
]
[
'
gpt_neox
'
[
'
GPTNeoXForCausalLM
'
GPTNeoXForCausalLM
]
]
[
'
codegen
'
[
'
CodeGenForCausalLM
'
CodeGenForCausalLM
]
]
[
'
llama
'
[
'
LlamaForCausalLM
'
LlamaForCausalLM
]
]
[
'
qwen2
'
[
'
Qwen2ForCausalLM
'
Qwen2ForCausalLM
]
]
[
'
phi
'
[
'
PhiForCausalLM
'
PhiForCausalLM
]
]
[
'
mpt
'
[
'
MptForCausalLM
'
MptForCausalLM
]
]
[
'
opt
'
[
'
OPTForCausalLM
'
OPTForCausalLM
]
]
[
'
mbart
'
[
'
MBartForCausalLM
'
MBartForCausalLM
]
]
[
'
mistral
'
[
'
MistralForCausalLM
'
MistralForCausalLM
]
]
[
'
starcoder2
'
[
'
Starcoder2ForCausalLM
'
Starcoder2ForCausalLM
]
]
[
'
falcon
'
[
'
FalconForCausalLM
'
FalconForCausalLM
]
]
[
'
trocr
'
[
'
TrOCRForCausalLM
'
TrOCRForCausalLM
]
]
[
'
stablelm
'
[
'
StableLmForCausalLM
'
StableLmForCausalLM
]
]
]
)
;
const
MODEL_FOR_MASKED_LM_MAPPING_NAMES
=
new
Map
(
[
[
'
bert
'
[
'
BertForMaskedLM
'
BertForMaskedLM
]
]
[
'
roformer
'
[
'
RoFormerForMaskedLM
'
RoFormerForMaskedLM
]
]
[
'
electra
'
[
'
ElectraForMaskedLM
'
ElectraForMaskedLM
]
]
[
'
esm
'
[
'
EsmForMaskedLM
'
EsmForMaskedLM
]
]
[
'
convbert
'
[
'
ConvBertForMaskedLM
'
ConvBertForMaskedLM
]
]
[
'
camembert
'
[
'
CamembertForMaskedLM
'
CamembertForMaskedLM
]
]
[
'
deberta
'
[
'
DebertaForMaskedLM
'
DebertaForMaskedLM
]
]
[
'
deberta
-
v2
'
[
'
DebertaV2ForMaskedLM
'
DebertaV2ForMaskedLM
]
]
[
'
mpnet
'
[
'
MPNetForMaskedLM
'
MPNetForMaskedLM
]
]
[
'
albert
'
[
'
AlbertForMaskedLM
'
AlbertForMaskedLM
]
]
[
'
distilbert
'
[
'
DistilBertForMaskedLM
'
DistilBertForMaskedLM
]
]
[
'
roberta
'
[
'
RobertaForMaskedLM
'
RobertaForMaskedLM
]
]
[
'
xlm
'
[
'
XLMWithLMHeadModel
'
XLMWithLMHeadModel
]
]
[
'
xlm
-
roberta
'
[
'
XLMRobertaForMaskedLM
'
XLMRobertaForMaskedLM
]
]
[
'
mobilebert
'
[
'
MobileBertForMaskedLM
'
MobileBertForMaskedLM
]
]
[
'
squeezebert
'
[
'
SqueezeBertForMaskedLM
'
SqueezeBertForMaskedLM
]
]
]
)
;
const
MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES
=
new
Map
(
[
[
'
bert
'
[
'
BertForQuestionAnswering
'
BertForQuestionAnswering
]
]
[
'
roformer
'
[
'
RoFormerForQuestionAnswering
'
RoFormerForQuestionAnswering
]
]
[
'
electra
'
[
'
ElectraForQuestionAnswering
'
ElectraForQuestionAnswering
]
]
[
'
convbert
'
[
'
ConvBertForQuestionAnswering
'
ConvBertForQuestionAnswering
]
]
[
'
camembert
'
[
'
CamembertForQuestionAnswering
'
CamembertForQuestionAnswering
]
]
[
'
deberta
'
[
'
DebertaForQuestionAnswering
'
DebertaForQuestionAnswering
]
]
[
'
deberta
-
v2
'
[
'
DebertaV2ForQuestionAnswering
'
DebertaV2ForQuestionAnswering
]
]
[
'
mpnet
'
[
'
MPNetForQuestionAnswering
'
MPNetForQuestionAnswering
]
]
[
'
albert
'
[
'
AlbertForQuestionAnswering
'
AlbertForQuestionAnswering
]
]
[
'
distilbert
'
[
'
DistilBertForQuestionAnswering
'
DistilBertForQuestionAnswering
]
]
[
'
roberta
'
[
'
RobertaForQuestionAnswering
'
RobertaForQuestionAnswering
]
]
[
'
xlm
'
[
'
XLMForQuestionAnswering
'
XLMForQuestionAnswering
]
]
[
'
xlm
-
roberta
'
[
'
XLMRobertaForQuestionAnswering
'
XLMRobertaForQuestionAnswering
]
]
[
'
mobilebert
'
[
'
MobileBertForQuestionAnswering
'
MobileBertForQuestionAnswering
]
]
[
'
squeezebert
'
[
'
SqueezeBertForQuestionAnswering
'
SqueezeBertForQuestionAnswering
]
]
]
)
;
const
MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES
=
new
Map
(
[
[
'
vision
-
encoder
-
decoder
'
[
'
VisionEncoderDecoderModel
'
VisionEncoderDecoderModel
]
]
]
)
;
const
MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES
=
new
Map
(
[
[
'
vision
-
encoder
-
decoder
'
[
'
VisionEncoderDecoderModel
'
VisionEncoderDecoderModel
]
]
]
)
;
const
MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES
=
new
Map
(
[
[
'
vit
'
[
'
ViTForImageClassification
'
ViTForImageClassification
]
]
[
'
mobilevit
'
[
'
MobileViTForImageClassification
'
MobileViTForImageClassification
]
]
[
'
beit
'
[
'
BeitForImageClassification
'
BeitForImageClassification
]
]
[
'
deit
'
[
'
DeiTForImageClassification
'
DeiTForImageClassification
]
]
[
'
convnext
'
[
'
ConvNextForImageClassification
'
ConvNextForImageClassification
]
]
[
'
convnextv2
'
[
'
ConvNextV2ForImageClassification
'
ConvNextV2ForImageClassification
]
]
[
'
dinov2
'
[
'
Dinov2ForImageClassification
'
Dinov2ForImageClassification
]
]
[
'
resnet
'
[
'
ResNetForImageClassification
'
ResNetForImageClassification
]
]
[
'
swin
'
[
'
SwinForImageClassification
'
SwinForImageClassification
]
]
[
'
segformer
'
[
'
SegformerForImageClassification
'
SegformerForImageClassification
]
]
[
'
efficientnet
'
[
'
EfficientNetForImageClassification
'
EfficientNetForImageClassification
]
]
]
)
;
const
MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES
=
new
Map
(
[
[
'
detr
'
[
'
DetrForObjectDetection
'
DetrForObjectDetection
]
]
[
'
table
-
transformer
'
[
'
TableTransformerForObjectDetection
'
TableTransformerForObjectDetection
]
]
[
'
yolos
'
[
'
YolosForObjectDetection
'
YolosForObjectDetection
]
]
]
)
;
const
MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES
=
new
Map
(
[
[
'
owlvit
'
[
'
OwlViTForObjectDetection
'
OwlViTForObjectDetection
]
]
[
'
owlv2
'
[
'
Owlv2ForObjectDetection
'
Owlv2ForObjectDetection
]
]
]
)
;
const
MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES
=
new
Map
(
[
[
'
detr
'
[
'
DetrForSegmentation
'
DetrForSegmentation
]
]
[
'
clipseg
'
[
'
CLIPSegForImageSegmentation
'
CLIPSegForImageSegmentation
]
]
]
)
;
const
MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES
=
new
Map
(
[
[
'
segformer
'
[
'
SegformerForSemanticSegmentation
'
SegformerForSemanticSegmentation
]
]
]
)
;
const
MODEL_FOR_MASK_GENERATION_MAPPING_NAMES
=
new
Map
(
[
[
'
sam
'
[
'
SamModel
'
SamModel
]
]
]
)
;
const
MODEL_FOR_CTC_MAPPING_NAMES
=
new
Map
(
[
[
'
wav2vec2
'
[
'
Wav2Vec2ForCTC
'
Wav2Vec2ForCTC
]
]
[
'
wav2vec2
-
bert
'
[
'
Wav2Vec2BertForCTC
'
Wav2Vec2BertForCTC
]
]
[
'
unispeech
'
[
'
UniSpeechForCTC
'
UniSpeechForCTC
]
]
[
'
unispeech
-
sat
'
[
'
UniSpeechSatForCTC
'
UniSpeechSatForCTC
]
]
[
'
wavlm
'
[
'
WavLMForCTC
'
WavLMForCTC
]
]
[
'
hubert
'
[
'
HubertForCTC
'
HubertForCTC
]
]
]
)
;
const
MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES
=
new
Map
(
[
[
'
wav2vec2
'
[
'
Wav2Vec2ForSequenceClassification
'
Wav2Vec2ForSequenceClassification
]
]
[
'
wav2vec2
-
bert
'
[
'
Wav2Vec2BertForSequenceClassification
'
Wav2Vec2BertForSequenceClassification
]
]
[
'
unispeech
'
[
'
UniSpeechForSequenceClassification
'
UniSpeechForSequenceClassification
]
]
[
'
unispeech
-
sat
'
[
'
UniSpeechSatForSequenceClassification
'
UniSpeechSatForSequenceClassification
]
]
[
'
wavlm
'
[
'
WavLMForSequenceClassification
'
WavLMForSequenceClassification
]
]
[
'
hubert
'
[
'
HubertForSequenceClassification
'
HubertForSequenceClassification
]
]
[
'
audio
-
spectrogram
-
transformer
'
[
'
ASTForAudioClassification
'
ASTForAudioClassification
]
]
]
)
;
const
MODEL_FOR_AUDIO_XVECTOR_MAPPING_NAMES
=
new
Map
(
[
[
'
wavlm
'
[
'
WavLMForXVector
'
WavLMForXVector
]
]
]
)
;
const
MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING_NAMES
=
new
Map
(
[
[
'
unispeech
-
sat
'
[
'
UniSpeechSatForAudioFrameClassification
'
UniSpeechSatForAudioFrameClassification
]
]
[
'
wavlm
'
[
'
WavLMForAudioFrameClassification
'
WavLMForAudioFrameClassification
]
]
[
'
wav2vec2
'
[
'
Wav2Vec2ForAudioFrameClassification
'
Wav2Vec2ForAudioFrameClassification
]
]
]
)
;
const
MODEL_FOR_IMAGE_MATTING_MAPPING_NAMES
=
new
Map
(
[
[
'
vitmatte
'
[
'
VitMatteForImageMatting
'
VitMatteForImageMatting
]
]
]
)
;
const
MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES
=
new
Map
(
[
[
'
swin2sr
'
[
'
Swin2SRForImageSuperResolution
'
Swin2SRForImageSuperResolution
]
]
]
)
const
MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES
=
new
Map
(
[
[
'
dpt
'
[
'
DPTForDepthEstimation
'
DPTForDepthEstimation
]
]
[
'
depth_anything
'
[
'
DepthAnythingForDepthEstimation
'
DepthAnythingForDepthEstimation
]
]
[
'
glpn
'
[
'
GLPNForDepthEstimation
'
GLPNForDepthEstimation
]
]
]
)
const
MODEL_FOR_IMAGE_FEATURE_EXTRACTION_MAPPING_NAMES
=
new
Map
(
[
[
'
clip
'
[
'
CLIPVisionModelWithProjection
'
CLIPVisionModelWithProjection
]
]
[
'
siglip
'
[
'
SiglipVisionModel
'
SiglipVisionModel
]
]
]
)
const
MODEL_CLASS_TYPE_MAPPING
=
[
[
MODEL_MAPPING_NAMES_ENCODER_ONLY
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_MAPPING_NAMES_ENCODER_DECODER
MODEL_TYPES
.
EncoderDecoder
]
[
MODEL_MAPPING_NAMES_DECODER_ONLY
MODEL_TYPES
.
DecoderOnly
]
[
MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES
MODEL_TYPES
.
Seq2Seq
]
[
MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES
MODEL_TYPES
.
Seq2Seq
]
[
MODEL_WITH_LM_HEAD_MAPPING_NAMES
MODEL_TYPES
.
DecoderOnly
]
[
MODEL_FOR_MASKED_LM_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES
MODEL_TYPES
.
Vision2Seq
]
[
MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_IMAGE_MATTING_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_MASK_GENERATION_MAPPING_NAMES
MODEL_TYPES
.
MaskGeneration
]
[
MODEL_FOR_CTC_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES
MODEL_TYPES
.
Seq2Seq
]
[
MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_AUDIO_XVECTOR_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
[
MODEL_FOR_IMAGE_FEATURE_EXTRACTION_MAPPING_NAMES
MODEL_TYPES
.
EncoderOnly
]
]
;
for
(
const
[
mappings
type
]
of
MODEL_CLASS_TYPE_MAPPING
)
{
for
(
const
[
name
model
]
of
mappings
.
values
(
)
)
{
MODEL_TYPE_MAPPING
.
set
(
name
type
)
;
MODEL_CLASS_TO_NAME_MAPPING
.
set
(
model
name
)
;
MODEL_NAME_TO_CLASS_MAPPING
.
set
(
name
model
)
;
}
}
const
CUSTOM_MAPPING
=
[
[
'
CLIPTextModelWithProjection
'
CLIPTextModelWithProjection
MODEL_TYPES
.
EncoderOnly
]
[
'
SiglipTextModel
'
SiglipTextModel
MODEL_TYPES
.
EncoderOnly
]
[
'
ClapTextModelWithProjection
'
ClapTextModelWithProjection
MODEL_TYPES
.
EncoderOnly
]
[
'
ClapAudioModelWithProjection
'
ClapAudioModelWithProjection
MODEL_TYPES
.
EncoderOnly
]
]
for
(
const
[
name
model
type
]
of
CUSTOM_MAPPING
)
{
MODEL_TYPE_MAPPING
.
set
(
name
type
)
;
MODEL_CLASS_TO_NAME_MAPPING
.
set
(
model
name
)
;
MODEL_NAME_TO_CLASS_MAPPING
.
set
(
name
model
)
;
}
class
AutoModel
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
MODEL_CLASS_TYPE_MAPPING
.
map
(
x
=
>
x
[
0
]
)
;
static
BASE_IF_FAIL
=
true
;
}
class
AutoModelForSequenceClassification
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES
]
;
}
class
AutoModelForTokenClassification
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES
]
;
}
class
AutoModelForSeq2SeqLM
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES
]
;
}
class
AutoModelForSpeechSeq2Seq
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES
]
;
}
class
AutoModelForTextToSpectrogram
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES
]
;
}
class
AutoModelForTextToWaveform
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING_NAMES
]
;
}
class
AutoModelForCausalLM
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_WITH_LM_HEAD_MAPPING_NAMES
]
;
}
class
AutoModelForMaskedLM
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_MASKED_LM_MAPPING_NAMES
]
;
}
class
AutoModelForQuestionAnswering
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES
]
;
}
class
AutoModelForVision2Seq
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES
]
;
}
class
AutoModelForImageClassification
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES
]
;
}
class
AutoModelForImageSegmentation
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES
]
;
}
class
AutoModelForSemanticSegmentation
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES
]
;
}
class
AutoModelForObjectDetection
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES
]
;
}
class
AutoModelForZeroShotObjectDetection
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES
]
;
}
class
AutoModelForMaskGeneration
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_MASK_GENERATION_MAPPING_NAMES
]
;
}
class
AutoModelForCTC
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_CTC_MAPPING_NAMES
]
;
}
class
AutoModelForAudioClassification
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES
]
;
}
class
AutoModelForXVector
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_AUDIO_XVECTOR_MAPPING_NAMES
]
;
}
class
AutoModelForAudioFrameClassification
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING_NAMES
]
;
}
class
AutoModelForDocumentQuestionAnswering
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES
]
;
}
class
AutoModelForImageMatting
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_IMAGE_MATTING_MAPPING_NAMES
]
;
}
class
AutoModelForImageToImage
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES
]
;
}
class
AutoModelForDepthEstimation
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES
]
;
}
class
AutoModelForImageFeatureExtraction
extends
PretrainedMixin
{
static
MODEL_CLASS_MAPPINGS
=
[
MODEL_FOR_IMAGE_FEATURE_EXTRACTION_MAPPING_NAMES
]
;
}
class
Seq2SeqLMOutput
extends
ModelOutput
{
constructor
(
{
logits
past_key_values
encoder_outputs
decoder_attentions
=
null
cross_attentions
=
null
}
)
{
super
(
)
;
this
.
logits
=
logits
;
this
.
past_key_values
=
past_key_values
;
this
.
encoder_outputs
=
encoder_outputs
;
this
.
decoder_attentions
=
decoder_attentions
;
this
.
cross_attentions
=
cross_attentions
;
}
}
class
SequenceClassifierOutput
extends
ModelOutput
{
constructor
(
{
logits
}
)
{
super
(
)
;
this
.
logits
=
logits
;
}
}
class
XVectorOutput
extends
ModelOutput
{
constructor
(
{
logits
embeddings
}
)
{
super
(
)
;
this
.
logits
=
logits
;
this
.
embeddings
=
embeddings
;
}
}
class
TokenClassifierOutput
extends
ModelOutput
{
constructor
(
{
logits
}
)
{
super
(
)
;
this
.
logits
=
logits
;
}
}
class
MaskedLMOutput
extends
ModelOutput
{
constructor
(
{
logits
}
)
{
super
(
)
;
this
.
logits
=
logits
;
}
}
class
QuestionAnsweringModelOutput
extends
ModelOutput
{
constructor
(
{
start_logits
end_logits
}
)
{
super
(
)
;
this
.
start_logits
=
start_logits
;
this
.
end_logits
=
end_logits
;
}
}
class
CausalLMOutput
extends
ModelOutput
{
constructor
(
{
logits
}
)
{
super
(
)
;
this
.
logits
=
logits
;
}
}
class
CausalLMOutputWithPast
extends
ModelOutput
{
constructor
(
{
logits
past_key_values
}
)
{
super
(
)
;
this
.
logits
=
logits
;
this
.
past_key_values
=
past_key_values
;
}
}
class
ImageMattingOutput
extends
ModelOutput
{
constructor
(
{
alphas
}
)
{
super
(
)
;
this
.
alphas
=
alphas
;
}
}
class
VitsModelOutput
extends
ModelOutput
{
constructor
(
{
waveform
spectrogram
}
)
{
super
(
)
;
this
.
waveform
=
waveform
;
this
.
spectrogram
=
spectrogram
;
}
}
}
)
"
.
/
src
/
pipelines
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
AudioClassificationPipeline
"
:
(
)
=
>
(
AudioClassificationPipeline
)
"
AutomaticSpeechRecognitionPipeline
"
:
(
)
=
>
(
AutomaticSpeechRecognitionPipeline
)
"
DepthEstimationPipeline
"
:
(
)
=
>
(
DepthEstimationPipeline
)
"
DocumentQuestionAnsweringPipeline
"
:
(
)
=
>
(
DocumentQuestionAnsweringPipeline
)
"
FeatureExtractionPipeline
"
:
(
)
=
>
(
FeatureExtractionPipeline
)
"
FillMaskPipeline
"
:
(
)
=
>
(
FillMaskPipeline
)
"
ImageClassificationPipeline
"
:
(
)
=
>
(
ImageClassificationPipeline
)
"
ImageFeatureExtractionPipeline
"
:
(
)
=
>
(
ImageFeatureExtractionPipeline
)
"
ImageSegmentationPipeline
"
:
(
)
=
>
(
ImageSegmentationPipeline
)
"
ImageToImagePipeline
"
:
(
)
=
>
(
ImageToImagePipeline
)
"
ImageToTextPipeline
"
:
(
)
=
>
(
ImageToTextPipeline
)
"
ObjectDetectionPipeline
"
:
(
)
=
>
(
ObjectDetectionPipeline
)
"
Pipeline
"
:
(
)
=
>
(
Pipeline
)
"
QuestionAnsweringPipeline
"
:
(
)
=
>
(
QuestionAnsweringPipeline
)
"
SummarizationPipeline
"
:
(
)
=
>
(
SummarizationPipeline
)
"
Text2TextGenerationPipeline
"
:
(
)
=
>
(
Text2TextGenerationPipeline
)
"
TextClassificationPipeline
"
:
(
)
=
>
(
TextClassificationPipeline
)
"
TextGenerationPipeline
"
:
(
)
=
>
(
TextGenerationPipeline
)
"
TextToAudioPipeline
"
:
(
)
=
>
(
TextToAudioPipeline
)
"
TokenClassificationPipeline
"
:
(
)
=
>
(
TokenClassificationPipeline
)
"
TranslationPipeline
"
:
(
)
=
>
(
TranslationPipeline
)
"
ZeroShotAudioClassificationPipeline
"
:
(
)
=
>
(
ZeroShotAudioClassificationPipeline
)
"
ZeroShotClassificationPipeline
"
:
(
)
=
>
(
ZeroShotClassificationPipeline
)
"
ZeroShotImageClassificationPipeline
"
:
(
)
=
>
(
ZeroShotImageClassificationPipeline
)
"
ZeroShotObjectDetectionPipeline
"
:
(
)
=
>
(
ZeroShotObjectDetectionPipeline
)
"
pipeline
"
:
(
)
=
>
(
pipeline
)
}
)
;
var
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
=
__webpack_require__
(
"
.
/
src
/
tokenizers
.
js
"
)
;
var
_models_js__WEBPACK_IMPORTED_MODULE_1__
=
__webpack_require__
(
"
.
/
src
/
models
.
js
"
)
;
var
_processors_js__WEBPACK_IMPORTED_MODULE_2__
=
__webpack_require__
(
"
.
/
src
/
processors
.
js
"
)
;
var
_utils_core_js__WEBPACK_IMPORTED_MODULE_3__
=
__webpack_require__
(
"
.
/
src
/
utils
/
core
.
js
"
)
;
var
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
=
__webpack_require__
(
"
.
/
src
/
utils
/
maths
.
js
"
)
;
var
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
=
__webpack_require__
(
"
.
/
src
/
utils
/
audio
.
js
"
)
;
var
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_6__
=
__webpack_require__
(
"
.
/
src
/
utils
/
tensor
.
js
"
)
;
var
_utils_image_js__WEBPACK_IMPORTED_MODULE_7__
=
__webpack_require__
(
"
.
/
src
/
utils
/
image
.
js
"
)
;
async
function
prepareImages
(
images
)
{
if
(
!
Array
.
isArray
(
images
)
)
{
images
=
[
images
]
;
}
return
await
Promise
.
all
(
images
.
map
(
x
=
>
_utils_image_js__WEBPACK_IMPORTED_MODULE_7__
.
RawImage
.
read
(
x
)
)
)
;
}
async
function
prepareAudios
(
audios
sampling_rate
)
{
if
(
!
Array
.
isArray
(
audios
)
)
{
audios
=
[
audios
]
;
}
return
await
Promise
.
all
(
audios
.
map
(
x
=
>
{
if
(
typeof
x
=
=
=
'
string
'
|
|
x
instanceof
URL
)
{
return
(
0
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
.
read_audio
)
(
x
sampling_rate
)
;
}
else
if
(
x
instanceof
Float64Array
)
{
return
new
Float32Array
(
x
)
;
}
return
x
;
}
)
)
;
}
function
get_bounding_box
(
box
asInteger
)
{
if
(
asInteger
)
{
box
=
box
.
map
(
x
=
>
x
|
0
)
;
}
const
[
xmin
ymin
xmax
ymax
]
=
box
;
return
{
xmin
ymin
xmax
ymax
}
;
}
class
Pipeline
extends
_utils_core_js__WEBPACK_IMPORTED_MODULE_3__
.
Callable
{
constructor
(
{
task
model
tokenizer
=
null
processor
=
null
}
)
{
super
(
)
;
this
.
task
=
task
;
this
.
model
=
model
;
this
.
tokenizer
=
tokenizer
;
this
.
processor
=
processor
;
}
async
dispose
(
)
{
await
this
.
model
.
dispose
(
)
;
}
}
class
TextClassificationPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
texts
{
topk
=
1
}
=
{
}
)
{
const
model_inputs
=
this
.
tokenizer
(
texts
{
padding
:
true
truncation
:
true
}
)
;
const
outputs
=
await
this
.
model
(
model_inputs
)
const
function_to_apply
=
this
.
model
.
config
.
problem_type
=
=
=
'
multi_label_classification
'
?
batch
=
>
batch
.
sigmoid
(
)
.
data
:
batch
=
>
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
softmax
)
(
batch
.
data
)
;
const
id2label
=
this
.
model
.
config
.
id2label
;
const
toReturn
=
[
]
;
for
(
const
batch
of
outputs
.
logits
)
{
const
output
=
function_to_apply
(
batch
)
;
const
scores
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
getTopItems
)
(
output
topk
)
;
const
vals
=
scores
.
map
(
x
=
>
(
{
label
:
id2label
[
x
[
0
]
]
score
:
x
[
1
]
}
)
)
;
if
(
topk
=
=
=
1
)
{
toReturn
.
push
(
.
.
.
vals
)
;
}
else
{
toReturn
.
push
(
vals
)
;
}
}
return
Array
.
isArray
(
texts
)
|
|
topk
=
=
=
1
?
(
toReturn
)
:
(
toReturn
)
[
0
]
;
}
}
class
TokenClassificationPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
texts
{
ignore_labels
=
[
'
O
'
]
}
=
{
}
)
{
const
isBatched
=
Array
.
isArray
(
texts
)
;
const
model_inputs
=
this
.
tokenizer
(
isBatched
?
texts
:
[
texts
]
{
padding
:
true
truncation
:
true
}
)
;
const
outputs
=
await
this
.
model
(
model_inputs
)
const
logits
=
outputs
.
logits
;
const
id2label
=
this
.
model
.
config
.
id2label
;
const
toReturn
=
[
]
;
for
(
let
i
=
0
;
i
<
logits
.
dims
[
0
]
;
+
+
i
)
{
const
ids
=
model_inputs
.
input_ids
[
i
]
;
const
batch
=
logits
[
i
]
;
const
tokens
=
[
]
;
for
(
let
j
=
0
;
j
<
batch
.
dims
[
0
]
;
+
+
j
)
{
const
tokenData
=
batch
[
j
]
;
const
topScoreIndex
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
max
)
(
tokenData
.
data
)
[
1
]
;
const
entity
=
id2label
?
id2label
[
topScoreIndex
]
:
LABEL_
{
topScoreIndex
}
;
if
(
ignore_labels
.
includes
(
entity
)
)
{
continue
;
}
const
word
=
this
.
tokenizer
.
decode
(
[
ids
[
j
]
.
item
(
)
]
{
skip_special_tokens
:
true
}
)
;
if
(
word
=
=
=
'
'
)
{
continue
;
}
const
scores
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
softmax
)
(
tokenData
.
data
)
;
tokens
.
push
(
{
entity
:
entity
score
:
scores
[
topScoreIndex
]
index
:
j
word
:
word
start
:
null
end
:
null
}
)
;
}
toReturn
.
push
(
tokens
)
;
}
return
isBatched
?
toReturn
:
toReturn
[
0
]
;
}
}
class
QuestionAnsweringPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
question
context
{
topk
=
1
}
=
{
}
)
{
const
inputs
=
this
.
tokenizer
(
question
{
text_pair
:
context
padding
:
true
truncation
:
true
}
)
;
const
output
=
await
this
.
model
(
inputs
)
;
const
toReturn
=
[
]
;
for
(
let
j
=
0
;
j
<
output
.
start_logits
.
dims
[
0
]
;
+
+
j
)
{
const
ids
=
inputs
.
input_ids
[
j
]
;
const
sepIndex
=
ids
.
indexOf
(
this
.
tokenizer
.
sep_token_id
)
;
const
s1
=
Array
.
from
(
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
softmax
)
(
output
.
start_logits
[
j
]
.
data
)
)
.
map
(
(
x
i
)
=
>
[
x
i
]
)
.
filter
(
x
=
>
x
[
1
]
>
sepIndex
)
;
const
e1
=
Array
.
from
(
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
softmax
)
(
output
.
end_logits
[
j
]
.
data
)
)
.
map
(
(
x
i
)
=
>
[
x
i
]
)
.
filter
(
x
=
>
x
[
1
]
>
sepIndex
)
;
const
options
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_3__
.
product
)
(
s1
e1
)
.
filter
(
x
=
>
x
[
0
]
[
1
]
<
=
x
[
1
]
[
1
]
)
.
map
(
x
=
>
[
x
[
0
]
[
1
]
x
[
1
]
[
1
]
x
[
0
]
[
0
]
*
x
[
1
]
[
0
]
]
)
.
sort
(
(
a
b
)
=
>
b
[
2
]
-
a
[
2
]
)
;
for
(
let
k
=
0
;
k
<
Math
.
min
(
options
.
length
topk
)
;
+
+
k
)
{
const
[
start
end
score
]
=
options
[
k
]
;
const
answer_tokens
=
[
.
.
.
ids
]
.
slice
(
start
end
+
1
)
const
answer
=
this
.
tokenizer
.
decode
(
answer_tokens
{
skip_special_tokens
:
true
}
)
;
toReturn
.
push
(
{
answer
score
}
)
;
}
}
return
(
topk
=
=
=
1
)
?
toReturn
[
0
]
:
toReturn
;
}
}
class
FillMaskPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
texts
{
topk
=
5
}
=
{
}
)
{
const
model_inputs
=
this
.
tokenizer
(
texts
{
padding
:
true
truncation
:
true
}
)
;
const
outputs
=
await
this
.
model
(
model_inputs
)
const
toReturn
=
[
]
;
for
(
let
i
=
0
;
i
<
model_inputs
.
input_ids
.
dims
[
0
]
;
+
+
i
)
{
const
ids
=
model_inputs
.
input_ids
[
i
]
;
const
mask_token_index
=
ids
.
indexOf
(
this
.
tokenizer
.
mask_token_id
)
if
(
mask_token_index
=
=
=
-
1
)
{
throw
Error
(
Mask
token
(
{
this
.
tokenizer
.
mask_token
}
)
not
found
in
text
.
)
}
const
logits
=
outputs
.
logits
[
i
]
;
const
itemLogits
=
logits
[
mask_token_index
]
;
const
scores
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
getTopItems
)
(
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
softmax
)
(
itemLogits
.
data
)
topk
)
;
toReturn
.
push
(
scores
.
map
(
x
=
>
{
const
sequence
=
[
.
.
.
ids
]
;
sequence
[
mask_token_index
]
=
x
[
0
]
;
return
{
score
:
x
[
1
]
token
:
x
[
0
]
token_str
:
this
.
tokenizer
.
model
.
vocab
[
x
[
0
]
]
sequence
:
this
.
tokenizer
.
decode
(
sequence
{
skip_special_tokens
:
true
}
)
}
}
)
)
;
}
return
Array
.
isArray
(
texts
)
?
toReturn
:
toReturn
[
0
]
;
}
}
class
Text2TextGenerationPipeline
extends
(
(
Pipeline
)
)
{
_key
=
'
generated_text
'
;
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
texts
generate_kwargs
=
{
}
)
{
if
(
!
Array
.
isArray
(
texts
)
)
{
texts
=
[
texts
]
;
}
if
(
this
.
model
.
config
.
prefix
)
{
texts
=
texts
.
map
(
x
=
>
this
.
model
.
config
.
prefix
+
x
)
}
const
task_specific_params
=
this
.
model
.
config
.
task_specific_params
if
(
task_specific_params
&
&
task_specific_params
[
this
.
task
]
)
{
if
(
task_specific_params
[
this
.
task
]
.
prefix
)
{
texts
=
texts
.
map
(
x
=
>
task_specific_params
[
this
.
task
]
.
prefix
+
x
)
}
}
const
tokenizer
=
this
.
tokenizer
;
const
tokenizer_options
=
{
padding
:
true
truncation
:
true
}
let
input_ids
;
if
(
this
instanceof
TranslationPipeline
&
&
'
_build_translation_inputs
'
in
tokenizer
)
{
input_ids
=
tokenizer
.
_build_translation_inputs
(
texts
tokenizer_options
generate_kwargs
)
.
input_ids
;
}
else
{
input_ids
=
tokenizer
(
texts
tokenizer_options
)
.
input_ids
;
}
const
outputTokenIds
=
await
this
.
model
.
generate
(
input_ids
generate_kwargs
)
;
return
tokenizer
.
batch_decode
(
outputTokenIds
{
skip_special_tokens
:
true
}
)
.
map
(
text
=
>
(
{
[
this
.
_key
]
:
text
}
)
)
;
}
}
class
SummarizationPipeline
extends
(
(
(
Text2TextGenerationPipeline
)
)
)
{
_key
=
'
summary_text
'
;
constructor
(
options
)
{
super
(
options
)
;
}
}
class
TranslationPipeline
extends
(
(
(
Text2TextGenerationPipeline
)
)
)
{
_key
=
'
translation_text
'
;
constructor
(
options
)
{
super
(
options
)
;
}
}
class
TextGenerationPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
texts
generate_kwargs
=
{
}
)
{
const
isBatched
=
Array
.
isArray
(
texts
)
;
if
(
!
isBatched
)
{
texts
=
[
(
texts
)
]
;
}
const
add_special_tokens
=
generate_kwargs
.
add_special_tokens
?
?
false
;
this
.
tokenizer
.
padding_side
=
'
left
'
;
const
{
input_ids
attention_mask
}
=
this
.
tokenizer
(
texts
{
add_special_tokens
padding
:
true
truncation
:
true
}
)
;
const
outputTokenIds
=
await
this
.
model
.
generate
(
input_ids
generate_kwargs
null
{
inputs_attention_mask
:
attention_mask
}
)
;
const
decoded
=
this
.
tokenizer
.
batch_decode
(
outputTokenIds
{
skip_special_tokens
:
true
}
)
;
const
toReturn
=
Array
.
from
(
{
length
:
texts
.
length
}
_
=
>
[
]
)
;
for
(
let
i
=
0
;
i
<
decoded
.
length
;
+
+
i
)
{
const
textIndex
=
Math
.
floor
(
i
/
outputTokenIds
.
length
*
texts
.
length
)
;
toReturn
[
textIndex
]
.
push
(
{
generated_text
:
decoded
[
i
]
}
)
;
}
return
(
!
isBatched
&
&
toReturn
.
length
=
=
=
1
)
?
toReturn
[
0
]
:
toReturn
;
}
}
class
ZeroShotClassificationPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
this
.
label2id
=
Object
.
fromEntries
(
Object
.
entries
(
(
(
this
)
.
model
)
.
config
.
label2id
)
.
map
(
(
[
k
v
]
)
=
>
[
k
.
toLowerCase
(
)
v
]
)
)
;
this
.
entailment_id
=
this
.
label2id
[
'
entailment
'
]
;
if
(
this
.
entailment_id
=
=
=
undefined
)
{
console
.
warn
(
"
Could
not
find
'
entailment
'
in
label2id
mapping
.
Using
2
as
entailment_id
.
"
)
;
this
.
entailment_id
=
2
;
}
this
.
contradiction_id
=
this
.
label2id
[
'
contradiction
'
]
?
?
this
.
label2id
[
'
not_entailment
'
]
;
if
(
this
.
contradiction_id
=
=
=
undefined
)
{
console
.
warn
(
"
Could
not
find
'
contradiction
'
in
label2id
mapping
.
Using
0
as
contradiction_id
.
"
)
;
this
.
contradiction_id
=
0
;
}
}
async
_call
(
texts
candidate_labels
{
hypothesis_template
=
"
This
example
is
{
}
.
"
multi_label
=
false
}
=
{
}
)
{
const
isBatched
=
Array
.
isArray
(
texts
)
;
if
(
!
isBatched
)
{
texts
=
[
(
texts
)
]
;
}
if
(
!
Array
.
isArray
(
candidate_labels
)
)
{
candidate_labels
=
[
candidate_labels
]
;
}
const
hypotheses
=
candidate_labels
.
map
(
x
=
>
hypothesis_template
.
replace
(
'
{
}
'
x
)
)
;
const
softmaxEach
=
multi_label
|
|
candidate_labels
.
length
=
=
=
1
;
const
toReturn
=
[
]
;
for
(
const
premise
of
texts
)
{
const
entails_logits
=
[
]
;
for
(
const
hypothesis
of
hypotheses
)
{
const
inputs
=
this
.
tokenizer
(
premise
{
text_pair
:
hypothesis
padding
:
true
truncation
:
true
}
)
const
outputs
=
await
this
.
model
(
inputs
)
if
(
softmaxEach
)
{
entails_logits
.
push
(
[
outputs
.
logits
.
data
[
this
.
contradiction_id
]
outputs
.
logits
.
data
[
this
.
entailment_id
]
]
)
}
else
{
entails_logits
.
push
(
outputs
.
logits
.
data
[
this
.
entailment_id
]
)
}
}
const
scores
=
softmaxEach
?
entails_logits
.
map
(
x
=
>
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
softmax
)
(
x
)
[
1
]
)
:
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
softmax
)
(
entails_logits
)
;
const
scores_sorted
=
scores
.
map
(
(
x
i
)
=
>
[
x
i
]
)
.
sort
(
(
a
b
)
=
>
(
b
[
0
]
-
a
[
0
]
)
)
;
toReturn
.
push
(
{
sequence
:
premise
labels
:
scores_sorted
.
map
(
x
=
>
candidate_labels
[
x
[
1
]
]
)
scores
:
scores_sorted
.
map
(
x
=
>
x
[
0
]
)
}
)
;
}
return
isBatched
?
toReturn
:
toReturn
[
0
]
;
}
}
class
FeatureExtractionPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
texts
{
pooling
=
(
'
none
'
)
normalize
=
false
}
=
{
}
)
{
const
model_inputs
=
this
.
tokenizer
(
texts
{
padding
:
true
truncation
:
true
}
)
;
const
outputs
=
await
this
.
model
(
model_inputs
)
let
result
=
outputs
.
last_hidden_state
?
?
outputs
.
logits
;
if
(
pooling
=
=
=
'
none
'
)
{
}
else
if
(
pooling
=
=
=
'
mean
'
)
{
result
=
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_6__
.
mean_pooling
)
(
result
model_inputs
.
attention_mask
)
;
}
else
if
(
pooling
=
=
=
'
cls
'
)
{
result
=
result
.
slice
(
null
0
)
;
}
else
{
throw
Error
(
Pooling
method
'
{
pooling
}
'
not
supported
.
)
;
}
if
(
normalize
)
{
result
=
result
.
normalize
(
2
-
1
)
;
}
return
result
;
}
}
class
ImageFeatureExtractionPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
images
{
pool
=
null
}
=
{
}
)
{
const
preparedImages
=
await
prepareImages
(
images
)
;
const
{
pixel_values
}
=
await
this
.
processor
(
preparedImages
)
;
const
outputs
=
await
this
.
model
(
{
pixel_values
}
)
;
let
result
;
if
(
pool
)
{
if
(
!
(
'
pooler_output
'
in
outputs
)
)
{
throw
Error
(
No
pooled
output
was
returned
.
Make
sure
the
model
has
a
'
pooler
'
layer
when
using
the
'
pool
'
option
.
)
;
}
result
=
outputs
.
pooler_output
;
}
else
{
result
=
outputs
.
last_hidden_state
?
?
outputs
.
logits
?
?
outputs
.
image_embeds
;
}
return
result
;
}
}
class
AudioClassificationPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
audio
{
topk
=
null
}
=
{
}
)
{
const
single
=
!
Array
.
isArray
(
audio
)
;
const
sampling_rate
=
this
.
processor
.
feature_extractor
.
config
.
sampling_rate
;
const
preparedAudios
=
await
prepareAudios
(
audio
sampling_rate
)
;
const
id2label
=
this
.
model
.
config
.
id2label
;
const
toReturn
=
[
]
;
for
(
const
aud
of
preparedAudios
)
{
const
inputs
=
await
this
.
processor
(
aud
)
;
const
output
=
await
this
.
model
(
inputs
)
;
const
logits
=
output
.
logits
[
0
]
;
const
scores
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
getTopItems
)
(
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
softmax
)
(
logits
.
data
)
topk
)
;
const
vals
=
scores
.
map
(
x
=
>
(
{
label
:
(
id2label
[
x
[
0
]
]
)
score
:
(
x
[
1
]
)
}
)
)
;
if
(
topk
=
=
=
1
)
{
toReturn
.
push
(
.
.
.
vals
)
;
}
else
{
toReturn
.
push
(
vals
)
;
}
}
return
!
single
|
|
topk
=
=
=
1
?
(
toReturn
)
:
(
toReturn
)
[
0
]
;
}
}
class
ZeroShotAudioClassificationPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
audio
candidate_labels
{
hypothesis_template
=
"
This
is
a
sound
of
{
}
.
"
}
=
{
}
)
{
const
single
=
!
Array
.
isArray
(
audio
)
;
if
(
single
)
{
audio
=
[
(
audio
)
]
;
}
const
texts
=
candidate_labels
.
map
(
x
=
>
hypothesis_template
.
replace
(
'
{
}
'
x
)
)
;
const
text_inputs
=
this
.
tokenizer
(
texts
{
padding
:
true
truncation
:
true
}
)
;
const
sampling_rate
=
this
.
processor
.
feature_extractor
.
config
.
sampling_rate
;
const
preparedAudios
=
await
prepareAudios
(
audio
sampling_rate
)
;
const
toReturn
=
[
]
;
for
(
const
aud
of
preparedAudios
)
{
const
audio_inputs
=
await
this
.
processor
(
aud
)
;
const
output
=
await
this
.
model
(
{
.
.
.
text_inputs
.
.
.
audio_inputs
}
)
;
const
probs
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
softmax
)
(
output
.
logits_per_audio
.
data
)
;
toReturn
.
push
(
[
.
.
.
probs
]
.
map
(
(
x
i
)
=
>
(
{
score
:
x
label
:
candidate_labels
[
i
]
}
)
)
)
;
}
return
single
?
toReturn
[
0
]
:
toReturn
;
}
}
class
AutomaticSpeechRecognitionPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
audio
kwargs
=
{
}
)
{
switch
(
this
.
model
.
config
.
model_type
)
{
case
'
whisper
'
:
return
this
.
_call_whisper
(
audio
kwargs
)
case
'
wav2vec2
'
:
case
'
wav2vec2
-
bert
'
:
case
'
unispeech
'
:
case
'
unispeech
-
sat
'
:
case
'
hubert
'
:
return
this
.
_call_wav2vec2
(
audio
kwargs
)
default
:
throw
new
Error
(
AutomaticSpeechRecognitionPipeline
does
not
support
model
type
'
{
this
.
model
.
config
.
model_type
}
'
.
)
}
}
async
_call_wav2vec2
(
audio
kwargs
=
{
}
)
{
if
(
kwargs
.
language
)
{
console
.
warn
(
'
language
parameter
is
not
yet
supported
for
wav2vec2
models
defaulting
to
"
English
"
.
'
)
;
}
if
(
kwargs
.
task
)
{
console
.
warn
(
'
task
parameter
is
not
yet
supported
for
wav2vec2
models
defaulting
to
"
transcribe
"
.
'
)
;
}
const
single
=
!
Array
.
isArray
(
audio
)
;
if
(
single
)
{
audio
=
[
(
audio
)
]
;
}
const
sampling_rate
=
this
.
processor
.
feature_extractor
.
config
.
sampling_rate
;
const
preparedAudios
=
await
prepareAudios
(
audio
sampling_rate
)
;
const
toReturn
=
[
]
;
for
(
const
aud
of
preparedAudios
)
{
const
inputs
=
await
this
.
processor
(
aud
)
;
const
output
=
await
this
.
model
(
inputs
)
;
const
logits
=
output
.
logits
[
0
]
;
const
predicted_ids
=
[
]
;
for
(
const
item
of
logits
)
{
predicted_ids
.
push
(
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
max
)
(
item
.
data
)
[
1
]
)
}
const
predicted_sentences
=
this
.
tokenizer
.
decode
(
predicted_ids
)
toReturn
.
push
(
{
text
:
predicted_sentences
}
)
}
return
single
?
toReturn
[
0
]
:
toReturn
;
}
async
_call_whisper
(
audio
kwargs
=
{
}
)
{
const
return_timestamps
=
kwargs
.
return_timestamps
?
?
false
;
const
chunk_length_s
=
kwargs
.
chunk_length_s
?
?
0
;
const
chunk_callback
=
kwargs
.
chunk_callback
?
?
null
;
const
force_full_sequences
=
kwargs
.
force_full_sequences
?
?
false
;
let
stride_length_s
=
kwargs
.
stride_length_s
?
?
null
;
if
(
return_timestamps
=
=
=
'
word
'
)
{
kwargs
[
'
return_token_timestamps
'
]
=
true
;
}
const
language
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_3__
.
pop
)
(
kwargs
'
language
'
null
)
;
const
task
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_3__
.
pop
)
(
kwargs
'
task
'
null
)
;
if
(
language
|
|
task
|
|
return_timestamps
)
{
if
(
kwargs
.
forced_decoder_ids
)
{
throw
new
Error
(
"
Cannot
specify
language
/
task
/
return_timestamps
and
forced_decoder_ids
at
the
same
time
.
"
)
}
const
decoder_prompt_ids
=
this
.
tokenizer
.
get_decoder_prompt_ids
(
{
language
task
no_timestamps
:
!
return_timestamps
}
)
if
(
decoder_prompt_ids
.
length
>
0
)
{
kwargs
.
forced_decoder_ids
=
decoder_prompt_ids
;
}
}
const
single
=
!
Array
.
isArray
(
audio
)
;
if
(
single
)
{
audio
=
[
(
audio
)
]
;
}
const
time_precision
=
this
.
processor
.
feature_extractor
.
config
.
chunk_length
/
this
.
model
.
config
.
max_source_positions
;
const
hop_length
=
this
.
processor
.
feature_extractor
.
config
.
hop_length
;
const
sampling_rate
=
this
.
processor
.
feature_extractor
.
config
.
sampling_rate
;
const
preparedAudios
=
await
prepareAudios
(
audio
sampling_rate
)
;
const
toReturn
=
[
]
;
for
(
const
aud
of
preparedAudios
)
{
let
chunks
=
[
]
;
if
(
chunk_length_s
>
0
)
{
if
(
stride_length_s
=
=
=
null
)
{
stride_length_s
=
chunk_length_s
/
6
;
}
else
if
(
chunk_length_s
<
=
stride_length_s
)
{
throw
Error
(
"
chunk_length_s
must
be
larger
than
stride_length_s
.
"
)
}
const
window
=
sampling_rate
*
chunk_length_s
;
const
stride
=
sampling_rate
*
stride_length_s
;
const
jump
=
window
-
2
*
stride
;
let
offset
=
0
;
while
(
offset
<
aud
.
length
)
{
const
subarr
=
aud
.
subarray
(
offset
offset
+
window
)
;
const
feature
=
await
this
.
processor
(
subarr
)
;
const
isFirst
=
offset
=
=
=
0
;
const
isLast
=
offset
+
jump
>
=
aud
.
length
;
chunks
.
push
(
{
stride
:
[
subarr
.
length
isFirst
?
0
:
stride
isLast
?
0
:
stride
]
input_features
:
feature
.
input_features
is_last
:
isLast
}
)
offset
+
=
jump
;
}
}
else
{
chunks
=
[
{
stride
:
[
aud
.
length
0
0
]
input_features
:
(
await
this
.
processor
(
aud
)
)
.
input_features
is_last
:
true
}
]
}
for
(
const
chunk
of
chunks
)
{
kwargs
.
num_frames
=
Math
.
floor
(
chunk
.
stride
[
0
]
/
hop_length
)
;
const
data
=
await
this
.
model
.
generate
(
chunk
.
input_features
kwargs
)
;
if
(
return_timestamps
=
=
=
'
word
'
)
{
chunk
.
tokens
=
data
.
sequences
[
0
]
;
chunk
.
token_timestamps
=
data
.
token_timestamps
.
tolist
(
)
[
0
]
.
map
(
(
x
)
=
>
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
round
)
(
x
2
)
)
;
}
else
{
chunk
.
tokens
=
data
[
0
]
;
}
chunk
.
stride
=
chunk
.
stride
.
map
(
x
=
>
x
/
sampling_rate
)
;
if
(
chunk_callback
!
=
=
null
)
{
chunk_callback
(
chunk
)
}
}
const
[
full_text
optional
]
=
this
.
tokenizer
.
_decode_asr
(
chunks
{
time_precision
return_timestamps
force_full_sequences
}
)
;
toReturn
.
push
(
{
text
:
full_text
.
.
.
optional
}
)
}
return
single
?
toReturn
[
0
]
:
toReturn
;
}
}
class
ImageToTextPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
images
generate_kwargs
=
{
}
)
{
const
isBatched
=
Array
.
isArray
(
images
)
;
const
preparedImages
=
await
prepareImages
(
images
)
;
const
{
pixel_values
}
=
await
this
.
processor
(
preparedImages
)
;
const
toReturn
=
[
]
;
for
(
const
batch
of
pixel_values
)
{
batch
.
dims
=
[
1
.
.
.
batch
.
dims
]
const
output
=
await
this
.
model
.
generate
(
batch
generate_kwargs
)
;
const
decoded
=
this
.
tokenizer
.
batch_decode
(
output
{
skip_special_tokens
:
true
}
)
.
map
(
x
=
>
(
{
generated_text
:
x
.
trim
(
)
}
)
)
toReturn
.
push
(
decoded
)
;
}
return
isBatched
?
toReturn
:
toReturn
[
0
]
;
}
}
class
ImageClassificationPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
images
{
topk
=
1
}
=
{
}
)
{
const
isBatched
=
Array
.
isArray
(
images
)
;
const
preparedImages
=
await
prepareImages
(
images
)
;
const
{
pixel_values
}
=
await
this
.
processor
(
preparedImages
)
;
const
output
=
await
this
.
model
(
{
pixel_values
}
)
;
const
id2label
=
this
.
model
.
config
.
id2label
;
const
toReturn
=
[
]
;
for
(
const
batch
of
output
.
logits
)
{
const
scores
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
getTopItems
)
(
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
softmax
)
(
batch
.
data
)
topk
)
;
const
vals
=
scores
.
map
(
x
=
>
(
{
label
:
id2label
[
x
[
0
]
]
score
:
x
[
1
]
}
)
)
;
if
(
topk
=
=
=
1
)
{
toReturn
.
push
(
.
.
.
vals
)
;
}
else
{
toReturn
.
push
(
vals
)
;
}
}
return
isBatched
|
|
topk
=
=
=
1
?
(
toReturn
)
:
(
toReturn
)
[
0
]
;
}
}
class
ImageSegmentationPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
this
.
subtasks_mapping
=
{
panoptic
:
'
post_process_panoptic_segmentation
'
instance
:
'
post_process_instance_segmentation
'
semantic
:
'
post_process_semantic_segmentation
'
}
}
async
_call
(
images
{
threshold
=
0
.
5
mask_threshold
=
0
.
5
overlap_mask_area_threshold
=
0
.
8
label_ids_to_fuse
=
null
target_sizes
=
null
subtask
=
null
}
=
{
}
)
{
const
isBatched
=
Array
.
isArray
(
images
)
;
if
(
isBatched
&
&
images
.
length
!
=
=
1
)
{
throw
Error
(
"
Image
segmentation
pipeline
currently
only
supports
a
batch
size
of
1
.
"
)
;
}
const
preparedImages
=
await
prepareImages
(
images
)
;
const
imageSizes
=
preparedImages
.
map
(
x
=
>
[
x
.
height
x
.
width
]
)
;
const
{
pixel_values
pixel_mask
}
=
await
this
.
processor
(
preparedImages
)
;
const
output
=
await
this
.
model
(
{
pixel_values
pixel_mask
}
)
;
let
fn
=
null
;
if
(
subtask
!
=
=
null
)
{
fn
=
this
.
subtasks_mapping
[
subtask
]
;
}
else
{
for
(
let
[
task
func
]
of
Object
.
entries
(
this
.
subtasks_mapping
)
)
{
if
(
func
in
this
.
processor
.
feature_extractor
)
{
fn
=
this
.
processor
.
feature_extractor
[
func
]
.
bind
(
this
.
processor
.
feature_extractor
)
;
subtask
=
task
;
break
;
}
}
}
const
id2label
=
this
.
model
.
config
.
id2label
;
const
annotation
=
[
]
;
if
(
subtask
=
=
=
'
panoptic
'
|
|
subtask
=
=
=
'
instance
'
)
{
const
processed
=
fn
(
output
threshold
mask_threshold
overlap_mask_area_threshold
label_ids_to_fuse
target_sizes
?
?
imageSizes
)
[
0
]
;
const
segmentation
=
processed
.
segmentation
;
for
(
const
segment
of
processed
.
segments_info
)
{
const
maskData
=
new
Uint8ClampedArray
(
segmentation
.
data
.
length
)
;
for
(
let
i
=
0
;
i
<
segmentation
.
data
.
length
;
+
+
i
)
{
if
(
segmentation
.
data
[
i
]
=
=
=
segment
.
id
)
{
maskData
[
i
]
=
255
;
}
}
const
mask
=
new
_utils_image_js__WEBPACK_IMPORTED_MODULE_7__
.
RawImage
(
maskData
segmentation
.
dims
[
1
]
segmentation
.
dims
[
0
]
1
)
annotation
.
push
(
{
score
:
segment
.
score
label
:
id2label
[
segment
.
label_id
]
mask
:
mask
}
)
}
}
else
if
(
subtask
=
=
=
'
semantic
'
)
{
const
{
segmentation
labels
}
=
fn
(
output
target_sizes
?
?
imageSizes
)
[
0
]
;
for
(
const
label
of
labels
)
{
const
maskData
=
new
Uint8ClampedArray
(
segmentation
.
data
.
length
)
;
for
(
let
i
=
0
;
i
<
segmentation
.
data
.
length
;
+
+
i
)
{
if
(
segmentation
.
data
[
i
]
=
=
=
label
)
{
maskData
[
i
]
=
255
;
}
}
const
mask
=
new
_utils_image_js__WEBPACK_IMPORTED_MODULE_7__
.
RawImage
(
maskData
segmentation
.
dims
[
1
]
segmentation
.
dims
[
0
]
1
)
;
annotation
.
push
(
{
score
:
null
label
:
id2label
[
label
]
mask
:
mask
}
)
;
}
}
else
{
throw
Error
(
Subtask
{
subtask
}
not
supported
.
)
;
}
return
annotation
;
}
}
class
ZeroShotImageClassificationPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
images
candidate_labels
{
hypothesis_template
=
"
This
is
a
photo
of
{
}
"
}
=
{
}
)
{
const
isBatched
=
Array
.
isArray
(
images
)
;
const
preparedImages
=
await
prepareImages
(
images
)
;
const
texts
=
candidate_labels
.
map
(
x
=
>
hypothesis_template
.
replace
(
'
{
}
'
x
)
)
;
const
text_inputs
=
this
.
tokenizer
(
texts
{
padding
:
this
.
model
.
config
.
model_type
=
=
=
'
siglip
'
?
'
max_length
'
:
true
truncation
:
true
}
)
;
const
{
pixel_values
}
=
await
this
.
processor
(
preparedImages
)
;
const
output
=
await
this
.
model
(
{
.
.
.
text_inputs
pixel_values
}
)
;
const
function_to_apply
=
this
.
model
.
config
.
model_type
=
=
=
'
siglip
'
?
batch
=
>
batch
.
sigmoid
(
)
.
data
:
batch
=
>
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
softmax
)
(
batch
.
data
)
;
const
toReturn
=
[
]
;
for
(
const
batch
of
output
.
logits_per_image
)
{
const
probs
=
function_to_apply
(
batch
)
;
const
result
=
[
.
.
.
probs
]
.
map
(
(
x
i
)
=
>
(
{
score
:
x
label
:
candidate_labels
[
i
]
}
)
)
;
result
.
sort
(
(
a
b
)
=
>
b
.
score
-
a
.
score
)
;
toReturn
.
push
(
result
)
;
}
return
isBatched
?
toReturn
:
toReturn
[
0
]
;
}
}
class
ObjectDetectionPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
images
{
threshold
=
0
.
9
percentage
=
false
}
=
{
}
)
{
const
isBatched
=
Array
.
isArray
(
images
)
;
if
(
isBatched
&
&
images
.
length
!
=
=
1
)
{
throw
Error
(
"
Object
detection
pipeline
currently
only
supports
a
batch
size
of
1
.
"
)
;
}
const
preparedImages
=
await
prepareImages
(
images
)
;
const
imageSizes
=
percentage
?
null
:
preparedImages
.
map
(
x
=
>
[
x
.
height
x
.
width
]
)
;
const
{
pixel_values
pixel_mask
}
=
await
this
.
processor
(
preparedImages
)
;
const
output
=
await
this
.
model
(
{
pixel_values
pixel_mask
}
)
;
const
processed
=
this
.
processor
.
feature_extractor
.
post_process_object_detection
(
output
threshold
imageSizes
)
;
const
id2label
=
this
.
model
.
config
.
id2label
;
const
result
=
processed
.
map
(
batch
=
>
(
batch
.
boxes
.
map
(
(
box
i
)
=
>
(
{
score
:
batch
.
scores
[
i
]
label
:
id2label
[
batch
.
classes
[
i
]
]
box
:
get_bounding_box
(
box
!
percentage
)
}
)
)
)
)
return
isBatched
?
result
:
result
[
0
]
;
}
}
class
ZeroShotObjectDetectionPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
images
candidate_labels
{
threshold
=
0
.
1
topk
=
null
percentage
=
false
}
=
{
}
)
{
const
isBatched
=
Array
.
isArray
(
images
)
;
const
preparedImages
=
await
prepareImages
(
images
)
;
const
text_inputs
=
this
.
tokenizer
(
candidate_labels
{
padding
:
true
truncation
:
true
}
)
;
const
model_inputs
=
await
this
.
processor
(
preparedImages
)
;
const
toReturn
=
[
]
;
for
(
let
i
=
0
;
i
<
preparedImages
.
length
;
+
+
i
)
{
const
image
=
preparedImages
[
i
]
;
const
imageSize
=
percentage
?
null
:
[
[
image
.
height
image
.
width
]
]
;
const
pixel_values
=
model_inputs
.
pixel_values
[
i
]
.
unsqueeze_
(
0
)
;
const
output
=
await
this
.
model
(
{
.
.
.
text_inputs
pixel_values
}
)
;
const
processed
=
this
.
processor
.
feature_extractor
.
post_process_object_detection
(
output
threshold
imageSize
true
)
[
0
]
;
let
result
=
processed
.
boxes
.
map
(
(
box
i
)
=
>
(
{
score
:
processed
.
scores
[
i
]
label
:
candidate_labels
[
processed
.
classes
[
i
]
]
box
:
get_bounding_box
(
box
!
percentage
)
}
)
)
.
sort
(
(
a
b
)
=
>
b
.
score
-
a
.
score
)
;
if
(
topk
!
=
=
null
)
{
result
=
result
.
slice
(
0
topk
)
;
}
toReturn
.
push
(
result
)
}
return
isBatched
?
toReturn
:
toReturn
[
0
]
;
}
}
class
DocumentQuestionAnsweringPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
image
question
generate_kwargs
=
{
}
)
{
const
preparedImage
=
(
await
prepareImages
(
image
)
)
[
0
]
;
const
{
pixel_values
}
=
await
this
.
processor
(
preparedImage
)
;
const
task_prompt
=
<
s_docvqa
>
<
s_question
>
{
question
}
<
/
s_question
>
<
s_answer
>
;
const
decoder_input_ids
=
this
.
tokenizer
(
task_prompt
{
add_special_tokens
:
false
padding
:
true
truncation
:
true
}
)
.
input_ids
;
const
output
=
await
this
.
model
.
generate
(
pixel_values
{
.
.
.
generate_kwargs
decoder_input_ids
max_length
:
this
.
model
.
config
.
decoder
.
max_position_embeddings
}
)
;
const
decoded
=
this
.
tokenizer
.
batch_decode
(
output
)
[
0
]
;
const
match
=
decoded
.
match
(
/
<
s_answer
>
(
.
*
?
)
<
\
/
s_answer
>
/
)
;
let
answer
=
null
;
if
(
match
&
&
match
.
length
>
=
2
)
{
answer
=
match
[
1
]
.
trim
(
)
;
}
return
[
{
answer
}
]
;
}
}
class
TextToAudioPipeline
extends
(
(
Pipeline
)
)
{
DEFAULT_VOCODER_ID
=
"
Xenova
/
speecht5_hifigan
"
constructor
(
options
)
{
super
(
options
)
;
this
.
vocoder
=
options
.
vocoder
?
?
null
;
}
async
_call
(
text_inputs
{
speaker_embeddings
=
null
}
=
{
}
)
{
if
(
this
.
processor
)
{
return
this
.
_call_text_to_spectrogram
(
text_inputs
{
speaker_embeddings
}
)
;
}
else
{
return
this
.
_call_text_to_waveform
(
text_inputs
)
;
}
}
async
_call_text_to_waveform
(
text_inputs
)
{
const
inputs
=
this
.
tokenizer
(
text_inputs
{
padding
:
true
truncation
:
true
}
)
;
const
{
waveform
}
=
await
this
.
model
(
inputs
)
;
const
sampling_rate
=
this
.
model
.
config
.
sampling_rate
;
return
{
audio
:
waveform
.
data
sampling_rate
}
}
async
_call_text_to_spectrogram
(
text_inputs
{
speaker_embeddings
}
)
{
if
(
!
this
.
vocoder
)
{
console
.
log
(
'
No
vocoder
specified
using
default
HifiGan
vocoder
.
'
)
;
this
.
vocoder
=
await
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModel
.
from_pretrained
(
this
.
DEFAULT_VOCODER_ID
{
quantized
:
false
}
)
;
}
if
(
typeof
speaker_embeddings
=
=
=
'
string
'
|
|
speaker_embeddings
instanceof
URL
)
{
speaker_embeddings
=
new
Float32Array
(
await
(
await
fetch
(
speaker_embeddings
)
)
.
arrayBuffer
(
)
)
;
}
if
(
speaker_embeddings
instanceof
Float32Array
)
{
speaker_embeddings
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_6__
.
Tensor
(
'
float32
'
speaker_embeddings
[
1
speaker_embeddings
.
length
]
)
}
else
if
(
!
(
speaker_embeddings
instanceof
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_6__
.
Tensor
)
)
{
throw
new
Error
(
"
Speaker
embeddings
must
be
a
Tensor
Float32Array
string
or
URL
.
"
)
}
const
{
input_ids
}
=
this
.
tokenizer
(
text_inputs
{
padding
:
true
truncation
:
true
}
)
;
const
{
waveform
}
=
await
this
.
model
.
generate_speech
(
input_ids
speaker_embeddings
{
vocoder
:
this
.
vocoder
}
)
;
const
sampling_rate
=
this
.
processor
.
feature_extractor
.
config
.
sampling_rate
;
return
{
audio
:
waveform
.
data
sampling_rate
}
}
}
class
ImageToImagePipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
images
)
{
const
preparedImages
=
await
prepareImages
(
images
)
;
const
inputs
=
await
this
.
processor
(
preparedImages
)
;
const
outputs
=
await
this
.
model
(
inputs
)
;
const
toReturn
=
[
]
;
for
(
const
batch
of
outputs
.
reconstruction
)
{
const
output
=
batch
.
squeeze
(
)
.
clamp_
(
0
1
)
.
mul_
(
255
)
.
round_
(
)
.
to
(
'
uint8
'
)
;
toReturn
.
push
(
_utils_image_js__WEBPACK_IMPORTED_MODULE_7__
.
RawImage
.
fromTensor
(
output
)
)
;
}
return
toReturn
.
length
>
1
?
toReturn
:
toReturn
[
0
]
;
}
}
class
DepthEstimationPipeline
extends
(
(
Pipeline
)
)
{
constructor
(
options
)
{
super
(
options
)
;
}
async
_call
(
images
)
{
const
preparedImages
=
await
prepareImages
(
images
)
;
const
inputs
=
await
this
.
processor
(
preparedImages
)
;
const
{
predicted_depth
}
=
await
this
.
model
(
inputs
)
;
const
toReturn
=
[
]
;
for
(
let
i
=
0
;
i
<
preparedImages
.
length
;
+
+
i
)
{
const
prediction
=
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_6__
.
interpolate
)
(
predicted_depth
[
i
]
preparedImages
[
i
]
.
size
.
reverse
(
)
'
bilinear
'
false
)
;
const
formatted
=
prediction
.
mul_
(
255
/
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_4__
.
max
)
(
prediction
.
data
)
[
0
]
)
.
to
(
'
uint8
'
)
;
toReturn
.
push
(
{
predicted_depth
:
predicted_depth
[
i
]
depth
:
_utils_image_js__WEBPACK_IMPORTED_MODULE_7__
.
RawImage
.
fromTensor
(
formatted
)
}
)
;
}
return
toReturn
.
length
>
1
?
toReturn
:
toReturn
[
0
]
;
}
}
const
SUPPORTED_TASKS
=
Object
.
freeze
(
{
"
text
-
classification
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
TextClassificationPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForSequenceClassification
"
default
"
:
{
"
model
"
:
"
Xenova
/
distilbert
-
base
-
uncased
-
finetuned
-
sst
-
2
-
english
"
}
"
type
"
:
"
text
"
}
"
token
-
classification
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
TokenClassificationPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForTokenClassification
"
default
"
:
{
"
model
"
:
"
Xenova
/
bert
-
base
-
multilingual
-
cased
-
ner
-
hrl
"
}
"
type
"
:
"
text
"
}
"
question
-
answering
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
QuestionAnsweringPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForQuestionAnswering
"
default
"
:
{
"
model
"
:
"
Xenova
/
distilbert
-
base
-
cased
-
distilled
-
squad
"
}
"
type
"
:
"
text
"
}
"
fill
-
mask
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
FillMaskPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForMaskedLM
"
default
"
:
{
"
model
"
:
"
Xenova
/
bert
-
base
-
uncased
"
}
"
type
"
:
"
text
"
}
"
summarization
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
SummarizationPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForSeq2SeqLM
"
default
"
:
{
"
model
"
:
"
Xenova
/
distilbart
-
cnn
-
6
-
6
"
}
"
type
"
:
"
text
"
}
"
translation
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
TranslationPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForSeq2SeqLM
"
default
"
:
{
"
model
"
:
"
Xenova
/
t5
-
small
"
}
"
type
"
:
"
text
"
}
"
text2text
-
generation
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
Text2TextGenerationPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForSeq2SeqLM
"
default
"
:
{
"
model
"
:
"
Xenova
/
flan
-
t5
-
small
"
}
"
type
"
:
"
text
"
}
"
text
-
generation
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
TextGenerationPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForCausalLM
"
default
"
:
{
"
model
"
:
"
Xenova
/
gpt2
"
}
"
type
"
:
"
text
"
}
"
zero
-
shot
-
classification
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
ZeroShotClassificationPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForSequenceClassification
"
default
"
:
{
"
model
"
:
"
Xenova
/
distilbert
-
base
-
uncased
-
mnli
"
}
"
type
"
:
"
text
"
}
"
audio
-
classification
"
:
{
"
pipeline
"
:
AudioClassificationPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForAudioClassification
"
processor
"
:
_processors_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoProcessor
"
default
"
:
{
"
model
"
:
"
Xenova
/
wav2vec2
-
base
-
superb
-
ks
"
}
"
type
"
:
"
audio
"
}
"
zero
-
shot
-
audio
-
classification
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
ZeroShotAudioClassificationPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModel
"
processor
"
:
_processors_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoProcessor
"
default
"
:
{
"
model
"
:
"
Xenova
/
clap
-
htsat
-
unfused
"
}
"
type
"
:
"
multimodal
"
}
"
automatic
-
speech
-
recognition
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
AutomaticSpeechRecognitionPipeline
"
model
"
:
[
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForSpeechSeq2Seq
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForCTC
]
"
processor
"
:
_processors_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoProcessor
"
default
"
:
{
"
model
"
:
"
Xenova
/
whisper
-
tiny
.
en
"
}
"
type
"
:
"
multimodal
"
}
"
text
-
to
-
audio
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
TextToAudioPipeline
"
model
"
:
[
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForTextToWaveform
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForTextToSpectrogram
]
"
processor
"
:
[
_processors_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoProcessor
null
]
"
default
"
:
{
"
model
"
:
"
Xenova
/
speecht5_tts
"
}
"
type
"
:
"
text
"
}
"
image
-
to
-
text
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
ImageToTextPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForVision2Seq
"
processor
"
:
_processors_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoProcessor
"
default
"
:
{
"
model
"
:
"
Xenova
/
vit
-
gpt2
-
image
-
captioning
"
}
"
type
"
:
"
multimodal
"
}
"
image
-
classification
"
:
{
"
pipeline
"
:
ImageClassificationPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForImageClassification
"
processor
"
:
_processors_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoProcessor
"
default
"
:
{
"
model
"
:
"
Xenova
/
vit
-
base
-
patch16
-
224
"
}
"
type
"
:
"
multimodal
"
}
"
image
-
segmentation
"
:
{
"
pipeline
"
:
ImageSegmentationPipeline
"
model
"
:
[
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForImageSegmentation
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForSemanticSegmentation
]
"
processor
"
:
_processors_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoProcessor
"
default
"
:
{
"
model
"
:
"
Xenova
/
detr
-
resnet
-
50
-
panoptic
"
}
"
type
"
:
"
multimodal
"
}
"
zero
-
shot
-
image
-
classification
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
ZeroShotImageClassificationPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModel
"
processor
"
:
_processors_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoProcessor
"
default
"
:
{
"
model
"
:
"
Xenova
/
clip
-
vit
-
base
-
patch32
"
}
"
type
"
:
"
multimodal
"
}
"
object
-
detection
"
:
{
"
pipeline
"
:
ObjectDetectionPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForObjectDetection
"
processor
"
:
_processors_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoProcessor
"
default
"
:
{
"
model
"
:
"
Xenova
/
detr
-
resnet
-
50
"
}
"
type
"
:
"
multimodal
"
}
"
zero
-
shot
-
object
-
detection
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
ZeroShotObjectDetectionPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForZeroShotObjectDetection
"
processor
"
:
_processors_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoProcessor
"
default
"
:
{
"
model
"
:
"
Xenova
/
owlvit
-
base
-
patch32
"
}
"
type
"
:
"
multimodal
"
}
"
document
-
question
-
answering
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
DocumentQuestionAnsweringPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForDocumentQuestionAnswering
"
processor
"
:
_processors_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoProcessor
"
default
"
:
{
"
model
"
:
"
Xenova
/
donut
-
base
-
finetuned
-
docvqa
"
}
"
type
"
:
"
multimodal
"
}
"
image
-
to
-
image
"
:
{
"
pipeline
"
:
ImageToImagePipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForImageToImage
"
processor
"
:
_processors_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoProcessor
"
default
"
:
{
"
model
"
:
"
Xenova
/
swin2SR
-
classical
-
sr
-
x2
-
64
"
}
"
type
"
:
"
image
"
}
"
depth
-
estimation
"
:
{
"
pipeline
"
:
DepthEstimationPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForDepthEstimation
"
processor
"
:
_processors_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoProcessor
"
default
"
:
{
"
model
"
:
"
Xenova
/
dpt
-
large
"
}
"
type
"
:
"
image
"
}
"
feature
-
extraction
"
:
{
"
tokenizer
"
:
_tokenizers_js__WEBPACK_IMPORTED_MODULE_0__
.
AutoTokenizer
"
pipeline
"
:
FeatureExtractionPipeline
"
model
"
:
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModel
"
default
"
:
{
"
model
"
:
"
Xenova
/
all
-
MiniLM
-
L6
-
v2
"
}
"
type
"
:
"
text
"
}
"
image
-
feature
-
extraction
"
:
{
"
processor
"
:
_processors_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoProcessor
"
pipeline
"
:
ImageFeatureExtractionPipeline
"
model
"
:
[
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModelForImageFeatureExtraction
_models_js__WEBPACK_IMPORTED_MODULE_1__
.
AutoModel
]
"
default
"
:
{
"
model
"
:
"
Xenova
/
vit
-
base
-
patch16
-
224
-
in21k
"
}
"
type
"
:
"
image
"
}
}
)
const
TASK_ALIASES
=
Object
.
freeze
(
{
"
sentiment
-
analysis
"
:
"
text
-
classification
"
"
ner
"
:
"
token
-
classification
"
"
asr
"
:
"
automatic
-
speech
-
recognition
"
"
text
-
to
-
speech
"
:
"
text
-
to
-
audio
"
"
embeddings
"
:
"
feature
-
extraction
"
}
)
;
async
function
pipeline
(
task
model
=
null
{
quantized
=
true
progress_callback
=
null
config
=
null
cache_dir
=
null
local_files_only
=
false
revision
=
'
main
'
}
=
{
}
)
{
task
=
TASK_ALIASES
[
task
]
?
?
task
;
const
pipelineInfo
=
SUPPORTED_TASKS
[
task
.
split
(
'
_
'
1
)
[
0
]
]
;
if
(
!
pipelineInfo
)
{
throw
Error
(
Unsupported
pipeline
:
{
task
}
.
Must
be
one
of
[
{
Object
.
keys
(
SUPPORTED_TASKS
)
}
]
)
}
if
(
!
model
)
{
model
=
pipelineInfo
.
default
.
model
console
.
log
(
No
model
specified
.
Using
default
model
:
"
{
model
}
"
.
)
;
}
const
pretrainedOptions
=
{
quantized
progress_callback
config
cache_dir
local_files_only
revision
}
const
classes
=
new
Map
(
[
[
'
tokenizer
'
pipelineInfo
.
tokenizer
]
[
'
model
'
pipelineInfo
.
model
]
[
'
processor
'
pipelineInfo
.
processor
]
]
)
;
const
results
=
await
loadItems
(
classes
model
pretrainedOptions
)
;
results
.
task
=
task
;
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_3__
.
dispatchCallback
)
(
progress_callback
{
'
status
'
:
'
ready
'
'
task
'
:
task
'
model
'
:
model
}
)
;
const
pipelineClass
=
pipelineInfo
.
pipeline
;
return
new
pipelineClass
(
results
)
;
}
async
function
loadItems
(
mapping
model
pretrainedOptions
)
{
const
result
=
Object
.
create
(
null
)
;
const
promises
=
[
]
;
for
(
let
[
name
cls
]
of
mapping
.
entries
(
)
)
{
if
(
!
cls
)
continue
;
let
promise
;
if
(
Array
.
isArray
(
cls
)
)
{
promise
=
new
Promise
(
async
(
resolve
reject
)
=
>
{
let
e
;
for
(
let
c
of
cls
)
{
if
(
c
=
=
=
null
)
{
resolve
(
null
)
;
return
;
}
try
{
resolve
(
await
c
.
from_pretrained
(
model
pretrainedOptions
)
)
;
return
;
}
catch
(
err
)
{
e
=
err
;
}
}
reject
(
e
)
;
}
)
}
else
{
promise
=
cls
.
from_pretrained
(
model
pretrainedOptions
)
;
}
result
[
name
]
=
promise
;
promises
.
push
(
promise
)
;
}
await
Promise
.
all
(
promises
)
;
for
(
let
[
name
promise
]
of
Object
.
entries
(
result
)
)
{
result
[
name
]
=
await
promise
;
}
return
result
;
}
}
)
"
.
/
src
/
processors
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
ASTFeatureExtractor
"
:
(
)
=
>
(
ASTFeatureExtractor
)
"
AutoProcessor
"
:
(
)
=
>
(
AutoProcessor
)
"
BeitFeatureExtractor
"
:
(
)
=
>
(
BeitFeatureExtractor
)
"
BitImageProcessor
"
:
(
)
=
>
(
BitImageProcessor
)
"
CLIPFeatureExtractor
"
:
(
)
=
>
(
CLIPFeatureExtractor
)
"
ChineseCLIPFeatureExtractor
"
:
(
)
=
>
(
ChineseCLIPFeatureExtractor
)
"
ClapFeatureExtractor
"
:
(
)
=
>
(
ClapFeatureExtractor
)
"
ConvNextFeatureExtractor
"
:
(
)
=
>
(
ConvNextFeatureExtractor
)
"
ConvNextImageProcessor
"
:
(
)
=
>
(
ConvNextImageProcessor
)
"
DPTFeatureExtractor
"
:
(
)
=
>
(
DPTFeatureExtractor
)
"
DPTImageProcessor
"
:
(
)
=
>
(
DPTImageProcessor
)
"
DeiTFeatureExtractor
"
:
(
)
=
>
(
DeiTFeatureExtractor
)
"
DetrFeatureExtractor
"
:
(
)
=
>
(
DetrFeatureExtractor
)
"
DonutFeatureExtractor
"
:
(
)
=
>
(
DonutFeatureExtractor
)
"
EfficientNetImageProcessor
"
:
(
)
=
>
(
EfficientNetImageProcessor
)
"
FeatureExtractor
"
:
(
)
=
>
(
FeatureExtractor
)
"
GLPNFeatureExtractor
"
:
(
)
=
>
(
GLPNFeatureExtractor
)
"
ImageFeatureExtractor
"
:
(
)
=
>
(
ImageFeatureExtractor
)
"
MobileViTFeatureExtractor
"
:
(
)
=
>
(
MobileViTFeatureExtractor
)
"
NougatImageProcessor
"
:
(
)
=
>
(
NougatImageProcessor
)
"
OwlViTFeatureExtractor
"
:
(
)
=
>
(
OwlViTFeatureExtractor
)
"
OwlViTProcessor
"
:
(
)
=
>
(
OwlViTProcessor
)
"
Owlv2ImageProcessor
"
:
(
)
=
>
(
Owlv2ImageProcessor
)
"
Processor
"
:
(
)
=
>
(
Processor
)
"
SamImageProcessor
"
:
(
)
=
>
(
SamImageProcessor
)
"
SamProcessor
"
:
(
)
=
>
(
SamProcessor
)
"
SeamlessM4TFeatureExtractor
"
:
(
)
=
>
(
SeamlessM4TFeatureExtractor
)
"
SegformerFeatureExtractor
"
:
(
)
=
>
(
SegformerFeatureExtractor
)
"
SiglipImageProcessor
"
:
(
)
=
>
(
SiglipImageProcessor
)
"
SpeechT5FeatureExtractor
"
:
(
)
=
>
(
SpeechT5FeatureExtractor
)
"
SpeechT5Processor
"
:
(
)
=
>
(
SpeechT5Processor
)
"
Swin2SRImageProcessor
"
:
(
)
=
>
(
Swin2SRImageProcessor
)
"
ViTFeatureExtractor
"
:
(
)
=
>
(
ViTFeatureExtractor
)
"
ViTImageProcessor
"
:
(
)
=
>
(
ViTImageProcessor
)
"
VitMatteImageProcessor
"
:
(
)
=
>
(
VitMatteImageProcessor
)
"
Wav2Vec2FeatureExtractor
"
:
(
)
=
>
(
Wav2Vec2FeatureExtractor
)
"
Wav2Vec2ProcessorWithLM
"
:
(
)
=
>
(
Wav2Vec2ProcessorWithLM
)
"
WhisperFeatureExtractor
"
:
(
)
=
>
(
WhisperFeatureExtractor
)
"
WhisperProcessor
"
:
(
)
=
>
(
WhisperProcessor
)
"
YolosFeatureExtractor
"
:
(
)
=
>
(
YolosFeatureExtractor
)
}
)
;
var
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
=
__webpack_require__
(
"
.
/
src
/
utils
/
core
.
js
"
)
;
var
_utils_hub_js__WEBPACK_IMPORTED_MODULE_1__
=
__webpack_require__
(
"
.
/
src
/
utils
/
hub
.
js
"
)
;
var
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
=
__webpack_require__
(
"
.
/
src
/
utils
/
maths
.
js
"
)
;
var
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
=
__webpack_require__
(
"
.
/
src
/
utils
/
tensor
.
js
"
)
;
var
_utils_image_js__WEBPACK_IMPORTED_MODULE_4__
=
__webpack_require__
(
"
.
/
src
/
utils
/
image
.
js
"
)
;
var
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
=
__webpack_require__
(
"
.
/
src
/
utils
/
audio
.
js
"
)
;
function
center_to_corners_format
(
[
centerX
centerY
width
height
]
)
{
return
[
centerX
-
width
/
2
centerY
-
height
/
2
centerX
+
width
/
2
centerY
+
height
/
2
]
;
}
function
post_process_object_detection
(
outputs
threshold
=
0
.
5
target_sizes
=
null
is_zero_shot
=
false
)
{
const
out_logits
=
outputs
.
logits
;
const
out_bbox
=
outputs
.
pred_boxes
;
const
[
batch_size
num_boxes
num_classes
]
=
out_logits
.
dims
;
if
(
target_sizes
!
=
=
null
&
&
target_sizes
.
length
!
=
=
batch_size
)
{
throw
Error
(
"
Make
sure
that
you
pass
in
as
many
target
sizes
as
the
batch
dimension
of
the
logits
"
)
}
let
toReturn
=
[
]
;
for
(
let
i
=
0
;
i
<
batch_size
;
+
+
i
)
{
let
target_size
=
target_sizes
!
=
=
null
?
target_sizes
[
i
]
:
null
;
let
info
=
{
boxes
:
[
]
classes
:
[
]
scores
:
[
]
}
let
logits
=
out_logits
[
i
]
;
let
bbox
=
out_bbox
[
i
]
;
for
(
let
j
=
0
;
j
<
num_boxes
;
+
+
j
)
{
let
logit
=
logits
[
j
]
;
let
indices
=
[
]
;
let
probs
;
if
(
is_zero_shot
)
{
probs
=
logit
.
sigmoid
(
)
.
data
;
for
(
let
k
=
0
;
k
<
probs
.
length
;
+
+
k
)
{
if
(
probs
[
k
]
>
threshold
)
{
indices
.
push
(
k
)
;
}
}
}
else
{
let
maxIndex
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
max
)
(
logit
.
data
)
[
1
]
;
if
(
maxIndex
=
=
=
num_classes
-
1
)
{
continue
;
}
indices
.
push
(
maxIndex
)
;
probs
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
softmax
)
(
logit
.
data
)
;
}
for
(
const
index
of
indices
)
{
let
box
=
bbox
[
j
]
.
data
;
box
=
center_to_corners_format
(
box
)
if
(
target_size
!
=
=
null
)
{
box
=
box
.
map
(
(
x
i
)
=
>
x
*
target_size
[
(
i
+
1
)
%
2
]
)
}
info
.
boxes
.
push
(
box
)
;
info
.
classes
.
push
(
index
)
;
info
.
scores
.
push
(
probs
[
index
]
)
;
}
}
toReturn
.
push
(
info
)
;
}
return
toReturn
;
}
function
validate_audio_inputs
(
audio
feature_extractor
)
{
if
(
!
(
audio
instanceof
Float32Array
|
|
audio
instanceof
Float64Array
)
)
{
throw
new
Error
(
{
feature_extractor
}
expects
input
to
be
a
Float32Array
or
a
Float64Array
but
got
{
audio
?
.
constructor
?
.
name
?
?
typeof
audio
}
instead
.
+
If
using
the
feature
extractor
directly
remember
to
use
\
read_audio
(
url
sampling_rate
)
\
to
obtain
the
raw
audio
data
of
the
file
/
url
.
)
}
}
function
constraint_to_multiple_of
(
val
multiple
minVal
=
0
maxVal
=
null
)
{
const
a
=
val
/
multiple
;
let
x
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
bankers_round
)
(
a
)
*
multiple
;
if
(
maxVal
!
=
=
null
&
&
x
>
maxVal
)
{
x
=
Math
.
floor
(
a
)
*
multiple
;
}
if
(
x
<
minVal
)
{
x
=
Math
.
ceil
(
a
)
*
multiple
;
}
return
x
;
}
function
enforce_size_divisibility
(
[
width
height
]
divisor
)
{
return
[
Math
.
max
(
Math
.
floor
(
width
/
divisor
)
1
)
*
divisor
Math
.
max
(
Math
.
floor
(
height
/
divisor
)
1
)
*
divisor
]
;
}
class
FeatureExtractor
extends
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
Callable
{
constructor
(
config
)
{
super
(
)
;
this
.
config
=
config
}
}
class
ImageFeatureExtractor
extends
FeatureExtractor
{
constructor
(
config
)
{
super
(
config
)
;
this
.
image_mean
=
this
.
config
.
image_mean
?
?
this
.
config
.
mean
;
this
.
image_std
=
this
.
config
.
image_std
?
?
this
.
config
.
std
;
this
.
resample
=
this
.
config
.
resample
?
?
2
;
this
.
do_rescale
=
this
.
config
.
do_rescale
?
?
true
;
this
.
rescale_factor
=
this
.
config
.
rescale_factor
?
?
(
1
/
255
)
;
this
.
do_normalize
=
this
.
config
.
do_normalize
;
this
.
do_resize
=
this
.
config
.
do_resize
;
this
.
do_thumbnail
=
this
.
config
.
do_thumbnail
;
this
.
size
=
this
.
config
.
size
;
this
.
size_divisibility
=
this
.
config
.
size_divisibility
?
?
this
.
config
.
size_divisor
;
this
.
do_center_crop
=
this
.
config
.
do_center_crop
;
this
.
crop_size
=
this
.
config
.
crop_size
;
this
.
do_convert_rgb
=
this
.
config
.
do_convert_rgb
?
?
true
;
this
.
do_crop_margin
=
this
.
config
.
do_crop_margin
;
this
.
pad_size
=
this
.
config
.
pad_size
;
this
.
do_pad
=
this
.
config
.
do_pad
;
if
(
this
.
do_pad
&
&
!
this
.
pad_size
&
&
this
.
size
&
&
this
.
size
.
width
!
=
=
undefined
&
&
this
.
size
.
height
!
=
=
undefined
)
{
this
.
pad_size
=
this
.
size
}
}
async
thumbnail
(
image
size
resample
=
2
)
{
const
input_height
=
image
.
height
;
const
input_width
=
image
.
width
;
const
output_height
=
size
.
height
;
const
output_width
=
size
.
width
;
let
height
=
Math
.
min
(
input_height
output_height
)
let
width
=
Math
.
min
(
input_width
output_width
)
if
(
height
=
=
=
input_height
&
&
width
=
=
=
input_width
)
{
return
image
;
}
if
(
input_height
>
input_width
)
{
width
=
Math
.
floor
(
input_width
*
height
/
input_height
)
;
}
else
if
(
input_width
>
input_height
)
{
height
=
Math
.
floor
(
input_height
*
width
/
input_width
)
;
}
return
await
image
.
resize
(
width
height
{
resample
}
)
;
}
async
crop_margin
(
image
gray_threshold
=
200
)
{
const
gray_image
=
image
.
clone
(
)
.
grayscale
(
)
;
const
minValue
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
min
)
(
gray_image
.
data
)
[
0
]
;
const
maxValue
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
max
)
(
gray_image
.
data
)
[
0
]
;
const
diff
=
maxValue
-
minValue
;
if
(
diff
=
=
=
0
)
{
return
image
;
}
const
threshold
=
gray_threshold
/
255
;
let
x_min
=
gray_image
.
width
y_min
=
gray_image
.
height
x_max
=
0
y_max
=
0
;
for
(
let
j
=
0
;
j
<
gray_image
.
height
;
+
+
j
)
{
const
row
=
j
*
gray_image
.
width
;
for
(
let
i
=
0
;
i
<
gray_image
.
width
;
+
+
i
)
{
if
(
(
gray_image
.
data
[
row
+
i
]
-
minValue
)
/
diff
<
threshold
)
{
x_min
=
Math
.
min
(
x_min
i
)
;
y_min
=
Math
.
min
(
y_min
j
)
;
x_max
=
Math
.
max
(
x_max
i
)
;
y_max
=
Math
.
max
(
y_max
j
)
;
}
}
}
image
=
await
image
.
crop
(
[
x_min
y_min
x_max
y_max
]
)
;
return
image
;
}
pad_image
(
pixelData
imgDims
padSize
{
mode
=
'
constant
'
center
=
false
constant_values
=
0
}
=
{
}
)
{
const
[
imageHeight
imageWidth
imageChannels
]
=
imgDims
;
let
paddedImageWidth
paddedImageHeight
;
if
(
typeof
padSize
=
=
=
'
number
'
)
{
paddedImageWidth
=
padSize
;
paddedImageHeight
=
padSize
;
}
else
{
paddedImageWidth
=
padSize
.
width
;
paddedImageHeight
=
padSize
.
height
;
}
if
(
paddedImageWidth
!
=
=
imageWidth
|
|
paddedImageHeight
!
=
=
imageHeight
)
{
const
paddedPixelData
=
new
Float32Array
(
paddedImageWidth
*
paddedImageHeight
*
imageChannels
)
;
if
(
Array
.
isArray
(
constant_values
)
)
{
for
(
let
i
=
0
;
i
<
paddedPixelData
.
length
;
+
+
i
)
{
paddedPixelData
[
i
]
=
constant_values
[
i
%
imageChannels
]
;
}
}
else
if
(
constant_values
!
=
=
0
)
{
paddedPixelData
.
fill
(
constant_values
)
;
}
const
[
left
top
]
=
center
?
[
Math
.
floor
(
(
paddedImageWidth
-
imageWidth
)
/
2
)
Math
.
floor
(
(
paddedImageHeight
-
imageHeight
)
/
2
)
]
:
[
0
0
]
;
for
(
let
i
=
0
;
i
<
imageHeight
;
+
+
i
)
{
const
a
=
(
i
+
top
)
*
paddedImageWidth
;
const
b
=
i
*
imageWidth
;
for
(
let
j
=
0
;
j
<
imageWidth
;
+
+
j
)
{
const
c
=
(
a
+
j
+
left
)
*
imageChannels
;
const
d
=
(
b
+
j
)
*
imageChannels
;
for
(
let
k
=
0
;
k
<
imageChannels
;
+
+
k
)
{
paddedPixelData
[
c
+
k
]
=
pixelData
[
d
+
k
]
;
}
}
}
if
(
mode
=
=
=
'
symmetric
'
)
{
if
(
center
)
{
throw
new
Error
(
'
center
padding
is
not
supported
when
mode
is
set
to
symmetric
.
'
)
;
}
const
h1
=
imageHeight
-
1
;
const
w1
=
imageWidth
-
1
;
for
(
let
i
=
0
;
i
<
paddedImageHeight
;
+
+
i
)
{
const
a
=
i
*
paddedImageWidth
;
const
b
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
calculateReflectOffset
)
(
i
h1
)
*
imageWidth
;
for
(
let
j
=
0
;
j
<
paddedImageWidth
;
+
+
j
)
{
if
(
i
<
imageHeight
&
&
j
<
imageWidth
)
continue
;
const
c
=
(
a
+
j
)
*
imageChannels
;
const
d
=
(
b
+
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
calculateReflectOffset
)
(
j
w1
)
)
*
imageChannels
;
for
(
let
k
=
0
;
k
<
imageChannels
;
+
+
k
)
{
paddedPixelData
[
c
+
k
]
=
pixelData
[
d
+
k
]
;
}
}
}
}
pixelData
=
paddedPixelData
;
imgDims
=
[
paddedImageHeight
paddedImageWidth
imageChannels
]
}
return
[
pixelData
imgDims
]
;
}
rescale
(
pixelData
)
{
for
(
let
i
=
0
;
i
<
pixelData
.
length
;
+
+
i
)
{
pixelData
[
i
]
=
this
.
rescale_factor
*
pixelData
[
i
]
;
}
}
get_resize_output_image_size
(
image
size
)
{
const
[
srcWidth
srcHeight
]
=
image
.
size
;
let
shortest_edge
;
let
longest_edge
;
if
(
this
.
do_thumbnail
)
{
const
{
height
width
}
=
size
;
shortest_edge
=
Math
.
min
(
height
width
)
}
else
if
(
Number
.
isInteger
(
size
)
)
{
shortest_edge
=
size
;
longest_edge
=
this
.
config
.
max_size
?
?
shortest_edge
;
}
else
if
(
size
!
=
=
undefined
)
{
shortest_edge
=
size
.
shortest_edge
;
longest_edge
=
size
.
longest_edge
;
}
if
(
shortest_edge
!
=
=
undefined
|
|
longest_edge
!
=
=
undefined
)
{
const
shortResizeFactor
=
shortest_edge
=
=
=
undefined
?
1
:
Math
.
max
(
shortest_edge
/
srcWidth
shortest_edge
/
srcHeight
)
;
const
newWidth
=
srcWidth
*
shortResizeFactor
;
const
newHeight
=
srcHeight
*
shortResizeFactor
;
const
longResizeFactor
=
longest_edge
=
=
=
undefined
?
1
:
Math
.
min
(
longest_edge
/
newWidth
longest_edge
/
newHeight
)
;
let
finalWidth
=
Math
.
floor
(
Number
(
(
newWidth
*
longResizeFactor
)
.
toFixed
(
2
)
)
)
;
let
finalHeight
=
Math
.
floor
(
Number
(
(
newHeight
*
longResizeFactor
)
.
toFixed
(
2
)
)
)
;
if
(
this
.
size_divisibility
!
=
=
undefined
)
{
[
finalWidth
finalHeight
]
=
enforce_size_divisibility
(
[
finalWidth
finalHeight
]
this
.
size_divisibility
)
}
return
[
finalWidth
finalHeight
]
;
}
else
if
(
size
!
=
=
undefined
&
&
size
.
width
!
=
=
undefined
&
&
size
.
height
!
=
=
undefined
)
{
let
newWidth
=
size
.
width
;
let
newHeight
=
size
.
height
;
if
(
this
.
config
.
keep_aspect_ratio
&
&
this
.
config
.
ensure_multiple_of
)
{
let
scale_height
=
newHeight
/
srcHeight
;
let
scale_width
=
newWidth
/
srcWidth
;
if
(
Math
.
abs
(
1
-
scale_width
)
<
Math
.
abs
(
1
-
scale_height
)
)
{
scale_height
=
scale_width
;
}
else
{
scale_width
=
scale_height
;
}
newHeight
=
constraint_to_multiple_of
(
scale_height
*
srcHeight
this
.
config
.
ensure_multiple_of
)
;
newWidth
=
constraint_to_multiple_of
(
scale_width
*
srcWidth
this
.
config
.
ensure_multiple_of
)
;
}
return
[
newWidth
newHeight
]
;
}
else
if
(
this
.
size_divisibility
!
=
=
undefined
)
{
return
enforce_size_divisibility
(
[
srcWidth
srcHeight
]
this
.
size_divisibility
)
;
}
else
{
throw
new
Error
(
Could
not
resize
image
due
to
unsupported
\
this
.
size
\
option
in
config
:
{
JSON
.
stringify
(
size
)
}
)
;
}
}
async
resize
(
image
)
{
const
[
newWidth
newHeight
]
=
this
.
get_resize_output_image_size
(
image
this
.
size
)
;
return
await
image
.
resize
(
newWidth
newHeight
{
resample
:
this
.
resample
}
)
;
}
async
preprocess
(
image
{
do_normalize
=
null
do_pad
=
null
do_convert_rgb
=
null
do_convert_grayscale
=
null
}
=
{
}
)
{
if
(
this
.
do_crop_margin
)
{
image
=
await
this
.
crop_margin
(
image
)
;
}
const
[
srcWidth
srcHeight
]
=
image
.
size
;
if
(
do_convert_rgb
?
?
this
.
do_convert_rgb
)
{
image
=
image
.
rgb
(
)
;
}
else
if
(
do_convert_grayscale
)
{
image
=
image
.
grayscale
(
)
;
}
if
(
this
.
do_resize
)
{
image
=
await
this
.
resize
(
image
)
;
}
if
(
this
.
do_thumbnail
)
{
image
=
await
this
.
thumbnail
(
image
this
.
size
this
.
resample
)
;
}
if
(
this
.
do_center_crop
)
{
let
crop_width
;
let
crop_height
;
if
(
Number
.
isInteger
(
this
.
crop_size
)
)
{
crop_width
=
this
.
crop_size
;
crop_height
=
this
.
crop_size
;
}
else
{
crop_width
=
this
.
crop_size
.
width
;
crop_height
=
this
.
crop_size
.
height
;
}
image
=
await
image
.
center_crop
(
crop_width
crop_height
)
;
}
const
reshaped_input_size
=
[
image
.
height
image
.
width
]
;
let
pixelData
=
Float32Array
.
from
(
image
.
data
)
;
let
imgDims
=
[
image
.
height
image
.
width
image
.
channels
]
;
if
(
this
.
do_rescale
)
{
this
.
rescale
(
pixelData
)
;
}
if
(
do_normalize
?
?
this
.
do_normalize
)
{
let
image_mean
=
this
.
image_mean
;
if
(
!
Array
.
isArray
(
this
.
image_mean
)
)
{
image_mean
=
new
Array
(
image
.
channels
)
.
fill
(
image_mean
)
;
}
let
image_std
=
this
.
image_std
;
if
(
!
Array
.
isArray
(
this
.
image_std
)
)
{
image_std
=
new
Array
(
image
.
channels
)
.
fill
(
image_mean
)
;
}
if
(
image_mean
.
length
!
=
=
image
.
channels
|
|
image_std
.
length
!
=
=
image
.
channels
)
{
throw
new
Error
(
When
set
to
arrays
the
length
of
\
image_mean
\
(
{
image_mean
.
length
}
)
and
\
image_std
\
(
{
image_std
.
length
}
)
must
match
the
number
of
channels
in
the
image
(
{
image
.
channels
}
)
.
)
;
}
for
(
let
i
=
0
;
i
<
pixelData
.
length
;
i
+
=
image
.
channels
)
{
for
(
let
j
=
0
;
j
<
image
.
channels
;
+
+
j
)
{
pixelData
[
i
+
j
]
=
(
pixelData
[
i
+
j
]
-
image_mean
[
j
]
)
/
image_std
[
j
]
;
}
}
}
if
(
do_pad
?
?
this
.
do_pad
)
{
if
(
this
.
pad_size
)
{
const
padded
=
this
.
pad_image
(
pixelData
[
image
.
height
image
.
width
image
.
channels
]
this
.
pad_size
)
;
[
pixelData
imgDims
]
=
padded
;
}
else
if
(
this
.
size_divisibility
)
{
const
[
paddedWidth
paddedHeight
]
=
enforce_size_divisibility
(
[
imgDims
[
1
]
imgDims
[
0
]
]
this
.
size_divisibility
)
;
[
pixelData
imgDims
]
=
this
.
pad_image
(
pixelData
imgDims
{
width
:
paddedWidth
height
:
paddedHeight
}
)
;
}
}
const
pixel_values
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
float32
'
pixelData
imgDims
)
.
permute
(
2
0
1
)
;
return
{
original_size
:
[
srcHeight
srcWidth
]
reshaped_input_size
:
reshaped_input_size
pixel_values
:
pixel_values
}
}
async
_call
(
images
.
.
.
args
)
{
if
(
!
Array
.
isArray
(
images
)
)
{
images
=
[
images
]
;
}
const
imageData
=
await
Promise
.
all
(
images
.
map
(
x
=
>
this
.
preprocess
(
x
)
)
)
;
const
pixel_values
=
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
stack
)
(
imageData
.
map
(
x
=
>
x
.
pixel_values
)
0
)
;
return
{
pixel_values
:
pixel_values
original_sizes
:
imageData
.
map
(
x
=
>
x
.
original_size
)
reshaped_input_sizes
:
imageData
.
map
(
x
=
>
x
.
reshaped_input_size
)
}
}
}
class
SegformerFeatureExtractor
extends
ImageFeatureExtractor
{
post_process_semantic_segmentation
(
outputs
target_sizes
=
null
)
{
const
logits
=
outputs
.
logits
;
const
batch_size
=
logits
.
dims
[
0
]
;
if
(
target_sizes
!
=
=
null
&
&
target_sizes
.
length
!
=
=
batch_size
)
{
throw
Error
(
"
Make
sure
that
you
pass
in
as
many
target
sizes
as
the
batch
dimension
of
the
logits
"
)
}
const
toReturn
=
[
]
;
for
(
let
i
=
0
;
i
<
batch_size
;
+
+
i
)
{
const
target_size
=
target_sizes
!
=
=
null
?
target_sizes
[
i
]
:
null
;
let
data
=
logits
[
i
]
;
if
(
target_size
!
=
=
null
)
{
data
=
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
interpolate
)
(
data
target_size
'
bilinear
'
false
)
;
}
const
[
height
width
]
=
target_size
?
?
data
.
dims
.
slice
(
-
2
)
;
const
segmentation
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
int32
'
new
Int32Array
(
height
*
width
)
[
height
width
]
)
;
const
buffer
=
data
[
0
]
.
data
;
for
(
let
j
=
1
;
j
<
data
.
dims
[
0
]
;
+
+
j
)
{
const
row
=
data
[
j
]
.
data
;
for
(
let
k
=
0
;
k
<
row
.
length
;
+
+
k
)
{
if
(
row
[
k
]
>
buffer
[
k
]
)
{
buffer
[
k
]
=
row
[
k
]
;
segmentation
.
data
[
k
]
=
j
;
}
}
}
const
hasLabel
=
new
Array
(
data
.
dims
[
0
]
)
;
const
out
=
segmentation
.
data
;
for
(
let
j
=
0
;
j
<
out
.
length
;
+
+
j
)
{
const
index
=
out
[
j
]
;
hasLabel
[
index
]
=
index
;
}
const
labels
=
hasLabel
.
filter
(
x
=
>
x
!
=
=
undefined
)
;
toReturn
.
push
(
{
segmentation
labels
}
)
;
}
return
toReturn
;
}
}
class
DPTFeatureExtractor
extends
ImageFeatureExtractor
{
}
class
DPTImageProcessor
extends
DPTFeatureExtractor
{
}
class
BitImageProcessor
extends
ImageFeatureExtractor
{
}
class
GLPNFeatureExtractor
extends
ImageFeatureExtractor
{
}
class
CLIPFeatureExtractor
extends
ImageFeatureExtractor
{
}
class
ChineseCLIPFeatureExtractor
extends
ImageFeatureExtractor
{
}
class
SiglipImageProcessor
extends
ImageFeatureExtractor
{
}
class
ConvNextFeatureExtractor
extends
ImageFeatureExtractor
{
constructor
(
config
)
{
super
(
config
)
;
this
.
crop_pct
=
this
.
config
.
crop_pct
?
?
(
224
/
256
)
;
}
async
resize
(
image
)
{
const
shortest_edge
=
this
.
size
?
.
shortest_edge
;
if
(
shortest_edge
=
=
=
undefined
)
{
throw
new
Error
(
Size
dictionary
must
contain
'
shortest_edge
'
key
.
)
;
}
if
(
shortest_edge
<
384
)
{
const
resize_shortest_edge
=
Math
.
floor
(
shortest_edge
/
this
.
crop_pct
)
;
const
[
newWidth
newHeight
]
=
this
.
get_resize_output_image_size
(
image
{
shortest_edge
:
resize_shortest_edge
}
)
;
image
=
await
image
.
resize
(
newWidth
newHeight
{
resample
:
this
.
resample
}
)
;
image
=
await
image
.
center_crop
(
shortest_edge
shortest_edge
)
;
}
else
{
image
=
await
image
.
resize
(
shortest_edge
shortest_edge
{
resample
:
this
.
resample
}
)
;
}
return
image
;
}
}
class
ConvNextImageProcessor
extends
ConvNextFeatureExtractor
{
}
class
ViTFeatureExtractor
extends
ImageFeatureExtractor
{
}
class
ViTImageProcessor
extends
ImageFeatureExtractor
{
}
class
EfficientNetImageProcessor
extends
ImageFeatureExtractor
{
constructor
(
config
)
{
super
(
config
)
;
this
.
include_top
=
this
.
config
.
include_top
?
?
true
;
if
(
this
.
include_top
)
{
this
.
image_std
=
this
.
image_std
.
map
(
x
=
>
x
*
x
)
;
}
}
}
class
MobileViTFeatureExtractor
extends
ImageFeatureExtractor
{
}
class
OwlViTFeatureExtractor
extends
ImageFeatureExtractor
{
post_process_object_detection
(
.
.
.
args
)
{
return
post_process_object_detection
(
.
.
.
args
)
;
}
}
class
Owlv2ImageProcessor
extends
OwlViTFeatureExtractor
{
}
class
DeiTFeatureExtractor
extends
ImageFeatureExtractor
{
}
class
BeitFeatureExtractor
extends
ImageFeatureExtractor
{
}
class
DonutFeatureExtractor
extends
ImageFeatureExtractor
{
pad_image
(
pixelData
imgDims
padSize
options
=
{
}
)
{
const
[
imageHeight
imageWidth
imageChannels
]
=
imgDims
;
let
image_mean
=
this
.
image_mean
;
if
(
!
Array
.
isArray
(
this
.
image_mean
)
)
{
image_mean
=
new
Array
(
imageChannels
)
.
fill
(
image_mean
)
;
}
let
image_std
=
this
.
image_std
;
if
(
!
Array
.
isArray
(
image_std
)
)
{
image_std
=
new
Array
(
imageChannels
)
.
fill
(
image_mean
)
;
}
const
constant_values
=
image_mean
.
map
(
(
x
i
)
=
>
-
x
/
image_std
[
i
]
)
;
return
super
.
pad_image
(
pixelData
imgDims
padSize
{
center
:
true
constant_values
:
constant_values
.
.
.
options
}
)
;
}
}
class
NougatImageProcessor
extends
DonutFeatureExtractor
{
}
class
DetrFeatureExtractor
extends
ImageFeatureExtractor
{
async
_call
(
images
)
{
const
result
=
await
super
.
_call
(
images
)
;
const
maskSize
=
[
result
.
pixel_values
.
dims
[
0
]
64
64
]
;
const
pixel_mask
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
int64
'
new
BigInt64Array
(
maskSize
.
reduce
(
(
a
b
)
=
>
a
*
b
)
)
.
fill
(
1n
)
maskSize
)
;
return
{
.
.
.
result
pixel_mask
}
;
}
post_process_object_detection
(
.
.
.
args
)
{
return
post_process_object_detection
(
.
.
.
args
)
;
}
remove_low_and_no_objects
(
class_logits
mask_logits
object_mask_threshold
num_labels
)
{
let
mask_probs_item
=
[
]
;
let
pred_scores_item
=
[
]
;
let
pred_labels_item
=
[
]
;
for
(
let
j
=
0
;
j
<
class_logits
.
dims
[
0
]
;
+
+
j
)
{
let
cls
=
class_logits
[
j
]
;
let
mask
=
mask_logits
[
j
]
;
let
pred_label
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
max
)
(
cls
.
data
)
[
1
]
;
if
(
pred_label
=
=
=
num_labels
)
{
continue
;
}
let
scores
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
softmax
)
(
cls
.
data
)
;
let
pred_score
=
scores
[
pred_label
]
;
if
(
pred_score
>
object_mask_threshold
)
{
mask_probs_item
.
push
(
mask
)
;
pred_scores_item
.
push
(
pred_score
)
;
pred_labels_item
.
push
(
pred_label
)
;
}
}
return
[
mask_probs_item
pred_scores_item
pred_labels_item
]
;
}
check_segment_validity
(
mask_labels
mask_probs
k
mask_threshold
=
0
.
5
overlap_mask_area_threshold
=
0
.
8
)
{
let
mask_k
=
[
]
;
let
mask_k_area
=
0
;
let
original_area
=
0
;
for
(
let
i
=
0
;
i
<
mask_labels
.
length
;
+
+
i
)
{
if
(
mask_labels
[
i
]
=
=
=
k
)
{
mask_k
.
push
(
i
)
;
+
+
mask_k_area
;
}
if
(
mask_probs
[
k
]
.
data
[
i
]
>
=
mask_threshold
)
{
+
+
original_area
;
}
}
let
mask_exists
=
mask_k_area
>
0
&
&
original_area
>
0
;
if
(
mask_exists
)
{
let
area_ratio
=
mask_k_area
/
original_area
;
mask_exists
=
area_ratio
>
overlap_mask_area_threshold
;
}
return
[
mask_exists
mask_k
]
}
compute_segments
(
mask_probs
pred_scores
pred_labels
mask_threshold
overlap_mask_area_threshold
label_ids_to_fuse
=
null
target_size
=
null
)
{
let
[
height
width
]
=
target_size
?
?
mask_probs
[
0
]
.
dims
;
let
segmentation
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
int32
'
new
Int32Array
(
height
*
width
)
[
height
width
]
)
;
let
segments
=
[
]
;
if
(
target_size
!
=
=
null
)
{
for
(
let
i
=
0
;
i
<
mask_probs
.
length
;
+
+
i
)
{
mask_probs
[
i
]
=
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
interpolate
)
(
mask_probs
[
i
]
target_size
'
bilinear
'
false
)
;
}
}
let
mask_labels
=
new
Int32Array
(
mask_probs
[
0
]
.
data
.
length
)
;
let
bestScores
=
new
Float32Array
(
mask_probs
[
0
]
.
data
.
length
)
;
for
(
let
i
=
0
;
i
<
mask_probs
.
length
;
+
+
i
)
{
let
score
=
pred_scores
[
i
]
;
for
(
let
j
=
0
;
j
<
mask_probs
[
i
]
.
data
.
length
;
+
+
j
)
{
mask_probs
[
i
]
.
data
[
j
]
*
=
score
if
(
mask_probs
[
i
]
.
data
[
j
]
>
bestScores
[
j
]
)
{
mask_labels
[
j
]
=
i
;
bestScores
[
j
]
=
mask_probs
[
i
]
.
data
[
j
]
;
}
}
}
let
current_segment_id
=
0
;
for
(
let
k
=
0
;
k
<
pred_labels
.
length
;
+
+
k
)
{
let
pred_class
=
pred_labels
[
k
]
;
let
[
mask_exists
mask_k
]
=
this
.
check_segment_validity
(
mask_labels
mask_probs
k
mask_threshold
overlap_mask_area_threshold
)
if
(
!
mask_exists
)
{
continue
;
}
+
+
current_segment_id
;
for
(
let
index
of
mask_k
)
{
segmentation
.
data
[
index
]
=
current_segment_id
;
}
segments
.
push
(
{
id
:
current_segment_id
label_id
:
pred_class
score
:
pred_scores
[
k
]
}
)
}
return
[
segmentation
segments
]
;
}
post_process_panoptic_segmentation
(
outputs
threshold
=
0
.
5
mask_threshold
=
0
.
5
overlap_mask_area_threshold
=
0
.
8
label_ids_to_fuse
=
null
target_sizes
=
null
)
{
if
(
label_ids_to_fuse
=
=
=
null
)
{
console
.
warn
(
"
label_ids_to_fuse
unset
.
No
instance
will
be
fused
.
"
)
label_ids_to_fuse
=
new
Set
(
)
;
}
const
class_queries_logits
=
outputs
.
logits
;
const
masks_queries_logits
=
outputs
.
pred_masks
;
const
mask_probs
=
masks_queries_logits
.
sigmoid
(
)
let
[
batch_size
num_queries
num_labels
]
=
class_queries_logits
.
dims
;
num_labels
-
=
1
;
if
(
target_sizes
!
=
=
null
&
&
target_sizes
.
length
!
=
=
batch_size
)
{
throw
Error
(
"
Make
sure
that
you
pass
in
as
many
target
sizes
as
the
batch
dimension
of
the
logits
"
)
}
let
toReturn
=
[
]
;
for
(
let
i
=
0
;
i
<
batch_size
;
+
+
i
)
{
let
target_size
=
target_sizes
!
=
=
null
?
target_sizes
[
i
]
:
null
;
let
class_logits
=
class_queries_logits
[
i
]
;
let
mask_logits
=
mask_probs
[
i
]
;
let
[
mask_probs_item
pred_scores_item
pred_labels_item
]
=
this
.
remove_low_and_no_objects
(
class_logits
mask_logits
threshold
num_labels
)
;
if
(
pred_labels_item
.
length
=
=
=
0
)
{
let
[
height
width
]
=
target_size
?
?
mask_logits
.
dims
.
slice
(
-
2
)
;
let
segmentation
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
int32
'
new
Int32Array
(
height
*
width
)
.
fill
(
-
1
)
[
height
width
]
)
toReturn
.
push
(
{
segmentation
:
segmentation
segments_info
:
[
]
}
)
;
continue
;
}
let
[
segmentation
segments
]
=
this
.
compute_segments
(
mask_probs_item
pred_scores_item
pred_labels_item
mask_threshold
overlap_mask_area_threshold
label_ids_to_fuse
target_size
)
toReturn
.
push
(
{
segmentation
:
segmentation
segments_info
:
segments
}
)
}
return
toReturn
;
}
post_process_instance_segmentation
(
)
{
throw
Error
(
"
Not
implemented
yet
"
)
;
}
}
class
YolosFeatureExtractor
extends
ImageFeatureExtractor
{
post_process_object_detection
(
.
.
.
args
)
{
return
post_process_object_detection
(
.
.
.
args
)
;
}
}
class
SamImageProcessor
extends
ImageFeatureExtractor
{
reshape_input_points
(
input_points
original_sizes
reshaped_input_sizes
)
{
input_points
=
structuredClone
(
input_points
)
;
let
shape
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
calculateDimensions
)
(
input_points
)
;
if
(
shape
.
length
=
=
=
3
)
{
shape
=
[
1
.
.
.
shape
]
;
input_points
=
[
input_points
]
;
}
else
if
(
shape
.
length
!
=
=
4
)
{
throw
Error
(
"
The
input_points
must
be
a
4D
tensor
of
shape
batch_size
point_batch_size
nb_points_per_image
2
.
"
)
}
for
(
let
i
=
0
;
i
<
input_points
.
length
;
+
+
i
)
{
let
originalImageSize
=
original_sizes
[
i
]
;
let
reshapedImageSize
=
reshaped_input_sizes
[
i
]
;
let
resizeFactors
=
[
reshapedImageSize
[
0
]
/
originalImageSize
[
0
]
reshapedImageSize
[
1
]
/
originalImageSize
[
1
]
]
for
(
let
j
=
0
;
j
<
input_points
[
i
]
.
length
;
+
+
j
)
{
for
(
let
k
=
0
;
k
<
input_points
[
i
]
[
j
]
.
length
;
+
+
k
)
{
for
(
let
w
=
0
;
w
<
input_points
[
i
]
[
j
]
[
k
]
.
length
;
+
+
w
)
{
input_points
[
i
]
[
j
]
[
k
]
[
w
]
*
=
resizeFactors
[
w
]
;
}
}
}
}
return
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
float32
'
Float32Array
.
from
(
input_points
.
flat
(
Infinity
)
)
shape
)
}
add_input_labels
(
input_labels
input_points
)
{
let
shape
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
calculateDimensions
)
(
input_labels
)
;
if
(
shape
.
length
=
=
=
2
)
{
shape
=
[
1
.
.
.
shape
]
;
input_labels
=
[
input_labels
]
;
}
else
if
(
shape
.
length
!
=
=
3
)
{
throw
Error
(
"
The
input_points
must
be
a
4D
tensor
of
shape
batch_size
point_batch_size
nb_points_per_image
2
.
"
)
}
if
(
shape
.
some
(
(
x
i
)
=
>
x
!
=
=
input_points
.
dims
[
i
]
)
)
{
throw
Error
(
The
first
{
shape
.
length
}
dimensions
of
'
input_points
'
and
'
input_labels
'
must
be
the
same
.
)
}
return
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
int64
'
input_labels
.
flat
(
Infinity
)
.
map
(
BigInt
)
shape
)
}
async
_call
(
images
input_points
=
null
input_labels
=
null
)
{
const
processed
=
await
super
.
_call
(
images
)
;
if
(
input_points
)
{
processed
.
input_points
=
this
.
reshape_input_points
(
input_points
processed
.
original_sizes
processed
.
reshaped_input_sizes
)
;
}
if
(
input_labels
)
{
if
(
!
processed
.
input_points
)
{
throw
Error
(
"
input_points
must
be
provided
if
input_labels
are
provided
.
"
)
}
processed
.
input_labels
=
this
.
add_input_labels
(
input_labels
processed
.
input_points
)
;
}
return
processed
;
}
post_process_masks
(
masks
original_sizes
reshaped_input_sizes
{
mask_threshold
=
0
.
0
binarize
=
true
pad_size
=
null
}
=
{
}
)
{
const
output_masks
=
[
]
;
pad_size
=
pad_size
?
?
this
.
pad_size
;
const
target_image_size
=
[
pad_size
.
height
pad_size
.
width
]
;
for
(
let
i
=
0
;
i
<
original_sizes
.
length
;
+
+
i
)
{
const
original_size
=
original_sizes
[
i
]
;
const
reshaped_input_size
=
reshaped_input_sizes
[
i
]
;
const
mask
=
masks
[
i
]
;
const
interpolated_masks
=
[
]
;
for
(
let
j
=
0
;
j
<
mask
.
dims
[
0
]
;
+
+
j
)
{
const
m
=
mask
[
j
]
;
let
interpolated_mask
=
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
interpolate
)
(
m
target_image_size
'
bilinear
'
false
)
;
interpolated_mask
=
interpolated_mask
.
slice
(
null
[
0
reshaped_input_size
[
0
]
]
[
0
reshaped_input_size
[
1
]
]
)
;
interpolated_mask
=
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
interpolate
)
(
interpolated_mask
original_size
'
bilinear
'
false
)
;
if
(
binarize
)
{
const
binarizedMaskData
=
new
Uint8Array
(
interpolated_mask
.
data
.
length
)
;
for
(
let
i
=
0
;
i
<
interpolated_mask
.
data
.
length
;
+
+
i
)
{
if
(
interpolated_mask
.
data
[
i
]
>
mask_threshold
)
{
binarizedMaskData
[
i
]
=
1
;
}
}
interpolated_mask
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
bool
'
binarizedMaskData
interpolated_mask
.
dims
)
}
interpolated_masks
.
push
(
interpolated_mask
)
;
}
output_masks
.
push
(
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
stack
)
(
interpolated_masks
)
)
;
}
return
output_masks
;
}
}
class
Swin2SRImageProcessor
extends
ImageFeatureExtractor
{
pad_image
(
pixelData
imgDims
padSize
options
=
{
}
)
{
const
[
imageHeight
imageWidth
imageChannels
]
=
imgDims
;
return
super
.
pad_image
(
pixelData
imgDims
{
width
:
imageWidth
+
(
padSize
-
imageWidth
%
padSize
)
%
padSize
height
:
imageHeight
+
(
padSize
-
imageHeight
%
padSize
)
%
padSize
}
{
mode
:
'
symmetric
'
center
:
false
constant_values
:
-
1
.
.
.
options
}
)
}
}
class
VitMatteImageProcessor
extends
ImageFeatureExtractor
{
async
_call
(
images
trimaps
)
{
if
(
!
Array
.
isArray
(
images
)
)
{
images
=
[
images
]
;
}
if
(
!
Array
.
isArray
(
trimaps
)
)
{
trimaps
=
[
trimaps
]
;
}
const
imageData
=
await
Promise
.
all
(
images
.
map
(
x
=
>
this
.
preprocess
(
x
)
)
)
;
const
trimapData
=
await
Promise
.
all
(
trimaps
.
map
(
x
=
>
this
.
preprocess
(
x
{
do_normalize
:
false
do_convert_rgb
:
false
do_convert_grayscale
:
true
}
)
)
)
;
const
pixel_values
=
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
stack
)
(
imageData
.
map
(
(
x
i
)
=
>
(
0
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
cat
)
(
[
x
.
pixel_values
trimapData
[
i
]
.
pixel_values
]
0
)
)
0
)
;
return
{
pixel_values
:
pixel_values
original_sizes
:
imageData
.
map
(
x
=
>
x
.
original_size
)
reshaped_input_sizes
:
imageData
.
map
(
x
=
>
x
.
reshaped_input_size
)
}
}
}
class
WhisperFeatureExtractor
extends
FeatureExtractor
{
constructor
(
config
)
{
super
(
config
)
;
this
.
config
.
mel_filters
?
?
=
(
0
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
.
mel_filter_bank
)
(
Math
.
floor
(
1
+
this
.
config
.
n_fft
/
2
)
this
.
config
.
feature_size
0
.
0
8000
.
0
this
.
config
.
sampling_rate
"
slaney
"
"
slaney
"
)
;
this
.
window
=
(
0
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
.
window_function
)
(
this
.
config
.
n_fft
'
hann
'
)
;
}
_extract_fbank_features
(
waveform
)
{
const
{
data
dims
}
=
(
0
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
.
spectrogram
)
(
waveform
this
.
window
this
.
config
.
n_fft
this
.
config
.
hop_length
{
power
:
2
.
0
mel_filters
:
this
.
config
.
mel_filters
log_mel
:
'
log10
'
max_num_frames
:
this
.
config
.
nb_max_frames
}
)
const
maxValue
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
max
)
(
data
)
[
0
]
;
for
(
let
i
=
0
;
i
<
data
.
length
;
+
+
i
)
{
data
[
i
]
=
(
Math
.
max
(
data
[
i
]
maxValue
-
8
.
0
)
+
4
.
0
)
/
4
.
0
;
}
return
{
data
dims
}
;
}
async
_call
(
audio
)
{
validate_audio_inputs
(
audio
'
WhisperFeatureExtractor
'
)
;
let
waveform
;
if
(
audio
.
length
>
this
.
config
.
n_samples
)
{
console
.
warn
(
"
Attempting
to
extract
features
for
audio
longer
than
30
seconds
.
"
+
"
If
using
a
pipeline
to
extract
transcript
from
a
long
audio
clip
"
+
"
remember
to
specify
chunk_length_s
and
/
or
stride_length_s
.
"
)
;
waveform
=
audio
.
slice
(
0
this
.
config
.
n_samples
)
;
}
else
{
waveform
=
new
Float32Array
(
this
.
config
.
n_samples
)
;
waveform
.
set
(
audio
)
;
}
const
{
data
dims
}
=
this
.
_extract_fbank_features
(
waveform
)
;
return
{
input_features
:
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
float32
'
data
[
1
.
.
.
dims
]
)
}
;
}
}
class
Wav2Vec2FeatureExtractor
extends
FeatureExtractor
{
_zero_mean_unit_var_norm
(
input_values
)
{
const
sum
=
input_values
.
reduce
(
(
a
b
)
=
>
a
+
b
0
)
;
const
mean
=
sum
/
input_values
.
length
;
const
variance
=
input_values
.
reduce
(
(
a
b
)
=
>
a
+
(
b
-
mean
)
*
*
2
0
)
/
input_values
.
length
;
return
input_values
.
map
(
x
=
>
(
x
-
mean
)
/
Math
.
sqrt
(
variance
+
1e
-
7
)
)
;
}
async
_call
(
audio
)
{
validate_audio_inputs
(
audio
'
Wav2Vec2FeatureExtractor
'
)
;
if
(
audio
instanceof
Float64Array
)
{
audio
=
new
Float32Array
(
audio
)
;
}
let
input_values
=
audio
;
if
(
this
.
config
.
do_normalize
)
{
input_values
=
this
.
_zero_mean_unit_var_norm
(
input_values
)
;
}
const
shape
=
[
1
input_values
.
length
]
;
return
{
input_values
:
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
float32
'
input_values
shape
)
attention_mask
:
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
int64
'
new
BigInt64Array
(
input_values
.
length
)
.
fill
(
1n
)
shape
)
}
;
}
}
class
SeamlessM4TFeatureExtractor
extends
FeatureExtractor
{
constructor
(
config
)
{
super
(
config
)
;
const
sampling_rate
=
this
.
config
.
sampling_rate
;
const
mel_filters
=
(
0
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
.
mel_filter_bank
)
(
256
this
.
config
.
num_mel_bins
20
Math
.
floor
(
sampling_rate
/
2
)
sampling_rate
null
"
kaldi
"
true
)
;
for
(
let
i
=
0
;
i
<
mel_filters
.
length
;
+
+
i
)
{
mel_filters
[
i
]
.
push
(
0
)
;
}
this
.
mel_filters
=
mel_filters
;
this
.
window
=
(
0
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
.
window_function
)
(
400
'
povey
'
{
periodic
:
false
}
)
}
_extract_fbank_features
(
waveform
max_length
)
{
waveform
=
waveform
.
map
(
(
x
)
=
>
x
*
32768
)
return
(
0
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
.
spectrogram
)
(
waveform
this
.
window
400
160
{
fft_length
:
512
power
:
2
.
0
center
:
false
preemphasis
:
0
.
97
mel_filters
:
this
.
mel_filters
log_mel
:
'
log
'
mel_floor
:
1
.
192092955078125e
-
07
remove_dc_offset
:
true
max_num_frames
:
max_length
transpose
:
true
}
)
}
async
_call
(
audio
{
padding
=
true
pad_to_multiple_of
=
2
do_normalize_per_mel_bins
=
true
return_attention_mask
=
true
}
=
{
}
)
{
validate_audio_inputs
(
audio
'
SeamlessM4TFeatureExtractor
'
)
;
let
features
=
this
.
_extract_fbank_features
(
audio
this
.
config
.
max_length
)
;
if
(
do_normalize_per_mel_bins
)
{
const
[
num_features
feature_size
]
=
features
.
dims
;
for
(
let
i
=
0
;
i
<
feature_size
;
+
+
i
)
{
let
sum
=
0
;
for
(
let
j
=
0
;
j
<
num_features
;
+
+
j
)
{
sum
+
=
features
.
data
[
j
*
feature_size
+
i
]
;
}
const
mean
=
sum
/
num_features
;
let
variance
=
0
;
for
(
let
j
=
0
;
j
<
num_features
;
+
+
j
)
{
variance
+
=
(
features
.
data
[
j
*
feature_size
+
i
]
-
mean
)
*
*
2
;
}
variance
/
=
num_features
-
1
;
const
std
=
Math
.
sqrt
(
variance
+
1e
-
7
)
;
for
(
let
j
=
0
;
j
<
num_features
;
+
+
j
)
{
const
index
=
j
*
feature_size
+
i
;
features
.
data
[
index
]
=
(
features
.
data
[
index
]
-
mean
)
/
std
;
}
}
}
let
padded_attention_mask
;
if
(
padding
)
{
const
[
num_frames
num_channels
]
=
features
.
dims
;
const
pad_size
=
num_frames
%
pad_to_multiple_of
;
if
(
pad_size
>
0
)
{
const
padded_data
=
new
Float32Array
(
num_channels
*
(
num_frames
+
pad_size
)
)
;
padded_data
.
set
(
features
.
data
)
padded_data
.
fill
(
this
.
config
.
padding_value
features
.
data
.
length
)
const
numPaddedFrames
=
num_frames
+
pad_size
;
features
=
{
data
:
padded_data
dims
:
[
numPaddedFrames
num_channels
]
}
if
(
return_attention_mask
)
{
padded_attention_mask
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
int64
'
new
BigInt64Array
(
numPaddedFrames
)
[
1
numPaddedFrames
]
)
padded_attention_mask
.
data
.
fill
(
1n
0
num_frames
)
;
}
}
}
const
[
num_frames
num_channels
]
=
features
.
dims
;
const
stride
=
this
.
config
.
stride
;
const
remainder
=
num_frames
%
stride
;
if
(
remainder
!
=
=
0
)
{
throw
new
Error
(
The
number
of
frames
(
{
num_frames
}
)
must
be
a
multiple
of
the
stride
(
{
stride
}
)
.
)
}
const
input_features
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
float32
'
features
.
data
features
.
dims
)
.
view
(
1
Math
.
floor
(
num_frames
/
stride
)
num_channels
*
stride
)
;
const
result
=
{
input_features
}
if
(
return_attention_mask
)
{
const
reshapedNumFrames
=
input_features
.
dims
[
1
]
;
const
attention_mask
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
int64
'
new
BigInt64Array
(
reshapedNumFrames
)
[
1
reshapedNumFrames
]
)
;
if
(
padded_attention_mask
)
{
for
(
let
i
=
1
j
=
0
;
i
<
num_frames
;
i
+
=
stride
+
+
j
)
{
attention_mask
.
data
[
j
]
=
padded_attention_mask
.
data
[
i
]
;
}
}
else
{
attention_mask
.
data
.
fill
(
1n
)
;
}
result
.
attention_mask
=
attention_mask
;
}
return
result
;
}
}
class
ASTFeatureExtractor
extends
FeatureExtractor
{
constructor
(
config
)
{
super
(
config
)
;
const
sampling_rate
=
this
.
config
.
sampling_rate
;
const
mel_filters
=
(
0
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
.
mel_filter_bank
)
(
256
this
.
config
.
num_mel_bins
20
Math
.
floor
(
sampling_rate
/
2
)
sampling_rate
null
"
kaldi
"
true
)
;
for
(
let
i
=
0
;
i
<
mel_filters
.
length
;
+
+
i
)
{
mel_filters
[
i
]
.
push
(
0
)
;
}
this
.
mel_filters
=
mel_filters
;
this
.
window
=
(
0
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
.
window_function
)
(
400
'
hann
'
{
periodic
:
false
}
)
this
.
mean
=
this
.
config
.
mean
;
this
.
std
=
this
.
config
.
std
;
}
_extract_fbank_features
(
waveform
max_length
)
{
return
(
0
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
.
spectrogram
)
(
waveform
this
.
window
400
160
{
fft_length
:
512
power
:
2
.
0
center
:
false
preemphasis
:
0
.
97
mel_filters
:
this
.
mel_filters
log_mel
:
'
log
'
mel_floor
:
1
.
192092955078125e
-
07
remove_dc_offset
:
true
max_num_frames
:
max_length
transpose
:
true
}
)
}
async
_call
(
audio
)
{
validate_audio_inputs
(
audio
'
ASTFeatureExtractor
'
)
;
const
features
=
this
.
_extract_fbank_features
(
audio
this
.
config
.
max_length
)
;
if
(
this
.
config
.
do_normalize
)
{
const
denom
=
this
.
std
*
2
;
for
(
let
i
=
0
;
i
<
features
.
data
.
length
;
+
+
i
)
{
features
.
data
[
i
]
=
(
features
.
data
[
i
]
-
this
.
mean
)
/
denom
;
}
}
return
{
input_values
:
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
float32
'
features
.
data
[
1
.
.
.
features
.
dims
]
)
}
;
}
}
class
ClapFeatureExtractor
extends
FeatureExtractor
{
constructor
(
config
)
{
super
(
config
)
;
this
.
mel_filters
=
(
0
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
.
mel_filter_bank
)
(
this
.
config
.
nb_frequency_bins
this
.
config
.
feature_size
this
.
config
.
frequency_min
this
.
config
.
frequency_max
this
.
config
.
sampling_rate
null
"
htk
"
)
;
this
.
mel_filters_slaney
=
(
0
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
.
mel_filter_bank
)
(
this
.
config
.
nb_frequency_bins
this
.
config
.
feature_size
this
.
config
.
frequency_min
this
.
config
.
frequency_max
this
.
config
.
sampling_rate
"
slaney
"
"
slaney
"
)
;
this
.
window
=
(
0
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
.
window_function
)
(
this
.
config
.
fft_window_size
'
hann
'
)
}
_get_input_mel
(
waveform
max_length
truncation
padding
)
{
let
input_mel
;
let
longer
=
false
;
const
diff
=
waveform
.
length
-
max_length
;
if
(
diff
>
0
)
{
if
(
truncation
=
=
=
'
rand_trunc
'
)
{
longer
=
true
;
const
idx
=
Math
.
floor
(
Math
.
random
(
)
*
(
diff
+
1
)
)
;
waveform
=
waveform
.
subarray
(
idx
idx
+
max_length
)
;
input_mel
=
this
.
_extract_fbank_features
(
waveform
this
.
mel_filters_slaney
this
.
config
.
nb_max_samples
)
;
input_mel
.
dims
=
[
1
.
.
.
input_mel
.
dims
]
;
}
else
{
throw
new
Error
(
Truncation
strategy
"
{
truncation
}
"
not
implemented
)
}
}
else
{
if
(
diff
<
0
)
{
let
padded
=
new
Float64Array
(
max_length
)
;
padded
.
set
(
waveform
)
;
if
(
padding
=
=
=
'
repeat
'
)
{
for
(
let
i
=
waveform
.
length
;
i
<
max_length
;
i
+
=
waveform
.
length
)
{
padded
.
set
(
waveform
.
subarray
(
0
Math
.
min
(
waveform
.
length
max_length
-
i
)
)
i
)
;
}
}
else
if
(
padding
=
=
=
'
repeatpad
'
)
{
for
(
let
i
=
waveform
.
length
;
i
<
-
diff
;
i
+
=
waveform
.
length
)
{
padded
.
set
(
waveform
i
)
;
}
}
waveform
=
padded
;
}
if
(
truncation
=
=
=
'
fusion
'
)
{
throw
new
Error
(
Truncation
strategy
"
{
truncation
}
"
not
implemented
)
}
input_mel
=
this
.
_extract_fbank_features
(
waveform
this
.
mel_filters_slaney
this
.
config
.
nb_max_samples
)
;
input_mel
.
dims
=
[
1
.
.
.
input_mel
.
dims
]
;
}
return
{
.
.
.
input_mel
longer
}
}
_extract_fbank_features
(
waveform
mel_filters
max_length
=
null
)
{
return
(
0
_utils_audio_js__WEBPACK_IMPORTED_MODULE_5__
.
spectrogram
)
(
waveform
this
.
window
this
.
config
.
fft_window_size
this
.
config
.
hop_length
{
power
:
2
.
0
mel_filters
log_mel
:
'
dB
'
max_num_frames
:
max_length
do_pad
:
false
transpose
:
true
}
)
}
async
_call
(
audio
{
max_length
=
null
}
=
{
}
)
{
validate_audio_inputs
(
audio
'
ClapFeatureExtractor
'
)
;
const
padded_inputs
=
this
.
_get_input_mel
(
audio
max_length
?
?
this
.
config
.
nb_max_samples
this
.
config
.
truncation
this
.
config
.
padding
)
;
return
{
input_features
:
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
float32
'
padded_inputs
.
data
[
1
.
.
.
padded_inputs
.
dims
]
)
}
;
}
}
class
SpeechT5FeatureExtractor
extends
FeatureExtractor
{
}
class
Processor
extends
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
Callable
{
constructor
(
feature_extractor
)
{
super
(
)
;
this
.
feature_extractor
=
feature_extractor
;
}
async
_call
(
input
.
.
.
args
)
{
return
await
this
.
feature_extractor
(
input
.
.
.
args
)
;
}
}
class
SamProcessor
extends
Processor
{
async
_call
(
.
.
.
args
)
{
return
await
this
.
feature_extractor
(
.
.
.
args
)
;
}
post_process_masks
(
.
.
.
args
)
{
return
this
.
feature_extractor
.
post_process_masks
(
.
.
.
args
)
;
}
reshape_input_points
(
.
.
.
args
)
{
return
this
.
feature_extractor
.
reshape_input_points
(
.
.
.
args
)
;
}
}
class
WhisperProcessor
extends
Processor
{
async
_call
(
audio
)
{
return
await
this
.
feature_extractor
(
audio
)
}
}
class
Wav2Vec2ProcessorWithLM
extends
Processor
{
async
_call
(
audio
)
{
return
await
this
.
feature_extractor
(
audio
)
}
}
class
SpeechT5Processor
extends
Processor
{
async
_call
(
input
)
{
return
await
this
.
feature_extractor
(
input
)
}
}
class
OwlViTProcessor
extends
Processor
{
}
class
AutoProcessor
{
static
FEATURE_EXTRACTOR_CLASS_MAPPING
=
{
ImageFeatureExtractor
WhisperFeatureExtractor
ViTFeatureExtractor
MobileViTFeatureExtractor
OwlViTFeatureExtractor
Owlv2ImageProcessor
CLIPFeatureExtractor
ChineseCLIPFeatureExtractor
SiglipImageProcessor
ConvNextFeatureExtractor
ConvNextImageProcessor
SegformerFeatureExtractor
BitImageProcessor
DPTImageProcessor
DPTFeatureExtractor
GLPNFeatureExtractor
BeitFeatureExtractor
DeiTFeatureExtractor
DetrFeatureExtractor
YolosFeatureExtractor
DonutFeatureExtractor
NougatImageProcessor
EfficientNetImageProcessor
ViTImageProcessor
VitMatteImageProcessor
SamImageProcessor
Swin2SRImageProcessor
Wav2Vec2FeatureExtractor
SeamlessM4TFeatureExtractor
SpeechT5FeatureExtractor
ASTFeatureExtractor
ClapFeatureExtractor
}
static
PROCESSOR_CLASS_MAPPING
=
{
WhisperProcessor
Wav2Vec2ProcessorWithLM
SamProcessor
SpeechT5Processor
OwlViTProcessor
}
static
async
from_pretrained
(
pretrained_model_name_or_path
{
progress_callback
=
null
config
=
null
cache_dir
=
null
local_files_only
=
false
revision
=
'
main
'
}
=
{
}
)
{
let
preprocessorConfig
=
config
?
?
await
(
0
_utils_hub_js__WEBPACK_IMPORTED_MODULE_1__
.
getModelJSON
)
(
pretrained_model_name_or_path
'
preprocessor_config
.
json
'
true
{
progress_callback
config
cache_dir
local_files_only
revision
}
)
let
key
=
preprocessorConfig
.
feature_extractor_type
?
?
preprocessorConfig
.
image_processor_type
;
let
feature_extractor_class
=
this
.
FEATURE_EXTRACTOR_CLASS_MAPPING
[
key
]
;
if
(
!
feature_extractor_class
)
{
if
(
preprocessorConfig
.
size
!
=
=
undefined
)
{
console
.
warn
(
Feature
extractor
type
"
{
key
}
"
not
found
assuming
ImageFeatureExtractor
due
to
size
parameter
in
config
.
)
;
feature_extractor_class
=
ImageFeatureExtractor
;
}
else
{
throw
new
Error
(
Unknown
Feature
Extractor
type
:
{
key
}
)
;
}
}
let
processor_class
=
this
.
PROCESSOR_CLASS_MAPPING
[
preprocessorConfig
.
processor_class
]
?
?
Processor
;
let
feature_extractor
=
new
feature_extractor_class
(
preprocessorConfig
)
;
return
new
processor_class
(
feature_extractor
)
;
}
}
}
)
"
.
/
src
/
tokenizers
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
AlbertTokenizer
"
:
(
)
=
>
(
AlbertTokenizer
)
"
AutoTokenizer
"
:
(
)
=
>
(
AutoTokenizer
)
"
BartTokenizer
"
:
(
)
=
>
(
BartTokenizer
)
"
BertTokenizer
"
:
(
)
=
>
(
BertTokenizer
)
"
BlenderbotSmallTokenizer
"
:
(
)
=
>
(
BlenderbotSmallTokenizer
)
"
BlenderbotTokenizer
"
:
(
)
=
>
(
BlenderbotTokenizer
)
"
BloomTokenizer
"
:
(
)
=
>
(
BloomTokenizer
)
"
CLIPTokenizer
"
:
(
)
=
>
(
CLIPTokenizer
)
"
CamembertTokenizer
"
:
(
)
=
>
(
CamembertTokenizer
)
"
CodeGenTokenizer
"
:
(
)
=
>
(
CodeGenTokenizer
)
"
CodeLlamaTokenizer
"
:
(
)
=
>
(
CodeLlamaTokenizer
)
"
CohereTokenizer
"
:
(
)
=
>
(
CohereTokenizer
)
"
ConvBertTokenizer
"
:
(
)
=
>
(
ConvBertTokenizer
)
"
DebertaTokenizer
"
:
(
)
=
>
(
DebertaTokenizer
)
"
DebertaV2Tokenizer
"
:
(
)
=
>
(
DebertaV2Tokenizer
)
"
DistilBertTokenizer
"
:
(
)
=
>
(
DistilBertTokenizer
)
"
ElectraTokenizer
"
:
(
)
=
>
(
ElectraTokenizer
)
"
EsmTokenizer
"
:
(
)
=
>
(
EsmTokenizer
)
"
FalconTokenizer
"
:
(
)
=
>
(
FalconTokenizer
)
"
GPT2Tokenizer
"
:
(
)
=
>
(
GPT2Tokenizer
)
"
GPTNeoXTokenizer
"
:
(
)
=
>
(
GPTNeoXTokenizer
)
"
GemmaTokenizer
"
:
(
)
=
>
(
GemmaTokenizer
)
"
Grok1Tokenizer
"
:
(
)
=
>
(
Grok1Tokenizer
)
"
HerbertTokenizer
"
:
(
)
=
>
(
HerbertTokenizer
)
"
LlamaTokenizer
"
:
(
)
=
>
(
LlamaTokenizer
)
"
M2M100Tokenizer
"
:
(
)
=
>
(
M2M100Tokenizer
)
"
MBart50Tokenizer
"
:
(
)
=
>
(
MBart50Tokenizer
)
"
MBartTokenizer
"
:
(
)
=
>
(
MBartTokenizer
)
"
MPNetTokenizer
"
:
(
)
=
>
(
MPNetTokenizer
)
"
MarianTokenizer
"
:
(
)
=
>
(
MarianTokenizer
)
"
MobileBertTokenizer
"
:
(
)
=
>
(
MobileBertTokenizer
)
"
NllbTokenizer
"
:
(
)
=
>
(
NllbTokenizer
)
"
NougatTokenizer
"
:
(
)
=
>
(
NougatTokenizer
)
"
PreTrainedTokenizer
"
:
(
)
=
>
(
PreTrainedTokenizer
)
"
Qwen2Tokenizer
"
:
(
)
=
>
(
Qwen2Tokenizer
)
"
RoFormerTokenizer
"
:
(
)
=
>
(
RoFormerTokenizer
)
"
RobertaTokenizer
"
:
(
)
=
>
(
RobertaTokenizer
)
"
SiglipTokenizer
"
:
(
)
=
>
(
SiglipTokenizer
)
"
SpeechT5Tokenizer
"
:
(
)
=
>
(
SpeechT5Tokenizer
)
"
SqueezeBertTokenizer
"
:
(
)
=
>
(
SqueezeBertTokenizer
)
"
T5Tokenizer
"
:
(
)
=
>
(
T5Tokenizer
)
"
TokenizerModel
"
:
(
)
=
>
(
TokenizerModel
)
"
VitsTokenizer
"
:
(
)
=
>
(
VitsTokenizer
)
"
Wav2Vec2CTCTokenizer
"
:
(
)
=
>
(
Wav2Vec2CTCTokenizer
)
"
WhisperTokenizer
"
:
(
)
=
>
(
WhisperTokenizer
)
"
XLMRobertaTokenizer
"
:
(
)
=
>
(
XLMRobertaTokenizer
)
"
XLMTokenizer
"
:
(
)
=
>
(
XLMTokenizer
)
}
)
;
var
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
=
__webpack_require__
(
"
.
/
src
/
utils
/
core
.
js
"
)
;
var
_utils_hub_js__WEBPACK_IMPORTED_MODULE_1__
=
__webpack_require__
(
"
.
/
src
/
utils
/
hub
.
js
"
)
;
var
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
=
__webpack_require__
(
"
.
/
src
/
utils
/
maths
.
js
"
)
;
var
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
=
__webpack_require__
(
"
.
/
src
/
utils
/
tensor
.
js
"
)
;
var
_utils_data_structures_js__WEBPACK_IMPORTED_MODULE_4__
=
__webpack_require__
(
"
.
/
src
/
utils
/
data
-
structures
.
js
"
)
;
var
_huggingface_jinja__WEBPACK_IMPORTED_MODULE_5__
=
__webpack_require__
(
"
.
/
node_modules
/
huggingface
/
jinja
/
dist
/
index
.
js
"
)
;
async
function
loadTokenizer
(
pretrained_model_name_or_path
options
)
{
const
info
=
await
Promise
.
all
(
[
(
0
_utils_hub_js__WEBPACK_IMPORTED_MODULE_1__
.
getModelJSON
)
(
pretrained_model_name_or_path
'
tokenizer
.
json
'
true
options
)
(
0
_utils_hub_js__WEBPACK_IMPORTED_MODULE_1__
.
getModelJSON
)
(
pretrained_model_name_or_path
'
tokenizer_config
.
json
'
true
options
)
]
)
if
(
options
.
legacy
!
=
=
null
)
{
info
[
1
]
.
legacy
=
options
.
legacy
;
}
return
info
;
}
function
regexSplit
(
text
regex
)
{
const
result
=
[
]
;
let
prev
=
0
;
for
(
const
match
of
text
.
matchAll
(
regex
)
)
{
const
fullMatch
=
match
[
0
]
;
if
(
prev
<
match
.
index
)
{
result
.
push
(
text
.
slice
(
prev
match
.
index
)
)
;
}
if
(
fullMatch
.
length
>
0
)
{
result
.
push
(
fullMatch
)
;
}
prev
=
match
.
index
+
fullMatch
.
length
;
}
if
(
prev
<
text
.
length
)
{
result
.
push
(
text
.
slice
(
prev
)
)
;
}
return
result
;
}
function
createPattern
(
pattern
invert
=
true
)
{
if
(
pattern
.
Regex
!
=
=
undefined
)
{
let
regex
=
pattern
.
Regex
.
replace
(
/
\
\
(
[
#
&
~
]
)
/
g
'
1
'
)
;
for
(
const
[
key
value
]
of
PROBLEMATIC_REGEX_MAP
)
{
regex
=
regex
.
replaceAll
(
key
value
)
;
}
return
new
RegExp
(
regex
'
gu
'
)
;
}
else
if
(
pattern
.
String
!
=
=
undefined
)
{
const
escaped
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
escapeRegExp
)
(
pattern
.
String
)
;
return
new
RegExp
(
invert
?
escaped
:
(
{
escaped
}
)
'
gu
'
)
;
}
else
{
console
.
warn
(
'
Unknown
pattern
type
:
'
pattern
)
return
null
;
}
}
function
objectToMap
(
obj
)
{
return
new
Map
(
Object
.
entries
(
obj
)
)
;
}
function
prepareTensorForDecode
(
tensor
)
{
const
dims
=
tensor
.
dims
;
switch
(
dims
.
length
)
{
case
1
:
return
tensor
.
tolist
(
)
;
case
2
:
if
(
dims
[
0
]
!
=
=
1
)
{
throw
new
Error
(
'
Unable
to
decode
tensor
with
batch
size
!
=
=
1
.
Use
tokenizer
.
batch_decode
(
.
.
.
)
for
batched
inputs
.
'
)
;
}
return
tensor
.
tolist
(
)
[
0
]
;
default
:
throw
new
Error
(
Expected
tensor
to
have
1
-
2
dimensions
got
{
dims
.
length
}
.
)
}
}
function
clean_up_tokenization
(
text
)
{
return
text
.
replace
(
/
\
.
/
g
'
.
'
)
.
replace
(
/
\
?
/
g
'
?
'
)
.
replace
(
/
\
!
/
g
'
!
'
)
.
replace
(
/
/
g
'
'
)
.
replace
(
/
\
'
/
g
"
'
"
)
.
replace
(
/
n
\
'
t
/
g
"
n
'
t
"
)
.
replace
(
/
\
'
m
/
g
"
'
m
"
)
.
replace
(
/
\
'
s
/
g
"
'
s
"
)
.
replace
(
/
\
'
ve
/
g
"
'
ve
"
)
.
replace
(
/
\
'
re
/
g
"
'
re
"
)
;
}
function
remove_accents
(
text
)
{
return
text
.
replace
(
/
[
\
u0300
-
\
u036f
]
/
g
'
'
)
;
}
function
lowercase_and_remove_accent
(
text
)
{
return
remove_accents
(
text
.
toLowerCase
(
)
)
;
}
function
fuse
(
arr
value
mapping
)
{
const
fused
=
[
]
;
let
i
=
0
;
while
(
i
<
arr
.
length
)
{
fused
.
push
(
arr
[
i
]
)
if
(
(
mapping
.
get
(
arr
[
i
]
)
?
?
value
)
!
=
=
value
)
{
+
+
i
;
continue
;
}
while
(
i
<
arr
.
length
&
&
(
mapping
.
get
(
arr
[
i
]
)
?
?
value
)
=
=
=
value
)
{
+
+
i
;
}
}
return
fused
;
}
function
whitespace_split
(
text
)
{
return
text
.
match
(
/
\
S
+
/
g
)
|
|
[
]
;
}
const
PUNCTUATION_REGEX
=
'
\
\
p
{
P
}
\
\
u0021
-
\
\
u002F
\
\
u003A
-
\
\
u0040
\
\
u005B
-
\
\
u0060
\
\
u007B
-
\
\
u007E
'
;
const
PROBLEMATIC_REGEX_MAP
=
new
Map
(
[
[
"
(
?
i
:
'
s
|
'
t
|
'
re
|
'
ve
|
'
m
|
'
ll
|
'
d
)
"
"
(
?
:
'
(
[
sS
]
|
[
tT
]
|
[
rR
]
[
eE
]
|
[
vV
]
[
eE
]
|
[
mM
]
|
[
lL
]
[
lL
]
|
[
dD
]
)
)
"
]
]
)
class
AddedToken
{
constructor
(
config
)
{
this
.
content
=
config
.
content
;
this
.
id
=
config
.
id
;
this
.
single_word
=
config
.
single_word
?
?
false
;
this
.
lstrip
=
config
.
lstrip
?
?
false
;
this
.
rstrip
=
config
.
rstrip
?
?
false
;
this
.
special
=
config
.
special
?
?
false
;
this
.
normalized
=
config
.
normalized
?
?
null
;
}
}
class
TokenizerModel
extends
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
Callable
{
constructor
(
config
)
{
super
(
)
;
this
.
config
=
config
;
this
.
vocab
=
[
]
;
this
.
tokens_to_ids
=
new
Map
(
)
;
this
.
unk_token_id
=
undefined
;
this
.
unk_token
=
undefined
;
this
.
end_of_word_suffix
=
undefined
;
this
.
fuse_unk
=
this
.
config
.
fuse_unk
?
?
false
;
}
static
fromConfig
(
config
.
.
.
args
)
{
switch
(
config
.
type
)
{
case
'
WordPiece
'
:
return
new
WordPieceTokenizer
(
config
)
;
case
'
Unigram
'
:
return
new
Unigram
(
config
.
.
.
args
)
;
case
'
BPE
'
:
return
new
BPE
(
config
)
;
default
:
if
(
config
.
vocab
)
{
return
new
LegacyTokenizerModel
(
config
.
.
.
args
)
;
}
throw
new
Error
(
Unknown
TokenizerModel
type
:
{
config
.
type
}
)
;
}
}
_call
(
tokens
)
{
let
ids
=
this
.
encode
(
tokens
)
;
if
(
this
.
fuse_unk
)
{
ids
=
fuse
(
ids
this
.
unk_token_id
this
.
tokens_to_ids
)
;
}
return
ids
;
}
encode
(
tokens
)
{
throw
Error
(
"
encode
should
be
implemented
in
subclass
.
"
)
}
convert_tokens_to_ids
(
tokens
)
{
return
tokens
.
map
(
t
=
>
this
.
tokens_to_ids
.
get
(
t
)
?
?
this
.
unk_token_id
)
;
}
convert_ids_to_tokens
(
ids
)
{
return
ids
.
map
(
i
=
>
this
.
vocab
[
i
]
?
?
this
.
unk_token
)
;
}
}
class
WordPieceTokenizer
extends
TokenizerModel
{
constructor
(
config
)
{
super
(
config
)
;
this
.
tokens_to_ids
=
objectToMap
(
config
.
vocab
)
;
this
.
unk_token_id
=
this
.
tokens_to_ids
.
get
(
config
.
unk_token
)
;
this
.
unk_token
=
config
.
unk_token
;
this
.
max_input_chars_per_word
=
config
.
max_input_chars_per_word
?
?
100
;
this
.
vocab
=
new
Array
(
this
.
tokens_to_ids
.
size
)
;
for
(
const
[
key
value
]
of
this
.
tokens_to_ids
)
{
this
.
vocab
[
value
]
=
key
;
}
}
encode
(
tokens
)
{
const
outputTokens
=
[
]
;
for
(
const
token
of
tokens
)
{
const
chars
=
[
.
.
.
token
]
;
if
(
chars
.
length
>
this
.
max_input_chars_per_word
)
{
outputTokens
.
push
(
this
.
unk_token
)
;
continue
;
}
let
isUnknown
=
false
;
let
start
=
0
;
const
subTokens
=
[
]
;
while
(
start
<
chars
.
length
)
{
let
end
=
chars
.
length
;
let
currentSubstring
=
null
;
while
(
start
<
end
)
{
let
substr
=
chars
.
slice
(
start
end
)
.
join
(
'
'
)
;
if
(
start
>
0
)
{
substr
=
this
.
config
.
continuing_subword_prefix
+
substr
;
}
if
(
this
.
tokens_to_ids
.
has
(
substr
)
)
{
currentSubstring
=
substr
;
break
;
}
-
-
end
;
}
if
(
currentSubstring
=
=
=
null
)
{
isUnknown
=
true
;
break
;
}
subTokens
.
push
(
currentSubstring
)
;
start
=
end
;
}
if
(
isUnknown
)
{
outputTokens
.
push
(
this
.
unk_token
)
;
}
else
{
outputTokens
.
push
(
.
.
.
subTokens
)
;
}
}
return
outputTokens
;
}
}
class
Unigram
extends
TokenizerModel
{
constructor
(
config
moreConfig
)
{
super
(
config
)
;
const
vocabSize
=
config
.
vocab
.
length
;
this
.
vocab
=
new
Array
(
vocabSize
)
;
this
.
scores
=
new
Array
(
vocabSize
)
;
for
(
let
i
=
0
;
i
<
vocabSize
;
+
+
i
)
{
const
piece
=
config
.
vocab
[
i
]
;
this
.
vocab
[
i
]
=
piece
[
0
]
;
this
.
scores
[
i
]
=
piece
[
1
]
;
}
this
.
unk_token_id
=
config
.
unk_id
;
this
.
unk_token
=
this
.
vocab
[
config
.
unk_id
]
;
this
.
tokens_to_ids
=
new
Map
(
this
.
vocab
.
map
(
(
x
i
)
=
>
[
x
i
]
)
)
;
this
.
bosToken
=
'
'
;
this
.
bosTokenId
=
this
.
tokens_to_ids
.
get
(
this
.
bosToken
)
;
this
.
eosToken
=
moreConfig
.
eos_token
;
this
.
eosTokenId
=
this
.
tokens_to_ids
.
get
(
this
.
eosToken
)
;
this
.
unkToken
=
this
.
vocab
[
this
.
unk_token_id
]
;
this
.
minScore
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
min
)
(
this
.
scores
)
[
0
]
;
this
.
unkScore
=
this
.
minScore
-
10
.
0
;
this
.
scores
[
this
.
unk_token_id
]
=
this
.
unkScore
;
this
.
trie
=
new
_utils_data_structures_js__WEBPACK_IMPORTED_MODULE_4__
.
CharTrie
(
)
;
this
.
trie
.
extend
(
this
.
vocab
)
;
this
.
fuse_unk
=
true
;
}
populateNodes
(
lattice
)
{
const
sentence
=
lattice
.
sentence
;
const
len
=
sentence
.
length
;
let
beginPos
=
0
;
while
(
beginPos
<
len
)
{
const
mblen
=
1
;
let
hasSingleNode
=
false
;
const
tokens
=
[
]
;
for
(
let
token
of
this
.
trie
.
commonPrefixSearch
(
sentence
.
slice
(
beginPos
)
)
)
{
tokens
.
push
(
token
)
;
const
tokenId
=
this
.
tokens_to_ids
.
get
(
token
)
;
const
tokenScore
=
this
.
scores
[
tokenId
]
;
const
n
=
token
.
length
;
lattice
.
insert
(
beginPos
n
tokenScore
tokenId
)
;
if
(
!
hasSingleNode
&
&
n
=
=
=
mblen
)
{
hasSingleNode
=
true
;
}
}
if
(
!
hasSingleNode
)
{
lattice
.
insert
(
beginPos
mblen
this
.
unkScore
this
.
unk_token_id
)
;
}
beginPos
+
=
mblen
;
}
}
tokenize
(
normalized
)
{
const
lattice
=
new
_utils_data_structures_js__WEBPACK_IMPORTED_MODULE_4__
.
TokenLattice
(
normalized
this
.
bosTokenId
this
.
eosTokenId
)
;
this
.
populateNodes
(
lattice
)
;
return
lattice
.
tokens
(
)
;
}
encode
(
tokens
)
{
const
toReturn
=
[
]
;
for
(
const
token
of
tokens
)
{
const
tokenized
=
this
.
tokenize
(
token
)
;
toReturn
.
push
(
.
.
.
tokenized
)
;
}
return
toReturn
;
}
}
const
BYTES_TO_UNICODE
=
(
(
)
=
>
{
const
bs
=
[
.
.
.
Array
.
from
(
{
length
:
"
~
"
.
charCodeAt
(
0
)
-
"
!
"
.
charCodeAt
(
0
)
+
1
}
(
_
i
)
=
>
i
+
"
!
"
.
charCodeAt
(
0
)
)
.
.
.
Array
.
from
(
{
length
:
"
"
.
charCodeAt
(
0
)
-
"
"
.
charCodeAt
(
0
)
+
1
}
(
_
i
)
=
>
i
+
"
"
.
charCodeAt
(
0
)
)
.
.
.
Array
.
from
(
{
length
:
"
"
.
charCodeAt
(
0
)
-
"
"
.
charCodeAt
(
0
)
+
1
}
(
_
i
)
=
>
i
+
"
"
.
charCodeAt
(
0
)
)
]
;
const
cs
=
bs
.
slice
(
)
;
let
n
=
0
;
for
(
let
b
=
0
;
b
<
256
;
+
+
b
)
{
if
(
!
bs
.
includes
(
b
)
)
{
bs
.
push
(
b
)
;
cs
.
push
(
256
+
n
)
;
n
+
=
1
;
}
}
const
ccs
=
cs
.
map
(
n
=
>
String
.
fromCharCode
(
n
)
)
;
return
Object
.
fromEntries
(
bs
.
map
(
(
b
i
)
=
>
[
b
ccs
[
i
]
]
)
)
;
}
)
(
)
;
const
UNICODE_TO_BYTES
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
reverseDictionary
)
(
BYTES_TO_UNICODE
)
;
class
BPE
extends
TokenizerModel
{
constructor
(
config
)
{
super
(
config
)
;
this
.
BPE_SPLIT_TOKEN
=
'
'
;
this
.
tokens_to_ids
=
objectToMap
(
config
.
vocab
)
;
this
.
unk_token_id
=
this
.
tokens_to_ids
.
get
(
config
.
unk_token
)
;
this
.
unk_token
=
config
.
unk_token
;
this
.
vocab
=
new
Array
(
this
.
tokens_to_ids
.
size
)
;
for
(
const
[
key
value
]
of
this
.
tokens_to_ids
)
{
this
.
vocab
[
value
]
=
key
;
}
this
.
bpe_ranks
=
new
Map
(
config
.
merges
.
map
(
(
x
i
)
=
>
[
x
i
]
)
)
;
this
.
merges
=
config
.
merges
.
map
(
x
=
>
x
.
split
(
this
.
BPE_SPLIT_TOKEN
)
)
;
this
.
end_of_word_suffix
=
config
.
end_of_word_suffix
;
this
.
continuing_subword_suffix
=
config
.
continuing_subword_suffix
?
?
null
;
this
.
byte_fallback
=
this
.
config
.
byte_fallback
?
?
false
;
if
(
this
.
byte_fallback
)
{
this
.
text_encoder
=
new
TextEncoder
(
)
;
}
this
.
cache
=
new
Map
(
)
;
}
bpe
(
token
)
{
if
(
token
.
length
=
=
=
0
)
{
return
[
]
;
}
const
cached
=
this
.
cache
.
get
(
token
)
;
if
(
cached
!
=
=
undefined
)
{
return
cached
;
}
const
word
=
Array
.
from
(
token
)
;
if
(
this
.
end_of_word_suffix
)
{
word
[
word
.
length
-
1
]
+
=
this
.
end_of_word_suffix
;
}
let
result
=
[
]
;
if
(
word
.
length
>
1
)
{
const
queue
=
new
_utils_data_structures_js__WEBPACK_IMPORTED_MODULE_4__
.
PriorityQueue
(
(
a
b
)
=
>
a
.
score
<
b
.
score
)
;
let
startingNode
=
{
token
:
word
[
0
]
bias
:
0
prev
:
null
next
:
null
}
let
previousNode
=
startingNode
for
(
let
i
=
1
;
i
<
word
.
length
;
+
+
i
)
{
const
currentNode
=
{
bias
:
i
/
word
.
length
token
:
word
[
i
]
prev
:
previousNode
next
:
null
}
previousNode
.
next
=
currentNode
this
.
_add_node
(
queue
previousNode
)
previousNode
=
currentNode
}
while
(
!
queue
.
isEmpty
(
)
)
{
const
node
=
queue
.
pop
(
)
;
if
(
node
.
deleted
|
|
!
node
.
next
|
|
node
.
next
.
deleted
)
continue
;
node
.
deleted
=
true
;
node
.
next
.
deleted
=
true
;
if
(
node
.
prev
)
{
const
newPreviousNode
=
{
.
.
.
node
.
prev
}
;
node
.
prev
.
deleted
=
true
;
node
.
prev
=
newPreviousNode
;
if
(
newPreviousNode
.
prev
)
{
newPreviousNode
.
prev
.
next
=
newPreviousNode
;
}
else
{
startingNode
=
newPreviousNode
;
}
}
const
merged
=
{
token
:
node
.
token
+
node
.
next
.
token
bias
:
node
.
bias
prev
:
node
.
prev
next
:
node
.
next
.
next
}
if
(
merged
.
prev
)
{
merged
.
prev
.
next
=
merged
;
this
.
_add_node
(
queue
merged
.
prev
)
;
}
else
{
startingNode
=
merged
;
}
if
(
merged
.
next
)
{
merged
.
next
.
prev
=
merged
;
this
.
_add_node
(
queue
merged
)
;
}
}
for
(
let
currentNode
=
startingNode
;
currentNode
!
=
=
null
;
currentNode
=
currentNode
.
next
)
{
result
.
push
(
currentNode
.
token
)
;
}
}
else
{
result
=
word
;
}
if
(
this
.
continuing_subword_suffix
)
{
for
(
let
i
=
0
;
i
<
result
.
length
-
1
;
+
+
i
)
{
result
[
i
]
+
=
this
.
continuing_subword_suffix
;
}
}
this
.
cache
.
set
(
token
result
)
;
return
result
;
}
_add_node
(
queue
node
)
{
const
rank
=
this
.
bpe_ranks
.
get
(
node
.
token
+
this
.
BPE_SPLIT_TOKEN
+
node
.
next
.
token
)
;
if
(
rank
!
=
=
undefined
)
{
node
.
score
=
rank
+
node
.
bias
;
queue
.
push
(
node
)
;
}
}
encode
(
tokens
)
{
const
outputTokens
=
[
]
;
for
(
const
token
of
tokens
)
{
const
bpe_token_list
=
this
.
bpe
(
token
)
;
for
(
const
t
of
bpe_token_list
)
{
if
(
this
.
tokens_to_ids
.
has
(
t
)
)
{
outputTokens
.
push
(
t
)
;
}
else
{
if
(
this
.
byte_fallback
)
{
outputTokens
.
push
(
.
.
.
Array
.
from
(
this
.
text_encoder
.
encode
(
t
)
)
.
map
(
x
=
>
<
0x
{
x
.
toString
(
16
)
.
toUpperCase
(
)
.
padStart
(
2
'
0
'
)
}
>
)
)
;
}
else
{
outputTokens
.
push
(
this
.
unk_token
)
;
}
}
}
}
return
outputTokens
;
}
}
class
LegacyTokenizerModel
extends
TokenizerModel
{
constructor
(
config
moreConfig
)
{
super
(
config
)
;
this
.
tokens_to_ids
=
objectToMap
(
moreConfig
.
target_lang
?
config
.
vocab
[
moreConfig
.
target_lang
]
:
config
.
vocab
)
;
this
.
bos_token
=
moreConfig
.
bos_token
;
this
.
bos_token_id
=
this
.
tokens_to_ids
.
get
(
this
.
bos_token
)
;
this
.
eos_token
=
moreConfig
.
eos_token
;
this
.
eos_token_id
=
this
.
tokens_to_ids
.
get
(
this
.
eos_token
)
;
this
.
pad_token
=
moreConfig
.
pad_token
;
this
.
pad_token_id
=
this
.
tokens_to_ids
.
get
(
this
.
pad_token
)
;
this
.
unk_token
=
moreConfig
.
unk_token
;
this
.
unk_token_id
=
this
.
tokens_to_ids
.
get
(
this
.
unk_token
)
;
this
.
vocab
=
new
Array
(
this
.
tokens_to_ids
.
size
)
;
for
(
const
[
key
value
]
of
this
.
tokens_to_ids
)
{
this
.
vocab
[
value
]
=
key
;
}
}
encode
(
tokens
)
{
return
tokens
;
}
}
class
Normalizer
extends
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
Callable
{
constructor
(
config
)
{
super
(
)
;
this
.
config
=
config
;
}
static
fromConfig
(
config
)
{
if
(
config
=
=
=
null
)
return
null
;
switch
(
config
.
type
)
{
case
'
BertNormalizer
'
:
return
new
BertNormalizer
(
config
)
;
case
'
Precompiled
'
:
return
new
Precompiled
(
config
)
;
case
'
Sequence
'
:
return
new
NormalizerSequence
(
config
)
;
case
'
Replace
'
:
return
new
Replace
(
config
)
;
case
'
NFC
'
:
return
new
NFC
(
config
)
;
case
'
NFKC
'
:
return
new
NFKC
(
config
)
;
case
'
NFKD
'
:
return
new
NFKD
(
config
)
;
case
'
Strip
'
:
return
new
StripNormalizer
(
config
)
;
case
'
StripAccents
'
:
return
new
StripAccents
(
config
)
;
case
'
Lowercase
'
:
return
new
Lowercase
(
config
)
;
case
'
Prepend
'
:
return
new
Prepend
(
config
)
;
default
:
throw
new
Error
(
Unknown
Normalizer
type
:
{
config
.
type
}
)
;
}
}
normalize
(
text
)
{
throw
Error
(
"
normalize
should
be
implemented
in
subclass
.
"
)
}
_call
(
text
)
{
return
this
.
normalize
(
text
)
;
}
}
class
Replace
extends
Normalizer
{
normalize
(
text
)
{
const
pattern
=
createPattern
(
this
.
config
.
pattern
)
;
return
pattern
=
=
=
null
?
text
:
text
.
replaceAll
(
pattern
this
.
config
.
content
)
;
}
}
class
NFC
extends
Normalizer
{
normalize
(
text
)
{
text
=
text
.
normalize
(
'
NFC
'
)
return
text
;
}
}
class
NFKC
extends
Normalizer
{
normalize
(
text
)
{
text
=
text
.
normalize
(
'
NFKC
'
)
return
text
;
}
}
class
NFKD
extends
Normalizer
{
normalize
(
text
)
{
text
=
text
.
normalize
(
'
NFKD
'
)
return
text
;
}
}
class
StripNormalizer
extends
Normalizer
{
normalize
(
text
)
{
if
(
this
.
config
.
strip_left
&
&
this
.
config
.
strip_right
)
{
text
=
text
.
trim
(
)
;
}
else
{
if
(
this
.
config
.
strip_left
)
{
text
=
text
.
trimStart
(
)
;
}
if
(
this
.
config
.
strip_right
)
{
text
=
text
.
trimEnd
(
)
;
}
}
return
text
;
}
}
class
StripAccents
extends
Normalizer
{
normalize
(
text
)
{
text
=
remove_accents
(
text
)
;
return
text
;
}
}
class
Lowercase
extends
Normalizer
{
normalize
(
text
)
{
text
=
text
.
toLowerCase
(
)
;
return
text
;
}
}
class
Prepend
extends
Normalizer
{
normalize
(
text
)
{
text
=
this
.
config
.
prepend
+
text
;
return
text
;
}
}
class
NormalizerSequence
extends
Normalizer
{
constructor
(
config
)
{
super
(
config
)
;
this
.
normalizers
=
config
.
normalizers
.
map
(
x
=
>
Normalizer
.
fromConfig
(
x
)
)
;
}
normalize
(
text
)
{
return
this
.
normalizers
.
reduce
(
(
t
normalizer
)
=
>
{
return
normalizer
.
normalize
(
t
)
;
}
text
)
;
}
}
class
BertNormalizer
extends
Normalizer
{
_tokenize_chinese_chars
(
text
)
{
const
output
=
[
]
;
for
(
let
i
=
0
;
i
<
text
.
length
;
+
+
i
)
{
const
char
=
text
[
i
]
;
const
cp
=
char
.
charCodeAt
(
0
)
;
if
(
this
.
_is_chinese_char
(
cp
)
)
{
output
.
push
(
"
"
)
;
output
.
push
(
char
)
;
output
.
push
(
"
"
)
;
}
else
{
output
.
push
(
char
)
;
}
}
return
output
.
join
(
"
"
)
;
}
_is_chinese_char
(
cp
)
{
return
(
(
cp
>
=
0x4E00
&
&
cp
<
=
0x9FFF
)
|
|
(
cp
>
=
0x3400
&
&
cp
<
=
0x4DBF
)
|
|
(
cp
>
=
0x20000
&
&
cp
<
=
0x2A6DF
)
|
|
(
cp
>
=
0x2A700
&
&
cp
<
=
0x2B73F
)
|
|
(
cp
>
=
0x2B740
&
&
cp
<
=
0x2B81F
)
|
|
(
cp
>
=
0x2B820
&
&
cp
<
=
0x2CEAF
)
|
|
(
cp
>
=
0xF900
&
&
cp
<
=
0xFAFF
)
|
|
(
cp
>
=
0x2F800
&
&
cp
<
=
0x2FA1F
)
)
}
stripAccents
(
text
)
{
return
text
.
normalize
(
'
NFD
'
)
.
replace
(
/
[
\
u0300
-
\
u036f
]
/
g
'
'
)
;
}
_is_control
(
char
)
{
switch
(
char
)
{
case
'
\
t
'
:
case
'
\
n
'
:
case
'
\
r
'
:
return
false
;
default
:
return
/
^
\
p
{
Cc
}
|
\
p
{
Cf
}
|
\
p
{
Co
}
|
\
p
{
Cs
}
/
u
.
test
(
char
)
;
}
}
_clean_text
(
text
)
{
const
output
=
[
]
;
for
(
const
char
of
text
)
{
const
cp
=
char
.
charCodeAt
(
0
)
;
if
(
cp
=
=
=
0
|
|
cp
=
=
=
0xFFFD
|
|
this
.
_is_control
(
char
)
)
{
continue
;
}
if
(
/
^
\
s
/
.
test
(
char
)
)
{
output
.
push
(
"
"
)
;
}
else
{
output
.
push
(
char
)
;
}
}
return
output
.
join
(
"
"
)
;
}
normalize
(
text
)
{
if
(
this
.
config
.
clean_text
)
{
text
=
this
.
_clean_text
(
text
)
;
}
if
(
this
.
config
.
handle_chinese_chars
)
{
text
=
this
.
_tokenize_chinese_chars
(
text
)
;
}
if
(
this
.
config
.
lowercase
)
{
text
=
text
.
toLowerCase
(
)
;
if
(
this
.
config
.
strip_accents
!
=
=
false
)
{
text
=
this
.
stripAccents
(
text
)
;
}
}
else
if
(
this
.
config
.
strip_accents
)
{
text
=
this
.
stripAccents
(
text
)
;
}
return
text
;
}
}
class
PreTokenizer
extends
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
Callable
{
static
fromConfig
(
config
)
{
if
(
config
=
=
=
null
)
return
null
;
switch
(
config
.
type
)
{
case
'
BertPreTokenizer
'
:
return
new
BertPreTokenizer
(
config
)
;
case
'
Sequence
'
:
return
new
PreTokenizerSequence
(
config
)
;
case
'
Whitespace
'
:
return
new
WhitespacePreTokenizer
(
config
)
;
case
'
WhitespaceSplit
'
:
return
new
WhitespaceSplit
(
config
)
;
case
'
Metaspace
'
:
return
new
MetaspacePreTokenizer
(
config
)
;
case
'
ByteLevel
'
:
return
new
ByteLevelPreTokenizer
(
config
)
;
case
'
Split
'
:
return
new
SplitPreTokenizer
(
config
)
;
case
'
Punctuation
'
:
return
new
PunctuationPreTokenizer
(
config
)
;
case
'
Digits
'
:
return
new
DigitsPreTokenizer
(
config
)
;
case
'
Replace
'
:
return
new
ReplacePreTokenizer
(
config
)
;
default
:
throw
new
Error
(
Unknown
PreTokenizer
type
:
{
config
.
type
}
)
;
}
}
pre_tokenize_text
(
text
options
)
{
throw
Error
(
"
pre_tokenize_text
should
be
implemented
in
subclass
.
"
)
}
pre_tokenize
(
text
options
)
{
return
(
Array
.
isArray
(
text
)
?
text
.
map
(
x
=
>
this
.
pre_tokenize_text
(
x
options
)
)
:
this
.
pre_tokenize_text
(
text
options
)
)
.
flat
(
)
;
}
_call
(
text
options
)
{
return
this
.
pre_tokenize
(
text
options
)
;
}
}
class
BertPreTokenizer
extends
PreTokenizer
{
constructor
(
config
)
{
super
(
)
;
this
.
pattern
=
new
RegExp
(
[
^
\
\
s
{
PUNCTUATION_REGEX
}
]
+
|
[
{
PUNCTUATION_REGEX
}
]
'
gu
'
)
;
}
pre_tokenize_text
(
text
options
)
{
return
text
.
trim
(
)
.
match
(
this
.
pattern
)
|
|
[
]
;
}
}
class
ByteLevelPreTokenizer
extends
PreTokenizer
{
constructor
(
config
)
{
super
(
)
;
this
.
config
=
config
;
this
.
add_prefix_space
=
this
.
config
.
add_prefix_space
;
this
.
trim_offsets
=
this
.
config
.
trim_offsets
;
this
.
use_regex
=
this
.
config
.
use_regex
?
?
true
;
this
.
pattern
=
/
'
s
|
'
t
|
'
re
|
'
ve
|
'
m
|
'
ll
|
'
d
|
?
\
p
{
L
}
+
|
?
\
p
{
N
}
+
|
?
[
^
\
s
\
p
{
L
}
\
p
{
N
}
]
+
|
\
s
+
(
?
!
\
S
)
|
\
s
+
/
gu
;
this
.
byte_encoder
=
BYTES_TO_UNICODE
;
this
.
text_encoder
=
new
TextEncoder
(
)
;
}
pre_tokenize_text
(
text
options
)
{
if
(
this
.
add_prefix_space
&
&
!
text
.
startsWith
(
'
'
)
)
{
text
=
'
'
+
text
;
}
const
tokens
=
this
.
use_regex
?
(
text
.
match
(
this
.
pattern
)
|
|
[
]
)
:
[
text
]
;
return
tokens
.
map
(
token
=
>
Array
.
from
(
this
.
text_encoder
.
encode
(
token
)
byte
=
>
this
.
byte_encoder
[
byte
]
)
.
join
(
'
'
)
)
;
}
}
class
SplitPreTokenizer
extends
PreTokenizer
{
constructor
(
config
)
{
super
(
)
;
this
.
config
=
config
;
this
.
pattern
=
createPattern
(
this
.
config
.
pattern
this
.
config
.
invert
)
;
}
pre_tokenize_text
(
text
options
)
{
if
(
this
.
pattern
=
=
=
null
)
{
return
[
]
;
}
if
(
this
.
config
.
invert
)
{
return
text
.
match
(
this
.
pattern
)
|
|
[
]
;
}
else
{
return
regexSplit
(
text
this
.
pattern
)
;
}
}
}
class
PunctuationPreTokenizer
extends
PreTokenizer
{
constructor
(
config
)
{
super
(
)
;
this
.
config
=
config
;
this
.
pattern
=
new
RegExp
(
[
^
{
PUNCTUATION_REGEX
}
]
+
|
[
{
PUNCTUATION_REGEX
}
]
+
'
gu
'
)
;
}
pre_tokenize_text
(
text
options
)
{
return
text
.
match
(
this
.
pattern
)
|
|
[
]
;
}
}
class
DigitsPreTokenizer
extends
PreTokenizer
{
constructor
(
config
)
{
super
(
)
;
this
.
config
=
config
;
const
digit_pattern
=
[
^
\
\
d
]
+
|
\
\
d
{
this
.
config
.
individual_digits
?
'
'
:
'
+
'
}
;
this
.
pattern
=
new
RegExp
(
digit_pattern
'
gu
'
)
;
}
pre_tokenize_text
(
text
options
)
{
return
text
.
match
(
this
.
pattern
)
|
|
[
]
;
}
}
class
PostProcessor
extends
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
Callable
{
constructor
(
config
)
{
super
(
)
;
this
.
config
=
config
;
}
static
fromConfig
(
config
)
{
if
(
config
=
=
=
null
)
return
null
;
switch
(
config
.
type
)
{
case
'
TemplateProcessing
'
:
return
new
TemplateProcessing
(
config
)
;
case
'
ByteLevel
'
:
return
new
ByteLevelPostProcessor
(
config
)
;
case
'
RobertaProcessing
'
:
return
new
RobertaProcessing
(
config
)
;
case
'
BertProcessing
'
:
return
new
BertProcessing
(
config
)
;
default
:
throw
new
Error
(
Unknown
PostProcessor
type
:
{
config
.
type
}
)
;
}
}
post_process
(
tokens
.
.
.
args
)
{
throw
Error
(
"
post_process
should
be
implemented
in
subclass
.
"
)
}
_call
(
tokens
.
.
.
args
)
{
return
this
.
post_process
(
tokens
.
.
.
args
)
;
}
}
class
BertProcessing
extends
PostProcessor
{
constructor
(
config
)
{
super
(
config
)
;
this
.
cls
=
config
.
cls
[
0
]
;
this
.
sep
=
config
.
sep
[
0
]
;
}
post_process
(
tokens
tokens_pair
=
null
{
add_special_tokens
=
true
}
=
{
}
)
{
if
(
add_special_tokens
)
{
tokens
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
[
this
.
cls
]
tokens
[
this
.
sep
]
)
;
}
let
token_type_ids
=
new
Array
(
tokens
.
length
)
.
fill
(
0
)
;
if
(
tokens_pair
!
=
=
null
)
{
const
middle
=
(
add_special_tokens
&
&
this
instanceof
RobertaProcessing
)
?
[
this
.
sep
]
:
[
]
;
const
after
=
add_special_tokens
?
[
this
.
sep
]
:
[
]
;
tokens
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
tokens
middle
tokens_pair
after
)
;
token_type_ids
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
token_type_ids
new
Array
(
tokens_pair
.
length
+
middle
.
length
+
after
.
length
)
.
fill
(
1
)
)
;
}
return
{
tokens
token_type_ids
}
;
}
}
class
RobertaProcessing
extends
BertProcessing
{
}
class
TemplateProcessing
extends
PostProcessor
{
constructor
(
config
)
{
super
(
config
)
;
this
.
single
=
config
.
single
;
this
.
pair
=
config
.
pair
;
}
post_process
(
tokens
tokens_pair
=
null
{
add_special_tokens
=
true
}
=
{
}
)
{
const
type
=
tokens_pair
=
=
=
null
?
this
.
single
:
this
.
pair
let
processedTokens
=
[
]
;
let
types
=
[
]
;
for
(
const
item
of
type
)
{
if
(
'
SpecialToken
'
in
item
)
{
if
(
add_special_tokens
)
{
processedTokens
.
push
(
item
.
SpecialToken
.
id
)
;
types
.
push
(
item
.
SpecialToken
.
type_id
)
;
}
}
else
if
(
'
Sequence
'
in
item
)
{
if
(
item
.
Sequence
.
id
=
=
=
'
A
'
)
{
processedTokens
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
processedTokens
tokens
)
;
types
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
types
new
Array
(
tokens
.
length
)
.
fill
(
item
.
Sequence
.
type_id
)
)
;
}
else
if
(
item
.
Sequence
.
id
=
=
=
'
B
'
)
{
processedTokens
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
processedTokens
tokens_pair
)
;
types
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
types
new
Array
(
tokens_pair
.
length
)
.
fill
(
item
.
Sequence
.
type_id
)
)
;
}
}
}
return
{
tokens
:
processedTokens
token_type_ids
:
types
}
;
}
}
class
ByteLevelPostProcessor
extends
PostProcessor
{
post_process
(
tokens
tokens_pair
=
null
)
{
if
(
tokens_pair
)
{
tokens
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
tokens
tokens_pair
)
;
}
return
{
tokens
}
;
}
}
class
Decoder
extends
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
Callable
{
constructor
(
config
)
{
super
(
)
;
this
.
config
=
config
;
this
.
added_tokens
=
[
]
;
this
.
end_of_word_suffix
=
null
;
this
.
trim_offsets
=
config
.
trim_offsets
;
}
static
fromConfig
(
config
)
{
if
(
config
=
=
=
null
)
return
null
;
switch
(
config
.
type
)
{
case
'
WordPiece
'
:
return
new
WordPieceDecoder
(
config
)
;
case
'
Metaspace
'
:
return
new
MetaspaceDecoder
(
config
)
;
case
'
ByteLevel
'
:
return
new
ByteLevelDecoder
(
config
)
;
case
'
Replace
'
:
return
new
ReplaceDecoder
(
config
)
;
case
'
ByteFallback
'
:
return
new
ByteFallback
(
config
)
;
case
'
Fuse
'
:
return
new
FuseDecoder
(
config
)
;
case
'
Strip
'
:
return
new
StripDecoder
(
config
)
;
case
'
Sequence
'
:
return
new
DecoderSequence
(
config
)
;
case
'
CTC
'
:
return
new
CTCDecoder
(
config
)
;
case
'
BPEDecoder
'
:
return
new
BPEDecoder
(
config
)
;
default
:
throw
new
Error
(
Unknown
Decoder
type
:
{
config
.
type
}
)
;
}
}
_call
(
tokens
)
{
return
this
.
decode
(
tokens
)
;
}
decode
(
tokens
)
{
return
this
.
decode_chain
(
tokens
)
.
join
(
'
'
)
;
}
decode_chain
(
tokens
)
{
throw
Error
(
"
decode_chain
should
be
implemented
in
subclass
.
"
)
}
}
class
ReplaceDecoder
extends
Decoder
{
decode_chain
(
tokens
)
{
const
pattern
=
createPattern
(
this
.
config
.
pattern
)
;
return
pattern
=
=
=
null
?
tokens
:
tokens
.
map
(
token
=
>
token
.
replaceAll
(
pattern
this
.
config
.
content
)
)
}
}
class
ByteFallback
extends
Decoder
{
constructor
(
config
)
{
super
(
config
)
;
this
.
text_decoder
=
new
TextDecoder
(
)
;
}
decode_chain
(
tokens
)
{
const
new_tokens
=
[
]
;
let
previous_byte_tokens
=
[
]
;
for
(
const
token
of
tokens
)
{
let
bytes
=
null
;
if
(
token
.
length
=
=
=
6
&
&
token
.
startsWith
(
'
<
0x
'
)
&
&
token
.
endsWith
(
'
>
'
)
)
{
const
byte
=
parseInt
(
token
.
slice
(
3
5
)
16
)
;
if
(
!
isNaN
(
byte
)
)
{
bytes
=
byte
;
}
}
if
(
bytes
!
=
=
null
)
{
previous_byte_tokens
.
push
(
bytes
)
;
}
else
{
if
(
previous_byte_tokens
.
length
>
0
)
{
const
string
=
this
.
text_decoder
.
decode
(
Uint8Array
.
from
(
previous_byte_tokens
)
)
;
new_tokens
.
push
(
string
)
;
previous_byte_tokens
=
[
]
;
}
new_tokens
.
push
(
token
)
;
}
}
if
(
previous_byte_tokens
.
length
>
0
)
{
const
string
=
this
.
text_decoder
.
decode
(
Uint8Array
.
from
(
previous_byte_tokens
)
)
;
new_tokens
.
push
(
string
)
;
previous_byte_tokens
=
[
]
;
}
return
new_tokens
;
}
}
class
FuseDecoder
extends
Decoder
{
decode_chain
(
tokens
)
{
return
[
tokens
.
join
(
'
'
)
]
;
}
}
class
StripDecoder
extends
Decoder
{
constructor
(
config
)
{
super
(
config
)
;
this
.
content
=
this
.
config
.
content
;
this
.
start
=
this
.
config
.
start
;
this
.
stop
=
this
.
config
.
stop
;
}
decode_chain
(
tokens
)
{
return
tokens
.
map
(
token
=
>
{
let
start_cut
=
0
;
for
(
let
i
=
0
;
i
<
this
.
start
;
+
+
i
)
{
if
(
token
[
i
]
=
=
=
this
.
content
)
{
start_cut
=
i
+
1
;
continue
;
}
else
{
break
;
}
}
let
stop_cut
=
token
.
length
;
for
(
let
i
=
0
;
i
<
this
.
stop
;
+
+
i
)
{
const
index
=
token
.
length
-
i
-
1
;
if
(
token
[
index
]
=
=
=
this
.
content
)
{
stop_cut
=
index
;
continue
;
}
else
{
break
;
}
}
return
token
.
slice
(
start_cut
stop_cut
)
}
)
;
}
}
class
WordPieceDecoder
extends
Decoder
{
constructor
(
config
)
{
super
(
config
)
;
this
.
cleanup
=
config
.
cleanup
;
}
decode_chain
(
tokens
)
{
return
tokens
.
map
(
(
token
i
)
=
>
{
if
(
i
!
=
=
0
)
{
if
(
token
.
startsWith
(
this
.
config
.
prefix
)
)
{
token
=
token
.
replace
(
this
.
config
.
prefix
'
'
)
;
}
else
{
token
=
'
'
+
token
;
}
}
if
(
this
.
cleanup
)
{
token
=
clean_up_tokenization
(
token
)
}
return
token
;
}
)
;
}
}
class
ByteLevelDecoder
extends
Decoder
{
constructor
(
config
)
{
super
(
config
)
;
this
.
byte_decoder
=
UNICODE_TO_BYTES
;
this
.
text_decoder
=
new
TextDecoder
(
"
utf
-
8
"
{
fatal
:
false
ignoreBOM
:
true
}
)
;
this
.
end_of_word_suffix
=
null
;
}
convert_tokens_to_string
(
tokens
)
{
const
text
=
tokens
.
join
(
'
'
)
;
const
byteArray
=
new
Uint8Array
(
[
.
.
.
text
]
.
map
(
c
=
>
this
.
byte_decoder
[
c
]
)
)
;
const
decoded_text
=
this
.
text_decoder
.
decode
(
byteArray
)
;
return
decoded_text
;
}
decode_chain
(
tokens
)
{
const
sub_texts
=
[
]
;
let
current_sub_text
=
[
]
;
for
(
const
token
of
tokens
)
{
if
(
this
.
added_tokens
.
find
(
x
=
>
x
.
content
=
=
=
token
)
!
=
=
undefined
)
{
if
(
current_sub_text
.
length
>
0
)
{
sub_texts
.
push
(
this
.
convert_tokens_to_string
(
current_sub_text
)
)
;
current_sub_text
=
[
]
;
}
sub_texts
.
push
(
token
)
;
}
else
{
current_sub_text
.
push
(
token
)
;
}
}
if
(
current_sub_text
.
length
>
0
)
{
sub_texts
.
push
(
this
.
convert_tokens_to_string
(
current_sub_text
)
)
;
}
return
sub_texts
;
}
}
class
CTCDecoder
extends
Decoder
{
constructor
(
config
)
{
super
(
config
)
;
this
.
pad_token
=
this
.
config
.
pad_token
;
this
.
word_delimiter_token
=
this
.
config
.
word_delimiter_token
;
this
.
cleanup
=
this
.
config
.
cleanup
;
}
convert_tokens_to_string
(
tokens
)
{
if
(
tokens
.
length
=
=
=
0
)
return
'
'
;
const
grouped_tokens
=
[
tokens
[
0
]
]
;
for
(
let
i
=
1
;
i
<
tokens
.
length
;
+
+
i
)
{
if
(
tokens
[
i
]
!
=
=
grouped_tokens
.
at
(
-
1
)
)
{
grouped_tokens
.
push
(
tokens
[
i
]
)
;
}
}
const
filtered_tokens
=
grouped_tokens
.
filter
(
token
=
>
token
!
=
=
this
.
pad_token
)
;
let
text
=
filtered_tokens
.
join
(
'
'
)
;
if
(
this
.
cleanup
)
{
text
=
clean_up_tokenization
(
text
)
.
replaceAll
(
this
.
word_delimiter_token
'
'
)
.
trim
(
)
;
}
return
text
;
}
decode_chain
(
tokens
)
{
return
[
this
.
convert_tokens_to_string
(
tokens
)
]
;
}
}
class
DecoderSequence
extends
Decoder
{
constructor
(
config
)
{
super
(
config
)
;
this
.
decoders
=
config
.
decoders
.
map
(
x
=
>
Decoder
.
fromConfig
(
x
)
)
;
}
decode_chain
(
tokens
)
{
return
this
.
decoders
.
reduce
(
(
toks
decoder
)
=
>
{
return
decoder
.
decode_chain
(
toks
)
;
}
tokens
)
;
}
}
class
BPEDecoder
extends
Decoder
{
constructor
(
config
)
{
super
(
config
)
;
this
.
suffix
=
this
.
config
.
suffix
;
}
decode_chain
(
tokens
)
{
return
tokens
.
map
(
(
token
i
)
=
>
{
return
token
.
replaceAll
(
this
.
suffix
(
i
=
=
=
tokens
.
length
-
1
)
?
'
'
:
'
'
)
}
)
;
}
}
class
VitsDecoder
extends
Decoder
{
decode_chain
(
tokens
)
{
let
decoded
=
'
'
;
for
(
let
i
=
1
;
i
<
tokens
.
length
;
i
+
=
2
)
{
decoded
+
=
tokens
[
i
]
;
}
return
[
decoded
]
;
}
}
class
MetaspacePreTokenizer
extends
PreTokenizer
{
constructor
(
config
)
{
super
(
)
;
this
.
addPrefixSpace
=
config
.
add_prefix_space
;
this
.
replacement
=
config
.
replacement
;
this
.
strRep
=
config
.
str_rep
|
|
this
.
replacement
;
this
.
prepend_scheme
=
config
.
prepend_scheme
?
?
'
always
'
;
}
pre_tokenize_text
(
text
{
section_index
=
undefined
}
=
{
}
)
{
let
normalized
=
text
.
replaceAll
(
'
'
this
.
strRep
)
;
if
(
(
this
.
addPrefixSpace
&
&
!
normalized
.
startsWith
(
this
.
replacement
)
)
&
&
(
this
.
prepend_scheme
=
=
=
'
always
'
|
|
(
this
.
prepend_scheme
=
=
=
'
first
'
&
&
section_index
=
=
=
0
)
)
)
{
normalized
=
this
.
strRep
+
normalized
;
}
return
[
normalized
]
;
}
}
class
MetaspaceDecoder
extends
Decoder
{
constructor
(
config
)
{
super
(
config
)
;
this
.
addPrefixSpace
=
config
.
add_prefix_space
;
this
.
replacement
=
config
.
replacement
;
}
decode_chain
(
tokens
)
{
const
result
=
[
]
;
for
(
let
i
=
0
;
i
<
tokens
.
length
;
+
+
i
)
{
let
normalized
=
tokens
[
i
]
.
replaceAll
(
this
.
replacement
'
'
)
;
if
(
this
.
addPrefixSpace
&
&
i
=
=
0
&
&
normalized
.
startsWith
(
'
'
)
)
{
normalized
=
normalized
.
substring
(
1
)
;
}
result
.
push
(
normalized
)
;
}
return
result
;
}
}
class
Precompiled
extends
Normalizer
{
constructor
(
config
)
{
super
(
config
)
;
this
.
charsmap
=
config
.
precompiled_charsmap
;
}
normalize
(
text
)
{
text
=
text
.
replace
(
/
[
\
u0001
-
\
u0008
\
u000B
\
u000E
-
\
u001F
\
u007F
\
u008F
\
u009F
]
/
gm
'
'
)
;
text
=
text
.
replace
(
/
[
\
u0009
\
u000A
\
u000C
\
u000D
\
u1680
\
u200B
\
u200C
\
u200E
\
u200F
\
u2028
\
u2029
\
u2581
\
uFEFF
\
uFFFD
]
/
gm
'
\
u0020
'
)
;
if
(
text
.
includes
(
'
\
uFF5E
'
)
)
{
const
parts
=
text
.
split
(
'
\
uFF5E
'
)
;
text
=
parts
.
map
(
part
=
>
part
.
normalize
(
'
NFKC
'
)
)
.
join
(
'
\
uFF5E
'
)
;
}
else
{
text
=
text
.
normalize
(
'
NFKC
'
)
;
}
return
text
;
}
}
class
PreTokenizerSequence
extends
PreTokenizer
{
constructor
(
config
)
{
super
(
)
;
this
.
tokenizers
=
config
.
pretokenizers
.
map
(
x
=
>
PreTokenizer
.
fromConfig
(
x
)
)
;
}
pre_tokenize_text
(
text
options
)
{
return
this
.
tokenizers
.
reduce
(
(
preTokenizedText
tokenizer
)
=
>
{
return
tokenizer
.
pre_tokenize
(
preTokenizedText
options
)
;
}
[
text
]
)
;
}
}
class
WhitespacePreTokenizer
extends
PreTokenizer
{
constructor
(
config
)
{
super
(
)
;
}
pre_tokenize_text
(
text
options
)
{
return
text
.
match
(
/
\
w
+
|
[
^
\
w
\
s
]
+
/
g
)
|
|
[
]
;
}
}
class
WhitespaceSplit
extends
PreTokenizer
{
constructor
(
config
)
{
super
(
)
;
}
pre_tokenize_text
(
text
options
)
{
return
whitespace_split
(
text
)
;
}
}
class
ReplacePreTokenizer
extends
PreTokenizer
{
constructor
(
config
)
{
super
(
)
;
this
.
config
=
config
;
this
.
pattern
=
createPattern
(
this
.
config
.
pattern
)
;
this
.
content
=
this
.
config
.
content
;
}
pre_tokenize_text
(
text
options
)
{
if
(
this
.
pattern
=
=
=
null
)
{
return
[
text
]
;
}
return
[
text
.
replaceAll
(
this
.
pattern
this
.
config
.
content
)
]
;
}
}
const
SPECIAL_TOKEN_ATTRIBUTES
=
[
'
bos_token
'
'
eos_token
'
'
unk_token
'
'
sep_token
'
'
pad_token
'
'
cls_token
'
'
mask_token
'
]
function
padHelper
(
item
length
value_fn
side
)
{
for
(
const
key
of
Object
.
keys
(
item
)
)
{
const
diff
=
length
-
item
[
key
]
.
length
;
const
value
=
value_fn
(
key
)
;
const
padData
=
new
Array
(
diff
)
.
fill
(
value
)
;
item
[
key
]
=
side
=
=
=
'
right
'
?
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
item
[
key
]
padData
)
:
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
padData
item
[
key
]
)
;
}
}
function
truncateHelper
(
item
length
)
{
for
(
const
key
of
Object
.
keys
(
item
)
)
{
item
[
key
]
.
length
=
length
;
}
}
class
PreTrainedTokenizer
extends
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
Callable
{
return_token_type_ids
=
false
;
_default_chat_template
=
{
%
for
message
in
messages
%
}
{
{
'
<
|
im_start
|
>
'
+
message
[
'
role
'
]
+
'
\
n
'
+
message
[
'
content
'
]
+
'
<
|
im_end
|
>
'
+
'
\
n
'
}
}
{
%
endfor
%
}
{
%
if
add_generation_prompt
%
}
{
{
'
<
|
im_start
|
>
assistant
\
n
'
}
}
{
%
endif
%
}
;
constructor
(
tokenizerJSON
tokenizerConfig
)
{
super
(
)
;
this
.
_tokenizer_config
=
tokenizerConfig
;
this
.
normalizer
=
Normalizer
.
fromConfig
(
tokenizerJSON
.
normalizer
)
;
this
.
pre_tokenizer
=
PreTokenizer
.
fromConfig
(
tokenizerJSON
.
pre_tokenizer
)
;
this
.
model
=
TokenizerModel
.
fromConfig
(
tokenizerJSON
.
model
tokenizerConfig
)
;
this
.
post_processor
=
PostProcessor
.
fromConfig
(
tokenizerJSON
.
post_processor
)
;
this
.
decoder
=
Decoder
.
fromConfig
(
tokenizerJSON
.
decoder
)
;
this
.
special_tokens
=
[
]
;
this
.
all_special_ids
=
[
]
;
this
.
added_tokens
=
[
]
;
for
(
const
addedToken
of
tokenizerJSON
.
added_tokens
)
{
const
token
=
new
AddedToken
(
addedToken
)
;
this
.
added_tokens
.
push
(
token
)
;
this
.
model
.
tokens_to_ids
.
set
(
token
.
content
token
.
id
)
;
this
.
model
.
vocab
[
token
.
id
]
=
token
.
content
;
if
(
token
.
special
)
{
this
.
special_tokens
.
push
(
token
.
content
)
;
this
.
all_special_ids
.
push
(
token
.
id
)
;
}
}
this
.
additional_special_tokens
=
tokenizerConfig
.
additional_special_tokens
?
?
[
]
;
this
.
special_tokens
.
push
(
.
.
.
this
.
additional_special_tokens
)
;
this
.
special_tokens
=
[
.
.
.
new
Set
(
this
.
special_tokens
)
]
;
if
(
this
.
decoder
)
{
this
.
decoder
.
added_tokens
=
this
.
added_tokens
;
this
.
decoder
.
end_of_word_suffix
=
this
.
model
.
end_of_word_suffix
;
}
this
.
added_tokens_regex
=
this
.
added_tokens
.
length
>
0
?
new
RegExp
(
this
.
added_tokens
.
map
(
x
=
>
{
x
.
lstrip
?
'
\
\
s
*
'
:
'
'
}
(
{
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
escapeRegExp
)
(
x
.
content
)
}
)
{
x
.
rstrip
?
'
\
\
s
*
'
:
'
'
}
)
.
join
(
'
|
'
)
)
:
null
;
this
.
mask_token
=
this
.
getToken
(
'
mask_token
'
)
;
this
.
mask_token_id
=
this
.
model
.
tokens_to_ids
.
get
(
this
.
mask_token
)
;
this
.
pad_token
=
this
.
getToken
(
'
pad_token
'
'
eos_token
'
)
;
this
.
pad_token_id
=
this
.
model
.
tokens_to_ids
.
get
(
this
.
pad_token
)
;
this
.
sep_token
=
this
.
getToken
(
'
sep_token
'
)
;
this
.
sep_token_id
=
this
.
model
.
tokens_to_ids
.
get
(
this
.
sep_token
)
;
this
.
unk_token
=
this
.
getToken
(
'
unk_token
'
)
;
this
.
unk_token_id
=
this
.
model
.
tokens_to_ids
.
get
(
this
.
unk_token
)
;
this
.
model_max_length
=
tokenizerConfig
.
model_max_length
;
this
.
remove_space
=
tokenizerConfig
.
remove_space
;
this
.
clean_up_tokenization_spaces
=
tokenizerConfig
.
clean_up_tokenization_spaces
?
?
true
;
this
.
do_lowercase_and_remove_accent
=
tokenizerConfig
.
do_lowercase_and_remove_accent
?
?
false
;
this
.
padding_side
=
'
right
'
;
this
.
legacy
=
false
;
this
.
chat_template
=
tokenizerConfig
.
chat_template
?
?
null
;
if
(
Array
.
isArray
(
this
.
chat_template
)
)
{
const
chat_template
=
Object
.
create
(
null
)
;
for
(
const
{
name
template
}
of
this
.
chat_template
)
{
if
(
typeof
name
!
=
=
'
string
'
|
|
typeof
template
!
=
=
'
string
'
)
{
throw
new
Error
(
'
Chat
template
must
be
a
list
of
objects
with
"
name
"
and
"
template
"
properties
'
)
;
}
chat_template
[
name
]
=
template
;
}
this
.
chat_template
=
chat_template
;
}
this
.
_compiled_template_cache
=
new
Map
(
)
;
}
getToken
(
.
.
.
keys
)
{
for
(
const
key
of
keys
)
{
const
item
=
this
.
_tokenizer_config
[
key
]
;
if
(
!
item
)
continue
;
if
(
typeof
item
=
=
=
'
object
'
)
{
if
(
item
.
__type
=
=
=
'
AddedToken
'
)
{
return
item
.
content
;
}
else
{
throw
Error
(
Unknown
token
:
{
item
}
)
;
}
}
else
{
return
item
;
}
}
return
null
;
}
static
async
from_pretrained
(
pretrained_model_name_or_path
{
progress_callback
=
null
config
=
null
cache_dir
=
null
local_files_only
=
false
revision
=
'
main
'
legacy
=
null
}
=
{
}
)
{
const
info
=
await
loadTokenizer
(
pretrained_model_name_or_path
{
progress_callback
config
cache_dir
local_files_only
revision
legacy
}
)
return
new
this
(
.
.
.
info
)
;
}
_call
(
text
{
text_pair
=
null
add_special_tokens
=
true
padding
=
false
truncation
=
null
max_length
=
null
return_tensor
=
true
}
=
{
}
)
{
const
isBatched
=
Array
.
isArray
(
text
)
;
let
encodedTokens
;
if
(
isBatched
)
{
if
(
text
.
length
=
=
=
0
)
{
throw
Error
(
'
text
array
must
be
non
-
empty
'
)
}
if
(
text_pair
!
=
=
null
)
{
if
(
!
Array
.
isArray
(
text_pair
)
)
{
throw
Error
(
'
text_pair
must
also
be
an
array
'
)
}
else
if
(
text
.
length
!
=
=
text_pair
.
length
)
{
throw
Error
(
'
text
and
text_pair
must
have
the
same
length
'
)
}
encodedTokens
=
text
.
map
(
(
t
i
)
=
>
this
.
_encode_plus
(
t
text_pair
[
i
]
{
add_special_tokens
}
)
)
}
else
{
encodedTokens
=
text
.
map
(
x
=
>
this
.
_encode_plus
(
x
null
{
add_special_tokens
}
)
)
;
}
}
else
{
if
(
text
=
=
=
null
)
{
throw
Error
(
'
text
may
not
be
null
'
)
}
if
(
Array
.
isArray
(
text_pair
)
)
{
throw
Error
(
'
When
specifying
text_pair
since
text
is
a
string
text_pair
must
also
be
a
string
(
i
.
e
.
not
an
array
)
.
'
)
}
encodedTokens
=
[
this
.
_encode_plus
(
text
text_pair
{
add_special_tokens
}
)
]
;
}
if
(
max_length
=
=
=
null
)
{
if
(
padding
=
=
=
'
max_length
'
)
{
max_length
=
this
.
model_max_length
;
}
else
{
max_length
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
max
)
(
encodedTokens
.
map
(
x
=
>
x
.
input_ids
.
length
)
)
[
0
]
;
}
}
else
{
if
(
!
truncation
)
{
console
.
warn
(
Truncation
was
not
explicitly
activated
but
\
max_length
\
is
provided
a
specific
value
please
use
\
truncation
=
true
\
to
explicitly
truncate
examples
to
max
length
.
)
}
}
max_length
=
Math
.
min
(
max_length
this
.
model_max_length
)
if
(
padding
|
|
truncation
)
{
for
(
let
i
=
0
;
i
<
encodedTokens
.
length
;
+
+
i
)
{
if
(
encodedTokens
[
i
]
.
input_ids
.
length
=
=
=
max_length
)
{
continue
;
}
else
if
(
encodedTokens
[
i
]
.
input_ids
.
length
>
max_length
)
{
if
(
truncation
)
{
truncateHelper
(
encodedTokens
[
i
]
max_length
)
;
}
}
else
{
if
(
padding
)
{
padHelper
(
encodedTokens
[
i
]
max_length
key
=
>
key
=
=
=
'
input_ids
'
?
this
.
pad_token_id
:
0
this
.
padding_side
)
;
}
}
}
}
const
result
=
{
}
;
if
(
return_tensor
)
{
if
(
!
(
padding
&
&
truncation
)
)
{
if
(
encodedTokens
.
some
(
x
=
>
{
for
(
const
key
of
Object
.
keys
(
x
)
)
{
if
(
x
[
key
]
.
length
!
=
=
encodedTokens
[
0
]
[
key
]
?
.
length
)
{
return
true
;
}
}
return
false
;
}
)
)
{
throw
Error
(
"
Unable
to
create
tensor
you
should
probably
activate
truncation
and
/
or
padding
"
+
"
with
'
padding
=
true
'
and
'
truncation
=
true
'
to
have
batched
tensors
with
the
same
length
.
"
)
}
}
const
dims
=
[
encodedTokens
.
length
encodedTokens
[
0
]
.
input_ids
.
length
]
;
for
(
const
key
of
Object
.
keys
(
encodedTokens
[
0
]
)
)
{
result
[
key
]
=
new
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
(
'
int64
'
BigInt64Array
.
from
(
encodedTokens
.
flatMap
(
x
=
>
x
[
key
]
)
.
map
(
BigInt
)
)
dims
)
;
}
}
else
{
for
(
const
key
of
Object
.
keys
(
encodedTokens
[
0
]
)
)
{
result
[
key
]
=
encodedTokens
.
map
(
x
=
>
x
[
key
]
)
;
}
if
(
!
isBatched
)
{
for
(
const
key
of
Object
.
keys
(
result
)
)
{
result
[
key
]
=
result
[
key
]
[
0
]
;
}
}
}
return
(
result
)
;
}
_encode_text
(
text
)
{
if
(
text
=
=
=
null
)
return
null
;
const
sections
=
this
.
added_tokens_regex
?
text
.
split
(
this
.
added_tokens_regex
)
.
filter
(
x
=
>
x
)
:
[
text
]
;
const
tokens
=
sections
.
map
(
(
x
section_index
)
=
>
{
const
addedToken
=
this
.
added_tokens
.
find
(
t
=
>
t
.
content
=
=
=
x
)
;
if
(
addedToken
!
=
=
undefined
)
{
return
x
}
else
{
if
(
this
.
remove_space
=
=
=
true
)
{
x
=
x
.
trim
(
)
.
split
(
/
\
s
+
/
)
.
join
(
'
'
)
;
}
if
(
this
.
do_lowercase_and_remove_accent
)
{
x
=
lowercase_and_remove_accent
(
x
)
;
}
if
(
this
.
normalizer
!
=
=
null
)
{
x
=
this
.
normalizer
(
x
)
;
}
if
(
x
.
length
=
=
=
0
)
{
return
[
]
;
}
const
sectionTokens
=
(
this
.
pre_tokenizer
!
=
=
null
)
?
this
.
pre_tokenizer
(
x
{
section_index
}
)
:
[
x
]
;
const
tokens
=
this
.
model
(
sectionTokens
)
;
return
tokens
;
}
}
)
.
flat
(
)
;
return
tokens
;
}
_encode_plus
(
text
text_pair
=
null
{
add_special_tokens
=
true
}
=
{
}
)
{
const
tokens
=
this
.
_encode_text
(
text
)
;
const
tokens2
=
this
.
_encode_text
(
text_pair
)
;
const
combinedTokens
=
this
.
post_processor
?
this
.
post_processor
(
tokens
tokens2
{
add_special_tokens
}
)
:
{
tokens
:
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
tokens
?
?
[
]
tokens2
?
?
[
]
)
}
;
const
input_ids
=
this
.
model
.
convert_tokens_to_ids
(
combinedTokens
.
tokens
)
;
const
result
=
{
input_ids
attention_mask
:
new
Array
(
input_ids
.
length
)
.
fill
(
1
)
}
if
(
this
.
return_token_type_ids
&
&
combinedTokens
.
token_type_ids
)
{
result
.
token_type_ids
=
combinedTokens
.
token_type_ids
;
}
return
result
;
}
encode
(
text
text_pair
=
null
{
add_special_tokens
=
true
}
=
{
}
)
{
const
{
input_ids
}
=
this
.
_encode_plus
(
text
text_pair
{
add_special_tokens
}
)
;
return
input_ids
;
}
batch_decode
(
batch
decode_args
=
{
}
)
{
if
(
batch
instanceof
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
)
{
batch
=
batch
.
tolist
(
)
;
}
return
batch
.
map
(
x
=
>
this
.
decode
(
x
decode_args
)
)
;
}
decode
(
token_ids
decode_args
=
{
}
)
{
if
(
token_ids
instanceof
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
)
{
token_ids
=
prepareTensorForDecode
(
token_ids
)
;
}
if
(
!
Array
.
isArray
(
token_ids
)
|
|
token_ids
.
length
=
=
=
0
|
|
!
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
isIntegralNumber
)
(
token_ids
[
0
]
)
)
{
throw
Error
(
"
token_ids
must
be
a
non
-
empty
array
of
integers
.
"
)
;
}
return
this
.
decode_single
(
token_ids
decode_args
)
}
decode_single
(
token_ids
{
skip_special_tokens
=
false
clean_up_tokenization_spaces
=
null
}
)
{
let
tokens
=
this
.
model
.
convert_ids_to_tokens
(
token_ids
)
;
if
(
skip_special_tokens
)
{
tokens
=
tokens
.
filter
(
x
=
>
!
this
.
special_tokens
.
includes
(
x
)
)
;
}
let
decoded
=
this
.
decoder
?
this
.
decoder
(
tokens
)
:
tokens
.
join
(
'
'
)
;
if
(
this
.
decoder
&
&
this
.
decoder
.
end_of_word_suffix
)
{
decoded
=
decoded
.
replaceAll
(
this
.
decoder
.
end_of_word_suffix
'
'
)
;
if
(
skip_special_tokens
)
{
decoded
=
decoded
.
trim
(
)
;
}
}
if
(
clean_up_tokenization_spaces
?
?
this
.
clean_up_tokenization_spaces
)
{
decoded
=
clean_up_tokenization
(
decoded
)
;
}
return
decoded
;
}
get
default_chat_template
(
)
{
if
(
!
this
.
_warned_about_chat_template
)
{
console
.
warn
(
"
No
chat
template
is
defined
for
this
tokenizer
-
using
a
default
chat
template
"
+
"
that
implements
the
ChatML
format
.
If
the
default
is
not
appropriate
for
"
+
"
your
model
please
set
tokenizer
.
chat_template
to
an
appropriate
template
.
"
+
"
See
https
:
/
/
huggingface
.
co
/
docs
/
transformers
/
main
/
chat_templating
for
more
information
.
"
)
this
.
_warned_about_chat_template
=
true
;
}
return
this
.
_default_chat_template
;
}
apply_chat_template
(
conversation
{
chat_template
=
null
add_generation_prompt
=
false
tokenize
=
true
padding
=
false
truncation
=
false
max_length
=
null
return_tensor
=
true
tokenizer_kwargs
=
{
}
.
.
.
kwargs
}
=
{
}
)
{
if
(
(
this
.
chat_template
&
&
typeof
this
.
chat_template
=
=
=
'
object
'
)
|
|
(
this
.
chat_template
=
=
=
null
&
&
this
.
default_chat_template
&
&
typeof
this
.
default_chat_template
=
=
=
'
object
'
)
)
{
const
template_dict
=
this
.
chat_template
?
?
this
.
default_chat_template
;
if
(
chat_template
!
=
=
null
&
&
Object
.
hasOwn
(
template_dict
chat_template
)
)
{
chat_template
=
template_dict
[
chat_template
]
;
}
else
if
(
chat_template
=
=
=
null
&
&
'
default
'
in
template_dict
)
{
chat_template
=
template_dict
[
'
default
'
]
;
}
else
if
(
chat_template
=
=
=
null
)
{
throw
Error
(
This
model
has
multiple
chat
templates
with
no
default
specified
!
Please
either
pass
a
chat
+
template
or
the
name
of
the
template
you
wish
to
use
to
the
'
chat_template
'
argument
.
Available
+
template
names
are
{
Object
.
keys
(
template_dict
)
.
sort
(
)
}
.
)
}
}
else
{
chat_template
?
?
=
this
.
chat_template
?
?
this
.
default_chat_template
;
}
if
(
typeof
chat_template
!
=
=
'
string
'
)
{
throw
Error
(
chat_template
must
be
a
string
but
got
{
typeof
chat_template
}
)
;
}
let
compiledTemplate
=
this
.
_compiled_template_cache
.
get
(
chat_template
)
;
if
(
compiledTemplate
=
=
=
undefined
)
{
compiledTemplate
=
new
_huggingface_jinja__WEBPACK_IMPORTED_MODULE_5__
.
Template
(
chat_template
)
;
this
.
_compiled_template_cache
.
set
(
chat_template
compiledTemplate
)
;
}
const
special_tokens_map
=
Object
.
create
(
null
)
;
for
(
const
key
of
SPECIAL_TOKEN_ATTRIBUTES
)
{
const
value
=
this
.
getToken
(
key
)
;
if
(
value
)
{
special_tokens_map
[
key
]
=
value
;
}
}
const
rendered
=
compiledTemplate
.
render
(
{
messages
:
conversation
add_generation_prompt
:
add_generation_prompt
.
.
.
special_tokens_map
.
.
.
kwargs
}
)
;
if
(
tokenize
)
{
return
this
.
_call
(
rendered
{
add_special_tokens
:
false
padding
truncation
max_length
return_tensor
.
.
.
tokenizer_kwargs
}
)
.
input_ids
;
}
return
rendered
;
}
}
class
BertTokenizer
extends
PreTrainedTokenizer
{
return_token_type_ids
=
true
;
}
class
AlbertTokenizer
extends
PreTrainedTokenizer
{
return_token_type_ids
=
true
;
}
class
MobileBertTokenizer
extends
PreTrainedTokenizer
{
return_token_type_ids
=
true
;
}
class
SqueezeBertTokenizer
extends
PreTrainedTokenizer
{
return_token_type_ids
=
true
;
}
class
DebertaTokenizer
extends
PreTrainedTokenizer
{
return_token_type_ids
=
true
;
}
class
DebertaV2Tokenizer
extends
PreTrainedTokenizer
{
return_token_type_ids
=
true
;
}
class
HerbertTokenizer
extends
PreTrainedTokenizer
{
return_token_type_ids
=
true
;
}
class
ConvBertTokenizer
extends
PreTrainedTokenizer
{
return_token_type_ids
=
true
;
}
class
RoFormerTokenizer
extends
PreTrainedTokenizer
{
return_token_type_ids
=
true
;
}
class
DistilBertTokenizer
extends
PreTrainedTokenizer
{
}
class
CamembertTokenizer
extends
PreTrainedTokenizer
{
}
class
XLMTokenizer
extends
PreTrainedTokenizer
{
return_token_type_ids
=
true
;
constructor
(
tokenizerJSON
tokenizerConfig
)
{
super
(
tokenizerJSON
tokenizerConfig
)
;
console
.
warn
(
'
WARNING
:
XLMTokenizer
is
not
yet
supported
by
Hugging
Face
\
'
s
"
fast
"
tokenizers
library
.
Therefore
you
may
experience
slightly
inaccurate
results
.
'
)
}
}
class
ElectraTokenizer
extends
PreTrainedTokenizer
{
return_token_type_ids
=
true
;
}
class
T5Tokenizer
extends
PreTrainedTokenizer
{
}
class
GPT2Tokenizer
extends
PreTrainedTokenizer
{
_default_chat_template
=
{
%
for
message
in
messages
%
}
"
"
{
{
message
.
content
}
}
{
{
eos_token
}
}
"
"
{
%
endfor
%
}
}
class
BartTokenizer
extends
PreTrainedTokenizer
{
}
class
MBartTokenizer
extends
PreTrainedTokenizer
{
constructor
(
tokenizerJSON
tokenizerConfig
)
{
super
(
tokenizerJSON
tokenizerConfig
)
;
this
.
languageRegex
=
/
^
[
a
-
z
]
{
2
}
_
[
A
-
Z
]
{
2
}
/
;
this
.
language_codes
=
this
.
special_tokens
.
filter
(
x
=
>
this
.
languageRegex
.
test
(
x
)
)
;
this
.
lang_to_token
=
x
=
>
x
;
}
_build_translation_inputs
(
raw_inputs
tokenizer_options
generate_kwargs
)
{
return
_build_translation_inputs
(
this
raw_inputs
tokenizer_options
generate_kwargs
)
;
}
}
class
MBart50Tokenizer
extends
MBartTokenizer
{
}
class
RobertaTokenizer
extends
PreTrainedTokenizer
{
}
class
BloomTokenizer
extends
GPT2Tokenizer
{
constructor
(
tokenizerJSON
tokenizerConfig
)
{
const
splitChars
=
'
.
!
?
\
u2026
\
u3002
\
uff0c
\
u3001
\
u0964
\
u06d4
\
u060c
'
;
const
patternObject
=
tokenizerJSON
.
pre_tokenizer
?
.
pretokenizers
[
0
]
?
.
pattern
;
if
(
patternObject
&
&
patternObject
.
Regex
=
=
=
?
[
^
(
\
\
s
|
[
{
splitChars
}
]
)
]
+
)
{
patternObject
.
Regex
=
?
[
^
\
\
s
{
splitChars
}
]
+
;
}
super
(
tokenizerJSON
tokenizerConfig
)
;
}
}
const
SPIECE_UNDERLINE
=
"
"
;
class
LlamaTokenizer
extends
PreTrainedTokenizer
{
_default_chat_template
=
{
%
if
messages
[
0
]
[
'
role
'
]
=
=
'
system
'
%
}
{
%
set
loop_messages
=
messages
[
1
:
]
%
}
{
%
set
system_message
=
messages
[
0
]
[
'
content
'
]
%
}
{
%
elif
USE_DEFAULT_PROMPT
=
=
true
and
not
'
<
<
SYS
>
>
'
in
messages
[
0
]
[
'
content
'
]
%
}
{
%
set
loop_messages
=
messages
%
}
{
%
set
system_message
=
'
DEFAULT_SYSTEM_MESSAGE
'
%
}
{
%
else
%
}
{
%
set
loop_messages
=
messages
%
}
{
%
set
system_message
=
false
%
}
{
%
endif
%
}
{
%
for
message
in
loop_messages
%
}
{
%
if
(
message
[
'
role
'
]
=
=
'
user
'
)
!
=
(
loop
.
index0
%
2
=
=
0
)
%
}
{
{
raise_exception
(
'
Conversation
roles
must
alternate
user
/
assistant
/
user
/
assistant
/
.
.
.
'
)
}
}
{
%
endif
%
}
{
%
if
loop
.
index0
=
=
0
and
system_message
!
=
false
%
}
{
%
set
content
=
'
<
<
SYS
>
>
\
n
'
+
system_message
+
'
\
n
<
<
/
SYS
>
>
\
n
\
n
'
+
message
[
'
content
'
]
%
}
{
%
else
%
}
{
%
set
content
=
message
[
'
content
'
]
%
}
{
%
endif
%
}
{
%
if
message
[
'
role
'
]
=
=
'
user
'
%
}
{
{
bos_token
+
'
[
INST
]
'
+
content
.
strip
(
)
+
'
[
/
INST
]
'
}
}
{
%
elif
message
[
'
role
'
]
=
=
'
system
'
%
}
{
{
'
<
<
SYS
>
>
\
n
'
+
content
.
strip
(
)
+
'
\
n
<
<
/
SYS
>
>
\
n
\
n
'
}
}
{
%
elif
message
[
'
role
'
]
=
=
'
assistant
'
%
}
{
{
'
'
+
content
.
strip
(
)
+
'
'
+
eos_token
}
}
{
%
endif
%
}
{
%
endfor
%
}
DEFAULT_SYSTEM_PROMPT
=
"
You
are
a
helpful
respectful
and
honest
assistant
.
Always
answer
as
helpfully
as
possible
while
being
safe
.
Your
"
+
"
answers
should
not
include
any
harmful
unethical
racist
sexist
toxic
dangerous
or
illegal
content
.
Please
ensure
"
+
"
that
your
responses
are
socially
unbiased
and
positive
in
nature
.
\
n
\
n
"
+
"
If
a
question
does
not
make
any
sense
or
is
not
factually
coherent
explain
why
instead
of
answering
something
not
"
+
"
correct
.
If
you
don
'
t
know
the
answer
to
a
question
please
don
'
t
share
false
information
.
"
constructor
(
tokenizerJSON
tokenizerConfig
)
{
super
(
tokenizerJSON
tokenizerConfig
)
;
this
.
use_default_system_prompt
=
tokenizerConfig
.
use_default_system_prompt
?
?
false
;
this
.
legacy
=
tokenizerConfig
.
legacy
?
?
true
;
if
(
!
this
.
legacy
)
{
this
.
normalizer
=
null
;
this
.
pre_tokenizer
=
new
MetaspacePreTokenizer
(
{
replacement
:
SPIECE_UNDERLINE
add_prefix_space
:
true
prepend_scheme
:
"
first
"
}
)
;
}
}
_encode_text
(
text
)
{
if
(
text
=
=
=
null
)
return
null
;
if
(
this
.
legacy
|
|
text
.
length
=
=
=
0
)
{
return
super
.
_encode_text
(
text
)
;
}
let
tokens
=
super
.
_encode_text
(
SPIECE_UNDERLINE
+
text
.
replaceAll
(
SPIECE_UNDERLINE
"
"
)
)
;
if
(
tokens
.
length
>
1
&
&
tokens
[
0
]
=
=
=
SPIECE_UNDERLINE
&
&
this
.
special_tokens
.
includes
(
tokens
[
1
]
)
)
{
tokens
=
tokens
.
slice
(
1
)
;
}
return
tokens
;
}
get
default_chat_template
(
)
{
return
super
.
default_chat_template
.
replaceAll
(
'
USE_DEFAULT_PROMPT
'
this
.
use_default_system_prompt
?
'
true
'
:
'
false
'
)
.
replaceAll
(
'
DEFAULT_SYSTEM_MESSAGE
'
this
.
DEFAULT_SYSTEM_PROMPT
.
replaceAll
(
"
\
n
"
"
\
\
n
"
)
.
replaceAll
(
"
'
"
"
\
\
'
"
)
)
;
}
}
class
CodeLlamaTokenizer
extends
LlamaTokenizer
{
}
class
XLMRobertaTokenizer
extends
PreTrainedTokenizer
{
}
class
MPNetTokenizer
extends
PreTrainedTokenizer
{
}
class
FalconTokenizer
extends
PreTrainedTokenizer
{
}
class
GPTNeoXTokenizer
extends
PreTrainedTokenizer
{
}
class
EsmTokenizer
extends
PreTrainedTokenizer
{
}
class
Qwen2Tokenizer
extends
PreTrainedTokenizer
{
}
class
GemmaTokenizer
extends
PreTrainedTokenizer
{
_default_chat_template
=
"
{
%
if
messages
[
0
]
[
'
role
'
]
=
=
'
system
'
%
}
{
{
raise_exception
(
'
System
role
not
supported
'
)
}
}
{
%
endif
%
}
{
%
for
message
in
messages
%
}
{
%
if
(
message
[
'
role
'
]
=
=
'
user
'
)
!
=
(
loop
.
index0
%
2
=
=
0
)
%
}
{
{
raise_exception
(
'
Conversation
roles
must
alternate
user
/
assistant
/
user
/
assistant
/
.
.
.
'
)
}
}
{
%
endif
%
}
{
%
if
(
message
[
'
role
'
]
=
=
'
assistant
'
)
%
}
{
%
set
role
=
'
model
'
%
}
{
%
else
%
}
{
%
set
role
=
message
[
'
role
'
]
%
}
{
%
endif
%
}
{
{
'
<
start_of_turn
>
'
+
role
+
'
\
n
'
+
message
[
'
content
'
]
|
trim
+
'
<
end_of_turn
>
\
n
'
}
}
{
%
endfor
%
}
{
%
if
add_generation_prompt
%
}
{
{
'
<
start_of_turn
>
model
\
n
'
}
}
{
%
endif
%
}
"
}
class
Grok1Tokenizer
extends
PreTrainedTokenizer
{
}
function
_build_translation_inputs
(
self
raw_inputs
tokenizer_options
generate_kwargs
)
{
if
(
!
(
'
language_codes
'
in
self
)
|
|
!
Array
.
isArray
(
self
.
language_codes
)
)
{
throw
new
Error
(
'
Tokenizer
must
have
language_codes
attribute
set
and
it
should
be
an
array
of
language
ids
.
'
)
}
if
(
!
(
'
languageRegex
'
in
self
)
|
|
!
(
self
.
languageRegex
instanceof
RegExp
)
)
{
throw
new
Error
(
'
Tokenizer
must
have
languageRegex
attribute
set
and
it
should
be
a
regular
expression
.
'
)
}
if
(
!
(
'
lang_to_token
'
in
self
)
|
|
typeof
self
.
lang_to_token
!
=
=
'
function
'
)
{
throw
new
Error
(
'
Tokenizer
must
have
lang_to_token
attribute
set
and
it
should
be
a
function
.
'
)
}
const
src_lang_token
=
generate_kwargs
.
src_lang
;
const
tgt_lang_token
=
generate_kwargs
.
tgt_lang
;
if
(
!
self
.
language_codes
.
includes
(
tgt_lang_token
)
)
{
throw
new
Error
(
Target
language
code
"
{
tgt_lang_token
}
"
is
not
valid
.
Must
be
one
of
:
{
{
self
.
language_codes
.
join
(
'
'
)
}
}
)
;
}
if
(
src_lang_token
!
=
=
undefined
)
{
if
(
!
self
.
language_codes
.
includes
(
src_lang_token
)
)
{
throw
new
Error
(
Source
language
code
"
{
src_lang_token
}
"
is
not
valid
.
Must
be
one
of
:
{
{
self
.
language_codes
.
join
(
'
'
)
}
}
)
;
}
for
(
const
item
of
self
.
post_processor
.
config
.
single
)
{
if
(
'
SpecialToken
'
in
item
&
&
self
.
languageRegex
.
test
(
item
.
SpecialToken
.
id
)
)
{
item
.
SpecialToken
.
id
=
self
.
lang_to_token
(
src_lang_token
)
;
break
;
}
}
}
generate_kwargs
.
forced_bos_token_id
=
self
.
model
.
convert_tokens_to_ids
(
[
self
.
lang_to_token
(
tgt_lang_token
)
]
)
[
0
]
;
return
self
.
_call
(
raw_inputs
tokenizer_options
)
;
}
class
NllbTokenizer
extends
PreTrainedTokenizer
{
constructor
(
tokenizerJSON
tokenizerConfig
)
{
super
(
tokenizerJSON
tokenizerConfig
)
;
this
.
languageRegex
=
/
^
[
a
-
z
]
{
3
}
_
[
A
-
Z
]
[
a
-
z
]
{
3
}
/
;
this
.
language_codes
=
this
.
special_tokens
.
filter
(
x
=
>
this
.
languageRegex
.
test
(
x
)
)
;
this
.
lang_to_token
=
x
=
>
x
;
}
_build_translation_inputs
(
raw_inputs
tokenizer_options
generate_kwargs
)
{
return
_build_translation_inputs
(
this
raw_inputs
tokenizer_options
generate_kwargs
)
;
}
}
class
M2M100Tokenizer
extends
PreTrainedTokenizer
{
constructor
(
tokenizerJSON
tokenizerConfig
)
{
super
(
tokenizerJSON
tokenizerConfig
)
;
this
.
languageRegex
=
/
^
__
[
a
-
z
]
{
2
3
}
__
/
;
this
.
language_codes
=
this
.
special_tokens
.
filter
(
x
=
>
this
.
languageRegex
.
test
(
x
)
)
.
map
(
x
=
>
x
.
slice
(
2
-
2
)
)
;
this
.
lang_to_token
=
x
=
>
__
{
x
}
__
;
}
_build_translation_inputs
(
raw_inputs
tokenizer_options
generate_kwargs
)
{
return
_build_translation_inputs
(
this
raw_inputs
tokenizer_options
generate_kwargs
)
;
}
}
const
WHISPER_LANGUAGES
=
[
[
"
en
"
"
english
"
]
[
"
zh
"
"
chinese
"
]
[
"
de
"
"
german
"
]
[
"
es
"
"
spanish
"
]
[
"
ru
"
"
russian
"
]
[
"
ko
"
"
korean
"
]
[
"
fr
"
"
french
"
]
[
"
ja
"
"
japanese
"
]
[
"
pt
"
"
portuguese
"
]
[
"
tr
"
"
turkish
"
]
[
"
pl
"
"
polish
"
]
[
"
ca
"
"
catalan
"
]
[
"
nl
"
"
dutch
"
]
[
"
ar
"
"
arabic
"
]
[
"
sv
"
"
swedish
"
]
[
"
it
"
"
italian
"
]
[
"
id
"
"
indonesian
"
]
[
"
hi
"
"
hindi
"
]
[
"
fi
"
"
finnish
"
]
[
"
vi
"
"
vietnamese
"
]
[
"
he
"
"
hebrew
"
]
[
"
uk
"
"
ukrainian
"
]
[
"
el
"
"
greek
"
]
[
"
ms
"
"
malay
"
]
[
"
cs
"
"
czech
"
]
[
"
ro
"
"
romanian
"
]
[
"
da
"
"
danish
"
]
[
"
hu
"
"
hungarian
"
]
[
"
ta
"
"
tamil
"
]
[
"
no
"
"
norwegian
"
]
[
"
th
"
"
thai
"
]
[
"
ur
"
"
urdu
"
]
[
"
hr
"
"
croatian
"
]
[
"
bg
"
"
bulgarian
"
]
[
"
lt
"
"
lithuanian
"
]
[
"
la
"
"
latin
"
]
[
"
mi
"
"
maori
"
]
[
"
ml
"
"
malayalam
"
]
[
"
cy
"
"
welsh
"
]
[
"
sk
"
"
slovak
"
]
[
"
te
"
"
telugu
"
]
[
"
fa
"
"
persian
"
]
[
"
lv
"
"
latvian
"
]
[
"
bn
"
"
bengali
"
]
[
"
sr
"
"
serbian
"
]
[
"
az
"
"
azerbaijani
"
]
[
"
sl
"
"
slovenian
"
]
[
"
kn
"
"
kannada
"
]
[
"
et
"
"
estonian
"
]
[
"
mk
"
"
macedonian
"
]
[
"
br
"
"
breton
"
]
[
"
eu
"
"
basque
"
]
[
"
is
"
"
icelandic
"
]
[
"
hy
"
"
armenian
"
]
[
"
ne
"
"
nepali
"
]
[
"
mn
"
"
mongolian
"
]
[
"
bs
"
"
bosnian
"
]
[
"
kk
"
"
kazakh
"
]
[
"
sq
"
"
albanian
"
]
[
"
sw
"
"
swahili
"
]
[
"
gl
"
"
galician
"
]
[
"
mr
"
"
marathi
"
]
[
"
pa
"
"
punjabi
"
]
[
"
si
"
"
sinhala
"
]
[
"
km
"
"
khmer
"
]
[
"
sn
"
"
shona
"
]
[
"
yo
"
"
yoruba
"
]
[
"
so
"
"
somali
"
]
[
"
af
"
"
afrikaans
"
]
[
"
oc
"
"
occitan
"
]
[
"
ka
"
"
georgian
"
]
[
"
be
"
"
belarusian
"
]
[
"
tg
"
"
tajik
"
]
[
"
sd
"
"
sindhi
"
]
[
"
gu
"
"
gujarati
"
]
[
"
am
"
"
amharic
"
]
[
"
yi
"
"
yiddish
"
]
[
"
lo
"
"
lao
"
]
[
"
uz
"
"
uzbek
"
]
[
"
fo
"
"
faroese
"
]
[
"
ht
"
"
haitian
creole
"
]
[
"
ps
"
"
pashto
"
]
[
"
tk
"
"
turkmen
"
]
[
"
nn
"
"
nynorsk
"
]
[
"
mt
"
"
maltese
"
]
[
"
sa
"
"
sanskrit
"
]
[
"
lb
"
"
luxembourgish
"
]
[
"
my
"
"
myanmar
"
]
[
"
bo
"
"
tibetan
"
]
[
"
tl
"
"
tagalog
"
]
[
"
mg
"
"
malagasy
"
]
[
"
as
"
"
assamese
"
]
[
"
tt
"
"
tatar
"
]
[
"
haw
"
"
hawaiian
"
]
[
"
ln
"
"
lingala
"
]
[
"
ha
"
"
hausa
"
]
[
"
ba
"
"
bashkir
"
]
[
"
jw
"
"
javanese
"
]
[
"
su
"
"
sundanese
"
]
]
const
WHISPER_LANGUAGE_MAPPING
=
new
Map
(
WHISPER_LANGUAGES
)
;
const
WHISPER_TO_LANGUAGE_CODE_MAPPING
=
new
Map
(
[
.
.
.
WHISPER_LANGUAGES
.
map
(
(
[
k
v
]
)
=
>
[
v
k
]
)
.
.
.
[
[
"
burmese
"
"
my
"
]
[
"
valencian
"
"
ca
"
]
[
"
flemish
"
"
nl
"
]
[
"
haitian
"
"
ht
"
]
[
"
letzeburgesch
"
"
lb
"
]
[
"
pushto
"
"
ps
"
]
[
"
panjabi
"
"
pa
"
]
[
"
moldavian
"
"
ro
"
]
[
"
moldovan
"
"
ro
"
]
[
"
sinhalese
"
"
si
"
]
[
"
castilian
"
"
es
"
]
]
]
)
;
class
WhisperTokenizer
extends
PreTrainedTokenizer
{
_default_chat_template
=
{
%
for
message
in
messages
%
}
"
"
{
{
message
.
content
}
}
{
{
eos_token
}
}
"
"
{
%
endfor
%
}
;
_decode_asr
(
sequences
{
return_timestamps
=
false
return_language
=
false
time_precision
=
null
force_full_sequences
=
true
}
=
{
}
)
{
if
(
time_precision
=
=
=
null
)
{
throw
Error
(
"
Must
specify
time_precision
"
)
}
let
last_language
=
null
;
const
returnWordTimestamps
=
return_timestamps
=
=
=
"
word
"
;
function
new_chunk
(
)
{
return
{
"
language
"
:
last_language
"
timestamp
"
:
[
null
null
]
"
text
"
:
"
"
}
;
}
const
chunks
=
[
]
;
let
chunk
=
new_chunk
(
)
;
let
time_offset
=
0
.
0
;
const
timestamp_begin
=
this
.
model
.
convert_tokens_to_ids
(
[
"
<
|
notimestamps
|
>
"
]
)
[
0
]
+
1
;
let
previous_tokens
=
[
]
;
let
previous_token_timestamps
=
[
]
;
let
skip
=
false
;
let
right_stride_start
=
null
;
const
all_special_ids
=
new
Set
(
this
.
all_special_ids
)
;
for
(
const
output
of
sequences
)
{
const
token_ids
=
output
.
tokens
;
const
token_timestamps
=
returnWordTimestamps
?
output
.
token_timestamps
:
null
;
let
last_timestamp
=
null
;
let
first_timestamp
=
timestamp_begin
;
if
(
"
stride
"
in
output
)
{
const
[
chunk_len
stride_left
stride_right
]
=
output
.
stride
;
time_offset
-
=
stride_left
;
right_stride_start
=
chunk_len
-
stride_right
;
if
(
stride_left
)
{
first_timestamp
=
stride_left
/
time_precision
+
timestamp_begin
;
}
if
(
stride_right
)
{
for
(
let
i
=
token_ids
.
length
-
1
;
i
>
=
0
;
-
-
i
)
{
const
token
=
token_ids
[
i
]
;
if
(
token
>
=
timestamp_begin
)
{
if
(
last_timestamp
!
=
=
null
&
&
(
token
-
timestamp_begin
)
*
time_precision
<
right_stride_start
)
{
break
;
}
last_timestamp
=
token
;
}
}
}
}
let
current_tokens
=
[
]
;
let
current_token_timestamps
=
[
]
;
for
(
let
i
=
0
;
i
<
token_ids
.
length
;
+
+
i
)
{
const
token
=
token_ids
[
i
]
;
if
(
all_special_ids
.
has
(
token
)
)
{
const
text
=
this
.
decode
(
[
token
]
)
;
const
language
=
WHISPER_LANGUAGE_MAPPING
.
get
(
text
.
slice
(
2
-
2
)
)
;
if
(
language
!
=
=
undefined
)
{
if
(
last_language
!
=
=
null
&
&
language
!
=
=
last_language
&
&
!
return_timestamps
)
{
previous_tokens
.
push
(
current_tokens
)
;
const
resolved_tokens
=
this
.
findLongestCommonSequence
(
previous_tokens
)
[
0
]
;
const
resolved_text
=
this
.
decode
(
resolved_tokens
)
;
chunk
.
text
=
resolved_text
;
chunks
.
push
(
chunk
)
;
previous_tokens
=
[
]
;
current_tokens
=
[
]
;
chunk
=
new_chunk
(
)
;
}
last_language
=
chunk
.
language
=
language
;
}
else
{
}
}
else
if
(
token
>
=
timestamp_begin
)
{
const
time
=
(
token
-
timestamp_begin
)
*
time_precision
+
time_offset
;
const
rounded_time
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
round
)
(
time
2
)
;
if
(
last_timestamp
!
=
=
null
&
&
token
>
=
last_timestamp
)
{
skip
=
true
;
}
else
if
(
skip
|
|
(
previous_tokens
.
length
>
0
&
&
token
<
first_timestamp
)
)
{
skip
=
false
;
}
else
if
(
chunk
.
timestamp
[
0
]
=
=
=
null
)
{
chunk
.
timestamp
[
0
]
=
rounded_time
;
}
else
{
if
(
rounded_time
=
=
=
chunk
.
timestamp
[
0
]
)
{
}
else
{
chunk
.
timestamp
[
1
]
=
rounded_time
;
previous_tokens
.
push
(
current_tokens
)
if
(
returnWordTimestamps
)
{
previous_token_timestamps
.
push
(
current_token_timestamps
)
;
}
const
[
resolved_tokens
resolved_token_timestamps
]
=
this
.
findLongestCommonSequence
(
previous_tokens
previous_token_timestamps
)
const
resolved_text
=
this
.
decode
(
resolved_tokens
)
chunk
.
text
=
resolved_text
if
(
returnWordTimestamps
)
{
chunk
.
words
=
this
.
collateWordTimestamps
(
resolved_tokens
resolved_token_timestamps
last_language
)
}
chunks
.
push
(
chunk
)
previous_tokens
=
[
]
current_tokens
=
[
]
previous_token_timestamps
=
[
]
current_token_timestamps
=
[
]
chunk
=
new_chunk
(
)
}
}
}
else
{
current_tokens
.
push
(
token
)
if
(
returnWordTimestamps
)
{
let
start_time
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
round
)
(
token_timestamps
[
i
]
+
time_offset
2
)
;
let
end_time
;
if
(
i
+
1
<
token_timestamps
.
length
)
{
end_time
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
round
)
(
token_timestamps
[
i
+
1
]
+
time_offset
2
)
;
}
else
{
end_time
=
null
;
}
current_token_timestamps
.
push
(
[
start_time
end_time
]
)
;
}
}
}
if
(
'
stride
'
in
output
)
{
const
[
chunk_len
stride_left
stride_right
]
=
output
.
stride
;
time_offset
+
=
chunk_len
-
stride_right
}
if
(
current_tokens
.
length
>
0
)
{
previous_tokens
.
push
(
current_tokens
)
if
(
returnWordTimestamps
)
{
previous_token_timestamps
.
push
(
current_token_timestamps
)
;
}
}
else
if
(
previous_tokens
.
every
(
p
=
>
p
.
length
=
=
=
0
)
)
{
chunk
=
new_chunk
(
)
previous_tokens
=
[
]
current_tokens
=
[
]
previous_token_timestamps
=
[
]
;
current_token_timestamps
=
[
]
;
}
}
if
(
previous_tokens
.
length
>
0
)
{
if
(
force_full_sequences
&
&
return_timestamps
)
{
throw
new
Error
(
"
Whisper
did
not
predict
an
ending
timestamp
which
can
happen
if
audio
is
cut
off
in
the
middle
of
a
word
.
"
+
"
Also
make
sure
WhisperTimeStampLogitsProcessor
was
used
during
generation
.
"
)
;
}
const
[
resolved_tokens
resolved_token_timestamps
]
=
this
.
findLongestCommonSequence
(
previous_tokens
previous_token_timestamps
)
;
const
resolved_text
=
this
.
decode
(
resolved_tokens
)
;
chunk
.
text
=
resolved_text
;
if
(
returnWordTimestamps
)
{
chunk
.
words
=
this
.
collateWordTimestamps
(
resolved_tokens
resolved_token_timestamps
last_language
)
}
chunks
.
push
(
chunk
)
;
}
let
optional
=
Object
.
create
(
null
)
;
const
full_text
=
chunks
.
map
(
chunk
=
>
chunk
.
text
)
.
join
(
'
'
)
;
if
(
return_timestamps
|
|
return_language
)
{
for
(
let
i
=
0
;
i
<
chunks
.
length
;
+
+
i
)
{
const
chunk
=
chunks
[
i
]
;
if
(
!
return_timestamps
)
{
delete
chunk
[
"
timestamp
"
]
;
}
if
(
!
return_language
)
{
delete
chunk
[
"
language
"
]
;
}
}
if
(
returnWordTimestamps
)
{
const
new_chunks
=
[
]
;
for
(
const
chunk
of
chunks
)
{
for
(
const
word
of
chunk
.
words
)
{
new_chunks
.
push
(
word
)
;
}
}
optional
=
{
"
chunks
"
:
new_chunks
}
;
}
else
{
optional
=
{
"
chunks
"
:
chunks
}
;
}
}
return
[
full_text
optional
]
;
}
findLongestCommonSequence
(
sequences
token_timestamp_sequences
=
null
)
{
let
leftSequence
=
sequences
[
0
]
;
let
leftLength
=
leftSequence
.
length
;
let
totalSequence
=
[
]
;
const
use_token_timestamp_sequences
=
Array
.
isArray
(
token_timestamp_sequences
)
&
&
token_timestamp_sequences
.
length
>
0
;
let
total_token_timestamp_sequence
=
use_token_timestamp_sequences
?
[
]
:
null
;
let
left_token_timestamp_sequence
=
use_token_timestamp_sequences
?
token_timestamp_sequences
[
0
]
:
null
;
for
(
let
i
=
1
;
i
<
sequences
.
length
;
+
+
i
)
{
const
rightSequence
=
sequences
[
i
]
;
let
max
=
0
.
0
;
let
maxIndices
=
[
leftLength
leftLength
0
0
]
;
const
rightLength
=
rightSequence
.
length
;
for
(
let
j
=
1
;
j
<
leftLength
+
rightLength
;
+
+
j
)
{
const
eps
=
j
/
10000
.
0
;
const
leftStart
=
Math
.
max
(
0
leftLength
-
j
)
;
const
leftStop
=
Math
.
min
(
leftLength
leftLength
+
rightLength
-
j
)
;
const
left
=
leftSequence
.
slice
(
leftStart
leftStop
)
;
const
rightStart
=
Math
.
max
(
0
j
-
leftLength
)
;
const
rightStop
=
Math
.
min
(
rightLength
j
)
;
const
right
=
rightSequence
.
slice
(
rightStart
rightStop
)
;
if
(
left
.
length
!
=
=
right
.
length
)
{
throw
new
Error
(
"
There
is
a
bug
within
whisper
decode_asr
function
please
report
it
.
Dropping
to
prevent
bad
inference
.
"
)
;
}
const
matches
=
left
.
filter
(
(
elem
idx
)
=
>
elem
=
=
=
right
[
idx
]
)
.
length
;
const
matching
=
matches
/
j
+
eps
;
if
(
matches
>
1
&
&
matching
>
max
)
{
max
=
matching
;
maxIndices
=
[
leftStart
leftStop
rightStart
rightStop
]
;
}
}
const
[
leftStart
leftStop
rightStart
rightStop
]
=
maxIndices
;
const
leftMid
=
Math
.
floor
(
(
leftStop
+
leftStart
)
/
2
)
;
const
rightMid
=
Math
.
floor
(
(
rightStop
+
rightStart
)
/
2
)
;
totalSequence
.
push
(
.
.
.
leftSequence
.
slice
(
0
leftMid
)
)
;
leftSequence
=
rightSequence
.
slice
(
rightMid
)
;
leftLength
=
leftSequence
.
length
;
if
(
use_token_timestamp_sequences
)
{
total_token_timestamp_sequence
.
push
(
.
.
.
left_token_timestamp_sequence
.
slice
(
0
leftMid
)
)
;
left_token_timestamp_sequence
=
token_timestamp_sequences
[
i
]
.
slice
(
rightMid
)
;
}
}
totalSequence
.
push
(
.
.
.
leftSequence
)
;
if
(
use_token_timestamp_sequences
)
{
total_token_timestamp_sequence
.
push
(
.
.
.
left_token_timestamp_sequence
)
;
return
[
totalSequence
total_token_timestamp_sequence
]
;
}
else
{
return
[
totalSequence
[
]
]
;
}
}
collateWordTimestamps
(
tokens
token_timestamps
language
)
{
const
[
words
_
token_indices
]
=
this
.
combineTokensIntoWords
(
tokens
language
)
;
const
timings
=
[
]
;
for
(
let
i
=
0
;
i
<
words
.
length
;
+
+
i
)
{
const
indices
=
token_indices
[
i
]
;
timings
.
push
(
{
text
:
words
[
i
]
timestamp
:
[
token_timestamps
[
indices
.
at
(
0
)
]
[
0
]
token_timestamps
[
indices
.
at
(
-
1
)
]
[
1
]
]
}
)
;
}
return
timings
;
}
combineTokensIntoWords
(
tokens
language
prepend_punctionations
=
"
\
"
'
(
[
{
-
"
append_punctuations
=
"
\
"
'
.
!
?
:
)
]
}
"
)
{
language
=
language
?
?
'
english
'
;
let
words
word_tokens
token_indices
;
if
(
[
"
chinese
"
"
japanese
"
"
thai
"
"
lao
"
"
myanmar
"
]
.
includes
(
language
)
)
{
[
words
word_tokens
token_indices
]
=
this
.
splitTokensOnUnicode
(
tokens
)
}
else
{
[
words
word_tokens
token_indices
]
=
this
.
splitTokensOnSpaces
(
tokens
)
}
return
this
.
mergePunctuations
(
words
word_tokens
token_indices
prepend_punctionations
append_punctuations
)
;
}
decode
(
token_ids
decode_args
)
{
let
text
;
if
(
decode_args
&
&
decode_args
.
decode_with_timestamps
)
{
if
(
token_ids
instanceof
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_3__
.
Tensor
)
{
token_ids
=
prepareTensorForDecode
(
token_ids
)
;
}
text
=
this
.
decodeWithTimestamps
(
token_ids
decode_args
)
;
}
else
{
text
=
super
.
decode
(
token_ids
decode_args
)
;
}
return
text
;
}
decodeWithTimestamps
(
token_ids
decode_args
)
{
const
time_precision
=
decode_args
?
.
time_precision
?
?
0
.
02
;
const
timestamp_begin
=
Array
.
from
(
this
.
all_special_ids
)
.
at
(
-
1
)
+
1
;
let
outputs
=
[
[
]
]
;
for
(
const
token
of
token_ids
)
{
if
(
token
>
=
timestamp_begin
)
{
const
timestamp
=
(
0
_utils_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
round
)
(
(
token
-
timestamp_begin
)
*
time_precision
2
)
;
outputs
.
push
(
<
|
{
timestamp
}
|
>
)
;
outputs
.
push
(
[
]
)
;
}
else
{
outputs
[
outputs
.
length
-
1
]
.
push
(
token
)
;
}
}
outputs
=
outputs
.
map
(
s
=
>
{
if
(
typeof
s
=
=
=
'
string
'
)
{
return
s
;
}
else
{
return
super
.
decode
(
s
decode_args
)
;
}
}
)
return
outputs
.
join
(
'
'
)
;
}
splitTokensOnUnicode
(
tokens
)
{
const
decoded_full
=
this
.
decode
(
tokens
{
decode_with_timestamps
:
true
}
)
;
const
replacement_char
=
'
\
uFFFD
'
;
const
words
=
[
]
const
word_tokens
=
[
]
const
token_indices
=
[
]
let
current_tokens
=
[
]
let
current_indices
=
[
]
let
unicode_offset
=
0
for
(
let
token_idx
=
0
;
token_idx
<
tokens
.
length
;
+
+
token_idx
)
{
const
token
=
tokens
[
token_idx
]
;
current_tokens
.
push
(
token
)
;
current_indices
.
push
(
token_idx
)
;
const
decoded
=
this
.
decode
(
current_tokens
{
decode_with_timestamps
:
true
}
)
;
if
(
!
decoded
.
includes
(
replacement_char
)
|
|
decoded_full
[
unicode_offset
+
decoded
.
indexOf
(
replacement_char
)
]
=
=
=
replacement_char
)
{
words
.
push
(
decoded
)
word_tokens
.
push
(
current_tokens
)
token_indices
.
push
(
current_indices
)
current_tokens
=
[
]
current_indices
=
[
]
unicode_offset
+
=
decoded
.
length
;
}
}
return
[
words
word_tokens
token_indices
]
}
splitTokensOnSpaces
(
tokens
)
{
const
[
subwords
subword_tokens_list
subword_indices_list
]
=
this
.
splitTokensOnUnicode
(
tokens
)
;
const
words
=
[
]
const
word_tokens
=
[
]
const
token_indices
=
[
]
const
punctuationRegex
=
new
RegExp
(
^
[
{
PUNCTUATION_REGEX
}
]
'
gu
'
)
;
for
(
let
i
=
0
;
i
<
subwords
.
length
;
+
+
i
)
{
const
subword
=
subwords
[
i
]
;
const
subword_tokens
=
subword_tokens_list
[
i
]
;
const
subword_indices
=
subword_indices_list
[
i
]
;
const
special
=
subword_tokens
[
0
]
>
=
this
.
model
.
tokens_to_ids
.
get
(
'
<
|
endoftext
|
>
'
)
;
const
with_space
=
subword
.
startsWith
(
'
'
)
;
const
trimmed
=
subword
.
trim
(
)
;
const
punctuation
=
punctuationRegex
.
test
(
trimmed
)
;
if
(
special
|
|
with_space
|
|
punctuation
|
|
words
.
length
=
=
=
0
)
{
words
.
push
(
subword
)
;
word_tokens
.
push
(
subword_tokens
)
;
token_indices
.
push
(
subword_indices
)
;
}
else
{
const
ix
=
words
.
length
-
1
;
words
[
ix
]
+
=
subword
;
word_tokens
[
ix
]
.
push
(
.
.
.
subword_tokens
)
;
token_indices
[
ix
]
.
push
(
.
.
.
subword_indices
)
;
}
}
return
[
words
word_tokens
token_indices
]
;
}
mergePunctuations
(
words
tokens
indices
prepended
appended
)
{
const
newWords
=
structuredClone
(
words
)
;
const
newTokens
=
structuredClone
(
tokens
)
;
const
newIndices
=
structuredClone
(
indices
)
;
let
i
=
newWords
.
length
-
2
;
let
j
=
newWords
.
length
-
1
;
while
(
i
>
=
0
)
{
if
(
newWords
[
i
]
.
startsWith
(
'
'
)
&
&
prepended
.
includes
(
newWords
[
i
]
.
trim
(
)
)
)
{
newWords
[
j
]
=
newWords
[
i
]
+
newWords
[
j
]
;
newTokens
[
j
]
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
newTokens
[
i
]
newTokens
[
j
]
)
;
newIndices
[
j
]
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
newIndices
[
i
]
newIndices
[
j
]
)
;
newWords
[
i
]
=
'
'
;
newTokens
[
i
]
=
[
]
;
newIndices
[
i
]
=
[
]
;
}
else
{
j
=
i
;
}
-
-
i
;
}
i
=
0
;
j
=
1
;
while
(
j
<
newWords
.
length
)
{
if
(
!
newWords
[
i
]
.
endsWith
(
'
'
)
&
&
appended
.
includes
(
newWords
[
j
]
)
)
{
newWords
[
i
]
+
=
newWords
[
j
]
;
newTokens
[
i
]
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
newTokens
[
i
]
newTokens
[
j
]
)
;
newIndices
[
i
]
=
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
newIndices
[
i
]
newIndices
[
j
]
)
;
newWords
[
j
]
=
'
'
;
newTokens
[
j
]
=
[
]
;
newIndices
[
j
]
=
[
]
;
}
else
{
i
=
j
;
}
+
+
j
;
}
return
[
newWords
.
filter
(
x
=
>
x
)
newTokens
.
filter
(
x
=
>
x
.
length
>
0
)
newIndices
.
filter
(
x
=
>
x
.
length
>
0
)
]
}
get_decoder_prompt_ids
(
{
language
=
null
task
=
null
no_timestamps
=
true
}
=
{
}
)
{
const
forced_decoder_ids
=
[
]
;
if
(
language
)
{
language
=
language
.
toLowerCase
(
)
;
let
language_code
=
WHISPER_TO_LANGUAGE_CODE_MAPPING
.
get
(
language
)
;
if
(
language_code
=
=
=
undefined
)
{
if
(
WHISPER_LANGUAGE_MAPPING
.
has
(
language
)
)
{
language_code
=
language
;
}
else
{
const
is_language_code
=
language
.
length
=
=
=
2
;
const
langs
=
is_language_code
?
WHISPER_LANGUAGE_MAPPING
.
keys
(
)
:
WHISPER_LANGUAGE_MAPPING
.
values
(
)
;
throw
new
Error
(
Language
"
{
language
}
"
is
not
supported
.
Must
be
one
of
:
{
JSON
.
stringify
(
langs
)
}
)
;
}
}
const
language_token_id
=
this
.
model
.
tokens_to_ids
.
get
(
<
|
{
language_code
}
|
>
)
;
if
(
language_token_id
=
=
=
undefined
)
{
throw
new
Error
(
Unable
to
find
language
"
{
language_code
}
"
in
model
vocabulary
.
Please
report
this
issue
at
https
:
/
/
github
.
com
/
xenova
/
transformers
.
js
/
issues
/
new
/
choose
.
)
}
forced_decoder_ids
.
push
(
language_token_id
)
;
}
else
{
forced_decoder_ids
.
push
(
null
)
;
}
if
(
task
)
{
task
=
task
.
toLowerCase
(
)
;
if
(
task
!
=
=
'
transcribe
'
&
&
task
!
=
=
'
translate
'
)
{
throw
new
Error
(
Task
"
{
task
}
"
is
not
supported
.
Must
be
one
of
:
[
"
transcribe
"
"
translate
"
]
)
;
}
const
task_token_id
=
this
.
model
.
tokens_to_ids
.
get
(
<
|
{
task
}
|
>
)
;
if
(
task_token_id
=
=
=
undefined
)
{
throw
new
Error
(
Unable
to
find
task
"
{
task
}
"
in
model
vocabulary
.
Please
report
this
issue
at
https
:
/
/
github
.
com
/
xenova
/
transformers
.
js
/
issues
/
new
/
choose
.
)
}
forced_decoder_ids
.
push
(
task_token_id
)
;
}
else
{
forced_decoder_ids
.
push
(
null
)
;
}
if
(
no_timestamps
)
{
const
no_timestamps_id
=
this
.
model
.
tokens_to_ids
.
get
(
<
|
notimestamps
|
>
)
;
if
(
no_timestamps_id
=
=
=
undefined
)
{
throw
new
Error
(
'
Unable
to
find
"
<
|
notimestamps
|
>
"
in
model
vocabulary
.
Please
report
this
issue
at
https
:
/
/
github
.
com
/
xenova
/
transformers
.
js
/
issues
/
new
/
choose
.
'
)
}
forced_decoder_ids
.
push
(
no_timestamps_id
)
;
}
return
forced_decoder_ids
.
map
(
(
x
i
)
=
>
[
i
+
1
x
]
)
.
filter
(
x
=
>
x
[
1
]
!
=
=
null
)
;
}
}
class
CodeGenTokenizer
extends
PreTrainedTokenizer
{
}
class
CLIPTokenizer
extends
PreTrainedTokenizer
{
}
class
SiglipTokenizer
extends
PreTrainedTokenizer
{
}
class
MarianTokenizer
extends
PreTrainedTokenizer
{
constructor
(
tokenizerJSON
tokenizerConfig
)
{
super
(
tokenizerJSON
tokenizerConfig
)
;
this
.
languageRegex
=
/
^
(
>
>
\
w
+
<
<
)
\
s
*
/
g
;
this
.
supported_language_codes
=
this
.
model
.
vocab
.
filter
(
x
=
>
this
.
languageRegex
.
test
(
x
)
)
;
console
.
warn
(
'
WARNING
:
MarianTokenizer
is
not
yet
supported
by
Hugging
Face
\
'
s
"
fast
"
tokenizers
library
.
Therefore
you
may
experience
slightly
inaccurate
results
.
'
)
}
_encode_text
(
text
)
{
if
(
text
=
=
=
null
)
return
null
;
const
[
matchInfo
.
.
.
remainder
]
=
text
.
trim
(
)
.
split
(
this
.
languageRegex
)
;
if
(
remainder
.
length
=
=
=
0
)
{
return
super
.
_encode_text
(
matchInfo
)
;
}
else
if
(
remainder
.
length
=
=
=
2
)
{
const
[
language
text
]
=
remainder
;
if
(
!
this
.
supported_language_codes
.
includes
(
language
)
)
{
console
.
warn
(
Unsupported
language
code
"
{
language
}
"
detected
which
may
lead
to
unexpected
behavior
.
Should
be
one
of
:
{
JSON
.
stringify
(
this
.
supported_language_codes
)
}
)
}
return
(
0
_utils_core_js__WEBPACK_IMPORTED_MODULE_0__
.
mergeArrays
)
(
[
language
]
super
.
_encode_text
(
text
)
)
;
}
}
}
class
Wav2Vec2CTCTokenizer
extends
PreTrainedTokenizer
{
}
class
BlenderbotTokenizer
extends
PreTrainedTokenizer
{
_default_chat_template
=
{
%
for
message
in
messages
%
}
{
%
if
message
[
'
role
'
]
=
=
'
user
'
%
}
{
{
'
'
}
}
{
%
endif
%
}
{
{
message
[
'
content
'
]
}
}
{
%
if
not
loop
.
last
%
}
{
{
'
'
}
}
{
%
endif
%
}
{
%
endfor
%
}
{
{
eos_token
}
}
;
}
class
BlenderbotSmallTokenizer
extends
BlenderbotTokenizer
{
}
class
SpeechT5Tokenizer
extends
PreTrainedTokenizer
{
}
class
NougatTokenizer
extends
PreTrainedTokenizer
{
}
class
VitsTokenizer
extends
PreTrainedTokenizer
{
constructor
(
tokenizerJSON
tokenizerConfig
)
{
super
(
tokenizerJSON
tokenizerConfig
)
;
this
.
decoder
=
new
VitsDecoder
(
{
}
)
;
}
}
class
CohereTokenizer
extends
PreTrainedTokenizer
{
}
class
AutoTokenizer
{
static
TOKENIZER_CLASS_MAPPING
=
{
T5Tokenizer
DistilBertTokenizer
CamembertTokenizer
DebertaTokenizer
DebertaV2Tokenizer
BertTokenizer
HerbertTokenizer
ConvBertTokenizer
RoFormerTokenizer
XLMTokenizer
ElectraTokenizer
MobileBertTokenizer
SqueezeBertTokenizer
AlbertTokenizer
GPT2Tokenizer
BartTokenizer
MBartTokenizer
MBart50Tokenizer
RobertaTokenizer
WhisperTokenizer
CodeGenTokenizer
CLIPTokenizer
SiglipTokenizer
MarianTokenizer
BloomTokenizer
NllbTokenizer
M2M100Tokenizer
LlamaTokenizer
CodeLlamaTokenizer
XLMRobertaTokenizer
MPNetTokenizer
FalconTokenizer
GPTNeoXTokenizer
EsmTokenizer
Wav2Vec2CTCTokenizer
BlenderbotTokenizer
BlenderbotSmallTokenizer
SpeechT5Tokenizer
NougatTokenizer
VitsTokenizer
Qwen2Tokenizer
GemmaTokenizer
Grok1Tokenizer
CohereTokenizer
PreTrainedTokenizer
}
static
async
from_pretrained
(
pretrained_model_name_or_path
{
quantized
=
true
progress_callback
=
null
config
=
null
cache_dir
=
null
local_files_only
=
false
revision
=
'
main
'
legacy
=
null
}
=
{
}
)
{
const
[
tokenizerJSON
tokenizerConfig
]
=
await
loadTokenizer
(
pretrained_model_name_or_path
{
quantized
progress_callback
config
cache_dir
local_files_only
revision
legacy
}
)
const
tokenizerName
=
tokenizerConfig
.
tokenizer_class
?
.
replace
(
/
Fast
/
'
'
)
?
?
'
PreTrainedTokenizer
'
;
let
cls
=
this
.
TOKENIZER_CLASS_MAPPING
[
tokenizerName
]
;
if
(
!
cls
)
{
console
.
warn
(
Unknown
tokenizer
class
"
{
tokenizerName
}
"
attempting
to
construct
from
base
class
.
)
;
cls
=
PreTrainedTokenizer
;
}
return
new
cls
(
tokenizerJSON
tokenizerConfig
)
;
}
}
}
)
"
.
/
src
/
transformers
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
ASTFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
ASTFeatureExtractor
)
"
ASTForAudioClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ASTForAudioClassification
)
"
ASTModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ASTModel
)
"
ASTPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ASTPreTrainedModel
)
"
AlbertForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AlbertForMaskedLM
)
"
AlbertForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AlbertForQuestionAnswering
)
"
AlbertForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AlbertForSequenceClassification
)
"
AlbertModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AlbertModel
)
"
AlbertPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AlbertPreTrainedModel
)
"
AlbertTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
AlbertTokenizer
)
"
AudioClassificationPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
AudioClassificationPipeline
)
"
AutoConfig
"
:
(
)
=
>
(
_configs_js__WEBPACK_IMPORTED_MODULE_5__
.
AutoConfig
)
"
AutoModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModel
)
"
AutoModelForAudioClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForAudioClassification
)
"
AutoModelForAudioFrameClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForAudioFrameClassification
)
"
AutoModelForCTC
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForCTC
)
"
AutoModelForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForCausalLM
)
"
AutoModelForDepthEstimation
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForDepthEstimation
)
"
AutoModelForDocumentQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForDocumentQuestionAnswering
)
"
AutoModelForImageClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForImageClassification
)
"
AutoModelForImageFeatureExtraction
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForImageFeatureExtraction
)
"
AutoModelForImageMatting
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForImageMatting
)
"
AutoModelForImageSegmentation
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForImageSegmentation
)
"
AutoModelForImageToImage
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForImageToImage
)
"
AutoModelForMaskGeneration
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForMaskGeneration
)
"
AutoModelForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForMaskedLM
)
"
AutoModelForObjectDetection
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForObjectDetection
)
"
AutoModelForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForQuestionAnswering
)
"
AutoModelForSemanticSegmentation
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForSemanticSegmentation
)
"
AutoModelForSeq2SeqLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForSeq2SeqLM
)
"
AutoModelForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForSequenceClassification
)
"
AutoModelForSpeechSeq2Seq
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForSpeechSeq2Seq
)
"
AutoModelForTextToSpectrogram
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForTextToSpectrogram
)
"
AutoModelForTextToWaveform
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForTextToWaveform
)
"
AutoModelForTokenClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForTokenClassification
)
"
AutoModelForVision2Seq
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForVision2Seq
)
"
AutoModelForXVector
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForXVector
)
"
AutoModelForZeroShotObjectDetection
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
AutoModelForZeroShotObjectDetection
)
"
AutoProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
AutoProcessor
)
"
AutoTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
AutoTokenizer
)
"
AutomaticSpeechRecognitionPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
AutomaticSpeechRecognitionPipeline
)
"
BartForConditionalGeneration
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BartForConditionalGeneration
)
"
BartForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BartForSequenceClassification
)
"
BartModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BartModel
)
"
BartPretrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BartPretrainedModel
)
"
BartTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
BartTokenizer
)
"
BaseModelOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BaseModelOutput
)
"
BeitFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
BeitFeatureExtractor
)
"
BeitForImageClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BeitForImageClassification
)
"
BeitModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BeitModel
)
"
BeitPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BeitPreTrainedModel
)
"
BertForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BertForMaskedLM
)
"
BertForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BertForQuestionAnswering
)
"
BertForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BertForSequenceClassification
)
"
BertForTokenClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BertForTokenClassification
)
"
BertModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BertModel
)
"
BertPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BertPreTrainedModel
)
"
BertTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
BertTokenizer
)
"
BitImageProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
BitImageProcessor
)
"
BlenderbotForConditionalGeneration
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BlenderbotForConditionalGeneration
)
"
BlenderbotModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BlenderbotModel
)
"
BlenderbotPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BlenderbotPreTrainedModel
)
"
BlenderbotSmallForConditionalGeneration
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BlenderbotSmallForConditionalGeneration
)
"
BlenderbotSmallModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BlenderbotSmallModel
)
"
BlenderbotSmallPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BlenderbotSmallPreTrainedModel
)
"
BlenderbotSmallTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
BlenderbotSmallTokenizer
)
"
BlenderbotTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
BlenderbotTokenizer
)
"
BloomForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BloomForCausalLM
)
"
BloomModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BloomModel
)
"
BloomPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
BloomPreTrainedModel
)
"
BloomTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
BloomTokenizer
)
"
CLIPFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
CLIPFeatureExtractor
)
"
CLIPModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CLIPModel
)
"
CLIPPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CLIPPreTrainedModel
)
"
CLIPSegForImageSegmentation
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CLIPSegForImageSegmentation
)
"
CLIPSegModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CLIPSegModel
)
"
CLIPSegPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CLIPSegPreTrainedModel
)
"
CLIPTextModelWithProjection
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CLIPTextModelWithProjection
)
"
CLIPTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
CLIPTokenizer
)
"
CLIPVisionModelWithProjection
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CLIPVisionModelWithProjection
)
"
CamembertForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CamembertForMaskedLM
)
"
CamembertForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CamembertForQuestionAnswering
)
"
CamembertForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CamembertForSequenceClassification
)
"
CamembertForTokenClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CamembertForTokenClassification
)
"
CamembertModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CamembertModel
)
"
CamembertPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CamembertPreTrainedModel
)
"
CamembertTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
CamembertTokenizer
)
"
CausalLMOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CausalLMOutput
)
"
CausalLMOutputWithPast
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CausalLMOutputWithPast
)
"
ChineseCLIPFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
ChineseCLIPFeatureExtractor
)
"
ChineseCLIPModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ChineseCLIPModel
)
"
ChineseCLIPPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ChineseCLIPPreTrainedModel
)
"
ClapAudioModelWithProjection
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ClapAudioModelWithProjection
)
"
ClapFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
ClapFeatureExtractor
)
"
ClapModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ClapModel
)
"
ClapPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ClapPreTrainedModel
)
"
ClapTextModelWithProjection
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ClapTextModelWithProjection
)
"
CodeGenForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CodeGenForCausalLM
)
"
CodeGenModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CodeGenModel
)
"
CodeGenPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
CodeGenPreTrainedModel
)
"
CodeGenTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
CodeGenTokenizer
)
"
CodeLlamaTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
CodeLlamaTokenizer
)
"
CohereTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
CohereTokenizer
)
"
ConvBertForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ConvBertForMaskedLM
)
"
ConvBertForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ConvBertForQuestionAnswering
)
"
ConvBertForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ConvBertForSequenceClassification
)
"
ConvBertForTokenClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ConvBertForTokenClassification
)
"
ConvBertModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ConvBertModel
)
"
ConvBertPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ConvBertPreTrainedModel
)
"
ConvBertTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
ConvBertTokenizer
)
"
ConvNextFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
ConvNextFeatureExtractor
)
"
ConvNextForImageClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ConvNextForImageClassification
)
"
ConvNextImageProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
ConvNextImageProcessor
)
"
ConvNextModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ConvNextModel
)
"
ConvNextPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ConvNextPreTrainedModel
)
"
ConvNextV2ForImageClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ConvNextV2ForImageClassification
)
"
ConvNextV2Model
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ConvNextV2Model
)
"
ConvNextV2PreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ConvNextV2PreTrainedModel
)
"
DPTFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
DPTFeatureExtractor
)
"
DPTForDepthEstimation
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DPTForDepthEstimation
)
"
DPTImageProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
DPTImageProcessor
)
"
DPTModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DPTModel
)
"
DPTPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DPTPreTrainedModel
)
"
DebertaForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DebertaForMaskedLM
)
"
DebertaForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DebertaForQuestionAnswering
)
"
DebertaForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DebertaForSequenceClassification
)
"
DebertaForTokenClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DebertaForTokenClassification
)
"
DebertaModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DebertaModel
)
"
DebertaPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DebertaPreTrainedModel
)
"
DebertaTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
DebertaTokenizer
)
"
DebertaV2ForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DebertaV2ForMaskedLM
)
"
DebertaV2ForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DebertaV2ForQuestionAnswering
)
"
DebertaV2ForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DebertaV2ForSequenceClassification
)
"
DebertaV2ForTokenClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DebertaV2ForTokenClassification
)
"
DebertaV2Model
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DebertaV2Model
)
"
DebertaV2PreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DebertaV2PreTrainedModel
)
"
DebertaV2Tokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
DebertaV2Tokenizer
)
"
DeiTFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
DeiTFeatureExtractor
)
"
DeiTForImageClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DeiTForImageClassification
)
"
DeiTModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DeiTModel
)
"
DeiTPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DeiTPreTrainedModel
)
"
DepthAnythingForDepthEstimation
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DepthAnythingForDepthEstimation
)
"
DepthAnythingPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DepthAnythingPreTrainedModel
)
"
DepthEstimationPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
DepthEstimationPipeline
)
"
DetrFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
DetrFeatureExtractor
)
"
DetrForObjectDetection
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DetrForObjectDetection
)
"
DetrForSegmentation
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DetrForSegmentation
)
"
DetrModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DetrModel
)
"
DetrObjectDetectionOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DetrObjectDetectionOutput
)
"
DetrPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DetrPreTrainedModel
)
"
DetrSegmentationOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DetrSegmentationOutput
)
"
Dinov2ForImageClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Dinov2ForImageClassification
)
"
Dinov2Model
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Dinov2Model
)
"
Dinov2PreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Dinov2PreTrainedModel
)
"
DistilBertForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DistilBertForMaskedLM
)
"
DistilBertForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DistilBertForQuestionAnswering
)
"
DistilBertForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DistilBertForSequenceClassification
)
"
DistilBertForTokenClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DistilBertForTokenClassification
)
"
DistilBertModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DistilBertModel
)
"
DistilBertPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DistilBertPreTrainedModel
)
"
DistilBertTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
DistilBertTokenizer
)
"
DocumentQuestionAnsweringPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
DocumentQuestionAnsweringPipeline
)
"
DonutFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
DonutFeatureExtractor
)
"
DonutSwinModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DonutSwinModel
)
"
DonutSwinPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
DonutSwinPreTrainedModel
)
"
EfficientNetForImageClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
EfficientNetForImageClassification
)
"
EfficientNetImageProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
EfficientNetImageProcessor
)
"
EfficientNetModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
EfficientNetModel
)
"
EfficientNetPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
EfficientNetPreTrainedModel
)
"
ElectraForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ElectraForMaskedLM
)
"
ElectraForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ElectraForQuestionAnswering
)
"
ElectraForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ElectraForSequenceClassification
)
"
ElectraForTokenClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ElectraForTokenClassification
)
"
ElectraModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ElectraModel
)
"
ElectraPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ElectraPreTrainedModel
)
"
ElectraTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
ElectraTokenizer
)
"
EsmForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
EsmForMaskedLM
)
"
EsmForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
EsmForSequenceClassification
)
"
EsmForTokenClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
EsmForTokenClassification
)
"
EsmModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
EsmModel
)
"
EsmPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
EsmPreTrainedModel
)
"
EsmTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
EsmTokenizer
)
"
FFT
"
:
(
)
=
>
(
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
.
FFT
)
"
FalconForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
FalconForCausalLM
)
"
FalconModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
FalconModel
)
"
FalconPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
FalconPreTrainedModel
)
"
FalconTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
FalconTokenizer
)
"
FeatureExtractionPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
FeatureExtractionPipeline
)
"
FeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
FeatureExtractor
)
"
FillMaskPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
FillMaskPipeline
)
"
GLPNFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
GLPNFeatureExtractor
)
"
GLPNForDepthEstimation
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GLPNForDepthEstimation
)
"
GLPNModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GLPNModel
)
"
GLPNPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GLPNPreTrainedModel
)
"
GPT2LMHeadModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPT2LMHeadModel
)
"
GPT2Model
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPT2Model
)
"
GPT2PreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPT2PreTrainedModel
)
"
GPT2Tokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
GPT2Tokenizer
)
"
GPTBigCodeForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPTBigCodeForCausalLM
)
"
GPTBigCodeModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPTBigCodeModel
)
"
GPTBigCodePreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPTBigCodePreTrainedModel
)
"
GPTJForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPTJForCausalLM
)
"
GPTJModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPTJModel
)
"
GPTJPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPTJPreTrainedModel
)
"
GPTNeoForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPTNeoForCausalLM
)
"
GPTNeoModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPTNeoModel
)
"
GPTNeoPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPTNeoPreTrainedModel
)
"
GPTNeoXForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPTNeoXForCausalLM
)
"
GPTNeoXModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPTNeoXModel
)
"
GPTNeoXPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
GPTNeoXPreTrainedModel
)
"
GPTNeoXTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
GPTNeoXTokenizer
)
"
GemmaTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
GemmaTokenizer
)
"
Grok1Tokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
Grok1Tokenizer
)
"
HerbertTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
HerbertTokenizer
)
"
HubertForCTC
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
HubertForCTC
)
"
HubertForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
HubertForSequenceClassification
)
"
HubertModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
HubertModel
)
"
HubertPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
HubertPreTrainedModel
)
"
ImageClassificationPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
ImageClassificationPipeline
)
"
ImageFeatureExtractionPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
ImageFeatureExtractionPipeline
)
"
ImageFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
ImageFeatureExtractor
)
"
ImageMattingOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ImageMattingOutput
)
"
ImageSegmentationPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
ImageSegmentationPipeline
)
"
ImageToImagePipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
ImageToImagePipeline
)
"
ImageToTextPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
ImageToTextPipeline
)
"
LlamaForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
LlamaForCausalLM
)
"
LlamaModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
LlamaModel
)
"
LlamaPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
LlamaPreTrainedModel
)
"
LlamaTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
LlamaTokenizer
)
"
LongT5ForConditionalGeneration
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
LongT5ForConditionalGeneration
)
"
LongT5Model
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
LongT5Model
)
"
LongT5PreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
LongT5PreTrainedModel
)
"
M2M100ForConditionalGeneration
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
M2M100ForConditionalGeneration
)
"
M2M100Model
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
M2M100Model
)
"
M2M100PreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
M2M100PreTrainedModel
)
"
M2M100Tokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
M2M100Tokenizer
)
"
MBart50Tokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
MBart50Tokenizer
)
"
MBartForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MBartForCausalLM
)
"
MBartForConditionalGeneration
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MBartForConditionalGeneration
)
"
MBartForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MBartForSequenceClassification
)
"
MBartModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MBartModel
)
"
MBartPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MBartPreTrainedModel
)
"
MBartTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
MBartTokenizer
)
"
MPNetForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MPNetForMaskedLM
)
"
MPNetForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MPNetForQuestionAnswering
)
"
MPNetForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MPNetForSequenceClassification
)
"
MPNetForTokenClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MPNetForTokenClassification
)
"
MPNetModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MPNetModel
)
"
MPNetPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MPNetPreTrainedModel
)
"
MPNetTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
MPNetTokenizer
)
"
MT5ForConditionalGeneration
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MT5ForConditionalGeneration
)
"
MT5Model
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MT5Model
)
"
MT5PreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MT5PreTrainedModel
)
"
MarianMTModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MarianMTModel
)
"
MarianModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MarianModel
)
"
MarianPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MarianPreTrainedModel
)
"
MarianTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
MarianTokenizer
)
"
MaskedLMOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MaskedLMOutput
)
"
MistralForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MistralForCausalLM
)
"
MistralModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MistralModel
)
"
MistralPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MistralPreTrainedModel
)
"
MobileBertForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MobileBertForMaskedLM
)
"
MobileBertForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MobileBertForQuestionAnswering
)
"
MobileBertForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MobileBertForSequenceClassification
)
"
MobileBertModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MobileBertModel
)
"
MobileBertPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MobileBertPreTrainedModel
)
"
MobileBertTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
MobileBertTokenizer
)
"
MobileViTFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
MobileViTFeatureExtractor
)
"
MobileViTForImageClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MobileViTForImageClassification
)
"
MobileViTModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MobileViTModel
)
"
MobileViTPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MobileViTPreTrainedModel
)
"
ModelOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ModelOutput
)
"
MptForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MptForCausalLM
)
"
MptModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MptModel
)
"
MptPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
MptPreTrainedModel
)
"
NllbTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
NllbTokenizer
)
"
NomicBertModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
NomicBertModel
)
"
NomicBertPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
NomicBertPreTrainedModel
)
"
NougatImageProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
NougatImageProcessor
)
"
NougatTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
NougatTokenizer
)
"
OPTForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
OPTForCausalLM
)
"
OPTModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
OPTModel
)
"
OPTPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
OPTPreTrainedModel
)
"
ObjectDetectionPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
ObjectDetectionPipeline
)
"
OwlViTFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
OwlViTFeatureExtractor
)
"
OwlViTForObjectDetection
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
OwlViTForObjectDetection
)
"
OwlViTModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
OwlViTModel
)
"
OwlViTPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
OwlViTPreTrainedModel
)
"
OwlViTProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
OwlViTProcessor
)
"
Owlv2ForObjectDetection
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Owlv2ForObjectDetection
)
"
Owlv2ImageProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
Owlv2ImageProcessor
)
"
Owlv2Model
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Owlv2Model
)
"
Owlv2PreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Owlv2PreTrainedModel
)
"
PhiForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
PhiForCausalLM
)
"
PhiModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
PhiModel
)
"
PhiPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
PhiPreTrainedModel
)
"
Pipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
Pipeline
)
"
PreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
PreTrainedModel
)
"
PreTrainedTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
PreTrainedTokenizer
)
"
PretrainedConfig
"
:
(
)
=
>
(
_configs_js__WEBPACK_IMPORTED_MODULE_5__
.
PretrainedConfig
)
"
PretrainedMixin
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
PretrainedMixin
)
"
Processor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
Processor
)
"
QuestionAnsweringModelOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
QuestionAnsweringModelOutput
)
"
QuestionAnsweringPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
QuestionAnsweringPipeline
)
"
Qwen2ForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Qwen2ForCausalLM
)
"
Qwen2Model
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Qwen2Model
)
"
Qwen2PreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Qwen2PreTrainedModel
)
"
Qwen2Tokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
Qwen2Tokenizer
)
"
RawImage
"
:
(
)
=
>
(
_utils_image_js__WEBPACK_IMPORTED_MODULE_7__
.
RawImage
)
"
ResNetForImageClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ResNetForImageClassification
)
"
ResNetModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ResNetModel
)
"
ResNetPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ResNetPreTrainedModel
)
"
RoFormerForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
RoFormerForMaskedLM
)
"
RoFormerForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
RoFormerForQuestionAnswering
)
"
RoFormerForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
RoFormerForSequenceClassification
)
"
RoFormerForTokenClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
RoFormerForTokenClassification
)
"
RoFormerModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
RoFormerModel
)
"
RoFormerPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
RoFormerPreTrainedModel
)
"
RoFormerTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
RoFormerTokenizer
)
"
RobertaForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
RobertaForMaskedLM
)
"
RobertaForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
RobertaForQuestionAnswering
)
"
RobertaForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
RobertaForSequenceClassification
)
"
RobertaForTokenClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
RobertaForTokenClassification
)
"
RobertaModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
RobertaModel
)
"
RobertaPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
RobertaPreTrainedModel
)
"
RobertaTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
RobertaTokenizer
)
"
SamImageProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
SamImageProcessor
)
"
SamImageSegmentationOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SamImageSegmentationOutput
)
"
SamModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SamModel
)
"
SamPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SamPreTrainedModel
)
"
SamProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
SamProcessor
)
"
SeamlessM4TFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
SeamlessM4TFeatureExtractor
)
"
SegformerFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
SegformerFeatureExtractor
)
"
SegformerForImageClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SegformerForImageClassification
)
"
SegformerForSemanticSegmentation
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SegformerForSemanticSegmentation
)
"
SegformerModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SegformerModel
)
"
SegformerPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SegformerPreTrainedModel
)
"
Seq2SeqLMOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Seq2SeqLMOutput
)
"
SequenceClassifierOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SequenceClassifierOutput
)
"
SiglipImageProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
SiglipImageProcessor
)
"
SiglipModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SiglipModel
)
"
SiglipPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SiglipPreTrainedModel
)
"
SiglipTextModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SiglipTextModel
)
"
SiglipTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
SiglipTokenizer
)
"
SiglipVisionModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SiglipVisionModel
)
"
SpeechT5FeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
SpeechT5FeatureExtractor
)
"
SpeechT5ForSpeechToText
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SpeechT5ForSpeechToText
)
"
SpeechT5ForTextToSpeech
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SpeechT5ForTextToSpeech
)
"
SpeechT5HifiGan
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SpeechT5HifiGan
)
"
SpeechT5Model
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SpeechT5Model
)
"
SpeechT5PreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SpeechT5PreTrainedModel
)
"
SpeechT5Processor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
SpeechT5Processor
)
"
SpeechT5Tokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
SpeechT5Tokenizer
)
"
SqueezeBertForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SqueezeBertForMaskedLM
)
"
SqueezeBertForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SqueezeBertForQuestionAnswering
)
"
SqueezeBertForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SqueezeBertForSequenceClassification
)
"
SqueezeBertModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SqueezeBertModel
)
"
SqueezeBertPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SqueezeBertPreTrainedModel
)
"
SqueezeBertTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
SqueezeBertTokenizer
)
"
StableLmForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
StableLmForCausalLM
)
"
StableLmModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
StableLmModel
)
"
StableLmPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
StableLmPreTrainedModel
)
"
Starcoder2ForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Starcoder2ForCausalLM
)
"
Starcoder2Model
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Starcoder2Model
)
"
Starcoder2PreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Starcoder2PreTrainedModel
)
"
SummarizationPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
SummarizationPipeline
)
"
Swin2SRForImageSuperResolution
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Swin2SRForImageSuperResolution
)
"
Swin2SRImageProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
Swin2SRImageProcessor
)
"
Swin2SRModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Swin2SRModel
)
"
Swin2SRPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Swin2SRPreTrainedModel
)
"
SwinForImageClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SwinForImageClassification
)
"
SwinModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SwinModel
)
"
SwinPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
SwinPreTrainedModel
)
"
T5ForConditionalGeneration
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
T5ForConditionalGeneration
)
"
T5Model
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
T5Model
)
"
T5PreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
T5PreTrainedModel
)
"
T5Tokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
T5Tokenizer
)
"
TableTransformerForObjectDetection
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
TableTransformerForObjectDetection
)
"
TableTransformerModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
TableTransformerModel
)
"
TableTransformerObjectDetectionOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
TableTransformerObjectDetectionOutput
)
"
TableTransformerPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
TableTransformerPreTrainedModel
)
"
Tensor
"
:
(
)
=
>
(
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_8__
.
Tensor
)
"
Text2TextGenerationPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
Text2TextGenerationPipeline
)
"
TextClassificationPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
TextClassificationPipeline
)
"
TextGenerationPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
TextGenerationPipeline
)
"
TextToAudioPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
TextToAudioPipeline
)
"
TokenClassificationPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
TokenClassificationPipeline
)
"
TokenClassifierOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
TokenClassifierOutput
)
"
TokenizerModel
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
TokenizerModel
)
"
TrOCRForCausalLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
TrOCRForCausalLM
)
"
TrOCRPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
TrOCRPreTrainedModel
)
"
TranslationPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
TranslationPipeline
)
"
UniSpeechForCTC
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
UniSpeechForCTC
)
"
UniSpeechForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
UniSpeechForSequenceClassification
)
"
UniSpeechModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
UniSpeechModel
)
"
UniSpeechPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
UniSpeechPreTrainedModel
)
"
UniSpeechSatForAudioFrameClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
UniSpeechSatForAudioFrameClassification
)
"
UniSpeechSatForCTC
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
UniSpeechSatForCTC
)
"
UniSpeechSatForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
UniSpeechSatForSequenceClassification
)
"
UniSpeechSatModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
UniSpeechSatModel
)
"
UniSpeechSatPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
UniSpeechSatPreTrainedModel
)
"
ViTFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
ViTFeatureExtractor
)
"
ViTForImageClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ViTForImageClassification
)
"
ViTImageProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
ViTImageProcessor
)
"
ViTModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ViTModel
)
"
ViTPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
ViTPreTrainedModel
)
"
VisionEncoderDecoderModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
VisionEncoderDecoderModel
)
"
VitMatteForImageMatting
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
VitMatteForImageMatting
)
"
VitMatteImageProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
VitMatteImageProcessor
)
"
VitMattePreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
VitMattePreTrainedModel
)
"
VitsModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
VitsModel
)
"
VitsModelOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
VitsModelOutput
)
"
VitsPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
VitsPreTrainedModel
)
"
VitsTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
VitsTokenizer
)
"
Wav2Vec2BertForCTC
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Wav2Vec2BertForCTC
)
"
Wav2Vec2BertForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Wav2Vec2BertForSequenceClassification
)
"
Wav2Vec2BertModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Wav2Vec2BertModel
)
"
Wav2Vec2BertPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Wav2Vec2BertPreTrainedModel
)
"
Wav2Vec2CTCTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
Wav2Vec2CTCTokenizer
)
"
Wav2Vec2FeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
Wav2Vec2FeatureExtractor
)
"
Wav2Vec2ForAudioFrameClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Wav2Vec2ForAudioFrameClassification
)
"
Wav2Vec2ForCTC
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Wav2Vec2ForCTC
)
"
Wav2Vec2ForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Wav2Vec2ForSequenceClassification
)
"
Wav2Vec2Model
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Wav2Vec2Model
)
"
Wav2Vec2PreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
Wav2Vec2PreTrainedModel
)
"
Wav2Vec2ProcessorWithLM
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
Wav2Vec2ProcessorWithLM
)
"
WavLMForAudioFrameClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
WavLMForAudioFrameClassification
)
"
WavLMForCTC
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
WavLMForCTC
)
"
WavLMForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
WavLMForSequenceClassification
)
"
WavLMForXVector
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
WavLMForXVector
)
"
WavLMModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
WavLMModel
)
"
WavLMPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
WavLMPreTrainedModel
)
"
WhisperFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
WhisperFeatureExtractor
)
"
WhisperForConditionalGeneration
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
WhisperForConditionalGeneration
)
"
WhisperModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
WhisperModel
)
"
WhisperPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
WhisperPreTrainedModel
)
"
WhisperProcessor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
WhisperProcessor
)
"
WhisperTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
WhisperTokenizer
)
"
XLMForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
XLMForQuestionAnswering
)
"
XLMForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
XLMForSequenceClassification
)
"
XLMForTokenClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
XLMForTokenClassification
)
"
XLMModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
XLMModel
)
"
XLMPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
XLMPreTrainedModel
)
"
XLMRobertaForMaskedLM
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
XLMRobertaForMaskedLM
)
"
XLMRobertaForQuestionAnswering
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
XLMRobertaForQuestionAnswering
)
"
XLMRobertaForSequenceClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
XLMRobertaForSequenceClassification
)
"
XLMRobertaForTokenClassification
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
XLMRobertaForTokenClassification
)
"
XLMRobertaModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
XLMRobertaModel
)
"
XLMRobertaPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
XLMRobertaPreTrainedModel
)
"
XLMRobertaTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
XLMRobertaTokenizer
)
"
XLMTokenizer
"
:
(
)
=
>
(
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
.
XLMTokenizer
)
"
XLMWithLMHeadModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
XLMWithLMHeadModel
)
"
XVectorOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
XVectorOutput
)
"
YolosFeatureExtractor
"
:
(
)
=
>
(
_processors_js__WEBPACK_IMPORTED_MODULE_4__
.
YolosFeatureExtractor
)
"
YolosForObjectDetection
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
YolosForObjectDetection
)
"
YolosModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
YolosModel
)
"
YolosObjectDetectionOutput
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
YolosObjectDetectionOutput
)
"
YolosPreTrainedModel
"
:
(
)
=
>
(
_models_js__WEBPACK_IMPORTED_MODULE_2__
.
YolosPreTrainedModel
)
"
ZeroShotAudioClassificationPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
ZeroShotAudioClassificationPipeline
)
"
ZeroShotClassificationPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
ZeroShotClassificationPipeline
)
"
ZeroShotImageClassificationPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
ZeroShotImageClassificationPipeline
)
"
ZeroShotObjectDetectionPipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
ZeroShotObjectDetectionPipeline
)
"
bankers_round
"
:
(
)
=
>
(
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
.
bankers_round
)
"
cat
"
:
(
)
=
>
(
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_8__
.
cat
)
"
cos_sim
"
:
(
)
=
>
(
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
.
cos_sim
)
"
dot
"
:
(
)
=
>
(
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
.
dot
)
"
dynamicTimeWarping
"
:
(
)
=
>
(
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_8__
.
dynamicTimeWarping
)
"
env
"
:
(
)
=
>
(
_env_js__WEBPACK_IMPORTED_MODULE_1__
.
env
)
"
getTopItems
"
:
(
)
=
>
(
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
.
getTopItems
)
"
hanning
"
:
(
)
=
>
(
_utils_audio_js__WEBPACK_IMPORTED_MODULE_6__
.
hanning
)
"
interpolate
"
:
(
)
=
>
(
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_8__
.
interpolate
)
"
interpolate_data
"
:
(
)
=
>
(
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
.
interpolate_data
)
"
layer_norm
"
:
(
)
=
>
(
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_8__
.
layer_norm
)
"
log_softmax
"
:
(
)
=
>
(
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
.
log_softmax
)
"
magnitude
"
:
(
)
=
>
(
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
.
magnitude
)
"
max
"
:
(
)
=
>
(
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
.
max
)
"
mean
"
:
(
)
=
>
(
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_8__
.
mean
)
"
mean_pooling
"
:
(
)
=
>
(
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_8__
.
mean_pooling
)
"
medianFilter
"
:
(
)
=
>
(
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
.
medianFilter
)
"
mel_filter_bank
"
:
(
)
=
>
(
_utils_audio_js__WEBPACK_IMPORTED_MODULE_6__
.
mel_filter_bank
)
"
min
"
:
(
)
=
>
(
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
.
min
)
"
ones
"
:
(
)
=
>
(
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_8__
.
ones
)
"
ones_like
"
:
(
)
=
>
(
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_8__
.
ones_like
)
"
permute
"
:
(
)
=
>
(
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_8__
.
permute
)
"
permute_data
"
:
(
)
=
>
(
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
.
permute_data
)
"
pipeline
"
:
(
)
=
>
(
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
.
pipeline
)
"
read_audio
"
:
(
)
=
>
(
_utils_audio_js__WEBPACK_IMPORTED_MODULE_6__
.
read_audio
)
"
round
"
:
(
)
=
>
(
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
.
round
)
"
softmax
"
:
(
)
=
>
(
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
.
softmax
)
"
spectrogram
"
:
(
)
=
>
(
_utils_audio_js__WEBPACK_IMPORTED_MODULE_6__
.
spectrogram
)
"
stack
"
:
(
)
=
>
(
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_8__
.
stack
)
"
std_mean
"
:
(
)
=
>
(
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_8__
.
std_mean
)
"
window_function
"
:
(
)
=
>
(
_utils_audio_js__WEBPACK_IMPORTED_MODULE_6__
.
window_function
)
}
)
;
var
_pipelines_js__WEBPACK_IMPORTED_MODULE_0__
=
__webpack_require__
(
"
.
/
src
/
pipelines
.
js
"
)
;
var
_env_js__WEBPACK_IMPORTED_MODULE_1__
=
__webpack_require__
(
"
.
/
src
/
env
.
js
"
)
;
var
_models_js__WEBPACK_IMPORTED_MODULE_2__
=
__webpack_require__
(
"
.
/
src
/
models
.
js
"
)
;
var
_tokenizers_js__WEBPACK_IMPORTED_MODULE_3__
=
__webpack_require__
(
"
.
/
src
/
tokenizers
.
js
"
)
;
var
_processors_js__WEBPACK_IMPORTED_MODULE_4__
=
__webpack_require__
(
"
.
/
src
/
processors
.
js
"
)
;
var
_configs_js__WEBPACK_IMPORTED_MODULE_5__
=
__webpack_require__
(
"
.
/
src
/
configs
.
js
"
)
;
var
_utils_audio_js__WEBPACK_IMPORTED_MODULE_6__
=
__webpack_require__
(
"
.
/
src
/
utils
/
audio
.
js
"
)
;
var
_utils_image_js__WEBPACK_IMPORTED_MODULE_7__
=
__webpack_require__
(
"
.
/
src
/
utils
/
image
.
js
"
)
;
var
_utils_tensor_js__WEBPACK_IMPORTED_MODULE_8__
=
__webpack_require__
(
"
.
/
src
/
utils
/
tensor
.
js
"
)
;
var
_utils_maths_js__WEBPACK_IMPORTED_MODULE_9__
=
__webpack_require__
(
"
.
/
src
/
utils
/
maths
.
js
"
)
;
}
)
"
.
/
src
/
utils
/
audio
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
hanning
"
:
(
)
=
>
(
hanning
)
"
mel_filter_bank
"
:
(
)
=
>
(
mel_filter_bank
)
"
read_audio
"
:
(
)
=
>
(
read_audio
)
"
spectrogram
"
:
(
)
=
>
(
spectrogram
)
"
window_function
"
:
(
)
=
>
(
window_function
)
}
)
;
var
_hub_js__WEBPACK_IMPORTED_MODULE_0__
=
__webpack_require__
(
"
.
/
src
/
utils
/
hub
.
js
"
)
;
var
_maths_js__WEBPACK_IMPORTED_MODULE_1__
=
__webpack_require__
(
"
.
/
src
/
utils
/
maths
.
js
"
)
;
var
_core_js__WEBPACK_IMPORTED_MODULE_2__
=
__webpack_require__
(
"
.
/
src
/
utils
/
core
.
js
"
)
;
async
function
read_audio
(
url
sampling_rate
)
{
if
(
typeof
AudioContext
=
=
=
'
undefined
'
)
{
throw
Error
(
"
Unable
to
load
audio
from
path
/
URL
since
AudioContext
is
not
available
in
your
environment
.
"
+
"
Instead
audio
data
should
be
passed
directly
to
the
pipeline
/
processor
.
"
+
"
For
more
information
and
some
example
code
see
https
:
/
/
huggingface
.
co
/
docs
/
transformers
.
js
/
guides
/
node
-
audio
-
processing
.
"
)
}
const
response
=
await
(
await
(
0
_hub_js__WEBPACK_IMPORTED_MODULE_0__
.
getFile
)
(
url
)
)
.
arrayBuffer
(
)
;
const
audioCTX
=
new
AudioContext
(
{
sampleRate
:
sampling_rate
}
)
;
if
(
typeof
sampling_rate
=
=
=
'
undefined
'
)
{
console
.
warn
(
No
sampling
rate
provided
using
default
of
{
audioCTX
.
sampleRate
}
Hz
.
)
}
const
decoded
=
await
audioCTX
.
decodeAudioData
(
response
)
;
let
audio
;
if
(
decoded
.
numberOfChannels
=
=
=
2
)
{
const
SCALING_FACTOR
=
Math
.
sqrt
(
2
)
;
const
left
=
decoded
.
getChannelData
(
0
)
;
const
right
=
decoded
.
getChannelData
(
1
)
;
audio
=
new
Float32Array
(
left
.
length
)
;
for
(
let
i
=
0
;
i
<
decoded
.
length
;
+
+
i
)
{
audio
[
i
]
=
SCALING_FACTOR
*
(
left
[
i
]
+
right
[
i
]
)
/
2
;
}
}
else
{
audio
=
decoded
.
getChannelData
(
0
)
;
}
return
audio
;
}
function
hanning
(
M
)
{
if
(
M
<
1
)
{
return
new
Float64Array
(
)
;
}
if
(
M
=
=
=
1
)
{
return
new
Float64Array
(
[
1
]
)
;
}
const
denom
=
M
-
1
;
const
factor
=
Math
.
PI
/
denom
;
const
cos_vals
=
new
Float64Array
(
M
)
;
for
(
let
i
=
0
;
i
<
M
;
+
+
i
)
{
const
n
=
2
*
i
-
denom
;
cos_vals
[
i
]
=
0
.
5
+
0
.
5
*
Math
.
cos
(
factor
*
n
)
;
}
return
cos_vals
;
}
const
HERTZ_TO_MEL_MAPPING
=
{
"
htk
"
:
(
freq
)
=
>
2595
.
0
*
Math
.
log10
(
1
.
0
+
(
freq
/
700
.
0
)
)
"
kaldi
"
:
(
freq
)
=
>
1127
.
0
*
Math
.
log
(
1
.
0
+
(
freq
/
700
.
0
)
)
"
slaney
"
:
(
freq
min_log_hertz
=
1000
.
0
min_log_mel
=
15
.
0
logstep
=
27
.
0
/
Math
.
log
(
6
.
4
)
)
=
>
freq
>
=
min_log_hertz
?
min_log_mel
+
Math
.
log
(
freq
/
min_log_hertz
)
*
logstep
:
3
.
0
*
freq
/
200
.
0
}
function
hertz_to_mel
(
freq
mel_scale
=
"
htk
"
)
{
const
fn
=
HERTZ_TO_MEL_MAPPING
[
mel_scale
]
;
if
(
!
fn
)
{
throw
new
Error
(
'
mel_scale
should
be
one
of
"
htk
"
"
slaney
"
or
"
kaldi
"
.
'
)
;
}
return
typeof
freq
=
=
=
'
number
'
?
fn
(
freq
)
:
freq
.
map
(
x
=
>
fn
(
x
)
)
;
}
const
MEL_TO_HERTZ_MAPPING
=
{
"
htk
"
:
(
mels
)
=
>
700
.
0
*
(
10
.
0
*
*
(
mels
/
2595
.
0
)
-
1
.
0
)
"
kaldi
"
:
(
mels
)
=
>
700
.
0
*
(
Math
.
exp
(
mels
/
1127
.
0
)
-
1
.
0
)
"
slaney
"
:
(
mels
min_log_hertz
=
1000
.
0
min_log_mel
=
15
.
0
logstep
=
Math
.
log
(
6
.
4
)
/
27
.
0
)
=
>
mels
>
=
min_log_mel
?
min_log_hertz
*
Math
.
exp
(
logstep
*
(
mels
-
min_log_mel
)
)
:
200
.
0
*
mels
/
3
.
0
}
function
mel_to_hertz
(
mels
mel_scale
=
"
htk
"
)
{
const
fn
=
MEL_TO_HERTZ_MAPPING
[
mel_scale
]
;
if
(
!
fn
)
{
throw
new
Error
(
'
mel_scale
should
be
one
of
"
htk
"
"
slaney
"
or
"
kaldi
"
.
'
)
;
}
return
typeof
mels
=
=
=
'
number
'
?
fn
(
mels
)
:
mels
.
map
(
x
=
>
fn
(
x
)
)
;
}
function
_create_triangular_filter_bank
(
fft_freqs
filter_freqs
)
{
const
filter_diff
=
Float64Array
.
from
(
{
length
:
filter_freqs
.
length
-
1
}
(
_
i
)
=
>
filter_freqs
[
i
+
1
]
-
filter_freqs
[
i
]
)
;
const
slopes
=
Array
.
from
(
{
length
:
fft_freqs
.
length
}
(
)
=
>
new
Array
(
filter_freqs
.
length
)
)
;
for
(
let
j
=
0
;
j
<
fft_freqs
.
length
;
+
+
j
)
{
const
slope
=
slopes
[
j
]
;
for
(
let
i
=
0
;
i
<
filter_freqs
.
length
;
+
+
i
)
{
slope
[
i
]
=
filter_freqs
[
i
]
-
fft_freqs
[
j
]
;
}
}
const
numFreqs
=
filter_freqs
.
length
-
2
;
const
ret
=
Array
.
from
(
{
length
:
numFreqs
}
(
)
=
>
new
Array
(
fft_freqs
.
length
)
)
;
for
(
let
j
=
0
;
j
<
fft_freqs
.
length
;
+
+
j
)
{
const
slope
=
slopes
[
j
]
;
for
(
let
i
=
0
;
i
<
numFreqs
;
+
+
i
)
{
const
down
=
-
slope
[
i
]
/
filter_diff
[
i
]
;
const
up
=
slope
[
i
+
2
]
/
filter_diff
[
i
+
1
]
;
ret
[
i
]
[
j
]
=
Math
.
max
(
0
Math
.
min
(
down
up
)
)
;
}
}
return
ret
;
}
function
linspace
(
start
end
num
)
{
const
step
=
(
end
-
start
)
/
(
num
-
1
)
;
return
Float64Array
.
from
(
{
length
:
num
}
(
_
i
)
=
>
start
+
step
*
i
)
;
}
function
mel_filter_bank
(
num_frequency_bins
num_mel_filters
min_frequency
max_frequency
sampling_rate
norm
=
null
mel_scale
=
"
htk
"
triangularize_in_mel_space
=
false
)
{
if
(
norm
!
=
=
null
&
&
norm
!
=
=
"
slaney
"
)
{
throw
new
Error
(
'
norm
must
be
one
of
null
or
"
slaney
"
'
)
;
}
const
mel_min
=
hertz_to_mel
(
min_frequency
mel_scale
)
;
const
mel_max
=
hertz_to_mel
(
max_frequency
mel_scale
)
;
const
mel_freqs
=
linspace
(
mel_min
mel_max
num_mel_filters
+
2
)
;
let
filter_freqs
=
mel_to_hertz
(
mel_freqs
mel_scale
)
;
let
fft_freqs
;
if
(
triangularize_in_mel_space
)
{
const
fft_bin_width
=
sampling_rate
/
(
num_frequency_bins
*
2
)
;
fft_freqs
=
hertz_to_mel
(
Float64Array
.
from
(
{
length
:
num_frequency_bins
}
(
_
i
)
=
>
i
*
fft_bin_width
)
mel_scale
)
;
filter_freqs
=
mel_freqs
;
}
else
{
fft_freqs
=
linspace
(
0
Math
.
floor
(
sampling_rate
/
2
)
num_frequency_bins
)
;
}
const
mel_filters
=
_create_triangular_filter_bank
(
fft_freqs
filter_freqs
)
;
if
(
norm
!
=
=
null
&
&
norm
=
=
=
"
slaney
"
)
{
for
(
let
i
=
0
;
i
<
num_mel_filters
;
+
+
i
)
{
const
filter
=
mel_filters
[
i
]
;
const
enorm
=
2
.
0
/
(
filter_freqs
[
i
+
2
]
-
filter_freqs
[
i
]
)
;
for
(
let
j
=
0
;
j
<
num_frequency_bins
;
+
+
j
)
{
filter
[
j
]
*
=
enorm
;
}
}
}
return
mel_filters
;
}
function
padReflect
(
array
left
right
)
{
const
padded
=
new
array
.
constructor
(
array
.
length
+
left
+
right
)
;
const
w
=
array
.
length
-
1
;
for
(
let
i
=
0
;
i
<
array
.
length
;
+
+
i
)
{
padded
[
left
+
i
]
=
array
[
i
]
;
}
for
(
let
i
=
1
;
i
<
=
left
;
+
+
i
)
{
padded
[
left
-
i
]
=
array
[
(
0
_core_js__WEBPACK_IMPORTED_MODULE_2__
.
calculateReflectOffset
)
(
i
w
)
]
;
}
for
(
let
i
=
1
;
i
<
=
right
;
+
+
i
)
{
padded
[
w
+
left
+
i
]
=
array
[
(
0
_core_js__WEBPACK_IMPORTED_MODULE_2__
.
calculateReflectOffset
)
(
w
-
i
w
)
]
;
}
return
padded
;
}
function
_db_conversion_helper
(
spectrogram
factor
reference
min_value
db_range
)
{
if
(
reference
<
=
0
)
{
throw
new
Error
(
'
reference
must
be
greater
than
zero
'
)
;
}
if
(
min_value
<
=
0
)
{
throw
new
Error
(
'
min_value
must
be
greater
than
zero
'
)
;
}
reference
=
Math
.
max
(
min_value
reference
)
;
const
logReference
=
Math
.
log10
(
reference
)
;
for
(
let
i
=
0
;
i
<
spectrogram
.
length
;
+
+
i
)
{
spectrogram
[
i
]
=
factor
*
Math
.
log10
(
Math
.
max
(
min_value
spectrogram
[
i
]
)
-
logReference
)
}
if
(
db_range
!
=
=
null
)
{
if
(
db_range
<
=
0
)
{
throw
new
Error
(
'
db_range
must
be
greater
than
zero
'
)
;
}
const
maxValue
=
(
0
_maths_js__WEBPACK_IMPORTED_MODULE_1__
.
max
)
(
spectrogram
)
[
0
]
-
db_range
;
for
(
let
i
=
0
;
i
<
spectrogram
.
length
;
+
+
i
)
{
spectrogram
[
i
]
=
Math
.
max
(
spectrogram
[
i
]
maxValue
)
;
}
}
return
spectrogram
;
}
function
amplitude_to_db
(
spectrogram
reference
=
1
.
0
min_value
=
1e
-
5
db_range
=
null
)
{
return
_db_conversion_helper
(
spectrogram
20
.
0
reference
min_value
db_range
)
;
}
function
power_to_db
(
spectrogram
reference
=
1
.
0
min_value
=
1e
-
10
db_range
=
null
)
{
return
_db_conversion_helper
(
spectrogram
10
.
0
reference
min_value
db_range
)
;
}
function
spectrogram
(
waveform
window
frame_length
hop_length
{
fft_length
=
null
power
=
1
.
0
center
=
true
pad_mode
=
"
reflect
"
onesided
=
true
preemphasis
=
null
mel_filters
=
null
mel_floor
=
1e
-
10
log_mel
=
null
reference
=
1
.
0
min_value
=
1e
-
10
db_range
=
null
remove_dc_offset
=
null
max_num_frames
=
null
do_pad
=
true
transpose
=
false
}
=
{
}
)
{
const
window_length
=
window
.
length
;
if
(
fft_length
=
=
=
null
)
{
fft_length
=
frame_length
;
}
if
(
frame_length
>
fft_length
)
{
throw
Error
(
frame_length
(
{
frame_length
}
)
may
not
be
larger
than
fft_length
(
{
fft_length
}
)
)
}
if
(
window_length
!
=
=
frame_length
)
{
throw
new
Error
(
Length
of
the
window
(
{
window_length
}
)
must
equal
frame_length
(
{
frame_length
}
)
)
;
}
if
(
hop_length
<
=
0
)
{
throw
new
Error
(
"
hop_length
must
be
greater
than
zero
"
)
;
}
if
(
center
)
{
if
(
pad_mode
!
=
=
'
reflect
'
)
{
throw
new
Error
(
pad_mode
=
"
{
pad_mode
}
"
not
implemented
yet
.
)
}
const
half_window
=
Math
.
floor
(
(
fft_length
-
1
)
/
2
)
+
1
;
waveform
=
padReflect
(
waveform
half_window
half_window
)
;
}
const
num_frames
=
Math
.
floor
(
1
+
Math
.
floor
(
(
waveform
.
length
-
frame_length
)
/
hop_length
)
)
const
num_frequency_bins
=
onesided
?
Math
.
floor
(
fft_length
/
2
)
+
1
:
fft_length
let
d1
=
num_frames
;
let
d1Max
=
num_frames
;
if
(
max_num_frames
!
=
=
null
)
{
if
(
max_num_frames
>
num_frames
)
{
if
(
do_pad
)
{
d1Max
=
max_num_frames
;
}
}
else
{
d1Max
=
d1
=
max_num_frames
;
}
}
const
fft
=
new
_maths_js__WEBPACK_IMPORTED_MODULE_1__
.
FFT
(
fft_length
)
;
const
inputBuffer
=
new
Float64Array
(
fft_length
)
;
const
outputBuffer
=
new
Float64Array
(
fft
.
outputBufferSize
)
;
const
magnitudes
=
new
Array
(
d1
)
;
for
(
let
i
=
0
;
i
<
d1
;
+
+
i
)
{
const
offset
=
i
*
hop_length
;
for
(
let
j
=
0
;
j
<
frame_length
;
+
+
j
)
{
inputBuffer
[
j
]
=
waveform
[
offset
+
j
]
;
}
if
(
remove_dc_offset
)
{
let
sum
=
0
;
for
(
let
j
=
0
;
j
<
frame_length
;
+
+
j
)
{
sum
+
=
inputBuffer
[
j
]
;
}
const
mean
=
sum
/
frame_length
;
for
(
let
j
=
0
;
j
<
frame_length
;
+
+
j
)
{
inputBuffer
[
j
]
-
=
mean
;
}
}
if
(
preemphasis
!
=
=
null
)
{
for
(
let
j
=
frame_length
-
1
;
j
>
=
1
;
-
-
j
)
{
inputBuffer
[
j
]
-
=
preemphasis
*
inputBuffer
[
j
-
1
]
;
}
inputBuffer
[
0
]
*
=
1
-
preemphasis
;
}
for
(
let
j
=
0
;
j
<
window
.
length
;
+
+
j
)
{
inputBuffer
[
j
]
*
=
window
[
j
]
;
}
fft
.
realTransform
(
outputBuffer
inputBuffer
)
;
const
row
=
new
Array
(
num_frequency_bins
)
;
for
(
let
j
=
0
;
j
<
row
.
length
;
+
+
j
)
{
const
j2
=
j
<
<
1
;
row
[
j
]
=
outputBuffer
[
j2
]
*
*
2
+
outputBuffer
[
j2
+
1
]
*
*
2
;
}
magnitudes
[
i
]
=
row
;
}
if
(
power
!
=
=
null
&
&
power
!
=
=
2
)
{
const
pow
=
2
/
power
;
for
(
let
i
=
0
;
i
<
magnitudes
.
length
;
+
+
i
)
{
const
magnitude
=
magnitudes
[
i
]
;
for
(
let
j
=
0
;
j
<
magnitude
.
length
;
+
+
j
)
{
magnitude
[
j
]
*
*
=
pow
;
}
}
}
const
num_mel_filters
=
mel_filters
.
length
;
const
mel_spec
=
new
Float32Array
(
num_mel_filters
*
d1Max
)
;
const
dims
=
transpose
?
[
d1Max
num_mel_filters
]
:
[
num_mel_filters
d1Max
]
;
for
(
let
i
=
0
;
i
<
num_mel_filters
;
+
+
i
)
{
const
filter
=
mel_filters
[
i
]
;
for
(
let
j
=
0
;
j
<
d1
;
+
+
j
)
{
const
magnitude
=
magnitudes
[
j
]
;
let
sum
=
0
;
for
(
let
k
=
0
;
k
<
num_frequency_bins
;
+
+
k
)
{
sum
+
=
filter
[
k
]
*
magnitude
[
k
]
;
}
mel_spec
[
transpose
?
j
*
num_mel_filters
+
i
:
i
*
d1
+
j
]
=
Math
.
max
(
mel_floor
sum
)
;
}
}
if
(
power
!
=
=
null
&
&
log_mel
!
=
=
null
)
{
const
o
=
Math
.
min
(
mel_spec
.
length
d1
*
num_mel_filters
)
;
switch
(
log_mel
)
{
case
'
log
'
:
for
(
let
i
=
0
;
i
<
o
;
+
+
i
)
{
mel_spec
[
i
]
=
Math
.
log
(
mel_spec
[
i
]
)
;
}
break
;
case
'
log10
'
:
for
(
let
i
=
0
;
i
<
o
;
+
+
i
)
{
mel_spec
[
i
]
=
Math
.
log10
(
mel_spec
[
i
]
)
;
}
break
;
case
'
dB
'
:
if
(
power
=
=
=
1
.
0
)
{
amplitude_to_db
(
mel_spec
reference
min_value
db_range
)
;
}
else
if
(
power
=
=
=
2
.
0
)
{
power_to_db
(
mel_spec
reference
min_value
db_range
)
;
}
else
{
throw
new
Error
(
Cannot
use
log_mel
option
'
{
log_mel
}
'
with
power
{
power
}
)
}
break
;
default
:
throw
new
Error
(
log_mel
must
be
one
of
null
'
log
'
'
log10
'
or
'
dB
'
.
Got
'
{
log_mel
}
'
)
;
}
}
return
{
data
:
mel_spec
dims
}
;
}
function
window_function
(
window_length
name
{
periodic
=
true
frame_length
=
null
center
=
true
}
=
{
}
)
{
const
length
=
periodic
?
window_length
+
1
:
window_length
;
let
window
;
switch
(
name
)
{
case
'
boxcar
'
:
window
=
new
Float64Array
(
length
)
.
fill
(
1
.
0
)
;
break
;
case
'
hann
'
:
case
'
hann_window
'
:
window
=
hanning
(
length
)
;
break
;
case
'
povey
'
:
window
=
hanning
(
length
)
.
map
(
x
=
>
Math
.
pow
(
x
0
.
85
)
)
;
break
;
default
:
throw
new
Error
(
Unknown
window
type
{
name
}
.
)
;
}
if
(
periodic
)
{
window
=
window
.
subarray
(
0
window_length
)
;
}
if
(
frame_length
=
=
=
null
)
{
return
window
;
}
if
(
window_length
>
frame_length
)
{
throw
new
Error
(
Length
of
the
window
(
{
window_length
}
)
may
not
be
larger
than
frame_length
(
{
frame_length
}
)
)
;
}
return
window
;
}
}
)
"
.
/
src
/
utils
/
core
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
Callable
"
:
(
)
=
>
(
Callable
)
"
calculateDimensions
"
:
(
)
=
>
(
calculateDimensions
)
"
calculateReflectOffset
"
:
(
)
=
>
(
calculateReflectOffset
)
"
dispatchCallback
"
:
(
)
=
>
(
dispatchCallback
)
"
escapeRegExp
"
:
(
)
=
>
(
escapeRegExp
)
"
exists
"
:
(
)
=
>
(
exists
)
"
isIntegralNumber
"
:
(
)
=
>
(
isIntegralNumber
)
"
isTypedArray
"
:
(
)
=
>
(
isTypedArray
)
"
mergeArrays
"
:
(
)
=
>
(
mergeArrays
)
"
pop
"
:
(
)
=
>
(
pop
)
"
product
"
:
(
)
=
>
(
product
)
"
reverseDictionary
"
:
(
)
=
>
(
reverseDictionary
)
}
)
;
function
dispatchCallback
(
progress_callback
data
)
{
if
(
progress_callback
)
progress_callback
(
data
)
;
}
function
reverseDictionary
(
data
)
{
return
Object
.
fromEntries
(
Object
.
entries
(
data
)
.
map
(
(
[
key
value
]
)
=
>
[
value
key
]
)
)
;
}
function
escapeRegExp
(
string
)
{
return
string
.
replace
(
/
[
.
*
+
?
^
{
}
(
)
|
[
\
]
\
\
]
/
g
'
\
\
&
'
)
;
}
const
Callable
=
(
class
{
constructor
(
)
{
let
closure
=
function
(
.
.
.
args
)
{
return
closure
.
_call
(
.
.
.
args
)
}
return
Object
.
setPrototypeOf
(
closure
new
.
target
.
prototype
)
}
_call
(
.
.
.
args
)
{
throw
Error
(
'
Must
implement
_call
method
in
subclass
'
)
}
}
)
;
function
isTypedArray
(
val
)
{
return
val
?
.
prototype
?
.
__proto__
?
.
constructor
?
.
name
=
=
=
'
TypedArray
'
;
}
function
isIntegralNumber
(
x
)
{
return
Number
.
isInteger
(
x
)
|
|
typeof
x
=
=
=
'
bigint
'
}
function
exists
(
x
)
{
return
x
!
=
=
undefined
&
&
x
!
=
=
null
;
}
function
calculateDimensions
(
arr
)
{
const
dimensions
=
[
]
;
let
current
=
arr
;
while
(
Array
.
isArray
(
current
)
)
{
dimensions
.
push
(
current
.
length
)
;
current
=
current
[
0
]
;
}
return
dimensions
;
}
function
pop
(
obj
key
defaultValue
=
undefined
)
{
const
value
=
obj
[
key
]
;
if
(
value
!
=
=
undefined
)
{
delete
obj
[
key
]
;
return
value
;
}
if
(
defaultValue
=
=
=
undefined
)
{
throw
Error
(
Key
{
key
}
does
not
exist
in
object
.
)
}
return
defaultValue
;
}
function
mergeArrays
(
.
.
.
arrs
)
{
return
Array
.
prototype
.
concat
.
apply
(
[
]
arrs
)
;
}
function
product
(
.
.
.
a
)
{
return
a
.
reduce
(
(
a
b
)
=
>
a
.
flatMap
(
d
=
>
b
.
map
(
e
=
>
[
d
e
]
)
)
)
;
}
function
calculateReflectOffset
(
i
w
)
{
return
Math
.
abs
(
(
i
+
w
)
%
(
2
*
w
)
-
w
)
;
}
}
)
"
.
/
src
/
utils
/
data
-
structures
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
CharTrie
"
:
(
)
=
>
(
CharTrie
)
"
PriorityQueue
"
:
(
)
=
>
(
PriorityQueue
)
"
TokenLattice
"
:
(
)
=
>
(
TokenLattice
)
}
)
;
class
PriorityQueue
{
constructor
(
comparator
=
(
a
b
)
=
>
a
>
b
)
{
this
.
_heap
=
[
]
;
this
.
_comparator
=
comparator
;
}
get
size
(
)
{
return
this
.
_heap
.
length
;
}
isEmpty
(
)
{
return
this
.
size
=
=
=
0
;
}
peek
(
)
{
return
this
.
_heap
[
0
]
;
}
push
(
.
.
.
values
)
{
return
this
.
extend
(
values
)
;
}
extend
(
values
)
{
for
(
const
value
of
values
)
{
this
.
_heap
.
push
(
value
)
;
this
.
_siftUp
(
)
;
}
return
this
.
size
;
}
pop
(
)
{
const
poppedValue
=
this
.
peek
(
)
;
const
bottom
=
this
.
size
-
1
;
if
(
bottom
>
0
)
{
this
.
_swap
(
0
bottom
)
;
}
this
.
_heap
.
pop
(
)
;
this
.
_siftDown
(
)
;
return
poppedValue
;
}
replace
(
value
)
{
const
replacedValue
=
this
.
peek
(
)
;
this
.
_heap
[
0
]
=
value
;
this
.
_siftDown
(
)
;
return
replacedValue
;
}
_parent
(
i
)
{
return
(
(
i
+
1
)
>
>
>
1
)
-
1
;
}
_left
(
i
)
{
return
(
i
<
<
1
)
+
1
;
}
_right
(
i
)
{
return
(
i
+
1
)
<
<
1
;
}
_greater
(
i
j
)
{
return
this
.
_comparator
(
this
.
_heap
[
i
]
this
.
_heap
[
j
]
)
;
}
_swap
(
i
j
)
{
const
temp
=
this
.
_heap
[
i
]
;
this
.
_heap
[
i
]
=
this
.
_heap
[
j
]
;
this
.
_heap
[
j
]
=
temp
;
}
_siftUp
(
)
{
let
node
=
this
.
size
-
1
;
while
(
node
>
0
&
&
this
.
_greater
(
node
this
.
_parent
(
node
)
)
)
{
this
.
_swap
(
node
this
.
_parent
(
node
)
)
;
node
=
this
.
_parent
(
node
)
;
}
}
_siftDown
(
)
{
let
node
=
0
;
while
(
(
this
.
_left
(
node
)
<
this
.
size
&
&
this
.
_greater
(
this
.
_left
(
node
)
node
)
)
|
|
(
this
.
_right
(
node
)
<
this
.
size
&
&
this
.
_greater
(
this
.
_right
(
node
)
node
)
)
)
{
const
maxChild
=
(
this
.
_right
(
node
)
<
this
.
size
&
&
this
.
_greater
(
this
.
_right
(
node
)
this
.
_left
(
node
)
)
)
?
this
.
_right
(
node
)
:
this
.
_left
(
node
)
;
this
.
_swap
(
node
maxChild
)
;
node
=
maxChild
;
}
}
}
class
CharTrie
{
constructor
(
)
{
this
.
root
=
CharTrieNode
.
default
(
)
;
}
extend
(
texts
)
{
for
(
let
text
of
texts
)
{
this
.
push
(
text
)
;
}
}
push
(
text
)
{
let
node
=
this
.
root
;
for
(
let
ch
of
text
)
{
let
child
=
node
.
children
.
get
(
ch
)
;
if
(
child
=
=
=
undefined
)
{
child
=
CharTrieNode
.
default
(
)
;
node
.
children
.
set
(
ch
child
)
;
}
node
=
child
;
}
node
.
isLeaf
=
true
;
}
*
commonPrefixSearch
(
text
)
{
let
node
=
this
.
root
;
let
prefix
=
"
"
;
for
(
let
i
=
0
;
i
<
text
.
length
&
&
node
!
=
=
undefined
;
+
+
i
)
{
const
ch
=
text
[
i
]
;
prefix
+
=
ch
;
node
=
node
.
children
.
get
(
ch
)
;
if
(
node
!
=
=
undefined
&
&
node
.
isLeaf
)
{
yield
prefix
;
}
}
}
}
class
CharTrieNode
{
constructor
(
isLeaf
children
)
{
this
.
isLeaf
=
isLeaf
;
this
.
children
=
children
;
}
static
default
(
)
{
return
new
CharTrieNode
(
false
new
Map
(
)
)
;
}
}
class
TokenLattice
{
constructor
(
sentence
bosTokenId
eosTokenId
)
{
this
.
sentence
=
sentence
;
this
.
len
=
sentence
.
length
;
this
.
bosTokenId
=
bosTokenId
;
this
.
eosTokenId
=
eosTokenId
;
this
.
nodes
=
[
]
;
this
.
beginNodes
=
Array
.
from
(
{
length
:
this
.
len
+
1
}
(
)
=
>
[
]
)
;
this
.
endNodes
=
Array
.
from
(
{
length
:
this
.
len
+
1
}
(
)
=
>
[
]
)
;
const
bos
=
new
TokenLatticeNode
(
this
.
bosTokenId
0
0
0
0
.
0
)
;
const
eos
=
new
TokenLatticeNode
(
this
.
eosTokenId
1
this
.
len
0
0
.
0
)
;
this
.
nodes
.
push
(
bos
.
clone
(
)
)
;
this
.
nodes
.
push
(
eos
.
clone
(
)
)
;
this
.
beginNodes
[
this
.
len
]
.
push
(
eos
)
;
this
.
endNodes
[
0
]
.
push
(
bos
)
;
}
insert
(
pos
length
score
tokenId
)
{
const
nodeId
=
this
.
nodes
.
length
;
const
node
=
new
TokenLatticeNode
(
tokenId
nodeId
pos
length
score
)
;
this
.
beginNodes
[
pos
]
.
push
(
node
)
;
this
.
endNodes
[
pos
+
length
]
.
push
(
node
)
;
this
.
nodes
.
push
(
node
)
;
}
viterbi
(
)
{
const
len
=
this
.
len
;
let
pos
=
0
;
while
(
pos
<
=
len
)
{
if
(
this
.
beginNodes
[
pos
]
.
length
=
=
0
)
{
return
[
]
;
}
for
(
let
rnode
of
this
.
beginNodes
[
pos
]
)
{
rnode
.
prev
=
null
;
let
bestScore
=
0
.
0
;
let
bestNode
=
null
;
for
(
let
lnode
of
this
.
endNodes
[
pos
]
)
{
const
score
=
lnode
.
backtraceScore
+
rnode
.
score
;
if
(
bestNode
=
=
=
null
|
|
score
>
bestScore
)
{
bestNode
=
lnode
.
clone
(
)
;
bestScore
=
score
;
}
}
if
(
bestNode
!
=
=
null
)
{
rnode
.
prev
=
bestNode
;
rnode
.
backtraceScore
=
bestScore
;
}
else
{
return
[
]
;
}
}
+
+
pos
;
}
const
results
=
[
]
;
const
root
=
this
.
beginNodes
[
len
]
[
0
]
;
const
prev
=
root
.
prev
;
if
(
prev
=
=
=
null
)
{
return
[
]
;
}
let
node
=
prev
.
clone
(
)
;
while
(
node
.
prev
!
=
=
null
)
{
results
.
push
(
node
.
clone
(
)
)
;
const
n
=
node
.
clone
(
)
;
node
=
n
.
prev
.
clone
(
)
;
}
results
.
reverse
(
)
;
return
results
;
}
piece
(
node
)
{
return
this
.
sentence
.
slice
(
node
.
pos
node
.
pos
+
node
.
length
)
;
}
tokens
(
)
{
const
nodes
=
this
.
viterbi
(
)
;
return
nodes
.
map
(
x
=
>
this
.
piece
(
x
)
)
;
}
tokenIds
(
)
{
const
nodes
=
this
.
viterbi
(
)
;
return
nodes
.
map
(
x
=
>
x
.
tokenId
)
;
}
}
class
TokenLatticeNode
{
constructor
(
tokenId
nodeId
pos
length
score
)
{
this
.
tokenId
=
tokenId
;
this
.
nodeId
=
nodeId
;
this
.
pos
=
pos
;
this
.
length
=
length
;
this
.
score
=
score
;
this
.
prev
=
null
;
this
.
backtraceScore
=
0
.
0
;
}
clone
(
)
{
const
n
=
new
TokenLatticeNode
(
this
.
tokenId
this
.
nodeId
this
.
pos
this
.
length
this
.
score
)
;
n
.
prev
=
this
.
prev
;
n
.
backtraceScore
=
this
.
backtraceScore
;
return
n
;
}
}
}
)
"
.
/
src
/
utils
/
generation
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
ForceTokensLogitsProcessor
"
:
(
)
=
>
(
ForceTokensLogitsProcessor
)
"
ForcedBOSTokenLogitsProcessor
"
:
(
)
=
>
(
ForcedBOSTokenLogitsProcessor
)
"
ForcedEOSTokenLogitsProcessor
"
:
(
)
=
>
(
ForcedEOSTokenLogitsProcessor
)
"
GenerationConfig
"
:
(
)
=
>
(
GenerationConfig
)
"
LogitsProcessor
"
:
(
)
=
>
(
LogitsProcessor
)
"
LogitsProcessorList
"
:
(
)
=
>
(
LogitsProcessorList
)
"
MinLengthLogitsProcessor
"
:
(
)
=
>
(
MinLengthLogitsProcessor
)
"
MinNewTokensLengthLogitsProcessor
"
:
(
)
=
>
(
MinNewTokensLengthLogitsProcessor
)
"
NoBadWordsLogitsProcessor
"
:
(
)
=
>
(
NoBadWordsLogitsProcessor
)
"
NoRepeatNGramLogitsProcessor
"
:
(
)
=
>
(
NoRepeatNGramLogitsProcessor
)
"
RepetitionPenaltyLogitsProcessor
"
:
(
)
=
>
(
RepetitionPenaltyLogitsProcessor
)
"
Sampler
"
:
(
)
=
>
(
Sampler
)
"
SuppressTokensAtBeginLogitsProcessor
"
:
(
)
=
>
(
SuppressTokensAtBeginLogitsProcessor
)
"
WhisperTimeStampLogitsProcessor
"
:
(
)
=
>
(
WhisperTimeStampLogitsProcessor
)
}
)
;
var
_tensor_js__WEBPACK_IMPORTED_MODULE_0__
=
__webpack_require__
(
"
.
/
src
/
utils
/
tensor
.
js
"
)
;
var
_core_js__WEBPACK_IMPORTED_MODULE_1__
=
__webpack_require__
(
"
.
/
src
/
utils
/
core
.
js
"
)
;
var
_maths_js__WEBPACK_IMPORTED_MODULE_2__
=
__webpack_require__
(
"
.
/
src
/
utils
/
maths
.
js
"
)
;
class
LogitsProcessorList
extends
_core_js__WEBPACK_IMPORTED_MODULE_1__
.
Callable
{
constructor
(
)
{
super
(
)
;
this
.
processors
=
[
]
;
}
push
(
item
)
{
this
.
processors
.
push
(
item
)
;
}
extend
(
items
)
{
this
.
processors
.
push
(
.
.
.
items
)
;
}
_call
(
input_ids
batchedLogits
)
{
for
(
let
logits
of
batchedLogits
)
{
this
.
processors
.
forEach
(
func
=
>
func
(
input_ids
logits
)
)
}
}
[
Symbol
.
iterator
]
(
)
{
return
this
.
processors
.
values
(
)
;
}
}
class
LogitsProcessor
extends
_core_js__WEBPACK_IMPORTED_MODULE_1__
.
Callable
{
_call
(
input_ids
logits
)
{
throw
Error
(
"
_call
should
be
implemented
in
a
subclass
"
)
}
}
class
ForceTokensLogitsProcessor
extends
LogitsProcessor
{
constructor
(
forced_decoder_ids
)
{
super
(
)
;
this
.
force_token_map
=
Object
.
fromEntries
(
forced_decoder_ids
?
?
[
]
)
;
}
_call
(
input_ids
logits
)
{
let
map
=
this
.
force_token_map
[
input_ids
.
length
]
;
if
(
(
0
_core_js__WEBPACK_IMPORTED_MODULE_1__
.
exists
)
(
map
)
)
{
logits
.
data
.
fill
(
-
Infinity
)
logits
.
data
[
map
]
=
0
;
}
return
logits
;
}
}
class
ForcedBOSTokenLogitsProcessor
extends
LogitsProcessor
{
constructor
(
bos_token_id
)
{
super
(
)
;
this
.
bos_token_id
=
bos_token_id
;
}
_call
(
input_ids
logits
)
{
if
(
input_ids
.
length
=
=
=
1
)
{
logits
.
data
.
fill
(
-
Infinity
)
logits
.
data
[
this
.
bos_token_id
]
=
0
;
}
return
logits
;
}
}
class
ForcedEOSTokenLogitsProcessor
extends
LogitsProcessor
{
constructor
(
max_length
forced_eos_token_id
)
{
super
(
)
;
this
.
max_length
=
max_length
;
this
.
forced_eos_token_id
=
forced_eos_token_id
;
}
_call
(
input_ids
logits
)
{
}
}
class
SuppressTokensAtBeginLogitsProcessor
extends
LogitsProcessor
{
constructor
(
begin_suppress_tokens
begin_index
)
{
super
(
)
;
this
.
begin_suppress_tokens
=
begin_suppress_tokens
;
this
.
begin_index
=
begin_index
;
}
_call
(
input_ids
logits
)
{
if
(
input_ids
.
length
=
=
=
this
.
begin_index
)
{
for
(
let
token_id
of
this
.
begin_suppress_tokens
)
{
logits
.
data
[
token_id
]
=
-
Infinity
;
}
}
return
logits
;
}
}
class
WhisperTimeStampLogitsProcessor
extends
LogitsProcessor
{
constructor
(
generate_config
)
{
super
(
)
;
this
.
eos_token_id
=
generate_config
.
eos_token_id
;
this
.
no_timestamps_token_id
=
generate_config
.
no_timestamps_token_id
;
this
.
timestamp_begin
=
this
.
no_timestamps_token_id
+
1
;
this
.
begin_index
=
(
generate_config
.
forced_decoder_ids
|
|
[
]
)
.
length
+
2
;
if
(
generate_config
.
forced_decoder_ids
.
slice
(
-
1
)
[
0
]
[
1
]
=
=
=
this
.
no_timestamps_token_id
)
{
this
.
begin_index
-
=
1
;
}
this
.
max_initial_timestamp_index
=
generate_config
.
max_initial_timestamp_index
;
}
_call
(
input_ids
logits
)
{
const
logitsData
=
(
logits
.
data
)
;
logitsData
[
this
.
no_timestamps_token_id
]
=
-
Infinity
;
if
(
input_ids
.
length
=
=
=
this
.
begin_index
-
1
)
{
logitsData
.
fill
(
-
Infinity
)
;
logitsData
[
this
.
timestamp_begin
]
=
0
;
return
logits
;
}
const
seq
=
input_ids
.
slice
(
this
.
begin_index
)
;
const
last_was_timestamp
=
seq
.
length
>
=
1
&
&
seq
[
seq
.
length
-
1
]
>
=
this
.
timestamp_begin
;
const
penultimate_was_timestamp
=
seq
.
length
<
2
|
|
seq
[
seq
.
length
-
2
]
>
=
this
.
timestamp_begin
;
if
(
last_was_timestamp
)
{
if
(
penultimate_was_timestamp
)
{
logitsData
.
subarray
(
this
.
timestamp_begin
)
.
fill
(
-
Infinity
)
;
}
else
{
logitsData
.
subarray
(
0
this
.
eos_token_id
)
.
fill
(
-
Infinity
)
;
}
}
if
(
input_ids
.
length
=
=
=
this
.
begin_index
&
&
this
.
max_initial_timestamp_index
!
=
=
null
)
{
const
last_allowed
=
this
.
timestamp_begin
+
this
.
max_initial_timestamp_index
;
logitsData
.
subarray
(
last_allowed
+
1
)
.
fill
(
-
Infinity
)
;
}
const
logprobs
=
(
0
_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
log_softmax
)
(
logitsData
)
;
const
timestamp_logprob
=
Math
.
log
(
logprobs
.
subarray
(
this
.
timestamp_begin
)
.
map
(
Math
.
exp
)
.
reduce
(
(
a
b
)
=
>
a
+
b
)
)
;
const
max_text_token_logprob
=
(
0
_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
max
)
(
logprobs
.
subarray
(
0
this
.
timestamp_begin
)
)
[
0
]
;
if
(
timestamp_logprob
>
max_text_token_logprob
)
{
logitsData
.
subarray
(
0
this
.
timestamp_begin
)
.
fill
(
-
Infinity
)
;
}
return
logits
;
}
}
class
NoRepeatNGramLogitsProcessor
extends
LogitsProcessor
{
constructor
(
no_repeat_ngram_size
)
{
super
(
)
;
this
.
no_repeat_ngram_size
=
no_repeat_ngram_size
;
}
getNgrams
(
prevInputIds
)
{
const
curLen
=
prevInputIds
.
length
;
const
ngrams
=
[
]
;
for
(
let
j
=
0
;
j
<
curLen
+
1
-
this
.
no_repeat_ngram_size
;
+
+
j
)
{
const
ngram
=
[
]
;
for
(
let
k
=
0
;
k
<
this
.
no_repeat_ngram_size
;
+
+
k
)
{
ngram
.
push
(
prevInputIds
[
j
+
k
]
)
;
}
ngrams
.
push
(
ngram
)
;
}
const
generatedNgram
=
new
Map
(
)
;
for
(
const
ngram
of
ngrams
)
{
const
prevNgram
=
ngram
.
slice
(
0
ngram
.
length
-
1
)
;
const
prevNgramKey
=
JSON
.
stringify
(
prevNgram
)
;
const
prevNgramValue
=
generatedNgram
.
get
(
prevNgramKey
)
?
?
[
]
;
prevNgramValue
.
push
(
ngram
[
ngram
.
length
-
1
]
)
;
generatedNgram
.
set
(
prevNgramKey
prevNgramValue
)
;
}
return
generatedNgram
;
}
getGeneratedNgrams
(
bannedNgrams
prevInputIds
)
{
const
ngramIdx
=
prevInputIds
.
slice
(
prevInputIds
.
length
+
1
-
this
.
no_repeat_ngram_size
prevInputIds
.
length
)
;
const
banned
=
bannedNgrams
.
get
(
JSON
.
stringify
(
ngramIdx
)
)
?
?
[
]
;
return
banned
;
}
calcBannedNgramTokens
(
prevInputIds
)
{
const
bannedTokens
=
[
]
;
if
(
prevInputIds
.
length
+
1
<
this
.
no_repeat_ngram_size
)
{
return
bannedTokens
;
}
else
{
const
generatedNgrams
=
this
.
getNgrams
(
prevInputIds
)
;
const
bannedTokens
=
this
.
getGeneratedNgrams
(
generatedNgrams
prevInputIds
)
;
return
bannedTokens
;
}
}
_call
(
input_ids
logits
)
{
const
bannedTokens
=
this
.
calcBannedNgramTokens
(
input_ids
)
;
for
(
const
token
of
bannedTokens
)
{
logits
.
data
[
token
]
=
-
Infinity
;
}
return
logits
;
}
}
class
RepetitionPenaltyLogitsProcessor
extends
LogitsProcessor
{
constructor
(
penalty
)
{
super
(
)
;
this
.
penalty
=
penalty
;
}
_call
(
input_ids
logits
)
{
for
(
const
input_id
of
input_ids
)
{
if
(
logits
.
data
[
input_id
]
<
0
)
{
logits
.
data
[
input_id
]
*
=
this
.
penalty
;
}
else
{
logits
.
data
[
input_id
]
/
=
this
.
penalty
;
}
}
return
logits
}
}
class
MinLengthLogitsProcessor
extends
LogitsProcessor
{
constructor
(
min_length
eos_token_id
)
{
super
(
)
;
this
.
min_length
=
min_length
;
this
.
eos_token_id
=
Array
.
isArray
(
eos_token_id
)
?
eos_token_id
:
[
eos_token_id
]
;
}
_call
(
input_ids
logits
)
{
if
(
input_ids
.
length
<
this
.
min_length
)
{
for
(
const
eos_token
of
this
.
eos_token_id
)
{
logits
.
data
[
eos_token
]
=
-
Infinity
;
}
}
return
logits
}
}
class
MinNewTokensLengthLogitsProcessor
extends
LogitsProcessor
{
constructor
(
prompt_length_to_skip
min_new_tokens
eos_token_id
)
{
super
(
)
;
this
.
prompt_length_to_skip
=
prompt_length_to_skip
;
this
.
min_new_tokens
=
min_new_tokens
;
this
.
eos_token_id
=
Array
.
isArray
(
eos_token_id
)
?
eos_token_id
:
[
eos_token_id
]
;
}
_call
(
input_ids
logits
)
{
const
new_tokens_length
=
input_ids
.
length
-
this
.
prompt_length_to_skip
;
if
(
new_tokens_length
<
this
.
min_new_tokens
)
{
for
(
const
eos_token
of
this
.
eos_token_id
)
{
logits
.
data
[
eos_token
]
=
-
Infinity
;
}
}
return
logits
}
}
class
NoBadWordsLogitsProcessor
extends
LogitsProcessor
{
constructor
(
bad_words_ids
eos_token_id
)
{
super
(
)
;
this
.
bad_words_ids
=
bad_words_ids
;
this
.
eos_token_id
=
Array
.
isArray
(
eos_token_id
)
?
eos_token_id
:
[
eos_token_id
]
;
}
_call
(
input_ids
logits
)
{
for
(
const
bad_word_ids
of
this
.
bad_words_ids
)
{
let
mark
=
true
;
for
(
let
i
=
1
;
i
<
=
bad_word_ids
.
length
-
1
&
&
bad_word_ids
.
length
<
input_ids
.
length
;
+
+
i
)
{
if
(
bad_word_ids
.
at
(
-
i
-
1
)
!
=
=
input_ids
.
at
(
-
i
)
)
{
mark
=
false
;
break
;
}
}
if
(
mark
)
{
logits
.
data
[
bad_word_ids
.
at
(
-
1
)
]
=
-
Infinity
;
}
}
return
logits
}
}
const
GenerationConfig
=
(
class
{
constructor
(
kwargs
=
{
}
)
{
this
.
max_length
=
kwargs
.
max_length
?
?
20
;
this
.
max_new_tokens
=
kwargs
.
max_new_tokens
?
?
null
;
this
.
min_length
=
kwargs
.
min_length
?
?
0
;
this
.
min_new_tokens
=
kwargs
.
min_new_tokens
?
?
null
;
this
.
early_stopping
=
kwargs
.
early_stopping
?
?
false
;
this
.
max_time
=
kwargs
.
max_time
?
?
null
;
this
.
do_sample
=
kwargs
.
do_sample
?
?
false
;
this
.
num_beams
=
kwargs
.
num_beams
?
?
1
;
this
.
num_beam_groups
=
kwargs
.
num_beam_groups
?
?
1
;
this
.
penalty_alpha
=
kwargs
.
penalty_alpha
?
?
null
;
this
.
use_cache
=
kwargs
.
use_cache
?
?
true
;
this
.
temperature
=
kwargs
.
temperature
?
?
1
.
0
;
this
.
top_k
=
kwargs
.
top_k
?
?
50
;
this
.
top_p
=
kwargs
.
top_p
?
?
1
.
0
;
this
.
typical_p
=
kwargs
.
typical_p
?
?
1
.
0
;
this
.
epsilon_cutoff
=
kwargs
.
epsilon_cutoff
?
?
0
.
0
;
this
.
eta_cutoff
=
kwargs
.
eta_cutoff
?
?
0
.
0
;
this
.
diversity_penalty
=
kwargs
.
diversity_penalty
?
?
0
.
0
;
this
.
repetition_penalty
=
kwargs
.
repetition_penalty
?
?
1
.
0
;
this
.
encoder_repetition_penalty
=
kwargs
.
encoder_repetition_penalty
?
?
1
.
0
;
this
.
length_penalty
=
kwargs
.
length_penalty
?
?
1
.
0
;
this
.
no_repeat_ngram_size
=
kwargs
.
no_repeat_ngram_size
?
?
0
;
this
.
bad_words_ids
=
kwargs
.
bad_words_ids
?
?
null
;
this
.
force_words_ids
=
kwargs
.
force_words_ids
?
?
null
;
this
.
renormalize_logits
=
kwargs
.
renormalize_logits
?
?
false
;
this
.
constraints
=
kwargs
.
constraints
?
?
null
;
this
.
forced_bos_token_id
=
kwargs
.
forced_bos_token_id
?
?
null
;
this
.
forced_eos_token_id
=
kwargs
.
forced_eos_token_id
?
?
null
;
this
.
remove_invalid_values
=
kwargs
.
remove_invalid_values
?
?
false
;
this
.
exponential_decay_length_penalty
=
kwargs
.
exponential_decay_length_penalty
?
?
null
;
this
.
suppress_tokens
=
kwargs
.
suppress_tokens
?
?
null
;
this
.
begin_suppress_tokens
=
kwargs
.
begin_suppress_tokens
?
?
null
;
this
.
forced_decoder_ids
=
kwargs
.
forced_decoder_ids
?
?
null
;
this
.
num_return_sequences
=
kwargs
.
num_return_sequences
?
?
1
;
this
.
output_attentions
=
kwargs
.
output_attentions
?
?
false
;
this
.
output_hidden_states
=
kwargs
.
output_hidden_states
?
?
false
;
this
.
output_scores
=
kwargs
.
output_scores
?
?
false
;
this
.
return_dict_in_generate
=
kwargs
.
return_dict_in_generate
?
?
false
;
this
.
pad_token_id
=
kwargs
.
pad_token_id
?
?
null
;
this
.
bos_token_id
=
kwargs
.
bos_token_id
?
?
null
;
this
.
eos_token_id
=
kwargs
.
eos_token_id
?
?
null
;
this
.
encoder_no_repeat_ngram_size
=
kwargs
.
encoder_no_repeat_ngram_size
?
?
0
;
this
.
decoder_start_token_id
=
kwargs
.
decoder_start_token_id
?
?
null
;
this
.
generation_kwargs
=
kwargs
.
generation_kwargs
?
?
{
}
;
}
}
)
;
class
Sampler
extends
_core_js__WEBPACK_IMPORTED_MODULE_1__
.
Callable
{
constructor
(
generation_config
)
{
super
(
)
;
this
.
generation_config
=
generation_config
;
}
_call
(
logits
index
=
-
1
)
{
return
this
.
sample
(
logits
index
)
;
}
sample
(
logits
index
)
{
throw
Error
(
"
sample
should
be
implemented
in
subclasses
.
"
)
}
getLogits
(
logits
index
)
{
let
vocabSize
=
logits
.
dims
.
at
(
-
1
)
;
let
logs
=
(
logits
.
data
)
;
if
(
index
=
=
=
-
1
)
{
logs
=
logs
.
slice
(
-
vocabSize
)
;
}
else
{
let
startIndex
=
index
*
vocabSize
;
logs
=
logs
.
slice
(
startIndex
startIndex
+
vocabSize
)
;
}
if
(
this
.
generation_config
.
temperature
>
0
)
{
logs
=
logs
.
map
(
x
=
>
x
/
this
.
generation_config
.
temperature
)
}
return
logs
;
}
randomSelect
(
probabilities
)
{
let
sumProbabilities
=
probabilities
.
reduce
(
(
acc
curr
)
=
>
acc
+
curr
0
)
;
let
r
=
Math
.
random
(
)
*
sumProbabilities
;
for
(
let
i
=
0
;
i
<
probabilities
.
length
;
+
+
i
)
{
r
-
=
probabilities
[
i
]
;
if
(
r
<
=
0
)
{
return
i
;
}
}
return
0
;
}
static
getSampler
(
generation_config
)
{
if
(
generation_config
.
do_sample
)
{
return
new
MultinomialSampler
(
generation_config
)
;
}
else
if
(
generation_config
.
num_beams
>
1
)
{
return
new
BeamSearchSampler
(
generation_config
)
;
}
else
{
if
(
generation_config
.
num_return_sequences
>
1
)
{
throw
Error
(
num_return_sequences
has
to
be
1
when
doing
greedy
search
but
is
{
generation_config
.
num_return_sequences
}
.
)
}
return
new
GreedySampler
(
generation_config
)
;
}
}
}
class
GreedySampler
extends
Sampler
{
sample
(
logits
index
=
-
1
)
{
let
logs
=
this
.
getLogits
(
logits
index
)
;
let
argmax
=
(
0
_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
max
)
(
logs
)
[
1
]
;
return
[
[
argmax
0
]
]
;
}
}
class
MultinomialSampler
extends
Sampler
{
sample
(
logits
index
=
-
1
)
{
let
k
=
logits
.
dims
.
at
(
-
1
)
;
if
(
this
.
generation_config
.
top_k
>
0
)
{
k
=
Math
.
min
(
this
.
generation_config
.
top_k
k
)
;
}
const
logs
=
this
.
getLogits
(
logits
index
)
;
const
topLogits
=
(
0
_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
getTopItems
)
(
logs
k
)
;
const
probabilities
=
(
0
_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
softmax
)
(
topLogits
.
map
(
x
=
>
x
[
1
]
)
)
;
return
Array
.
from
(
{
length
:
this
.
generation_config
.
num_beams
}
(
)
=
>
{
const
sampledIndex
=
this
.
randomSelect
(
probabilities
)
;
return
[
topLogits
[
sampledIndex
]
[
0
]
Math
.
log
(
probabilities
[
sampledIndex
]
)
]
;
}
)
;
}
}
class
BeamSearchSampler
extends
Sampler
{
sample
(
logits
index
=
-
1
)
{
let
k
=
logits
.
dims
.
at
(
-
1
)
;
if
(
this
.
generation_config
.
top_k
>
0
)
{
k
=
Math
.
min
(
this
.
generation_config
.
top_k
k
)
;
}
const
logs
=
this
.
getLogits
(
logits
index
)
;
const
topLogits
=
(
0
_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
getTopItems
)
(
logs
k
)
;
const
probabilities
=
(
0
_maths_js__WEBPACK_IMPORTED_MODULE_2__
.
softmax
)
(
topLogits
.
map
(
x
=
>
x
[
1
]
)
)
;
return
Array
.
from
(
{
length
:
this
.
generation_config
.
num_beams
}
(
_
i
)
=
>
{
return
[
topLogits
[
i
]
[
0
]
Math
.
log
(
probabilities
[
i
]
)
]
;
}
)
;
}
}
}
)
"
.
/
src
/
utils
/
hub
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
getFile
"
:
(
)
=
>
(
getFile
)
"
getModelFile
"
:
(
)
=
>
(
getModelFile
)
"
getModelJSON
"
:
(
)
=
>
(
getModelJSON
)
}
)
;
var
fs__WEBPACK_IMPORTED_MODULE_0__
=
__webpack_require__
(
"
?
7a2c
"
)
;
var
path__WEBPACK_IMPORTED_MODULE_1__
=
__webpack_require__
(
"
?
a42a
"
)
;
var
stream_web__WEBPACK_IMPORTED_MODULE_2__
=
__webpack_require__
(
"
?
e65c
"
)
;
var
_env_js__WEBPACK_IMPORTED_MODULE_3__
=
__webpack_require__
(
"
.
/
src
/
env
.
js
"
)
;
var
_core_js__WEBPACK_IMPORTED_MODULE_4__
=
__webpack_require__
(
"
.
/
src
/
utils
/
core
.
js
"
)
;
if
(
!
globalThis
.
ReadableStream
)
{
globalThis
.
ReadableStream
=
stream_web__WEBPACK_IMPORTED_MODULE_2__
.
ReadableStream
;
}
class
FileResponse
{
_CONTENT_TYPE_MAP
=
{
'
txt
'
:
'
text
/
plain
'
'
html
'
:
'
text
/
html
'
'
css
'
:
'
text
/
css
'
'
js
'
:
'
text
/
javascript
'
'
json
'
:
'
application
/
json
'
'
png
'
:
'
image
/
png
'
'
jpg
'
:
'
image
/
jpeg
'
'
jpeg
'
:
'
image
/
jpeg
'
'
gif
'
:
'
image
/
gif
'
}
constructor
(
filePath
)
{
this
.
filePath
=
filePath
;
this
.
headers
=
new
Headers
(
)
;
this
.
exists
=
fs__WEBPACK_IMPORTED_MODULE_0__
.
existsSync
(
filePath
)
;
if
(
this
.
exists
)
{
this
.
status
=
200
;
this
.
statusText
=
'
OK
'
;
let
stats
=
fs__WEBPACK_IMPORTED_MODULE_0__
.
statSync
(
filePath
)
;
this
.
headers
.
set
(
'
content
-
length
'
stats
.
size
.
toString
(
)
)
;
this
.
updateContentType
(
)
;
let
self
=
this
;
this
.
body
=
new
ReadableStream
(
{
start
(
controller
)
{
self
.
arrayBuffer
(
)
.
then
(
buffer
=
>
{
controller
.
enqueue
(
new
Uint8Array
(
buffer
)
)
;
controller
.
close
(
)
;
}
)
}
}
)
;
}
else
{
this
.
status
=
404
;
this
.
statusText
=
'
Not
Found
'
;
this
.
body
=
null
;
}
}
updateContentType
(
)
{
const
extension
=
this
.
filePath
.
toString
(
)
.
split
(
'
.
'
)
.
pop
(
)
.
toLowerCase
(
)
;
this
.
headers
.
set
(
'
content
-
type
'
this
.
_CONTENT_TYPE_MAP
[
extension
]
?
?
'
application
/
octet
-
stream
'
)
;
}
clone
(
)
{
let
response
=
new
FileResponse
(
this
.
filePath
)
;
response
.
exists
=
this
.
exists
;
response
.
status
=
this
.
status
;
response
.
statusText
=
this
.
statusText
;
response
.
headers
=
new
Headers
(
this
.
headers
)
;
return
response
;
}
async
arrayBuffer
(
)
{
const
data
=
await
fs__WEBPACK_IMPORTED_MODULE_0__
.
promises
.
readFile
(
this
.
filePath
)
;
return
data
.
buffer
;
}
async
blob
(
)
{
const
data
=
await
fs__WEBPACK_IMPORTED_MODULE_0__
.
promises
.
readFile
(
this
.
filePath
)
;
return
new
Blob
(
[
data
]
{
type
:
this
.
headers
.
get
(
'
content
-
type
'
)
}
)
;
}
async
text
(
)
{
const
data
=
await
fs__WEBPACK_IMPORTED_MODULE_0__
.
promises
.
readFile
(
this
.
filePath
'
utf8
'
)
;
return
data
;
}
async
json
(
)
{
return
JSON
.
parse
(
await
this
.
text
(
)
)
;
}
}
function
isValidHttpUrl
(
string
validHosts
=
null
)
{
let
url
;
try
{
url
=
new
URL
(
string
)
;
}
catch
(
_
)
{
return
false
;
}
if
(
validHosts
&
&
!
validHosts
.
includes
(
url
.
hostname
)
)
{
return
false
;
}
return
url
.
protocol
=
=
=
"
http
:
"
|
|
url
.
protocol
=
=
=
"
https
:
"
;
}
async
function
getFile
(
urlOrPath
)
{
if
(
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
useFS
&
&
!
isValidHttpUrl
(
urlOrPath
)
)
{
return
new
FileResponse
(
urlOrPath
)
;
}
else
if
(
typeof
process
!
=
=
'
undefined
'
&
&
process
?
.
release
?
.
name
=
=
=
'
node
'
)
{
const
IS_CI
=
!
!
process
.
env
?
.
TESTING_REMOTELY
;
const
version
=
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
version
;
const
headers
=
new
Headers
(
)
;
headers
.
set
(
'
User
-
Agent
'
transformers
.
js
/
{
version
}
;
is_ci
/
{
IS_CI
}
;
)
;
const
isHFURL
=
isValidHttpUrl
(
urlOrPath
[
'
huggingface
.
co
'
'
hf
.
co
'
]
)
;
if
(
isHFURL
)
{
const
token
=
process
.
env
?
.
HF_TOKEN
?
?
process
.
env
?
.
HF_ACCESS_TOKEN
;
if
(
token
)
{
headers
.
set
(
'
Authorization
'
Bearer
{
token
}
)
;
}
}
return
fetch
(
urlOrPath
{
headers
}
)
;
}
else
{
return
fetch
(
urlOrPath
)
;
}
}
const
ERROR_MAPPING
=
{
400
:
'
Bad
request
error
occurred
while
trying
to
load
file
'
401
:
'
Unauthorized
access
to
file
'
403
:
'
Forbidden
access
to
file
'
404
:
'
Could
not
locate
file
'
408
:
'
Request
timeout
error
occurred
while
trying
to
load
file
'
500
:
'
Internal
server
error
error
occurred
while
trying
to
load
file
'
502
:
'
Bad
gateway
error
occurred
while
trying
to
load
file
'
503
:
'
Service
unavailable
error
occurred
while
trying
to
load
file
'
504
:
'
Gateway
timeout
error
occurred
while
trying
to
load
file
'
}
function
handleError
(
status
remoteURL
fatal
)
{
if
(
!
fatal
)
{
return
null
;
}
const
message
=
ERROR_MAPPING
[
status
]
?
?
Error
(
{
status
}
)
occurred
while
trying
to
load
file
;
throw
Error
(
{
message
}
:
"
{
remoteURL
}
"
.
)
;
}
class
FileCache
{
constructor
(
path
)
{
this
.
path
=
path
;
}
async
match
(
request
)
{
let
filePath
=
path__WEBPACK_IMPORTED_MODULE_1__
.
join
(
this
.
path
request
)
;
let
file
=
new
FileResponse
(
filePath
)
;
if
(
file
.
exists
)
{
return
file
;
}
else
{
return
undefined
;
}
}
async
put
(
request
response
)
{
const
buffer
=
Buffer
.
from
(
await
response
.
arrayBuffer
(
)
)
;
let
outputPath
=
path__WEBPACK_IMPORTED_MODULE_1__
.
join
(
this
.
path
request
)
;
try
{
await
fs__WEBPACK_IMPORTED_MODULE_0__
.
promises
.
mkdir
(
path__WEBPACK_IMPORTED_MODULE_1__
.
dirname
(
outputPath
)
{
recursive
:
true
}
)
;
await
fs__WEBPACK_IMPORTED_MODULE_0__
.
promises
.
writeFile
(
outputPath
buffer
)
;
}
catch
(
err
)
{
console
.
warn
(
'
An
error
occurred
while
writing
the
file
to
cache
:
'
err
)
}
}
}
async
function
tryCache
(
cache
.
.
.
names
)
{
for
(
let
name
of
names
)
{
try
{
let
result
=
await
cache
.
match
(
name
)
;
if
(
result
)
return
result
;
}
catch
(
e
)
{
continue
;
}
}
return
undefined
;
}
async
function
getModelFile
(
path_or_repo_id
filename
fatal
=
true
options
=
{
}
)
{
if
(
!
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
allowLocalModels
)
{
if
(
options
.
local_files_only
)
{
throw
Error
(
"
Invalid
configuration
detected
:
local
models
are
disabled
(
env
.
allowLocalModels
=
false
)
but
you
have
requested
to
only
use
local
models
(
local_files_only
=
true
)
.
"
)
}
else
if
(
!
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
allowRemoteModels
)
{
throw
Error
(
"
Invalid
configuration
detected
:
both
local
and
remote
models
are
disabled
.
Fix
by
setting
env
.
allowLocalModels
or
env
.
allowRemoteModels
to
true
.
"
)
}
}
(
0
_core_js__WEBPACK_IMPORTED_MODULE_4__
.
dispatchCallback
)
(
options
.
progress_callback
{
status
:
'
initiate
'
name
:
path_or_repo_id
file
:
filename
}
)
let
cache
;
if
(
!
cache
&
&
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
useBrowserCache
)
{
if
(
typeof
caches
=
=
=
'
undefined
'
)
{
throw
Error
(
'
Browser
cache
is
not
available
in
this
environment
.
'
)
}
try
{
cache
=
await
caches
.
open
(
'
transformers
-
cache
'
)
;
}
catch
(
e
)
{
console
.
warn
(
'
An
error
occurred
while
opening
the
browser
cache
:
'
e
)
;
}
}
if
(
!
cache
&
&
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
useFSCache
)
{
cache
=
new
FileCache
(
options
.
cache_dir
?
?
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
cacheDir
)
;
}
if
(
!
cache
&
&
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
useCustomCache
)
{
if
(
!
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
customCache
)
{
throw
Error
(
'
env
.
useCustomCache
=
true
but
env
.
customCache
is
not
defined
.
'
)
}
if
(
!
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
customCache
.
match
|
|
!
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
customCache
.
put
)
{
throw
new
Error
(
"
env
.
customCache
must
be
an
object
which
implements
the
match
and
put
functions
of
the
Web
Cache
API
.
"
+
"
For
more
information
see
https
:
/
/
developer
.
mozilla
.
org
/
en
-
US
/
docs
/
Web
/
API
/
Cache
"
)
}
cache
=
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
customCache
;
}
const
revision
=
options
.
revision
?
?
'
main
'
;
let
requestURL
=
pathJoin
(
path_or_repo_id
filename
)
;
let
localPath
=
pathJoin
(
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
localModelPath
requestURL
)
;
let
remoteURL
=
pathJoin
(
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
remoteHost
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
remotePathTemplate
.
replaceAll
(
'
{
model
}
'
path_or_repo_id
)
.
replaceAll
(
'
{
revision
}
'
encodeURIComponent
(
revision
)
)
filename
)
;
let
fsCacheKey
=
revision
=
=
=
'
main
'
?
requestURL
:
pathJoin
(
path_or_repo_id
revision
filename
)
;
let
cacheKey
;
let
proposedCacheKey
=
cache
instanceof
FileCache
?
fsCacheKey
:
remoteURL
;
let
toCacheResponse
=
false
;
let
response
;
if
(
cache
)
{
response
=
await
tryCache
(
cache
localPath
proposedCacheKey
)
;
}
const
cacheHit
=
response
!
=
=
undefined
;
if
(
response
=
=
=
undefined
)
{
if
(
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
allowLocalModels
)
{
const
isURL
=
isValidHttpUrl
(
requestURL
)
;
if
(
!
isURL
)
{
try
{
response
=
await
getFile
(
localPath
)
;
cacheKey
=
localPath
;
}
catch
(
e
)
{
console
.
warn
(
Unable
to
load
from
local
path
"
{
localPath
}
"
:
"
{
e
}
"
)
;
}
}
else
if
(
options
.
local_files_only
)
{
throw
new
Error
(
\
local_files_only
=
true
\
but
attempted
to
load
a
remote
file
from
:
{
requestURL
}
.
)
;
}
else
if
(
!
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
allowRemoteModels
)
{
throw
new
Error
(
\
env
.
allowRemoteModels
=
false
\
but
attempted
to
load
a
remote
file
from
:
{
requestURL
}
.
)
;
}
}
if
(
response
=
=
=
undefined
|
|
response
.
status
=
=
=
404
)
{
if
(
options
.
local_files_only
|
|
!
_env_js__WEBPACK_IMPORTED_MODULE_3__
.
env
.
allowRemoteModels
)
{
if
(
fatal
)
{
throw
Error
(
\
local_files_only
=
true
\
or
\
env
.
allowRemoteModels
=
false
\
and
file
was
not
found
locally
at
"
{
localPath
}
"
.
)
;
}
else
{
return
null
;
}
}
response
=
await
getFile
(
remoteURL
)
;
if
(
response
.
status
!
=
=
200
)
{
return
handleError
(
response
.
status
remoteURL
fatal
)
;
}
cacheKey
=
proposedCacheKey
;
}
toCacheResponse
=
cache
&
&
typeof
Response
!
=
=
'
undefined
'
&
&
response
instanceof
Response
&
&
response
.
status
=
=
=
200
}
(
0
_core_js__WEBPACK_IMPORTED_MODULE_4__
.
dispatchCallback
)
(
options
.
progress_callback
{
status
:
'
download
'
name
:
path_or_repo_id
file
:
filename
}
)
const
progressInfo
=
{
status
:
'
progress
'
name
:
path_or_repo_id
file
:
filename
}
let
buffer
;
if
(
!
options
.
progress_callback
)
{
buffer
=
new
Uint8Array
(
await
response
.
arrayBuffer
(
)
)
;
}
else
if
(
cacheHit
&
&
typeof
navigator
!
=
=
'
undefined
'
&
&
/
firefox
/
i
.
test
(
navigator
.
userAgent
)
)
{
buffer
=
new
Uint8Array
(
await
response
.
arrayBuffer
(
)
)
;
(
0
_core_js__WEBPACK_IMPORTED_MODULE_4__
.
dispatchCallback
)
(
options
.
progress_callback
{
.
.
.
progressInfo
progress
:
100
loaded
:
buffer
.
length
total
:
buffer
.
length
}
)
}
else
{
buffer
=
await
readResponse
(
response
data
=
>
{
(
0
_core_js__WEBPACK_IMPORTED_MODULE_4__
.
dispatchCallback
)
(
options
.
progress_callback
{
.
.
.
progressInfo
.
.
.
data
}
)
}
)
}
if
(
toCacheResponse
&
&
cacheKey
&
&
(
await
cache
.
match
(
cacheKey
)
=
=
=
undefined
)
)
{
await
cache
.
put
(
cacheKey
new
Response
(
buffer
{
headers
:
response
.
headers
}
)
)
.
catch
(
err
=
>
{
console
.
warn
(
Unable
to
add
response
to
browser
cache
:
{
err
}
.
)
;
}
)
;
}
(
0
_core_js__WEBPACK_IMPORTED_MODULE_4__
.
dispatchCallback
)
(
options
.
progress_callback
{
status
:
'
done
'
name
:
path_or_repo_id
file
:
filename
}
)
;
return
buffer
;
}
async
function
getModelJSON
(
modelPath
fileName
fatal
=
true
options
=
{
}
)
{
let
buffer
=
await
getModelFile
(
modelPath
fileName
fatal
options
)
;
if
(
buffer
=
=
=
null
)
{
return
{
}
}
let
decoder
=
new
TextDecoder
(
'
utf
-
8
'
)
;
let
jsonData
=
decoder
.
decode
(
buffer
)
;
return
JSON
.
parse
(
jsonData
)
;
}
async
function
readResponse
(
response
progress_callback
)
{
const
contentLength
=
response
.
headers
.
get
(
'
Content
-
Length
'
)
;
if
(
contentLength
=
=
=
null
)
{
console
.
warn
(
'
Unable
to
determine
content
-
length
from
response
headers
.
Will
expand
buffer
when
needed
.
'
)
}
let
total
=
parseInt
(
contentLength
?
?
'
0
'
)
;
let
buffer
=
new
Uint8Array
(
total
)
;
let
loaded
=
0
;
const
reader
=
response
.
body
.
getReader
(
)
;
async
function
read
(
)
{
const
{
done
value
}
=
await
reader
.
read
(
)
;
if
(
done
)
return
;
let
newLoaded
=
loaded
+
value
.
length
;
if
(
newLoaded
>
total
)
{
total
=
newLoaded
;
let
newBuffer
=
new
Uint8Array
(
total
)
;
newBuffer
.
set
(
buffer
)
;
buffer
=
newBuffer
;
}
buffer
.
set
(
value
loaded
)
loaded
=
newLoaded
;
const
progress
=
(
loaded
/
total
)
*
100
;
progress_callback
(
{
progress
:
progress
loaded
:
loaded
total
:
total
}
)
return
read
(
)
;
}
await
read
(
)
;
return
buffer
;
}
function
pathJoin
(
.
.
.
parts
)
{
parts
=
parts
.
map
(
(
part
index
)
=
>
{
if
(
index
)
{
part
=
part
.
replace
(
new
RegExp
(
'
^
/
'
)
'
'
)
;
}
if
(
index
!
=
=
parts
.
length
-
1
)
{
part
=
part
.
replace
(
new
RegExp
(
'
/
'
)
'
'
)
;
}
return
part
;
}
)
return
parts
.
join
(
'
/
'
)
;
}
}
)
"
.
/
src
/
utils
/
image
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
RawImage
"
:
(
)
=
>
(
RawImage
)
}
)
;
var
_hub_js__WEBPACK_IMPORTED_MODULE_0__
=
__webpack_require__
(
"
.
/
src
/
utils
/
hub
.
js
"
)
;
var
_env_js__WEBPACK_IMPORTED_MODULE_1__
=
__webpack_require__
(
"
.
/
src
/
env
.
js
"
)
;
var
_tensor_js__WEBPACK_IMPORTED_MODULE_2__
=
__webpack_require__
(
"
.
/
src
/
utils
/
tensor
.
js
"
)
;
var
sharp__WEBPACK_IMPORTED_MODULE_3__
=
__webpack_require__
(
"
?
2b25
"
)
;
const
BROWSER_ENV
=
typeof
self
!
=
=
'
undefined
'
;
const
WEBWORKER_ENV
=
BROWSER_ENV
&
&
self
.
constructor
.
name
=
=
=
'
DedicatedWorkerGlobalScope
'
;
let
createCanvasFunction
;
let
ImageDataClass
;
let
loadImageFunction
;
if
(
BROWSER_ENV
)
{
createCanvasFunction
=
(
width
height
)
=
>
{
if
(
!
self
.
OffscreenCanvas
)
{
throw
new
Error
(
'
OffscreenCanvas
not
supported
by
this
browser
.
'
)
;
}
return
new
self
.
OffscreenCanvas
(
width
height
)
}
;
loadImageFunction
=
self
.
createImageBitmap
;
ImageDataClass
=
self
.
ImageData
;
}
else
if
(
sharp__WEBPACK_IMPORTED_MODULE_3__
)
{
loadImageFunction
=
async
(
img
)
=
>
{
const
metadata
=
await
img
.
metadata
(
)
;
const
rawChannels
=
metadata
.
channels
;
let
{
data
info
}
=
await
img
.
raw
(
)
.
toBuffer
(
{
resolveWithObject
:
true
}
)
;
const
newImage
=
new
RawImage
(
new
Uint8ClampedArray
(
data
)
info
.
width
info
.
height
info
.
channels
)
;
if
(
rawChannels
!
=
=
undefined
&
&
rawChannels
!
=
=
info
.
channels
)
{
newImage
.
convert
(
rawChannels
)
;
}
return
newImage
;
}
}
else
{
throw
new
Error
(
'
Unable
to
load
image
processing
library
.
'
)
;
}
const
RESAMPLING_MAPPING
=
{
0
:
'
nearest
'
1
:
'
lanczos
'
2
:
'
bilinear
'
3
:
'
bicubic
'
4
:
'
box
'
5
:
'
hamming
'
}
const
CONTENT_TYPE_MAP
=
new
Map
(
[
[
'
png
'
'
image
/
png
'
]
[
'
jpg
'
'
image
/
jpeg
'
]
[
'
jpeg
'
'
image
/
jpeg
'
]
[
'
gif
'
'
image
/
gif
'
]
]
)
;
class
RawImage
{
constructor
(
data
width
height
channels
)
{
this
.
data
=
data
;
this
.
width
=
width
;
this
.
height
=
height
;
this
.
channels
=
channels
;
}
get
size
(
)
{
return
[
this
.
width
this
.
height
]
;
}
static
async
read
(
input
)
{
if
(
input
instanceof
RawImage
)
{
return
input
;
}
else
if
(
typeof
input
=
=
=
'
string
'
|
|
input
instanceof
URL
)
{
return
await
this
.
fromURL
(
input
)
;
}
else
{
throw
new
Error
(
Unsupported
input
type
:
{
typeof
input
}
)
;
}
}
static
async
fromURL
(
url
)
{
let
response
=
await
(
0
_hub_js__WEBPACK_IMPORTED_MODULE_0__
.
getFile
)
(
url
)
;
if
(
response
.
status
!
=
=
200
)
{
throw
new
Error
(
Unable
to
read
image
from
"
{
url
}
"
(
{
response
.
status
}
{
response
.
statusText
}
)
)
;
}
let
blob
=
await
response
.
blob
(
)
;
return
this
.
fromBlob
(
blob
)
;
}
static
async
fromBlob
(
blob
)
{
if
(
BROWSER_ENV
)
{
let
img
=
await
loadImageFunction
(
blob
)
;
const
ctx
=
createCanvasFunction
(
img
.
width
img
.
height
)
.
getContext
(
'
2d
'
)
;
ctx
.
drawImage
(
img
0
0
)
;
return
new
this
(
ctx
.
getImageData
(
0
0
img
.
width
img
.
height
)
.
data
img
.
width
img
.
height
4
)
;
}
else
{
let
img
=
sharp__WEBPACK_IMPORTED_MODULE_3__
(
await
blob
.
arrayBuffer
(
)
)
;
return
await
loadImageFunction
(
img
)
;
}
}
static
fromTensor
(
tensor
channel_format
=
'
CHW
'
)
{
if
(
tensor
.
dims
.
length
!
=
=
3
)
{
throw
new
Error
(
Tensor
should
have
3
dimensions
but
has
{
tensor
.
dims
.
length
}
dimensions
.
)
;
}
if
(
channel_format
=
=
=
'
CHW
'
)
{
tensor
=
tensor
.
transpose
(
1
2
0
)
;
}
else
if
(
channel_format
=
=
=
'
HWC
'
)
{
}
else
{
throw
new
Error
(
Unsupported
channel
format
:
{
channel_format
}
)
;
}
if
(
!
(
tensor
.
data
instanceof
Uint8ClampedArray
|
|
tensor
.
data
instanceof
Uint8Array
)
)
{
throw
new
Error
(
Unsupported
tensor
type
:
{
tensor
.
type
}
)
;
}
switch
(
tensor
.
dims
[
2
]
)
{
case
1
:
case
2
:
case
3
:
case
4
:
return
new
RawImage
(
tensor
.
data
tensor
.
dims
[
1
]
tensor
.
dims
[
0
]
tensor
.
dims
[
2
]
)
;
default
:
throw
new
Error
(
Unsupported
number
of
channels
:
{
tensor
.
dims
[
2
]
}
)
;
}
}
grayscale
(
)
{
if
(
this
.
channels
=
=
=
1
)
{
return
this
;
}
let
newData
=
new
Uint8ClampedArray
(
this
.
width
*
this
.
height
*
1
)
;
switch
(
this
.
channels
)
{
case
3
:
case
4
:
for
(
let
i
=
0
offset
=
0
;
i
<
this
.
data
.
length
;
i
+
=
this
.
channels
)
{
const
red
=
this
.
data
[
i
]
;
const
green
=
this
.
data
[
i
+
1
]
;
const
blue
=
this
.
data
[
i
+
2
]
;
newData
[
offset
+
+
]
=
Math
.
round
(
0
.
2989
*
red
+
0
.
5870
*
green
+
0
.
1140
*
blue
)
;
}
break
;
default
:
throw
new
Error
(
Conversion
failed
due
to
unsupported
number
of
channels
:
{
this
.
channels
}
)
;
}
return
this
.
_update
(
newData
this
.
width
this
.
height
1
)
;
}
rgb
(
)
{
if
(
this
.
channels
=
=
=
3
)
{
return
this
;
}
let
newData
=
new
Uint8ClampedArray
(
this
.
width
*
this
.
height
*
3
)
;
switch
(
this
.
channels
)
{
case
1
:
for
(
let
i
=
0
offset
=
0
;
i
<
this
.
data
.
length
;
+
+
i
)
{
newData
[
offset
+
+
]
=
this
.
data
[
i
]
;
newData
[
offset
+
+
]
=
this
.
data
[
i
]
;
newData
[
offset
+
+
]
=
this
.
data
[
i
]
;
}
break
;
case
4
:
for
(
let
i
=
0
offset
=
0
;
i
<
this
.
data
.
length
;
i
+
=
4
)
{
newData
[
offset
+
+
]
=
this
.
data
[
i
]
;
newData
[
offset
+
+
]
=
this
.
data
[
i
+
1
]
;
newData
[
offset
+
+
]
=
this
.
data
[
i
+
2
]
;
}
break
;
default
:
throw
new
Error
(
Conversion
failed
due
to
unsupported
number
of
channels
:
{
this
.
channels
}
)
;
}
return
this
.
_update
(
newData
this
.
width
this
.
height
3
)
;
}
rgba
(
)
{
if
(
this
.
channels
=
=
=
4
)
{
return
this
;
}
let
newData
=
new
Uint8ClampedArray
(
this
.
width
*
this
.
height
*
4
)
;
switch
(
this
.
channels
)
{
case
1
:
for
(
let
i
=
0
offset
=
0
;
i
<
this
.
data
.
length
;
+
+
i
)
{
newData
[
offset
+
+
]
=
this
.
data
[
i
]
;
newData
[
offset
+
+
]
=
this
.
data
[
i
]
;
newData
[
offset
+
+
]
=
this
.
data
[
i
]
;
newData
[
offset
+
+
]
=
255
;
}
break
;
case
3
:
for
(
let
i
=
0
offset
=
0
;
i
<
this
.
data
.
length
;
i
+
=
3
)
{
newData
[
offset
+
+
]
=
this
.
data
[
i
]
;
newData
[
offset
+
+
]
=
this
.
data
[
i
+
1
]
;
newData
[
offset
+
+
]
=
this
.
data
[
i
+
2
]
;
newData
[
offset
+
+
]
=
255
;
}
break
;
default
:
throw
new
Error
(
Conversion
failed
due
to
unsupported
number
of
channels
:
{
this
.
channels
}
)
;
}
return
this
.
_update
(
newData
this
.
width
this
.
height
4
)
;
}
async
resize
(
width
height
{
resample
=
2
}
=
{
}
)
{
let
resampleMethod
=
RESAMPLING_MAPPING
[
resample
]
?
?
resample
;
if
(
BROWSER_ENV
)
{
let
numChannels
=
this
.
channels
;
let
canvas
=
this
.
toCanvas
(
)
;
const
ctx
=
createCanvasFunction
(
width
height
)
.
getContext
(
'
2d
'
)
;
ctx
.
drawImage
(
canvas
0
0
width
height
)
;
let
resizedImage
=
new
RawImage
(
ctx
.
getImageData
(
0
0
width
height
)
.
data
width
height
4
)
;
return
resizedImage
.
convert
(
numChannels
)
;
}
else
{
let
img
=
this
.
toSharp
(
)
;
switch
(
resampleMethod
)
{
case
'
box
'
:
case
'
hamming
'
:
if
(
resampleMethod
=
=
=
'
box
'
|
|
resampleMethod
=
=
=
'
hamming
'
)
{
console
.
warn
(
Resampling
method
{
resampleMethod
}
is
not
yet
supported
.
Using
bilinear
instead
.
)
;
resampleMethod
=
'
bilinear
'
;
}
case
'
nearest
'
:
case
'
bilinear
'
:
case
'
bicubic
'
:
img
=
img
.
affine
(
[
width
/
this
.
width
0
0
height
/
this
.
height
]
{
interpolator
:
resampleMethod
}
)
;
break
;
case
'
lanczos
'
:
img
=
img
.
resize
(
{
width
height
fit
:
'
fill
'
kernel
:
'
lanczos3
'
}
)
;
break
;
default
:
throw
new
Error
(
Resampling
method
{
resampleMethod
}
is
not
supported
.
)
;
}
return
await
loadImageFunction
(
img
)
;
}
}
async
pad
(
[
left
right
top
bottom
]
)
{
left
=
Math
.
max
(
left
0
)
;
right
=
Math
.
max
(
right
0
)
;
top
=
Math
.
max
(
top
0
)
;
bottom
=
Math
.
max
(
bottom
0
)
;
if
(
left
=
=
=
0
&
&
right
=
=
=
0
&
&
top
=
=
=
0
&
&
bottom
=
=
=
0
)
{
return
this
;
}
if
(
BROWSER_ENV
)
{
let
numChannels
=
this
.
channels
;
let
canvas
=
this
.
toCanvas
(
)
;
let
newWidth
=
this
.
width
+
left
+
right
;
let
newHeight
=
this
.
height
+
top
+
bottom
;
const
ctx
=
createCanvasFunction
(
newWidth
newHeight
)
.
getContext
(
'
2d
'
)
;
ctx
.
drawImage
(
canvas
0
0
this
.
width
this
.
height
left
top
newWidth
newHeight
)
;
let
paddedImage
=
new
RawImage
(
ctx
.
getImageData
(
0
0
newWidth
newHeight
)
.
data
newWidth
newHeight
4
)
;
return
paddedImage
.
convert
(
numChannels
)
;
}
else
{
let
img
=
this
.
toSharp
(
)
.
extend
(
{
left
right
top
bottom
}
)
;
return
await
loadImageFunction
(
img
)
;
}
}
async
crop
(
[
x_min
y_min
x_max
y_max
]
)
{
x_min
=
Math
.
max
(
x_min
0
)
;
y_min
=
Math
.
max
(
y_min
0
)
;
x_max
=
Math
.
min
(
x_max
this
.
width
-
1
)
;
y_max
=
Math
.
min
(
y_max
this
.
height
-
1
)
;
if
(
x_min
=
=
=
0
&
&
y_min
=
=
=
0
&
&
x_max
=
=
=
this
.
width
-
1
&
&
y_max
=
=
=
this
.
height
-
1
)
{
return
this
;
}
const
crop_width
=
x_max
-
x_min
+
1
;
const
crop_height
=
y_max
-
y_min
+
1
;
if
(
BROWSER_ENV
)
{
const
numChannels
=
this
.
channels
;
const
canvas
=
this
.
toCanvas
(
)
;
const
ctx
=
createCanvasFunction
(
crop_width
crop_height
)
.
getContext
(
'
2d
'
)
;
ctx
.
drawImage
(
canvas
x_min
y_min
crop_width
crop_height
0
0
crop_width
crop_height
)
;
const
resizedImage
=
new
RawImage
(
ctx
.
getImageData
(
0
0
crop_width
crop_height
)
.
data
crop_width
crop_height
4
)
;
return
resizedImage
.
convert
(
numChannels
)
;
}
else
{
const
img
=
this
.
toSharp
(
)
.
extract
(
{
left
:
x_min
top
:
y_min
width
:
crop_width
height
:
crop_height
}
)
;
return
await
loadImageFunction
(
img
)
;
}
}
async
center_crop
(
crop_width
crop_height
)
{
if
(
this
.
width
=
=
=
crop_width
&
&
this
.
height
=
=
=
crop_height
)
{
return
this
;
}
let
width_offset
=
(
this
.
width
-
crop_width
)
/
2
;
let
height_offset
=
(
this
.
height
-
crop_height
)
/
2
;
if
(
BROWSER_ENV
)
{
let
numChannels
=
this
.
channels
;
let
canvas
=
this
.
toCanvas
(
)
;
const
ctx
=
createCanvasFunction
(
crop_width
crop_height
)
.
getContext
(
'
2d
'
)
;
let
sourceX
=
0
;
let
sourceY
=
0
;
let
destX
=
0
;
let
destY
=
0
;
if
(
width_offset
>
=
0
)
{
sourceX
=
width_offset
;
}
else
{
destX
=
-
width_offset
;
}
if
(
height_offset
>
=
0
)
{
sourceY
=
height_offset
;
}
else
{
destY
=
-
height_offset
;
}
ctx
.
drawImage
(
canvas
sourceX
sourceY
crop_width
crop_height
destX
destY
crop_width
crop_height
)
;
let
resizedImage
=
new
RawImage
(
ctx
.
getImageData
(
0
0
crop_width
crop_height
)
.
data
crop_width
crop_height
4
)
;
return
resizedImage
.
convert
(
numChannels
)
;
}
else
{
let
img
=
this
.
toSharp
(
)
;
if
(
width_offset
>
=
0
&
&
height_offset
>
=
0
)
{
img
=
img
.
extract
(
{
left
:
Math
.
floor
(
width_offset
)
top
:
Math
.
floor
(
height_offset
)
width
:
crop_width
height
:
crop_height
}
)
}
else
if
(
width_offset
<
=
0
&
&
height_offset
<
=
0
)
{
let
top
=
Math
.
floor
(
-
height_offset
)
;
let
left
=
Math
.
floor
(
-
width_offset
)
;
img
=
img
.
extend
(
{
top
:
top
left
:
left
right
:
crop_width
-
this
.
width
-
left
bottom
:
crop_height
-
this
.
height
-
top
}
)
;
}
else
{
let
y_padding
=
[
0
0
]
;
let
y_extract
=
0
;
if
(
height_offset
<
0
)
{
y_padding
[
0
]
=
Math
.
floor
(
-
height_offset
)
;
y_padding
[
1
]
=
crop_height
-
this
.
height
-
y_padding
[
0
]
;
}
else
{
y_extract
=
Math
.
floor
(
height_offset
)
;
}
let
x_padding
=
[
0
0
]
;
let
x_extract
=
0
;
if
(
width_offset
<
0
)
{
x_padding
[
0
]
=
Math
.
floor
(
-
width_offset
)
;
x_padding
[
1
]
=
crop_width
-
this
.
width
-
x_padding
[
0
]
;
}
else
{
x_extract
=
Math
.
floor
(
width_offset
)
;
}
img
=
img
.
extend
(
{
top
:
y_padding
[
0
]
bottom
:
y_padding
[
1
]
left
:
x_padding
[
0
]
right
:
x_padding
[
1
]
}
)
.
extract
(
{
left
:
x_extract
top
:
y_extract
width
:
crop_width
height
:
crop_height
}
)
}
return
await
loadImageFunction
(
img
)
;
}
}
async
toBlob
(
type
=
'
image
/
png
'
quality
=
1
)
{
if
(
!
BROWSER_ENV
)
{
throw
new
Error
(
'
toBlob
(
)
is
only
supported
in
browser
environments
.
'
)
}
const
canvas
=
this
.
toCanvas
(
)
;
return
await
canvas
.
convertToBlob
(
{
type
quality
}
)
;
}
toTensor
(
channel_format
=
'
CHW
'
)
{
let
tensor
=
new
_tensor_js__WEBPACK_IMPORTED_MODULE_2__
.
Tensor
(
'
uint8
'
new
Uint8Array
(
this
.
data
)
[
this
.
height
this
.
width
this
.
channels
]
)
;
if
(
channel_format
=
=
=
'
HWC
'
)
{
}
else
if
(
channel_format
=
=
=
'
CHW
'
)
{
tensor
=
tensor
.
permute
(
2
0
1
)
;
}
else
{
throw
new
Error
(
Unsupported
channel
format
:
{
channel_format
}
)
;
}
return
tensor
;
}
toCanvas
(
)
{
if
(
!
BROWSER_ENV
)
{
throw
new
Error
(
'
toCanvas
(
)
is
only
supported
in
browser
environments
.
'
)
}
let
cloned
=
this
.
clone
(
)
.
rgba
(
)
;
let
clonedCanvas
=
createCanvasFunction
(
cloned
.
width
cloned
.
height
)
;
let
data
=
new
ImageDataClass
(
cloned
.
data
cloned
.
width
cloned
.
height
)
;
clonedCanvas
.
getContext
(
'
2d
'
)
.
putImageData
(
data
0
0
)
;
return
clonedCanvas
;
}
_update
(
data
width
height
channels
=
null
)
{
this
.
data
=
data
;
this
.
width
=
width
;
this
.
height
=
height
;
if
(
channels
!
=
=
null
)
{
this
.
channels
=
channels
;
}
return
this
;
}
clone
(
)
{
return
new
RawImage
(
this
.
data
.
slice
(
)
this
.
width
this
.
height
this
.
channels
)
;
}
convert
(
numChannels
)
{
if
(
this
.
channels
=
=
=
numChannels
)
return
this
;
switch
(
numChannels
)
{
case
1
:
this
.
grayscale
(
)
;
break
;
case
3
:
this
.
rgb
(
)
;
break
;
case
4
:
this
.
rgba
(
)
;
break
;
default
:
throw
new
Error
(
Conversion
failed
due
to
unsupported
number
of
channels
:
{
this
.
channels
}
)
;
}
return
this
;
}
async
save
(
path
)
{
if
(
BROWSER_ENV
)
{
if
(
WEBWORKER_ENV
)
{
throw
new
Error
(
'
Unable
to
save
an
image
from
a
Web
Worker
.
'
)
}
const
extension
=
path
.
split
(
'
.
'
)
.
pop
(
)
.
toLowerCase
(
)
;
const
mime
=
CONTENT_TYPE_MAP
.
get
(
extension
)
?
?
'
image
/
png
'
;
const
blob
=
await
this
.
toBlob
(
mime
)
;
const
dataURL
=
URL
.
createObjectURL
(
blob
)
;
const
downloadLink
=
document
.
createElement
(
'
a
'
)
;
downloadLink
.
href
=
dataURL
;
downloadLink
.
download
=
path
;
downloadLink
.
click
(
)
;
downloadLink
.
remove
(
)
;
}
else
if
(
!
_env_js__WEBPACK_IMPORTED_MODULE_1__
.
env
.
useFS
)
{
throw
new
Error
(
'
Unable
to
save
the
image
because
filesystem
is
disabled
in
this
environment
.
'
)
}
else
{
const
img
=
this
.
toSharp
(
)
;
return
await
img
.
toFile
(
path
)
;
}
}
toSharp
(
)
{
if
(
BROWSER_ENV
)
{
throw
new
Error
(
'
toSharp
(
)
is
only
supported
in
server
-
side
environments
.
'
)
}
return
sharp__WEBPACK_IMPORTED_MODULE_3__
(
this
.
data
{
raw
:
{
width
:
this
.
width
height
:
this
.
height
channels
:
this
.
channels
}
}
)
;
}
}
}
)
"
.
/
src
/
utils
/
maths
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
FFT
"
:
(
)
=
>
(
FFT
)
"
bankers_round
"
:
(
)
=
>
(
bankers_round
)
"
cos_sim
"
:
(
)
=
>
(
cos_sim
)
"
dot
"
:
(
)
=
>
(
dot
)
"
getTopItems
"
:
(
)
=
>
(
getTopItems
)
"
interpolate_data
"
:
(
)
=
>
(
interpolate_data
)
"
log_softmax
"
:
(
)
=
>
(
log_softmax
)
"
magnitude
"
:
(
)
=
>
(
magnitude
)
"
max
"
:
(
)
=
>
(
max
)
"
medianFilter
"
:
(
)
=
>
(
medianFilter
)
"
min
"
:
(
)
=
>
(
min
)
"
permute_data
"
:
(
)
=
>
(
permute_data
)
"
round
"
:
(
)
=
>
(
round
)
"
softmax
"
:
(
)
=
>
(
softmax
)
}
)
;
function
interpolate_data
(
input
[
in_channels
in_height
in_width
]
[
out_height
out_width
]
mode
=
'
bilinear
'
align_corners
=
false
)
{
const
x_scale
=
out_width
/
in_width
;
const
y_scale
=
out_height
/
in_height
;
const
out_img
=
new
input
.
constructor
(
out_height
*
out_width
*
in_channels
)
;
const
inStride
=
in_height
*
in_width
;
const
outStride
=
out_height
*
out_width
;
for
(
let
i
=
0
;
i
<
out_height
;
+
+
i
)
{
for
(
let
j
=
0
;
j
<
out_width
;
+
+
j
)
{
const
outOffset
=
i
*
out_width
+
j
;
const
x
=
(
j
+
0
.
5
)
/
x_scale
-
0
.
5
;
const
y
=
(
i
+
0
.
5
)
/
y_scale
-
0
.
5
;
let
x1
=
Math
.
floor
(
x
)
;
let
y1
=
Math
.
floor
(
y
)
;
const
x2
=
Math
.
min
(
x1
+
1
in_width
-
1
)
;
const
y2
=
Math
.
min
(
y1
+
1
in_height
-
1
)
;
x1
=
Math
.
max
(
x1
0
)
;
y1
=
Math
.
max
(
y1
0
)
;
const
s
=
x
-
x1
;
const
t
=
y
-
y1
;
const
w1
=
(
1
-
s
)
*
(
1
-
t
)
;
const
w2
=
s
*
(
1
-
t
)
;
const
w3
=
(
1
-
s
)
*
t
;
const
w4
=
s
*
t
;
const
yStride
=
y1
*
in_width
;
const
xStride
=
y2
*
in_width
;
const
idx1
=
yStride
+
x1
;
const
idx2
=
yStride
+
x2
;
const
idx3
=
xStride
+
x1
;
const
idx4
=
xStride
+
x2
;
for
(
let
k
=
0
;
k
<
in_channels
;
+
+
k
)
{
const
cOffset
=
k
*
inStride
;
out_img
[
k
*
outStride
+
outOffset
]
=
w1
*
input
[
cOffset
+
idx1
]
+
w2
*
input
[
cOffset
+
idx2
]
+
w3
*
input
[
cOffset
+
idx3
]
+
w4
*
input
[
cOffset
+
idx4
]
;
}
}
}
return
out_img
;
}
function
permute_data
(
array
dims
axes
)
{
const
shape
=
new
Array
(
axes
.
length
)
;
const
stride
=
new
Array
(
axes
.
length
)
;
for
(
let
i
=
axes
.
length
-
1
s
=
1
;
i
>
=
0
;
-
-
i
)
{
stride
[
i
]
=
s
;
shape
[
i
]
=
dims
[
axes
[
i
]
]
;
s
*
=
shape
[
i
]
;
}
const
invStride
=
axes
.
map
(
(
_
i
)
=
>
stride
[
axes
.
indexOf
(
i
)
]
)
;
const
permutedData
=
new
array
.
constructor
(
array
.
length
)
;
for
(
let
i
=
0
;
i
<
array
.
length
;
+
+
i
)
{
let
newIndex
=
0
;
for
(
let
j
=
dims
.
length
-
1
k
=
i
;
j
>
=
0
;
-
-
j
)
{
newIndex
+
=
(
k
%
dims
[
j
]
)
*
invStride
[
j
]
;
k
=
Math
.
floor
(
k
/
dims
[
j
]
)
;
}
permutedData
[
newIndex
]
=
array
[
i
]
;
}
return
[
permutedData
shape
]
;
}
function
softmax
(
arr
)
{
const
maxVal
=
max
(
arr
)
[
0
]
;
const
exps
=
arr
.
map
(
x
=
>
Math
.
exp
(
x
-
maxVal
)
)
;
const
sumExps
=
exps
.
reduce
(
(
acc
val
)
=
>
acc
+
val
0
)
;
const
softmaxArr
=
exps
.
map
(
x
=
>
x
/
sumExps
)
;
return
(
softmaxArr
)
;
}
function
log_softmax
(
arr
)
{
const
softmaxArr
=
softmax
(
arr
)
;
const
logSoftmaxArr
=
softmaxArr
.
map
(
x
=
>
Math
.
log
(
x
)
)
;
return
(
logSoftmaxArr
)
;
}
function
dot
(
arr1
arr2
)
{
return
arr1
.
reduce
(
(
acc
val
i
)
=
>
acc
+
val
*
arr2
[
i
]
0
)
;
}
function
getTopItems
(
items
top_k
=
0
)
{
items
=
Array
.
from
(
items
)
.
map
(
(
x
i
)
=
>
[
i
x
]
)
.
sort
(
(
a
b
)
=
>
b
[
1
]
-
a
[
1
]
)
if
(
top_k
!
=
=
null
&
&
top_k
>
0
)
{
items
=
items
.
slice
(
0
top_k
)
;
}
return
items
}
function
cos_sim
(
arr1
arr2
)
{
const
dotProduct
=
dot
(
arr1
arr2
)
;
const
magnitudeA
=
magnitude
(
arr1
)
;
const
magnitudeB
=
magnitude
(
arr2
)
;
const
cosineSimilarity
=
dotProduct
/
(
magnitudeA
*
magnitudeB
)
;
return
cosineSimilarity
;
}
function
magnitude
(
arr
)
{
return
Math
.
sqrt
(
arr
.
reduce
(
(
acc
val
)
=
>
acc
+
val
*
val
0
)
)
;
}
function
min
(
arr
)
{
if
(
arr
.
length
=
=
=
0
)
throw
Error
(
'
Array
must
not
be
empty
'
)
;
let
min
=
arr
[
0
]
;
let
indexOfMin
=
0
;
for
(
let
i
=
1
;
i
<
arr
.
length
;
+
+
i
)
{
if
(
arr
[
i
]
<
min
)
{
min
=
arr
[
i
]
;
indexOfMin
=
i
;
}
}
return
[
min
indexOfMin
]
;
}
function
max
(
arr
)
{
if
(
arr
.
length
=
=
=
0
)
throw
Error
(
'
Array
must
not
be
empty
'
)
;
let
max
=
arr
[
0
]
;
let
indexOfMax
=
0
;
for
(
let
i
=
1
;
i
<
arr
.
length
;
+
+
i
)
{
if
(
arr
[
i
]
>
max
)
{
max
=
arr
[
i
]
;
indexOfMax
=
i
;
}
}
return
[
Number
(
max
)
indexOfMax
]
;
}
function
isPowerOfTwo
(
number
)
{
return
(
number
>
0
)
&
&
(
(
number
&
(
number
-
1
)
)
=
=
=
0
)
;
}
class
P2FFT
{
constructor
(
size
)
{
this
.
size
=
size
|
0
;
if
(
this
.
size
<
=
1
|
|
!
isPowerOfTwo
(
this
.
size
)
)
throw
new
Error
(
'
FFT
size
must
be
a
power
of
two
larger
than
1
'
)
;
this
.
_csize
=
size
<
<
1
;
this
.
table
=
new
Float64Array
(
this
.
size
*
2
)
;
for
(
let
i
=
0
;
i
<
this
.
table
.
length
;
i
+
=
2
)
{
const
angle
=
Math
.
PI
*
i
/
this
.
size
;
this
.
table
[
i
]
=
Math
.
cos
(
angle
)
;
this
.
table
[
i
+
1
]
=
-
Math
.
sin
(
angle
)
;
}
let
power
=
0
;
for
(
let
t
=
1
;
this
.
size
>
t
;
t
<
<
=
1
)
+
+
power
;
this
.
_width
=
power
%
2
=
=
=
0
?
power
-
1
:
power
;
this
.
_bitrev
=
new
Int32Array
(
1
<
<
this
.
_width
)
;
for
(
let
j
=
0
;
j
<
this
.
_bitrev
.
length
;
+
+
j
)
{
this
.
_bitrev
[
j
]
=
0
;
for
(
let
shift
=
0
;
shift
<
this
.
_width
;
shift
+
=
2
)
{
const
revShift
=
this
.
_width
-
shift
-
2
;
this
.
_bitrev
[
j
]
|
=
(
(
j
>
>
>
shift
)
&
3
)
<
<
revShift
;
}
}
}
createComplexArray
(
)
{
return
new
Float64Array
(
this
.
_csize
)
;
}
fromComplexArray
(
complex
storage
)
{
const
res
=
storage
|
|
new
Array
(
complex
.
length
>
>
>
1
)
;
for
(
let
i
=
0
;
i
<
complex
.
length
;
i
+
=
2
)
res
[
i
>
>
>
1
]
=
complex
[
i
]
;
return
res
;
}
toComplexArray
(
input
storage
)
{
const
res
=
storage
|
|
this
.
createComplexArray
(
)
;
for
(
let
i
=
0
;
i
<
res
.
length
;
i
+
=
2
)
{
res
[
i
]
=
input
[
i
>
>
>
1
]
;
res
[
i
+
1
]
=
0
;
}
return
res
;
}
completeSpectrum
(
spectrum
)
{
const
size
=
this
.
_csize
;
const
half
=
size
>
>
>
1
;
for
(
let
i
=
2
;
i
<
half
;
i
+
=
2
)
{
spectrum
[
size
-
i
]
=
spectrum
[
i
]
;
spectrum
[
size
-
i
+
1
]
=
-
spectrum
[
i
+
1
]
;
}
}
transform
(
out
data
)
{
if
(
out
=
=
=
data
)
throw
new
Error
(
'
Input
and
output
buffers
must
be
different
'
)
;
this
.
_transform4
(
out
data
1
)
;
}
realTransform
(
out
data
)
{
if
(
out
=
=
=
data
)
throw
new
Error
(
'
Input
and
output
buffers
must
be
different
'
)
;
this
.
_realTransform4
(
out
data
1
)
;
}
inverseTransform
(
out
data
)
{
if
(
out
=
=
=
data
)
throw
new
Error
(
'
Input
and
output
buffers
must
be
different
'
)
;
this
.
_transform4
(
out
data
-
1
)
;
for
(
let
i
=
0
;
i
<
out
.
length
;
+
+
i
)
out
[
i
]
/
=
this
.
size
;
}
_transform4
(
out
data
inv
)
{
const
size
=
this
.
_csize
;
const
width
=
this
.
_width
;
let
step
=
1
<
<
width
;
let
len
=
(
size
/
step
)
<
<
1
;
let
outOff
;
let
t
;
const
bitrev
=
this
.
_bitrev
;
if
(
len
=
=
=
4
)
{
for
(
outOff
=
0
t
=
0
;
outOff
<
size
;
outOff
+
=
len
+
+
t
)
{
const
off
=
bitrev
[
t
]
;
this
.
_singleTransform2
(
data
out
outOff
off
step
)
;
}
}
else
{
for
(
outOff
=
0
t
=
0
;
outOff
<
size
;
outOff
+
=
len
+
+
t
)
{
const
off
=
bitrev
[
t
]
;
this
.
_singleTransform4
(
data
out
outOff
off
step
inv
)
;
}
}
for
(
step
>
>
=
2
;
step
>
=
2
;
step
>
>
=
2
)
{
len
=
(
size
/
step
)
<
<
1
;
const
quarterLen
=
len
>
>
>
2
;
for
(
outOff
=
0
;
outOff
<
size
;
outOff
+
=
len
)
{
const
limit
=
outOff
+
quarterLen
-
1
;
for
(
let
i
=
outOff
k
=
0
;
i
<
limit
;
i
+
=
2
k
+
=
step
)
{
const
A
=
i
;
const
B
=
A
+
quarterLen
;
const
C
=
B
+
quarterLen
;
const
D
=
C
+
quarterLen
;
const
Ar
=
out
[
A
]
;
const
Ai
=
out
[
A
+
1
]
;
const
Br
=
out
[
B
]
;
const
Bi
=
out
[
B
+
1
]
;
const
Cr
=
out
[
C
]
;
const
Ci
=
out
[
C
+
1
]
;
const
Dr
=
out
[
D
]
;
const
Di
=
out
[
D
+
1
]
;
const
tableBr
=
this
.
table
[
k
]
;
const
tableBi
=
inv
*
this
.
table
[
k
+
1
]
;
const
MBr
=
Br
*
tableBr
-
Bi
*
tableBi
;
const
MBi
=
Br
*
tableBi
+
Bi
*
tableBr
;
const
tableCr
=
this
.
table
[
2
*
k
]
;
const
tableCi
=
inv
*
this
.
table
[
2
*
k
+
1
]
;
const
MCr
=
Cr
*
tableCr
-
Ci
*
tableCi
;
const
MCi
=
Cr
*
tableCi
+
Ci
*
tableCr
;
const
tableDr
=
this
.
table
[
3
*
k
]
;
const
tableDi
=
inv
*
this
.
table
[
3
*
k
+
1
]
;
const
MDr
=
Dr
*
tableDr
-
Di
*
tableDi
;
const
MDi
=
Dr
*
tableDi
+
Di
*
tableDr
;
const
T0r
=
Ar
+
MCr
;
const
T0i
=
Ai
+
MCi
;
const
T1r
=
Ar
-
MCr
;
const
T1i
=
Ai
-
MCi
;
const
T2r
=
MBr
+
MDr
;
const
T2i
=
MBi
+
MDi
;
const
T3r
=
inv
*
(
MBr
-
MDr
)
;
const
T3i
=
inv
*
(
MBi
-
MDi
)
;
out
[
A
]
=
T0r
+
T2r
;
out
[
A
+
1
]
=
T0i
+
T2i
;
out
[
B
]
=
T1r
+
T3i
;
out
[
B
+
1
]
=
T1i
-
T3r
;
out
[
C
]
=
T0r
-
T2r
;
out
[
C
+
1
]
=
T0i
-
T2i
;
out
[
D
]
=
T1r
-
T3i
;
out
[
D
+
1
]
=
T1i
+
T3r
;
}
}
}
}
_singleTransform2
(
data
out
outOff
off
step
)
{
const
evenR
=
data
[
off
]
;
const
evenI
=
data
[
off
+
1
]
;
const
oddR
=
data
[
off
+
step
]
;
const
oddI
=
data
[
off
+
step
+
1
]
;
out
[
outOff
]
=
evenR
+
oddR
;
out
[
outOff
+
1
]
=
evenI
+
oddI
;
out
[
outOff
+
2
]
=
evenR
-
oddR
;
out
[
outOff
+
3
]
=
evenI
-
oddI
;
}
_singleTransform4
(
data
out
outOff
off
step
inv
)
{
const
step2
=
step
*
2
;
const
step3
=
step
*
3
;
const
Ar
=
data
[
off
]
;
const
Ai
=
data
[
off
+
1
]
;
const
Br
=
data
[
off
+
step
]
;
const
Bi
=
data
[
off
+
step
+
1
]
;
const
Cr
=
data
[
off
+
step2
]
;
const
Ci
=
data
[
off
+
step2
+
1
]
;
const
Dr
=
data
[
off
+
step3
]
;
const
Di
=
data
[
off
+
step3
+
1
]
;
const
T0r
=
Ar
+
Cr
;
const
T0i
=
Ai
+
Ci
;
const
T1r
=
Ar
-
Cr
;
const
T1i
=
Ai
-
Ci
;
const
T2r
=
Br
+
Dr
;
const
T2i
=
Bi
+
Di
;
const
T3r
=
inv
*
(
Br
-
Dr
)
;
const
T3i
=
inv
*
(
Bi
-
Di
)
;
out
[
outOff
]
=
T0r
+
T2r
;
out
[
outOff
+
1
]
=
T0i
+
T2i
;
out
[
outOff
+
2
]
=
T1r
+
T3i
;
out
[
outOff
+
3
]
=
T1i
-
T3r
;
out
[
outOff
+
4
]
=
T0r
-
T2r
;
out
[
outOff
+
5
]
=
T0i
-
T2i
;
out
[
outOff
+
6
]
=
T1r
-
T3i
;
out
[
outOff
+
7
]
=
T1i
+
T3r
;
}
_realTransform4
(
out
data
inv
)
{
const
size
=
this
.
_csize
;
const
width
=
this
.
_width
;
let
step
=
1
<
<
width
;
let
len
=
(
size
/
step
)
<
<
1
;
let
outOff
;
let
t
;
const
bitrev
=
this
.
_bitrev
;
if
(
len
=
=
=
4
)
{
for
(
outOff
=
0
t
=
0
;
outOff
<
size
;
outOff
+
=
len
+
+
t
)
{
const
off
=
bitrev
[
t
]
;
this
.
_singleRealTransform2
(
data
out
outOff
off
>
>
>
1
step
>
>
>
1
)
;
}
}
else
{
for
(
outOff
=
0
t
=
0
;
outOff
<
size
;
outOff
+
=
len
+
+
t
)
{
const
off
=
bitrev
[
t
]
;
this
.
_singleRealTransform4
(
data
out
outOff
off
>
>
>
1
step
>
>
>
1
inv
)
;
}
}
for
(
step
>
>
=
2
;
step
>
=
2
;
step
>
>
=
2
)
{
len
=
(
size
/
step
)
<
<
1
;
const
quarterLen
=
len
>
>
>
2
;
for
(
outOff
=
0
;
outOff
<
size
;
outOff
+
=
len
)
{
const
limit
=
outOff
+
quarterLen
-
1
;
for
(
let
i
=
outOff
k
=
0
;
i
<
limit
;
i
+
=
2
k
+
=
step
)
{
const
A
=
i
;
const
B
=
A
+
quarterLen
;
const
C
=
B
+
quarterLen
;
const
D
=
C
+
quarterLen
;
const
Ar
=
out
[
A
]
;
const
Ai
=
out
[
A
+
1
]
;
const
Br
=
out
[
B
]
;
const
Bi
=
out
[
B
+
1
]
;
const
Cr
=
out
[
C
]
;
const
Ci
=
out
[
C
+
1
]
;
const
Dr
=
out
[
D
]
;
const
Di
=
out
[
D
+
1
]
;
const
tableBr
=
this
.
table
[
k
]
;
const
tableBi
=
inv
*
this
.
table
[
k
+
1
]
;
const
MBr
=
Br
*
tableBr
-
Bi
*
tableBi
;
const
MBi
=
Br
*
tableBi
+
Bi
*
tableBr
;
const
tableCr
=
this
.
table
[
2
*
k
]
;
const
tableCi
=
inv
*
this
.
table
[
2
*
k
+
1
]
;
const
MCr
=
Cr
*
tableCr
-
Ci
*
tableCi
;
const
MCi
=
Cr
*
tableCi
+
Ci
*
tableCr
;
const
tableDr
=
this
.
table
[
3
*
k
]
;
const
tableDi
=
inv
*
this
.
table
[
3
*
k
+
1
]
;
const
MDr
=
Dr
*
tableDr
-
Di
*
tableDi
;
const
MDi
=
Dr
*
tableDi
+
Di
*
tableDr
;
const
T0r
=
Ar
+
MCr
;
const
T0i
=
Ai
+
MCi
;
const
T1r
=
Ar
-
MCr
;
const
T1i
=
Ai
-
MCi
;
const
T2r
=
MBr
+
MDr
;
const
T2i
=
MBi
+
MDi
;
const
T3r
=
inv
*
(
MBr
-
MDr
)
;
const
T3i
=
inv
*
(
MBi
-
MDi
)
;
out
[
A
]
=
T0r
+
T2r
;
out
[
A
+
1
]
=
T0i
+
T2i
;
out
[
B
]
=
T1r
+
T3i
;
out
[
B
+
1
]
=
T1i
-
T3r
;
out
[
C
]
=
T0r
-
T2r
;
out
[
C
+
1
]
=
T0i
-
T2i
;
out
[
D
]
=
T1r
-
T3i
;
out
[
D
+
1
]
=
T1i
+
T3r
;
}
}
}
}
_singleRealTransform2
(
data
out
outOff
off
step
)
{
const
evenR
=
data
[
off
]
;
const
oddR
=
data
[
off
+
step
]
;
out
[
outOff
]
=
evenR
+
oddR
;
out
[
outOff
+
1
]
=
0
;
out
[
outOff
+
2
]
=
evenR
-
oddR
;
out
[
outOff
+
3
]
=
0
;
}
_singleRealTransform4
(
data
out
outOff
off
step
inv
)
{
const
step2
=
step
*
2
;
const
step3
=
step
*
3
;
const
Ar
=
data
[
off
]
;
const
Br
=
data
[
off
+
step
]
;
const
Cr
=
data
[
off
+
step2
]
;
const
Dr
=
data
[
off
+
step3
]
;
const
T0r
=
Ar
+
Cr
;
const
T1r
=
Ar
-
Cr
;
const
T2r
=
Br
+
Dr
;
const
T3r
=
inv
*
(
Br
-
Dr
)
;
out
[
outOff
]
=
T0r
+
T2r
;
out
[
outOff
+
1
]
=
0
;
out
[
outOff
+
2
]
=
T1r
;
out
[
outOff
+
3
]
=
-
T3r
;
out
[
outOff
+
4
]
=
T0r
-
T2r
;
out
[
outOff
+
5
]
=
0
;
out
[
outOff
+
6
]
=
T1r
;
out
[
outOff
+
7
]
=
T3r
;
}
}
class
NP2FFT
{
constructor
(
fft_length
)
{
const
a
=
2
*
(
fft_length
-
1
)
;
const
b
=
2
*
(
2
*
fft_length
-
1
)
;
const
nextP2
=
2
*
*
(
Math
.
ceil
(
Math
.
log2
(
b
)
)
)
this
.
bufferSize
=
nextP2
;
this
.
_a
=
a
;
const
chirp
=
new
Float64Array
(
b
)
;
const
ichirp
=
new
Float64Array
(
nextP2
)
;
this
.
_chirpBuffer
=
new
Float64Array
(
nextP2
)
;
this
.
_buffer1
=
new
Float64Array
(
nextP2
)
;
this
.
_buffer2
=
new
Float64Array
(
nextP2
)
;
this
.
_outBuffer1
=
new
Float64Array
(
nextP2
)
;
this
.
_outBuffer2
=
new
Float64Array
(
nextP2
)
;
const
theta
=
-
2
*
Math
.
PI
/
fft_length
;
const
baseR
=
Math
.
cos
(
theta
)
;
const
baseI
=
Math
.
sin
(
theta
)
;
for
(
let
i
=
0
;
i
<
b
>
>
1
;
+
+
i
)
{
const
e
=
(
i
+
1
-
fft_length
)
*
*
2
/
2
.
0
;
const
result_mod
=
Math
.
sqrt
(
baseR
*
*
2
+
baseI
*
*
2
)
*
*
e
;
const
result_arg
=
e
*
Math
.
atan2
(
baseI
baseR
)
;
const
i2
=
2
*
i
;
chirp
[
i2
]
=
result_mod
*
Math
.
cos
(
result_arg
)
;
chirp
[
i2
+
1
]
=
result_mod
*
Math
.
sin
(
result_arg
)
;
ichirp
[
i2
]
=
chirp
[
i2
]
;
ichirp
[
i2
+
1
]
=
-
chirp
[
i2
+
1
]
;
}
this
.
_slicedChirpBuffer
=
chirp
.
subarray
(
a
b
)
;
this
.
_f
=
new
P2FFT
(
nextP2
>
>
1
)
;
this
.
_f
.
transform
(
this
.
_chirpBuffer
ichirp
)
;
}
_transform
(
output
input
real
)
{
const
ib1
=
this
.
_buffer1
;
const
ib2
=
this
.
_buffer2
;
const
ob2
=
this
.
_outBuffer1
;
const
ob3
=
this
.
_outBuffer2
;
const
cb
=
this
.
_chirpBuffer
;
const
sb
=
this
.
_slicedChirpBuffer
;
const
a
=
this
.
_a
;
if
(
real
)
{
for
(
let
j
=
0
;
j
<
sb
.
length
;
j
+
=
2
)
{
const
j2
=
j
+
1
const
j3
=
j
>
>
1
;
const
a_real
=
input
[
j3
]
;
ib1
[
j
]
=
a_real
*
sb
[
j
]
;
ib1
[
j2
]
=
a_real
*
sb
[
j2
]
;
}
}
else
{
for
(
let
j
=
0
;
j
<
sb
.
length
;
j
+
=
2
)
{
const
j2
=
j
+
1
ib1
[
j
]
=
input
[
j
]
*
sb
[
j
]
-
input
[
j2
]
*
sb
[
j2
]
;
ib1
[
j2
]
=
input
[
j
]
*
sb
[
j2
]
+
input
[
j2
]
*
sb
[
j
]
;
}
}
this
.
_f
.
transform
(
ob2
ib1
)
;
for
(
let
j
=
0
;
j
<
cb
.
length
;
j
+
=
2
)
{
const
j2
=
j
+
1
;
ib2
[
j
]
=
ob2
[
j
]
*
cb
[
j
]
-
ob2
[
j2
]
*
cb
[
j2
]
;
ib2
[
j2
]
=
ob2
[
j
]
*
cb
[
j2
]
+
ob2
[
j2
]
*
cb
[
j
]
;
}
this
.
_f
.
inverseTransform
(
ob3
ib2
)
;
for
(
let
j
=
0
;
j
<
ob3
.
length
;
j
+
=
2
)
{
const
a_real
=
ob3
[
j
+
a
]
;
const
a_imag
=
ob3
[
j
+
a
+
1
]
;
const
b_real
=
sb
[
j
]
;
const
b_imag
=
sb
[
j
+
1
]
;
output
[
j
]
=
a_real
*
b_real
-
a_imag
*
b_imag
;
output
[
j
+
1
]
=
a_real
*
b_imag
+
a_imag
*
b_real
;
}
}
transform
(
output
input
)
{
this
.
_transform
(
output
input
false
)
;
}
realTransform
(
output
input
)
{
this
.
_transform
(
output
input
true
)
;
}
}
class
FFT
{
constructor
(
fft_length
)
{
this
.
fft_length
=
fft_length
;
this
.
isPowerOfTwo
=
isPowerOfTwo
(
fft_length
)
;
if
(
this
.
isPowerOfTwo
)
{
this
.
fft
=
new
P2FFT
(
fft_length
)
;
this
.
outputBufferSize
=
2
*
fft_length
;
}
else
{
this
.
fft
=
new
NP2FFT
(
fft_length
)
;
this
.
outputBufferSize
=
this
.
fft
.
bufferSize
;
}
}
realTransform
(
out
input
)
{
this
.
fft
.
realTransform
(
out
input
)
;
}
transform
(
out
input
)
{
this
.
fft
.
transform
(
out
input
)
;
}
}
function
medianFilter
(
data
windowSize
)
{
if
(
windowSize
%
2
=
=
=
0
|
|
windowSize
<
=
0
)
{
throw
new
Error
(
'
Window
size
must
be
a
positive
odd
number
'
)
;
}
const
outputArray
=
new
data
.
constructor
(
data
.
length
)
;
const
buffer
=
new
data
.
constructor
(
windowSize
)
;
const
halfWindowSize
=
Math
.
floor
(
windowSize
/
2
)
;
for
(
let
i
=
0
;
i
<
data
.
length
;
+
+
i
)
{
let
valuesIndex
=
0
;
for
(
let
j
=
-
halfWindowSize
;
j
<
=
halfWindowSize
;
+
+
j
)
{
let
index
=
i
+
j
;
if
(
index
<
0
)
{
index
=
Math
.
abs
(
index
)
;
}
else
if
(
index
>
=
data
.
length
)
{
index
=
2
*
(
data
.
length
-
1
)
-
index
;
}
buffer
[
valuesIndex
+
+
]
=
data
[
index
]
;
}
buffer
.
sort
(
)
;
outputArray
[
i
]
=
buffer
[
halfWindowSize
]
;
}
return
outputArray
;
}
function
round
(
num
decimals
)
{
const
pow
=
Math
.
pow
(
10
decimals
)
;
return
Math
.
round
(
num
*
pow
)
/
pow
;
}
function
bankers_round
(
x
)
{
const
r
=
Math
.
round
(
x
)
;
const
br
=
Math
.
abs
(
x
)
%
1
=
=
=
0
.
5
?
(
r
%
2
=
=
=
0
?
r
:
r
-
1
)
:
r
;
return
br
;
}
}
)
"
.
/
src
/
utils
/
tensor
.
js
"
:
(
(
__unused_webpack___webpack_module__
__webpack_exports__
__webpack_require__
)
=
>
{
__webpack_require__
.
r
(
__webpack_exports__
)
;
__webpack_require__
.
d
(
__webpack_exports__
{
"
Tensor
"
:
(
)
=
>
(
Tensor
)
"
cat
"
:
(
)
=
>
(
cat
)
"
dynamicTimeWarping
"
:
(
)
=
>
(
dynamicTimeWarping
)
"
interpolate
"
:
(
)
=
>
(
interpolate
)
"
layer_norm
"
:
(
)
=
>
(
layer_norm
)
"
mean
"
:
(
)
=
>
(
mean
)
"
mean_pooling
"
:
(
)
=
>
(
mean_pooling
)
"
ones
"
:
(
)
=
>
(
ones
)
"
ones_like
"
:
(
)
=
>
(
ones_like
)
"
permute
"
:
(
)
=
>
(
permute
)
"
stack
"
:
(
)
=
>
(
stack
)
"
std_mean
"
:
(
)
=
>
(
std_mean
)
}
)
;
var
_backends_onnx_js__WEBPACK_IMPORTED_MODULE_0__
=
__webpack_require__
(
"
.
/
src
/
backends
/
onnx
.
js
"
)
;
var
_maths_js__WEBPACK_IMPORTED_MODULE_1__
=
__webpack_require__
(
"
.
/
src
/
utils
/
maths
.
js
"
)
;
const
DataTypeMap
=
Object
.
freeze
(
{
float32
:
Float32Array
float64
:
Float64Array
string
:
Array
int8
:
Int8Array
uint8
:
Uint8Array
int16
:
Int16Array
uint16
:
Uint16Array
int32
:
Int32Array
uint32
:
Uint32Array
int64
:
BigInt64Array
uint64
:
BigUint64Array
bool
:
Uint8Array
}
)
;
const
ONNXTensor
=
_backends_onnx_js__WEBPACK_IMPORTED_MODULE_0__
.
ONNX
.
Tensor
;
class
Tensor
{
dims
;
type
;
data
;
size
;
constructor
(
.
.
.
args
)
{
if
(
args
[
0
]
instanceof
ONNXTensor
)
{
Object
.
assign
(
this
args
[
0
]
)
;
}
else
{
Object
.
assign
(
this
new
ONNXTensor
(
(
args
[
0
]
)
(
args
[
1
]
)
args
[
2
]
)
)
;
}
return
new
Proxy
(
this
{
get
:
(
obj
key
)
=
>
{
if
(
typeof
key
=
=
=
'
string
'
)
{
let
index
=
Number
(
key
)
;
if
(
Number
.
isInteger
(
index
)
)
{
return
obj
.
_getitem
(
index
)
;
}
}
return
obj
[
key
]
;
}
set
:
(
obj
key
value
)
=
>
{
return
obj
[
key
]
=
value
;
}
}
)
;
}
*
[
Symbol
.
iterator
]
(
)
{
const
[
iterLength
.
.
.
iterDims
]
=
this
.
dims
;
if
(
iterDims
.
length
>
0
)
{
const
iterSize
=
iterDims
.
reduce
(
(
a
b
)
=
>
a
*
b
)
;
for
(
let
i
=
0
;
i
<
iterLength
;
+
+
i
)
{
yield
this
.
_subarray
(
i
iterSize
iterDims
)
;
}
}
else
{
yield
*
this
.
data
}
}
_getitem
(
index
)
{
const
[
iterLength
.
.
.
iterDims
]
=
this
.
dims
;
index
=
safeIndex
(
index
iterLength
)
;
if
(
iterDims
.
length
>
0
)
{
const
iterSize
=
iterDims
.
reduce
(
(
a
b
)
=
>
a
*
b
)
;
return
this
.
_subarray
(
index
iterSize
iterDims
)
;
}
else
{
return
new
Tensor
(
this
.
type
[
this
.
data
[
index
]
]
iterDims
)
;
}
}
indexOf
(
item
)
{
for
(
let
index
=
0
;
index
<
this
.
data
.
length
;
+
+
index
)
{
if
(
this
.
data
[
index
]
=
=
item
)
{
return
index
;
}
}
return
-
1
;
}
_subarray
(
index
iterSize
iterDims
)
{
const
o1
=
index
*
iterSize
;
const
o2
=
(
index
+
1
)
*
iterSize
;
const
data
=
(
'
subarray
'
in
this
.
data
)
?
this
.
data
.
subarray
(
o1
o2
)
:
this
.
data
.
slice
(
o1
o2
)
;
return
new
Tensor
(
this
.
type
data
iterDims
)
;
}
item
(
)
{
if
(
this
.
data
.
length
!
=
=
1
)
{
throw
new
Error
(
a
Tensor
with
{
this
.
data
.
length
}
elements
cannot
be
converted
to
Scalar
)
;
}
return
this
.
data
[
0
]
;
}
tolist
(
)
{
return
reshape
(
this
.
data
this
.
dims
)
}
sigmoid
(
)
{
return
this
.
clone
(
)
.
sigmoid_
(
)
;
}
sigmoid_
(
)
{
for
(
let
i
=
0
;
i
<
this
.
data
.
length
;
+
+
i
)
{
this
.
data
[
i
]
=
1
/
(
1
+
Math
.
exp
(
-
this
.
data
[
i
]
)
)
;
}
return
this
;
}
mul
(
val
)
{
return
this
.
clone
(
)
.
mul_
(
val
)
;
}
mul_
(
val
)
{
for
(
let
i
=
0
;
i
<
this
.
data
.
length
;
+
+
i
)
{
this
.
data
[
i
]
*
=
val
;
}
return
this
;
}
add
(
val
)
{
return
this
.
clone
(
)
.
add_
(
val
)
;
}
add_
(
val
)
{
for
(
let
i
=
0
;
i
<
this
.
data
.
length
;
+
+
i
)
{
this
.
data
[
i
]
+
=
val
;
}
return
this
;
}
clone
(
)
{
return
new
Tensor
(
this
.
type
this
.
data
.
slice
(
)
this
.
dims
.
slice
(
)
)
;
}
slice
(
.
.
.
slices
)
{
let
newTensorDims
=
[
]
;
let
newOffsets
=
[
]
;
for
(
let
sliceIndex
=
0
;
sliceIndex
<
this
.
dims
.
length
;
+
+
sliceIndex
)
{
let
slice
=
slices
[
sliceIndex
]
;
if
(
slice
=
=
=
null
|
|
slice
=
=
=
undefined
)
{
newOffsets
.
push
(
[
0
this
.
dims
[
sliceIndex
]
]
)
;
newTensorDims
.
push
(
this
.
dims
[
sliceIndex
]
)
;
}
else
if
(
typeof
slice
=
=
=
'
number
'
)
{
slice
=
safeIndex
(
slice
this
.
dims
[
sliceIndex
]
sliceIndex
)
;
newOffsets
.
push
(
[
slice
slice
+
1
]
)
;
}
else
if
(
Array
.
isArray
(
slice
)
&
&
slice
.
length
=
=
=
2
)
{
if
(
slice
[
0
]
>
slice
[
1
]
)
{
throw
new
Error
(
Invalid
slice
:
{
slice
}
)
;
}
let
offsets
=
[
Math
.
max
(
slice
[
0
]
0
)
Math
.
min
(
slice
[
1
]
this
.
dims
[
sliceIndex
]
)
]
;
newOffsets
.
push
(
offsets
)
;
newTensorDims
.
push
(
offsets
[
1
]
-
offsets
[
0
]
)
;
}
else
{
throw
new
Error
(
Invalid
slice
:
{
slice
}
)
;
}
}
let
newDims
=
newOffsets
.
map
(
(
[
start
end
]
)
=
>
end
-
start
)
;
let
newBufferSize
=
newDims
.
reduce
(
(
a
b
)
=
>
a
*
b
)
;
let
data
=
new
this
.
data
.
constructor
(
newBufferSize
)
;
const
stride
=
this
.
stride
(
)
;
for
(
let
i
=
0
;
i
<
newBufferSize
;
+
+
i
)
{
let
originalIndex
=
0
;
for
(
let
j
=
newDims
.
length
-
1
num
=
i
;
j
>
=
0
;
-
-
j
)
{
const
size
=
newDims
[
j
]
;
originalIndex
+
=
(
(
num
%
size
)
+
newOffsets
[
j
]
[
0
]
)
*
stride
[
j
]
;
num
=
Math
.
floor
(
num
/
size
)
;
}
data
[
i
]
=
this
.
data
[
originalIndex
]
;
}
return
new
Tensor
(
this
.
type
data
newTensorDims
)
;
}
permute
(
.
.
.
dims
)
{
return
permute
(
this
dims
)
;
}
transpose
(
.
.
.
dims
)
{
return
this
.
permute
(
.
.
.
dims
)
;
}
sum
(
dim
=
null
keepdim
=
false
)
{
return
this
.
norm
(
1
dim
keepdim
)
;
}
norm
(
p
=
'
fro
'
dim
=
null
keepdim
=
false
)
{
if
(
p
=
=
=
'
fro
'
)
{
p
=
2
;
}
else
if
(
typeof
p
=
=
=
'
string
'
)
{
throw
Error
(
Unsupported
norm
:
{
p
}
)
;
}
if
(
dim
=
=
=
null
)
{
let
val
=
this
.
data
.
reduce
(
(
a
b
)
=
>
a
+
(
b
*
*
p
)
0
)
*
*
(
1
/
p
)
;
return
new
Tensor
(
this
.
type
[
val
]
[
]
)
;
}
dim
=
safeIndex
(
dim
this
.
dims
.
length
)
;
const
resultDims
=
this
.
dims
.
slice
(
)
;
resultDims
[
dim
]
=
1
;
const
result
=
new
this
.
data
.
constructor
(
this
.
data
.
length
/
this
.
dims
[
dim
]
)
;
for
(
let
i
=
0
;
i
<
this
.
data
.
length
;
+
+
i
)
{
let
resultIndex
=
0
;
for
(
let
j
=
this
.
dims
.
length
-
1
num
=
i
resultMultiplier
=
1
;
j
>
=
0
;
-
-
j
)
{
const
size
=
this
.
dims
[
j
]
;
if
(
j
!
=
=
dim
)
{
const
index
=
num
%
size
;
resultIndex
+
=
index
*
resultMultiplier
;
resultMultiplier
*
=
resultDims
[
j
]
;
}
num
=
Math
.
floor
(
num
/
size
)
;
}
result
[
resultIndex
]
+
=
(
this
.
data
[
i
]
)
*
*
p
;
}
if
(
p
!
=
=
1
)
{
for
(
let
i
=
0
;
i
<
result
.
length
;
+
+
i
)
{
result
[
i
]
=
result
[
i
]
*
*
(
1
/
p
)
;
}
}
if
(
!
keepdim
)
{
resultDims
.
splice
(
dim
1
)
;
}
return
new
Tensor
(
this
.
type
result
resultDims
)
;
}
normalize_
(
p
=
2
.
0
dim
=
1
)
{
dim
=
safeIndex
(
dim
this
.
dims
.
length
)
;
const
norm
=
this
.
norm
(
p
dim
true
)
;
for
(
let
i
=
0
;
i
<
this
.
data
.
length
;
+
+
i
)
{
let
resultIndex
=
0
;
for
(
let
j
=
this
.
dims
.
length
-
1
num
=
i
resultMultiplier
=
1
;
j
>
=
0
;
-
-
j
)
{
const
size
=
this
.
dims
[
j
]
;
if
(
j
!
=
=
dim
)
{
const
index
=
num
%
size
;
resultIndex
+
=
index
*
resultMultiplier
;
resultMultiplier
*
=
this
.
dims
[
j
]
;
}
num
=
Math
.
floor
(
num
/
size
)
;
}
this
.
data
[
i
]
/
=
norm
.
data
[
resultIndex
]
;
}
return
this
;
}
normalize
(
p
=
2
.
0
dim
=
1
)
{
return
this
.
clone
(
)
.
normalize_
(
p
dim
)
;
}
stride
(
)
{
return
dimsToStride
(
this
.
dims
)
;
}
squeeze
(
dim
=
null
)
{
return
new
Tensor
(
this
.
type
this
.
data
calc_squeeze_dims
(
this
.
dims
dim
)
)
}
squeeze_
(
dim
=
null
)
{
this
.
dims
=
calc_squeeze_dims
(
this
.
dims
dim
)
;
return
this
;
}
unsqueeze
(
dim
=
null
)
{
return
new
Tensor
(
this
.
type
this
.
data
calc_unsqueeze_dims
(
this
.
dims
dim
)
)
;
}
unsqueeze_
(
dim
=
null
)
{
this
.
dims
=
calc_unsqueeze_dims
(
this
.
dims
dim
)
;
return
this
;
}
flatten_
(
start_dim
=
0
end_dim
=
-
1
)
{
end_dim
=
(
end_dim
+
this
.
dims
.
length
)
%
this
.
dims
.
length
;
let
dimsToKeepBefore
=
this
.
dims
.
slice
(
0
start_dim
)
;
let
dimsToFlatten
=
this
.
dims
.
slice
(
start_dim
end_dim
+
1
)
;
let
dimsToKeepAfter
=
this
.
dims
.
slice
(
end_dim
+
1
)
;
this
.
dims
=
[
.
.
.
dimsToKeepBefore
dimsToFlatten
.
reduce
(
(
a
b
)
=
>
a
*
b
1
)
.
.
.
dimsToKeepAfter
]
return
this
;
}
flatten
(
start_dim
=
0
end_dim
=
-
1
)
{
return
this
.
clone
(
)
.
flatten_
(
start_dim
end_dim
)
;
}
view
(
.
.
.
dims
)
{
let
inferredIndex
=
-
1
;
for
(
let
i
=
0
;
i
<
dims
.
length
;
+
+
i
)
{
if
(
dims
[
i
]
=
=
=
-
1
)
{
if
(
inferredIndex
!
=
=
-
1
)
{
throw
new
Error
(
"
Only
one
dimension
can
be
inferred
"
)
;
}
inferredIndex
=
i
;
}
}
if
(
inferredIndex
!
=
=
-
1
)
{
const
productOther
=
dims
.
reduce
(
(
product
curr
index
)
=
>
{
return
index
!
=
=
inferredIndex
?
product
*
curr
:
product
}
1
)
;
dims
[
inferredIndex
]
=
this
.
data
.
length
/
productOther
;
}
return
new
Tensor
(
this
.
type
this
.
data
dims
)
;
}
neg_
(
)
{
for
(
let
i
=
0
;
i
<
this
.
data
.
length
;
+
+
i
)
{
this
.
data
[
i
]
=
-
this
.
data
[
i
]
;
}
return
this
;
}
neg
(
)
{
return
this
.
clone
(
)
.
neg_
(
)
;
}
clamp_
(
min
max
)
{
for
(
let
i
=
0
;
i
<
this
.
data
.
length
;
+
+
i
)
{
this
.
data
[
i
]
=
Math
.
min
(
Math
.
max
(
this
.
data
[
i
]
min
)
max
)
;
}
return
this
;
}
clamp
(
min
max
)
{
return
this
.
clone
(
)
.
clamp_
(
min
max
)
;
}
round_
(
)
{
for
(
let
i
=
0
;
i
<
this
.
data
.
length
;
+
+
i
)
{
this
.
data
[
i
]
=
Math
.
round
(
this
.
data
[
i
]
)
;
}
return
this
;
}
round
(
)
{
return
this
.
clone
(
)
.
round_
(
)
;
}
to
(
type
)
{
if
(
this
.
type
=
=
=
type
)
return
this
;
if
(
!
DataTypeMap
.
hasOwnProperty
(
type
)
)
{
throw
new
Error
(
Unsupported
type
:
{
type
}
)
;
}
return
new
Tensor
(
type
DataTypeMap
[
type
]
.
from
(
this
.
data
)
this
.
dims
)
;
}
}
function
reshape
(
data
dimensions
)
{
const
totalElements
=
data
.
length
;
const
dimensionSize
=
dimensions
.
reduce
(
(
a
b
)
=
>
a
*
b
)
;
if
(
totalElements
!
=
=
dimensionSize
)
{
throw
Error
(
cannot
reshape
array
of
size
{
totalElements
}
into
shape
(
{
dimensions
}
)
)
;
}
let
reshapedArray
=
data
;
for
(
let
i
=
dimensions
.
length
-
1
;
i
>
=
0
;
i
-
-
)
{
reshapedArray
=
reshapedArray
.
reduce
(
(
acc
val
)
=
>
{
let
lastArray
=
acc
[
acc
.
length
-
1
]
;
if
(
lastArray
.
length
<
dimensions
[
i
]
)
{
lastArray
.
push
(
val
)
;
}
else
{
acc
.
push
(
[
val
]
)
;
}
return
acc
;
}
[
[
]
]
)
;
}
return
reshapedArray
[
0
]
;
}
function
permute
(
tensor
axes
)
{
const
[
permutedData
shape
]
=
(
0
_maths_js__WEBPACK_IMPORTED_MODULE_1__
.
permute_data
)
(
tensor
.
data
tensor
.
dims
axes
)
;
return
new
Tensor
(
tensor
.
type
permutedData
shape
)
;
}
function
interpolate
(
input
[
out_height
out_width
]
mode
=
'
bilinear
'
align_corners
=
false
)
{
const
in_channels
=
input
.
dims
.
at
(
-
3
)
?
?
1
;
const
in_height
=
input
.
dims
.
at
(
-
2
)
;
const
in_width
=
input
.
dims
.
at
(
-
1
)
;
let
output
=
(
0
_maths_js__WEBPACK_IMPORTED_MODULE_1__
.
interpolate_data
)
(
(
input
.
data
)
[
in_channels
in_height
in_width
]
[
out_height
out_width
]
mode
align_corners
)
;
return
new
Tensor
(
input
.
type
output
[
in_channels
out_height
out_width
]
)
;
}
function
mean_pooling
(
last_hidden_state
attention_mask
)
{
let
shape
=
[
last_hidden_state
.
dims
[
0
]
last_hidden_state
.
dims
[
2
]
]
;
let
returnedData
=
new
last_hidden_state
.
data
.
constructor
(
shape
[
0
]
*
shape
[
1
]
)
;
let
[
batchSize
seqLength
embedDim
]
=
last_hidden_state
.
dims
;
let
outIndex
=
0
;
for
(
let
i
=
0
;
i
<
batchSize
;
+
+
i
)
{
let
offset
=
i
*
embedDim
*
seqLength
;
for
(
let
k
=
0
;
k
<
embedDim
;
+
+
k
)
{
let
sum
=
0
;
let
count
=
0
;
let
attnMaskOffset
=
i
*
seqLength
;
let
offset2
=
offset
+
k
;
for
(
let
j
=
0
;
j
<
seqLength
;
+
+
j
)
{
let
attn
=
Number
(
attention_mask
.
data
[
attnMaskOffset
+
j
]
)
;
count
+
=
attn
;
sum
+
=
last_hidden_state
.
data
[
offset2
+
j
*
embedDim
]
*
attn
;
}
let
avg
=
sum
/
count
;
returnedData
[
outIndex
+
+
]
=
avg
;
}
}
return
new
Tensor
(
last_hidden_state
.
type
returnedData
shape
)
}
function
layer_norm
(
input
normalized_shape
{
eps
=
1e
-
5
}
=
{
}
)
{
if
(
input
.
dims
.
length
!
=
=
2
)
{
throw
new
Error
(
'
layer_norm
currently
only
supports
2D
input
.
'
)
;
}
const
[
batchSize
featureDim
]
=
input
.
dims
;
if
(
normalized_shape
.
length
!
=
=
1
&
&
normalized_shape
[
0
]
!
=
=
featureDim
)
{
throw
new
Error
(
'
normalized_shape
must
be
a
1D
array
with
shape
[
input
.
dims
[
1
]
]
.
'
)
;
}
const
[
std
mean
]
=
std_mean
(
input
1
0
true
)
;
const
returnedData
=
new
input
.
data
.
constructor
(
input
.
data
.
length
)
;
for
(
let
i
=
0
;
i
<
batchSize
;
+
+
i
)
{
const
offset
=
i
*
featureDim
;
for
(
let
j
=
0
;
j
<
featureDim
;
+
+
j
)
{
const
offset2
=
offset
+
j
;
returnedData
[
offset2
]
=
(
input
.
data
[
offset2
]
-
mean
.
data
[
i
]
)
/
(
std
.
data
[
i
]
+
eps
)
;
}
}
return
new
Tensor
(
input
.
type
returnedData
input
.
dims
)
;
}
function
calc_squeeze_dims
(
dims
dim
)
{
dims
=
dims
.
slice
(
)
;
if
(
dim
=
=
=
null
)
{
dims
=
dims
.
filter
(
(
d
)
=
>
d
!
=
=
1
)
;
}
else
if
(
typeof
dim
=
=
=
'
number
'
)
{
if
(
dims
[
dim
]
=
=
=
1
)
{
dims
.
splice
(
dim
1
)
;
}
}
else
if
(
Array
.
isArray
(
dim
)
)
{
dims
=
dims
.
filter
(
(
x
i
)
=
>
{
return
x
!
=
=
1
|
|
!
dim
.
includes
(
i
)
;
}
)
;
}
return
dims
;
}
function
calc_unsqueeze_dims
(
dims
dim
)
{
dim
=
safeIndex
(
dim
dims
.
length
+
1
)
;
dims
=
dims
.
slice
(
)
;
dims
.
splice
(
dim
0
1
)
;
return
dims
;
}
function
safeIndex
(
index
size
dimension
=
null
)
{
if
(
index
<
-
size
|
|
index
>
=
size
)
{
throw
new
Error
(
IndexError
:
index
{
index
}
is
out
of
bounds
for
dimension
{
dimension
=
=
=
null
?
'
'
:
'
'
+
dimension
}
with
size
{
size
}
)
;
}
if
(
index
<
0
)
{
index
=
(
(
index
%
size
)
+
size
)
%
size
;
}
return
index
;
}
function
cat
(
tensors
dim
=
0
)
{
dim
=
safeIndex
(
dim
tensors
[
0
]
.
dims
.
length
)
;
const
resultDims
=
tensors
[
0
]
.
dims
.
slice
(
)
;
resultDims
[
dim
]
=
tensors
.
reduce
(
(
a
b
)
=
>
a
+
b
.
dims
[
dim
]
0
)
;
const
resultSize
=
resultDims
.
reduce
(
(
a
b
)
=
>
a
*
b
1
)
;
const
result
=
new
tensors
[
0
]
.
data
.
constructor
(
resultSize
)
;
const
resultType
=
tensors
[
0
]
.
type
;
if
(
dim
=
=
=
0
)
{
let
offset
=
0
;
for
(
let
t
of
tensors
)
{
result
.
set
(
t
.
data
offset
)
;
offset
+
=
t
.
data
.
length
;
}
}
else
{
let
currentDim
=
0
;
for
(
let
t
=
0
;
t
<
tensors
.
length
;
+
+
t
)
{
let
tensor
=
tensors
[
t
]
;
for
(
let
i
=
0
;
i
<
tensor
.
data
.
length
;
+
+
i
)
{
let
resultIndex
=
0
;
for
(
let
j
=
tensor
.
dims
.
length
-
1
num
=
i
resultMultiplier
=
1
;
j
>
=
0
;
-
-
j
)
{
const
size
=
tensor
.
dims
[
j
]
;
let
index
=
num
%
size
;
if
(
j
=
=
=
dim
)
{
index
+
=
currentDim
;
}
resultIndex
+
=
index
*
resultMultiplier
;
resultMultiplier
*
=
resultDims
[
j
]
;
num
=
Math
.
floor
(
num
/
size
)
;
}
result
[
resultIndex
]
=
tensor
.
data
[
i
]
;
}
currentDim
+
=
tensor
.
dims
[
dim
]
;
}
}
return
new
Tensor
(
resultType
result
resultDims
)
;
}
function
stack
(
tensors
dim
=
0
)
{
return
cat
(
tensors
.
map
(
t
=
>
t
.
unsqueeze
(
dim
)
)
dim
)
;
}
function
std_mean
(
input
dim
=
null
correction
=
1
keepdim
=
false
)
{
if
(
dim
=
=
=
null
)
{
const
sum
=
input
.
data
.
reduce
(
(
a
b
)
=
>
a
+
b
0
)
;
const
mean
=
sum
/
input
.
data
.
length
;
const
std
=
Math
.
sqrt
(
input
.
data
.
reduce
(
(
a
b
)
=
>
a
+
(
b
-
mean
)
*
*
2
0
)
/
(
input
.
data
.
length
-
correction
)
)
;
const
meanTensor
=
new
Tensor
(
input
.
type
[
mean
]
[
]
)
;
const
stdTensor
=
new
Tensor
(
input
.
type
[
std
]
[
]
)
;
return
[
stdTensor
meanTensor
]
;
}
dim
=
safeIndex
(
dim
input
.
dims
.
length
)
;
const
meanTensor
=
mean
(
input
dim
keepdim
)
;
const
resultDims
=
input
.
dims
.
slice
(
)
;
resultDims
[
dim
]
=
1
;
const
result
=
new
input
.
data
.
constructor
(
input
.
data
.
length
/
input
.
dims
[
dim
]
)
;
for
(
let
i
=
0
;
i
<
input
.
data
.
length
;
+
+
i
)
{
let
resultIndex
=
0
;
for
(
let
j
=
input
.
dims
.
length
-
1
num
=
i
resultMultiplier
=
1
;
j
>
=
0
;
-
-
j
)
{
const
size
=
input
.
dims
[
j
]
;
if
(
j
!
=
=
dim
)
{
const
index
=
num
%
size
;
resultIndex
+
=
index
*
resultMultiplier
;
resultMultiplier
*
=
resultDims
[
j
]
;
}
num
=
Math
.
floor
(
num
/
size
)
;
}
result
[
resultIndex
]
+
=
(
input
.
data
[
i
]
-
meanTensor
.
data
[
resultIndex
]
)
*
*
2
;
}
for
(
let
i
=
0
;
i
<
result
.
length
;
+
+
i
)
{
result
[
i
]
=
Math
.
sqrt
(
result
[
i
]
/
(
input
.
dims
[
dim
]
-
correction
)
)
;
}
if
(
!
keepdim
)
{
resultDims
.
splice
(
dim
1
)
;
}
const
stdTensor
=
new
Tensor
(
input
.
type
result
resultDims
)
;
return
[
stdTensor
meanTensor
]
;
}
function
mean
(
input
dim
=
null
keepdim
=
false
)
{
if
(
dim
=
=
=
null
)
{
let
val
=
input
.
data
.
reduce
(
(
a
b
)
=
>
a
+
b
0
)
;
return
new
Tensor
(
input
.
type
[
val
/
input
.
data
.
length
]
[
]
)
;
}
dim
=
safeIndex
(
dim
input
.
dims
.
length
)
;
const
resultDims
=
input
.
dims
.
slice
(
)
;
resultDims
[
dim
]
=
1
;
const
result
=
new
input
.
data
.
constructor
(
input
.
data
.
length
/
input
.
dims
[
dim
]
)
;
for
(
let
i
=
0
;
i
<
input
.
data
.
length
;
+
+
i
)
{
let
resultIndex
=
0
;
for
(
let
j
=
input
.
dims
.
length
-
1
num
=
i
resultMultiplier
=
1
;
j
>
=
0
;
-
-
j
)
{
const
size
=
input
.
dims
[
j
]
;
if
(
j
!
=
=
dim
)
{
const
index
=
num
%
size
;
resultIndex
+
=
index
*
resultMultiplier
;
resultMultiplier
*
=
resultDims
[
j
]
;
}
num
=
Math
.
floor
(
num
/
size
)
;
}
result
[
resultIndex
]
+
=
input
.
data
[
i
]
;
}
if
(
input
.
dims
[
dim
]
!
=
=
1
)
{
for
(
let
i
=
0
;
i
<
result
.
length
;
+
+
i
)
{
result
[
i
]
=
result
[
i
]
/
input
.
dims
[
dim
]
;
}
}
if
(
!
keepdim
)
{
resultDims
.
splice
(
dim
1
)
;
}
return
new
Tensor
(
input
.
type
result
resultDims
)
;
}
function
dynamicTimeWarping
(
matrix
)
{
const
[
output_length
input_length
]
=
matrix
.
dims
;
const
outputShape
=
[
output_length
+
1
input_length
+
1
]
;
const
cost
=
new
Tensor
(
'
float32
'
new
Float32Array
(
outputShape
[
0
]
*
outputShape
[
1
]
)
.
fill
(
Infinity
)
outputShape
)
;
const
trace
=
new
Tensor
(
'
float32
'
new
Float32Array
(
outputShape
[
0
]
*
outputShape
[
1
]
)
.
fill
(
-
1
)
outputShape
)
cost
[
0
]
.
data
[
0
]
=
0
;
for
(
let
j
=
1
;
j
<
input_length
+
1
;
+
+
j
)
{
for
(
let
i
=
1
;
i
<
output_length
+
1
;
+
+
i
)
{
const
c0
=
cost
[
i
-
1
]
[
j
-
1
]
.
item
(
)
;
const
c1
=
cost
[
i
-
1
]
[
j
]
.
item
(
)
;
const
c2
=
cost
[
i
]
[
j
-
1
]
.
item
(
)
;
let
c
t
;
if
(
c0
<
c1
&
&
c0
<
c2
)
{
c
=
c0
;
t
=
0
;
}
else
if
(
c1
<
c0
&
&
c1
<
c2
)
{
c
=
c1
;
t
=
1
;
}
else
{
c
=
c2
;
t
=
2
;
}
cost
[
i
]
.
data
[
j
]
=
matrix
[
i
-
1
]
[
j
-
1
]
.
item
(
)
+
c
;
trace
[
i
]
.
data
[
j
]
=
t
;
}
}
let
i
=
output_length
;
let
j
=
input_length
;
trace
.
data
.
fill
(
2
0
outputShape
[
1
]
)
for
(
let
i
=
0
;
i
<
outputShape
[
0
]
;
+
+
i
)
{
trace
[
i
]
.
data
[
0
]
=
1
;
}
let
text_indices
=
[
]
;
let
time_indices
=
[
]
;
while
(
i
>
0
|
|
j
>
0
)
{
text_indices
.
push
(
i
-
1
)
;
time_indices
.
push
(
j
-
1
)
;
const
t
=
trace
[
i
]
[
j
]
.
item
(
)
;
switch
(
t
)
{
case
0
:
-
-
i
;
-
-
j
;
break
;
case
1
:
-
-
i
;
break
;
case
2
:
-
-
j
;
break
;
default
:
throw
new
Error
(
Internal
error
in
dynamic
time
warping
.
Unexpected
trace
[
{
i
}
{
j
}
]
.
Please
file
a
bug
report
.
)
}
}
text_indices
.
reverse
(
)
;
time_indices
.
reverse
(
)
;
return
[
text_indices
time_indices
]
;
}
function
dimsToStride
(
dims
)
{
const
stride
=
new
Array
(
dims
.
length
)
;
for
(
let
i
=
dims
.
length
-
1
s2
=
1
;
i
>
=
0
;
-
-
i
)
{
stride
[
i
]
=
s2
;
s2
*
=
dims
[
i
]
;
}
return
stride
;
}
function
ones
(
size
)
{
const
numElements
=
size
.
reduce
(
(
a
b
)
=
>
a
*
b
1
)
;
return
new
Tensor
(
'
int64
'
new
BigInt64Array
(
numElements
)
.
fill
(
1n
)
size
)
}
function
ones_like
(
tensor
)
{
return
ones
(
tensor
.
dims
)
;
}
}
)
}
)
;
var
__webpack_module_cache__
=
{
}
;
function
__webpack_require__
(
moduleId
)
{
var
cachedModule
=
__webpack_module_cache__
[
moduleId
]
;
if
(
cachedModule
!
=
=
undefined
)
{
return
cachedModule
.
exports
;
}
var
module
=
__webpack_module_cache__
[
moduleId
]
=
{
exports
:
{
}
}
;
__webpack_modules__
[
moduleId
]
(
module
module
.
exports
__webpack_require__
)
;
return
module
.
exports
;
}
(
(
)
=
>
{
__webpack_require__
.
d
=
(
exports
definition
)
=
>
{
for
(
var
key
in
definition
)
{
if
(
__webpack_require__
.
o
(
definition
key
)
&
&
!
__webpack_require__
.
o
(
exports
key
)
)
{
Object
.
defineProperty
(
exports
key
{
enumerable
:
true
get
:
definition
[
key
]
}
)
;
}
}
}
;
}
)
(
)
;
(
(
)
=
>
{
__webpack_require__
.
o
=
(
obj
prop
)
=
>
(
Object
.
prototype
.
hasOwnProperty
.
call
(
obj
prop
)
)
}
)
(
)
;
(
(
)
=
>
{
__webpack_require__
.
r
=
(
exports
)
=
>
{
if
(
typeof
Symbol
!
=
=
'
undefined
'
&
&
Symbol
.
toStringTag
)
{
Object
.
defineProperty
(
exports
Symbol
.
toStringTag
{
value
:
'
Module
'
}
)
;
}
Object
.
defineProperty
(
exports
'
__esModule
'
{
value
:
true
}
)
;
}
;
}
)
(
)
;
var
__webpack_exports__
=
__webpack_require__
(
"
.
/
src
/
transformers
.
js
"
)
;
var
__webpack_exports__ASTFeatureExtractor
=
__webpack_exports__
.
ASTFeatureExtractor
;
var
__webpack_exports__ASTForAudioClassification
=
__webpack_exports__
.
ASTForAudioClassification
;
var
__webpack_exports__ASTModel
=
__webpack_exports__
.
ASTModel
;
var
__webpack_exports__ASTPreTrainedModel
=
__webpack_exports__
.
ASTPreTrainedModel
;
var
__webpack_exports__AlbertForMaskedLM
=
__webpack_exports__
.
AlbertForMaskedLM
;
var
__webpack_exports__AlbertForQuestionAnswering
=
__webpack_exports__
.
AlbertForQuestionAnswering
;
var
__webpack_exports__AlbertForSequenceClassification
=
__webpack_exports__
.
AlbertForSequenceClassification
;
var
__webpack_exports__AlbertModel
=
__webpack_exports__
.
AlbertModel
;
var
__webpack_exports__AlbertPreTrainedModel
=
__webpack_exports__
.
AlbertPreTrainedModel
;
var
__webpack_exports__AlbertTokenizer
=
__webpack_exports__
.
AlbertTokenizer
;
var
__webpack_exports__AudioClassificationPipeline
=
__webpack_exports__
.
AudioClassificationPipeline
;
var
__webpack_exports__AutoConfig
=
__webpack_exports__
.
AutoConfig
;
var
__webpack_exports__AutoModel
=
__webpack_exports__
.
AutoModel
;
var
__webpack_exports__AutoModelForAudioClassification
=
__webpack_exports__
.
AutoModelForAudioClassification
;
var
__webpack_exports__AutoModelForAudioFrameClassification
=
__webpack_exports__
.
AutoModelForAudioFrameClassification
;
var
__webpack_exports__AutoModelForCTC
=
__webpack_exports__
.
AutoModelForCTC
;
var
__webpack_exports__AutoModelForCausalLM
=
__webpack_exports__
.
AutoModelForCausalLM
;
var
__webpack_exports__AutoModelForDepthEstimation
=
__webpack_exports__
.
AutoModelForDepthEstimation
;
var
__webpack_exports__AutoModelForDocumentQuestionAnswering
=
__webpack_exports__
.
AutoModelForDocumentQuestionAnswering
;
var
__webpack_exports__AutoModelForImageClassification
=
__webpack_exports__
.
AutoModelForImageClassification
;
var
__webpack_exports__AutoModelForImageFeatureExtraction
=
__webpack_exports__
.
AutoModelForImageFeatureExtraction
;
var
__webpack_exports__AutoModelForImageMatting
=
__webpack_exports__
.
AutoModelForImageMatting
;
var
__webpack_exports__AutoModelForImageSegmentation
=
__webpack_exports__
.
AutoModelForImageSegmentation
;
var
__webpack_exports__AutoModelForImageToImage
=
__webpack_exports__
.
AutoModelForImageToImage
;
var
__webpack_exports__AutoModelForMaskGeneration
=
__webpack_exports__
.
AutoModelForMaskGeneration
;
var
__webpack_exports__AutoModelForMaskedLM
=
__webpack_exports__
.
AutoModelForMaskedLM
;
var
__webpack_exports__AutoModelForObjectDetection
=
__webpack_exports__
.
AutoModelForObjectDetection
;
var
__webpack_exports__AutoModelForQuestionAnswering
=
__webpack_exports__
.
AutoModelForQuestionAnswering
;
var
__webpack_exports__AutoModelForSemanticSegmentation
=
__webpack_exports__
.
AutoModelForSemanticSegmentation
;
var
__webpack_exports__AutoModelForSeq2SeqLM
=
__webpack_exports__
.
AutoModelForSeq2SeqLM
;
var
__webpack_exports__AutoModelForSequenceClassification
=
__webpack_exports__
.
AutoModelForSequenceClassification
;
var
__webpack_exports__AutoModelForSpeechSeq2Seq
=
__webpack_exports__
.
AutoModelForSpeechSeq2Seq
;
var
__webpack_exports__AutoModelForTextToSpectrogram
=
__webpack_exports__
.
AutoModelForTextToSpectrogram
;
var
__webpack_exports__AutoModelForTextToWaveform
=
__webpack_exports__
.
AutoModelForTextToWaveform
;
var
__webpack_exports__AutoModelForTokenClassification
=
__webpack_exports__
.
AutoModelForTokenClassification
;
var
__webpack_exports__AutoModelForVision2Seq
=
__webpack_exports__
.
AutoModelForVision2Seq
;
var
__webpack_exports__AutoModelForXVector
=
__webpack_exports__
.
AutoModelForXVector
;
var
__webpack_exports__AutoModelForZeroShotObjectDetection
=
__webpack_exports__
.
AutoModelForZeroShotObjectDetection
;
var
__webpack_exports__AutoProcessor
=
__webpack_exports__
.
AutoProcessor
;
var
__webpack_exports__AutoTokenizer
=
__webpack_exports__
.
AutoTokenizer
;
var
__webpack_exports__AutomaticSpeechRecognitionPipeline
=
__webpack_exports__
.
AutomaticSpeechRecognitionPipeline
;
var
__webpack_exports__BartForConditionalGeneration
=
__webpack_exports__
.
BartForConditionalGeneration
;
var
__webpack_exports__BartForSequenceClassification
=
__webpack_exports__
.
BartForSequenceClassification
;
var
__webpack_exports__BartModel
=
__webpack_exports__
.
BartModel
;
var
__webpack_exports__BartPretrainedModel
=
__webpack_exports__
.
BartPretrainedModel
;
var
__webpack_exports__BartTokenizer
=
__webpack_exports__
.
BartTokenizer
;
var
__webpack_exports__BaseModelOutput
=
__webpack_exports__
.
BaseModelOutput
;
var
__webpack_exports__BeitFeatureExtractor
=
__webpack_exports__
.
BeitFeatureExtractor
;
var
__webpack_exports__BeitForImageClassification
=
__webpack_exports__
.
BeitForImageClassification
;
var
__webpack_exports__BeitModel
=
__webpack_exports__
.
BeitModel
;
var
__webpack_exports__BeitPreTrainedModel
=
__webpack_exports__
.
BeitPreTrainedModel
;
var
__webpack_exports__BertForMaskedLM
=
__webpack_exports__
.
BertForMaskedLM
;
var
__webpack_exports__BertForQuestionAnswering
=
__webpack_exports__
.
BertForQuestionAnswering
;
var
__webpack_exports__BertForSequenceClassification
=
__webpack_exports__
.
BertForSequenceClassification
;
var
__webpack_exports__BertForTokenClassification
=
__webpack_exports__
.
BertForTokenClassification
;
var
__webpack_exports__BertModel
=
__webpack_exports__
.
BertModel
;
var
__webpack_exports__BertPreTrainedModel
=
__webpack_exports__
.
BertPreTrainedModel
;
var
__webpack_exports__BertTokenizer
=
__webpack_exports__
.
BertTokenizer
;
var
__webpack_exports__BitImageProcessor
=
__webpack_exports__
.
BitImageProcessor
;
var
__webpack_exports__BlenderbotForConditionalGeneration
=
__webpack_exports__
.
BlenderbotForConditionalGeneration
;
var
__webpack_exports__BlenderbotModel
=
__webpack_exports__
.
BlenderbotModel
;
var
__webpack_exports__BlenderbotPreTrainedModel
=
__webpack_exports__
.
BlenderbotPreTrainedModel
;
var
__webpack_exports__BlenderbotSmallForConditionalGeneration
=
__webpack_exports__
.
BlenderbotSmallForConditionalGeneration
;
var
__webpack_exports__BlenderbotSmallModel
=
__webpack_exports__
.
BlenderbotSmallModel
;
var
__webpack_exports__BlenderbotSmallPreTrainedModel
=
__webpack_exports__
.
BlenderbotSmallPreTrainedModel
;
var
__webpack_exports__BlenderbotSmallTokenizer
=
__webpack_exports__
.
BlenderbotSmallTokenizer
;
var
__webpack_exports__BlenderbotTokenizer
=
__webpack_exports__
.
BlenderbotTokenizer
;
var
__webpack_exports__BloomForCausalLM
=
__webpack_exports__
.
BloomForCausalLM
;
var
__webpack_exports__BloomModel
=
__webpack_exports__
.
BloomModel
;
var
__webpack_exports__BloomPreTrainedModel
=
__webpack_exports__
.
BloomPreTrainedModel
;
var
__webpack_exports__BloomTokenizer
=
__webpack_exports__
.
BloomTokenizer
;
var
__webpack_exports__CLIPFeatureExtractor
=
__webpack_exports__
.
CLIPFeatureExtractor
;
var
__webpack_exports__CLIPModel
=
__webpack_exports__
.
CLIPModel
;
var
__webpack_exports__CLIPPreTrainedModel
=
__webpack_exports__
.
CLIPPreTrainedModel
;
var
__webpack_exports__CLIPSegForImageSegmentation
=
__webpack_exports__
.
CLIPSegForImageSegmentation
;
var
__webpack_exports__CLIPSegModel
=
__webpack_exports__
.
CLIPSegModel
;
var
__webpack_exports__CLIPSegPreTrainedModel
=
__webpack_exports__
.
CLIPSegPreTrainedModel
;
var
__webpack_exports__CLIPTextModelWithProjection
=
__webpack_exports__
.
CLIPTextModelWithProjection
;
var
__webpack_exports__CLIPTokenizer
=
__webpack_exports__
.
CLIPTokenizer
;
var
__webpack_exports__CLIPVisionModelWithProjection
=
__webpack_exports__
.
CLIPVisionModelWithProjection
;
var
__webpack_exports__CamembertForMaskedLM
=
__webpack_exports__
.
CamembertForMaskedLM
;
var
__webpack_exports__CamembertForQuestionAnswering
=
__webpack_exports__
.
CamembertForQuestionAnswering
;
var
__webpack_exports__CamembertForSequenceClassification
=
__webpack_exports__
.
CamembertForSequenceClassification
;
var
__webpack_exports__CamembertForTokenClassification
=
__webpack_exports__
.
CamembertForTokenClassification
;
var
__webpack_exports__CamembertModel
=
__webpack_exports__
.
CamembertModel
;
var
__webpack_exports__CamembertPreTrainedModel
=
__webpack_exports__
.
CamembertPreTrainedModel
;
var
__webpack_exports__CamembertTokenizer
=
__webpack_exports__
.
CamembertTokenizer
;
var
__webpack_exports__CausalLMOutput
=
__webpack_exports__
.
CausalLMOutput
;
var
__webpack_exports__CausalLMOutputWithPast
=
__webpack_exports__
.
CausalLMOutputWithPast
;
var
__webpack_exports__ChineseCLIPFeatureExtractor
=
__webpack_exports__
.
ChineseCLIPFeatureExtractor
;
var
__webpack_exports__ChineseCLIPModel
=
__webpack_exports__
.
ChineseCLIPModel
;
var
__webpack_exports__ChineseCLIPPreTrainedModel
=
__webpack_exports__
.
ChineseCLIPPreTrainedModel
;
var
__webpack_exports__ClapAudioModelWithProjection
=
__webpack_exports__
.
ClapAudioModelWithProjection
;
var
__webpack_exports__ClapFeatureExtractor
=
__webpack_exports__
.
ClapFeatureExtractor
;
var
__webpack_exports__ClapModel
=
__webpack_exports__
.
ClapModel
;
var
__webpack_exports__ClapPreTrainedModel
=
__webpack_exports__
.
ClapPreTrainedModel
;
var
__webpack_exports__ClapTextModelWithProjection
=
__webpack_exports__
.
ClapTextModelWithProjection
;
var
__webpack_exports__CodeGenForCausalLM
=
__webpack_exports__
.
CodeGenForCausalLM
;
var
__webpack_exports__CodeGenModel
=
__webpack_exports__
.
CodeGenModel
;
var
__webpack_exports__CodeGenPreTrainedModel
=
__webpack_exports__
.
CodeGenPreTrainedModel
;
var
__webpack_exports__CodeGenTokenizer
=
__webpack_exports__
.
CodeGenTokenizer
;
var
__webpack_exports__CodeLlamaTokenizer
=
__webpack_exports__
.
CodeLlamaTokenizer
;
var
__webpack_exports__CohereTokenizer
=
__webpack_exports__
.
CohereTokenizer
;
var
__webpack_exports__ConvBertForMaskedLM
=
__webpack_exports__
.
ConvBertForMaskedLM
;
var
__webpack_exports__ConvBertForQuestionAnswering
=
__webpack_exports__
.
ConvBertForQuestionAnswering
;
var
__webpack_exports__ConvBertForSequenceClassification
=
__webpack_exports__
.
ConvBertForSequenceClassification
;
var
__webpack_exports__ConvBertForTokenClassification
=
__webpack_exports__
.
ConvBertForTokenClassification
;
var
__webpack_exports__ConvBertModel
=
__webpack_exports__
.
ConvBertModel
;
var
__webpack_exports__ConvBertPreTrainedModel
=
__webpack_exports__
.
ConvBertPreTrainedModel
;
var
__webpack_exports__ConvBertTokenizer
=
__webpack_exports__
.
ConvBertTokenizer
;
var
__webpack_exports__ConvNextFeatureExtractor
=
__webpack_exports__
.
ConvNextFeatureExtractor
;
var
__webpack_exports__ConvNextForImageClassification
=
__webpack_exports__
.
ConvNextForImageClassification
;
var
__webpack_exports__ConvNextImageProcessor
=
__webpack_exports__
.
ConvNextImageProcessor
;
var
__webpack_exports__ConvNextModel
=
__webpack_exports__
.
ConvNextModel
;
var
__webpack_exports__ConvNextPreTrainedModel
=
__webpack_exports__
.
ConvNextPreTrainedModel
;
var
__webpack_exports__ConvNextV2ForImageClassification
=
__webpack_exports__
.
ConvNextV2ForImageClassification
;
var
__webpack_exports__ConvNextV2Model
=
__webpack_exports__
.
ConvNextV2Model
;
var
__webpack_exports__ConvNextV2PreTrainedModel
=
__webpack_exports__
.
ConvNextV2PreTrainedModel
;
var
__webpack_exports__DPTFeatureExtractor
=
__webpack_exports__
.
DPTFeatureExtractor
;
var
__webpack_exports__DPTForDepthEstimation
=
__webpack_exports__
.
DPTForDepthEstimation
;
var
__webpack_exports__DPTImageProcessor
=
__webpack_exports__
.
DPTImageProcessor
;
var
__webpack_exports__DPTModel
=
__webpack_exports__
.
DPTModel
;
var
__webpack_exports__DPTPreTrainedModel
=
__webpack_exports__
.
DPTPreTrainedModel
;
var
__webpack_exports__DebertaForMaskedLM
=
__webpack_exports__
.
DebertaForMaskedLM
;
var
__webpack_exports__DebertaForQuestionAnswering
=
__webpack_exports__
.
DebertaForQuestionAnswering
;
var
__webpack_exports__DebertaForSequenceClassification
=
__webpack_exports__
.
DebertaForSequenceClassification
;
var
__webpack_exports__DebertaForTokenClassification
=
__webpack_exports__
.
DebertaForTokenClassification
;
var
__webpack_exports__DebertaModel
=
__webpack_exports__
.
DebertaModel
;
var
__webpack_exports__DebertaPreTrainedModel
=
__webpack_exports__
.
DebertaPreTrainedModel
;
var
__webpack_exports__DebertaTokenizer
=
__webpack_exports__
.
DebertaTokenizer
;
var
__webpack_exports__DebertaV2ForMaskedLM
=
__webpack_exports__
.
DebertaV2ForMaskedLM
;
var
__webpack_exports__DebertaV2ForQuestionAnswering
=
__webpack_exports__
.
DebertaV2ForQuestionAnswering
;
var
__webpack_exports__DebertaV2ForSequenceClassification
=
__webpack_exports__
.
DebertaV2ForSequenceClassification
;
var
__webpack_exports__DebertaV2ForTokenClassification
=
__webpack_exports__
.
DebertaV2ForTokenClassification
;
var
__webpack_exports__DebertaV2Model
=
__webpack_exports__
.
DebertaV2Model
;
var
__webpack_exports__DebertaV2PreTrainedModel
=
__webpack_exports__
.
DebertaV2PreTrainedModel
;
var
__webpack_exports__DebertaV2Tokenizer
=
__webpack_exports__
.
DebertaV2Tokenizer
;
var
__webpack_exports__DeiTFeatureExtractor
=
__webpack_exports__
.
DeiTFeatureExtractor
;
var
__webpack_exports__DeiTForImageClassification
=
__webpack_exports__
.
DeiTForImageClassification
;
var
__webpack_exports__DeiTModel
=
__webpack_exports__
.
DeiTModel
;
var
__webpack_exports__DeiTPreTrainedModel
=
__webpack_exports__
.
DeiTPreTrainedModel
;
var
__webpack_exports__DepthAnythingForDepthEstimation
=
__webpack_exports__
.
DepthAnythingForDepthEstimation
;
var
__webpack_exports__DepthAnythingPreTrainedModel
=
__webpack_exports__
.
DepthAnythingPreTrainedModel
;
var
__webpack_exports__DepthEstimationPipeline
=
__webpack_exports__
.
DepthEstimationPipeline
;
var
__webpack_exports__DetrFeatureExtractor
=
__webpack_exports__
.
DetrFeatureExtractor
;
var
__webpack_exports__DetrForObjectDetection
=
__webpack_exports__
.
DetrForObjectDetection
;
var
__webpack_exports__DetrForSegmentation
=
__webpack_exports__
.
DetrForSegmentation
;
var
__webpack_exports__DetrModel
=
__webpack_exports__
.
DetrModel
;
var
__webpack_exports__DetrObjectDetectionOutput
=
__webpack_exports__
.
DetrObjectDetectionOutput
;
var
__webpack_exports__DetrPreTrainedModel
=
__webpack_exports__
.
DetrPreTrainedModel
;
var
__webpack_exports__DetrSegmentationOutput
=
__webpack_exports__
.
DetrSegmentationOutput
;
var
__webpack_exports__Dinov2ForImageClassification
=
__webpack_exports__
.
Dinov2ForImageClassification
;
var
__webpack_exports__Dinov2Model
=
__webpack_exports__
.
Dinov2Model
;
var
__webpack_exports__Dinov2PreTrainedModel
=
__webpack_exports__
.
Dinov2PreTrainedModel
;
var
__webpack_exports__DistilBertForMaskedLM
=
__webpack_exports__
.
DistilBertForMaskedLM
;
var
__webpack_exports__DistilBertForQuestionAnswering
=
__webpack_exports__
.
DistilBertForQuestionAnswering
;
var
__webpack_exports__DistilBertForSequenceClassification
=
__webpack_exports__
.
DistilBertForSequenceClassification
;
var
__webpack_exports__DistilBertForTokenClassification
=
__webpack_exports__
.
DistilBertForTokenClassification
;
var
__webpack_exports__DistilBertModel
=
__webpack_exports__
.
DistilBertModel
;
var
__webpack_exports__DistilBertPreTrainedModel
=
__webpack_exports__
.
DistilBertPreTrainedModel
;
var
__webpack_exports__DistilBertTokenizer
=
__webpack_exports__
.
DistilBertTokenizer
;
var
__webpack_exports__DocumentQuestionAnsweringPipeline
=
__webpack_exports__
.
DocumentQuestionAnsweringPipeline
;
var
__webpack_exports__DonutFeatureExtractor
=
__webpack_exports__
.
DonutFeatureExtractor
;
var
__webpack_exports__DonutSwinModel
=
__webpack_exports__
.
DonutSwinModel
;
var
__webpack_exports__DonutSwinPreTrainedModel
=
__webpack_exports__
.
DonutSwinPreTrainedModel
;
var
__webpack_exports__EfficientNetForImageClassification
=
__webpack_exports__
.
EfficientNetForImageClassification
;
var
__webpack_exports__EfficientNetImageProcessor
=
__webpack_exports__
.
EfficientNetImageProcessor
;
var
__webpack_exports__EfficientNetModel
=
__webpack_exports__
.
EfficientNetModel
;
var
__webpack_exports__EfficientNetPreTrainedModel
=
__webpack_exports__
.
EfficientNetPreTrainedModel
;
var
__webpack_exports__ElectraForMaskedLM
=
__webpack_exports__
.
ElectraForMaskedLM
;
var
__webpack_exports__ElectraForQuestionAnswering
=
__webpack_exports__
.
ElectraForQuestionAnswering
;
var
__webpack_exports__ElectraForSequenceClassification
=
__webpack_exports__
.
ElectraForSequenceClassification
;
var
__webpack_exports__ElectraForTokenClassification
=
__webpack_exports__
.
ElectraForTokenClassification
;
var
__webpack_exports__ElectraModel
=
__webpack_exports__
.
ElectraModel
;
var
__webpack_exports__ElectraPreTrainedModel
=
__webpack_exports__
.
ElectraPreTrainedModel
;
var
__webpack_exports__ElectraTokenizer
=
__webpack_exports__
.
ElectraTokenizer
;
var
__webpack_exports__EsmForMaskedLM
=
__webpack_exports__
.
EsmForMaskedLM
;
var
__webpack_exports__EsmForSequenceClassification
=
__webpack_exports__
.
EsmForSequenceClassification
;
var
__webpack_exports__EsmForTokenClassification
=
__webpack_exports__
.
EsmForTokenClassification
;
var
__webpack_exports__EsmModel
=
__webpack_exports__
.
EsmModel
;
var
__webpack_exports__EsmPreTrainedModel
=
__webpack_exports__
.
EsmPreTrainedModel
;
var
__webpack_exports__EsmTokenizer
=
__webpack_exports__
.
EsmTokenizer
;
var
__webpack_exports__FFT
=
__webpack_exports__
.
FFT
;
var
__webpack_exports__FalconForCausalLM
=
__webpack_exports__
.
FalconForCausalLM
;
var
__webpack_exports__FalconModel
=
__webpack_exports__
.
FalconModel
;
var
__webpack_exports__FalconPreTrainedModel
=
__webpack_exports__
.
FalconPreTrainedModel
;
var
__webpack_exports__FalconTokenizer
=
__webpack_exports__
.
FalconTokenizer
;
var
__webpack_exports__FeatureExtractionPipeline
=
__webpack_exports__
.
FeatureExtractionPipeline
;
var
__webpack_exports__FeatureExtractor
=
__webpack_exports__
.
FeatureExtractor
;
var
__webpack_exports__FillMaskPipeline
=
__webpack_exports__
.
FillMaskPipeline
;
var
__webpack_exports__GLPNFeatureExtractor
=
__webpack_exports__
.
GLPNFeatureExtractor
;
var
__webpack_exports__GLPNForDepthEstimation
=
__webpack_exports__
.
GLPNForDepthEstimation
;
var
__webpack_exports__GLPNModel
=
__webpack_exports__
.
GLPNModel
;
var
__webpack_exports__GLPNPreTrainedModel
=
__webpack_exports__
.
GLPNPreTrainedModel
;
var
__webpack_exports__GPT2LMHeadModel
=
__webpack_exports__
.
GPT2LMHeadModel
;
var
__webpack_exports__GPT2Model
=
__webpack_exports__
.
GPT2Model
;
var
__webpack_exports__GPT2PreTrainedModel
=
__webpack_exports__
.
GPT2PreTrainedModel
;
var
__webpack_exports__GPT2Tokenizer
=
__webpack_exports__
.
GPT2Tokenizer
;
var
__webpack_exports__GPTBigCodeForCausalLM
=
__webpack_exports__
.
GPTBigCodeForCausalLM
;
var
__webpack_exports__GPTBigCodeModel
=
__webpack_exports__
.
GPTBigCodeModel
;
var
__webpack_exports__GPTBigCodePreTrainedModel
=
__webpack_exports__
.
GPTBigCodePreTrainedModel
;
var
__webpack_exports__GPTJForCausalLM
=
__webpack_exports__
.
GPTJForCausalLM
;
var
__webpack_exports__GPTJModel
=
__webpack_exports__
.
GPTJModel
;
var
__webpack_exports__GPTJPreTrainedModel
=
__webpack_exports__
.
GPTJPreTrainedModel
;
var
__webpack_exports__GPTNeoForCausalLM
=
__webpack_exports__
.
GPTNeoForCausalLM
;
var
__webpack_exports__GPTNeoModel
=
__webpack_exports__
.
GPTNeoModel
;
var
__webpack_exports__GPTNeoPreTrainedModel
=
__webpack_exports__
.
GPTNeoPreTrainedModel
;
var
__webpack_exports__GPTNeoXForCausalLM
=
__webpack_exports__
.
GPTNeoXForCausalLM
;
var
__webpack_exports__GPTNeoXModel
=
__webpack_exports__
.
GPTNeoXModel
;
var
__webpack_exports__GPTNeoXPreTrainedModel
=
__webpack_exports__
.
GPTNeoXPreTrainedModel
;
var
__webpack_exports__GPTNeoXTokenizer
=
__webpack_exports__
.
GPTNeoXTokenizer
;
var
__webpack_exports__GemmaTokenizer
=
__webpack_exports__
.
GemmaTokenizer
;
var
__webpack_exports__Grok1Tokenizer
=
__webpack_exports__
.
Grok1Tokenizer
;
var
__webpack_exports__HerbertTokenizer
=
__webpack_exports__
.
HerbertTokenizer
;
var
__webpack_exports__HubertForCTC
=
__webpack_exports__
.
HubertForCTC
;
var
__webpack_exports__HubertForSequenceClassification
=
__webpack_exports__
.
HubertForSequenceClassification
;
var
__webpack_exports__HubertModel
=
__webpack_exports__
.
HubertModel
;
var
__webpack_exports__HubertPreTrainedModel
=
__webpack_exports__
.
HubertPreTrainedModel
;
var
__webpack_exports__ImageClassificationPipeline
=
__webpack_exports__
.
ImageClassificationPipeline
;
var
__webpack_exports__ImageFeatureExtractionPipeline
=
__webpack_exports__
.
ImageFeatureExtractionPipeline
;
var
__webpack_exports__ImageFeatureExtractor
=
__webpack_exports__
.
ImageFeatureExtractor
;
var
__webpack_exports__ImageMattingOutput
=
__webpack_exports__
.
ImageMattingOutput
;
var
__webpack_exports__ImageSegmentationPipeline
=
__webpack_exports__
.
ImageSegmentationPipeline
;
var
__webpack_exports__ImageToImagePipeline
=
__webpack_exports__
.
ImageToImagePipeline
;
var
__webpack_exports__ImageToTextPipeline
=
__webpack_exports__
.
ImageToTextPipeline
;
var
__webpack_exports__LlamaForCausalLM
=
__webpack_exports__
.
LlamaForCausalLM
;
var
__webpack_exports__LlamaModel
=
__webpack_exports__
.
LlamaModel
;
var
__webpack_exports__LlamaPreTrainedModel
=
__webpack_exports__
.
LlamaPreTrainedModel
;
var
__webpack_exports__LlamaTokenizer
=
__webpack_exports__
.
LlamaTokenizer
;
var
__webpack_exports__LongT5ForConditionalGeneration
=
__webpack_exports__
.
LongT5ForConditionalGeneration
;
var
__webpack_exports__LongT5Model
=
__webpack_exports__
.
LongT5Model
;
var
__webpack_exports__LongT5PreTrainedModel
=
__webpack_exports__
.
LongT5PreTrainedModel
;
var
__webpack_exports__M2M100ForConditionalGeneration
=
__webpack_exports__
.
M2M100ForConditionalGeneration
;
var
__webpack_exports__M2M100Model
=
__webpack_exports__
.
M2M100Model
;
var
__webpack_exports__M2M100PreTrainedModel
=
__webpack_exports__
.
M2M100PreTrainedModel
;
var
__webpack_exports__M2M100Tokenizer
=
__webpack_exports__
.
M2M100Tokenizer
;
var
__webpack_exports__MBart50Tokenizer
=
__webpack_exports__
.
MBart50Tokenizer
;
var
__webpack_exports__MBartForCausalLM
=
__webpack_exports__
.
MBartForCausalLM
;
var
__webpack_exports__MBartForConditionalGeneration
=
__webpack_exports__
.
MBartForConditionalGeneration
;
var
__webpack_exports__MBartForSequenceClassification
=
__webpack_exports__
.
MBartForSequenceClassification
;
var
__webpack_exports__MBartModel
=
__webpack_exports__
.
MBartModel
;
var
__webpack_exports__MBartPreTrainedModel
=
__webpack_exports__
.
MBartPreTrainedModel
;
var
__webpack_exports__MBartTokenizer
=
__webpack_exports__
.
MBartTokenizer
;
var
__webpack_exports__MPNetForMaskedLM
=
__webpack_exports__
.
MPNetForMaskedLM
;
var
__webpack_exports__MPNetForQuestionAnswering
=
__webpack_exports__
.
MPNetForQuestionAnswering
;
var
__webpack_exports__MPNetForSequenceClassification
=
__webpack_exports__
.
MPNetForSequenceClassification
;
var
__webpack_exports__MPNetForTokenClassification
=
__webpack_exports__
.
MPNetForTokenClassification
;
var
__webpack_exports__MPNetModel
=
__webpack_exports__
.
MPNetModel
;
var
__webpack_exports__MPNetPreTrainedModel
=
__webpack_exports__
.
MPNetPreTrainedModel
;
var
__webpack_exports__MPNetTokenizer
=
__webpack_exports__
.
MPNetTokenizer
;
var
__webpack_exports__MT5ForConditionalGeneration
=
__webpack_exports__
.
MT5ForConditionalGeneration
;
var
__webpack_exports__MT5Model
=
__webpack_exports__
.
MT5Model
;
var
__webpack_exports__MT5PreTrainedModel
=
__webpack_exports__
.
MT5PreTrainedModel
;
var
__webpack_exports__MarianMTModel
=
__webpack_exports__
.
MarianMTModel
;
var
__webpack_exports__MarianModel
=
__webpack_exports__
.
MarianModel
;
var
__webpack_exports__MarianPreTrainedModel
=
__webpack_exports__
.
MarianPreTrainedModel
;
var
__webpack_exports__MarianTokenizer
=
__webpack_exports__
.
MarianTokenizer
;
var
__webpack_exports__MaskedLMOutput
=
__webpack_exports__
.
MaskedLMOutput
;
var
__webpack_exports__MistralForCausalLM
=
__webpack_exports__
.
MistralForCausalLM
;
var
__webpack_exports__MistralModel
=
__webpack_exports__
.
MistralModel
;
var
__webpack_exports__MistralPreTrainedModel
=
__webpack_exports__
.
MistralPreTrainedModel
;
var
__webpack_exports__MobileBertForMaskedLM
=
__webpack_exports__
.
MobileBertForMaskedLM
;
var
__webpack_exports__MobileBertForQuestionAnswering
=
__webpack_exports__
.
MobileBertForQuestionAnswering
;
var
__webpack_exports__MobileBertForSequenceClassification
=
__webpack_exports__
.
MobileBertForSequenceClassification
;
var
__webpack_exports__MobileBertModel
=
__webpack_exports__
.
MobileBertModel
;
var
__webpack_exports__MobileBertPreTrainedModel
=
__webpack_exports__
.
MobileBertPreTrainedModel
;
var
__webpack_exports__MobileBertTokenizer
=
__webpack_exports__
.
MobileBertTokenizer
;
var
__webpack_exports__MobileViTFeatureExtractor
=
__webpack_exports__
.
MobileViTFeatureExtractor
;
var
__webpack_exports__MobileViTForImageClassification
=
__webpack_exports__
.
MobileViTForImageClassification
;
var
__webpack_exports__MobileViTModel
=
__webpack_exports__
.
MobileViTModel
;
var
__webpack_exports__MobileViTPreTrainedModel
=
__webpack_exports__
.
MobileViTPreTrainedModel
;
var
__webpack_exports__ModelOutput
=
__webpack_exports__
.
ModelOutput
;
var
__webpack_exports__MptForCausalLM
=
__webpack_exports__
.
MptForCausalLM
;
var
__webpack_exports__MptModel
=
__webpack_exports__
.
MptModel
;
var
__webpack_exports__MptPreTrainedModel
=
__webpack_exports__
.
MptPreTrainedModel
;
var
__webpack_exports__NllbTokenizer
=
__webpack_exports__
.
NllbTokenizer
;
var
__webpack_exports__NomicBertModel
=
__webpack_exports__
.
NomicBertModel
;
var
__webpack_exports__NomicBertPreTrainedModel
=
__webpack_exports__
.
NomicBertPreTrainedModel
;
var
__webpack_exports__NougatImageProcessor
=
__webpack_exports__
.
NougatImageProcessor
;
var
__webpack_exports__NougatTokenizer
=
__webpack_exports__
.
NougatTokenizer
;
var
__webpack_exports__OPTForCausalLM
=
__webpack_exports__
.
OPTForCausalLM
;
var
__webpack_exports__OPTModel
=
__webpack_exports__
.
OPTModel
;
var
__webpack_exports__OPTPreTrainedModel
=
__webpack_exports__
.
OPTPreTrainedModel
;
var
__webpack_exports__ObjectDetectionPipeline
=
__webpack_exports__
.
ObjectDetectionPipeline
;
var
__webpack_exports__OwlViTFeatureExtractor
=
__webpack_exports__
.
OwlViTFeatureExtractor
;
var
__webpack_exports__OwlViTForObjectDetection
=
__webpack_exports__
.
OwlViTForObjectDetection
;
var
__webpack_exports__OwlViTModel
=
__webpack_exports__
.
OwlViTModel
;
var
__webpack_exports__OwlViTPreTrainedModel
=
__webpack_exports__
.
OwlViTPreTrainedModel
;
var
__webpack_exports__OwlViTProcessor
=
__webpack_exports__
.
OwlViTProcessor
;
var
__webpack_exports__Owlv2ForObjectDetection
=
__webpack_exports__
.
Owlv2ForObjectDetection
;
var
__webpack_exports__Owlv2ImageProcessor
=
__webpack_exports__
.
Owlv2ImageProcessor
;
var
__webpack_exports__Owlv2Model
=
__webpack_exports__
.
Owlv2Model
;
var
__webpack_exports__Owlv2PreTrainedModel
=
__webpack_exports__
.
Owlv2PreTrainedModel
;
var
__webpack_exports__PhiForCausalLM
=
__webpack_exports__
.
PhiForCausalLM
;
var
__webpack_exports__PhiModel
=
__webpack_exports__
.
PhiModel
;
var
__webpack_exports__PhiPreTrainedModel
=
__webpack_exports__
.
PhiPreTrainedModel
;
var
__webpack_exports__Pipeline
=
__webpack_exports__
.
Pipeline
;
var
__webpack_exports__PreTrainedModel
=
__webpack_exports__
.
PreTrainedModel
;
var
__webpack_exports__PreTrainedTokenizer
=
__webpack_exports__
.
PreTrainedTokenizer
;
var
__webpack_exports__PretrainedConfig
=
__webpack_exports__
.
PretrainedConfig
;
var
__webpack_exports__PretrainedMixin
=
__webpack_exports__
.
PretrainedMixin
;
var
__webpack_exports__Processor
=
__webpack_exports__
.
Processor
;
var
__webpack_exports__QuestionAnsweringModelOutput
=
__webpack_exports__
.
QuestionAnsweringModelOutput
;
var
__webpack_exports__QuestionAnsweringPipeline
=
__webpack_exports__
.
QuestionAnsweringPipeline
;
var
__webpack_exports__Qwen2ForCausalLM
=
__webpack_exports__
.
Qwen2ForCausalLM
;
var
__webpack_exports__Qwen2Model
=
__webpack_exports__
.
Qwen2Model
;
var
__webpack_exports__Qwen2PreTrainedModel
=
__webpack_exports__
.
Qwen2PreTrainedModel
;
var
__webpack_exports__Qwen2Tokenizer
=
__webpack_exports__
.
Qwen2Tokenizer
;
var
__webpack_exports__RawImage
=
__webpack_exports__
.
RawImage
;
var
__webpack_exports__ResNetForImageClassification
=
__webpack_exports__
.
ResNetForImageClassification
;
var
__webpack_exports__ResNetModel
=
__webpack_exports__
.
ResNetModel
;
var
__webpack_exports__ResNetPreTrainedModel
=
__webpack_exports__
.
ResNetPreTrainedModel
;
var
__webpack_exports__RoFormerForMaskedLM
=
__webpack_exports__
.
RoFormerForMaskedLM
;
var
__webpack_exports__RoFormerForQuestionAnswering
=
__webpack_exports__
.
RoFormerForQuestionAnswering
;
var
__webpack_exports__RoFormerForSequenceClassification
=
__webpack_exports__
.
RoFormerForSequenceClassification
;
var
__webpack_exports__RoFormerForTokenClassification
=
__webpack_exports__
.
RoFormerForTokenClassification
;
var
__webpack_exports__RoFormerModel
=
__webpack_exports__
.
RoFormerModel
;
var
__webpack_exports__RoFormerPreTrainedModel
=
__webpack_exports__
.
RoFormerPreTrainedModel
;
var
__webpack_exports__RoFormerTokenizer
=
__webpack_exports__
.
RoFormerTokenizer
;
var
__webpack_exports__RobertaForMaskedLM
=
__webpack_exports__
.
RobertaForMaskedLM
;
var
__webpack_exports__RobertaForQuestionAnswering
=
__webpack_exports__
.
RobertaForQuestionAnswering
;
var
__webpack_exports__RobertaForSequenceClassification
=
__webpack_exports__
.
RobertaForSequenceClassification
;
var
__webpack_exports__RobertaForTokenClassification
=
__webpack_exports__
.
RobertaForTokenClassification
;
var
__webpack_exports__RobertaModel
=
__webpack_exports__
.
RobertaModel
;
var
__webpack_exports__RobertaPreTrainedModel
=
__webpack_exports__
.
RobertaPreTrainedModel
;
var
__webpack_exports__RobertaTokenizer
=
__webpack_exports__
.
RobertaTokenizer
;
var
__webpack_exports__SamImageProcessor
=
__webpack_exports__
.
SamImageProcessor
;
var
__webpack_exports__SamImageSegmentationOutput
=
__webpack_exports__
.
SamImageSegmentationOutput
;
var
__webpack_exports__SamModel
=
__webpack_exports__
.
SamModel
;
var
__webpack_exports__SamPreTrainedModel
=
__webpack_exports__
.
SamPreTrainedModel
;
var
__webpack_exports__SamProcessor
=
__webpack_exports__
.
SamProcessor
;
var
__webpack_exports__SeamlessM4TFeatureExtractor
=
__webpack_exports__
.
SeamlessM4TFeatureExtractor
;
var
__webpack_exports__SegformerFeatureExtractor
=
__webpack_exports__
.
SegformerFeatureExtractor
;
var
__webpack_exports__SegformerForImageClassification
=
__webpack_exports__
.
SegformerForImageClassification
;
var
__webpack_exports__SegformerForSemanticSegmentation
=
__webpack_exports__
.
SegformerForSemanticSegmentation
;
var
__webpack_exports__SegformerModel
=
__webpack_exports__
.
SegformerModel
;
var
__webpack_exports__SegformerPreTrainedModel
=
__webpack_exports__
.
SegformerPreTrainedModel
;
var
__webpack_exports__Seq2SeqLMOutput
=
__webpack_exports__
.
Seq2SeqLMOutput
;
var
__webpack_exports__SequenceClassifierOutput
=
__webpack_exports__
.
SequenceClassifierOutput
;
var
__webpack_exports__SiglipImageProcessor
=
__webpack_exports__
.
SiglipImageProcessor
;
var
__webpack_exports__SiglipModel
=
__webpack_exports__
.
SiglipModel
;
var
__webpack_exports__SiglipPreTrainedModel
=
__webpack_exports__
.
SiglipPreTrainedModel
;
var
__webpack_exports__SiglipTextModel
=
__webpack_exports__
.
SiglipTextModel
;
var
__webpack_exports__SiglipTokenizer
=
__webpack_exports__
.
SiglipTokenizer
;
var
__webpack_exports__SiglipVisionModel
=
__webpack_exports__
.
SiglipVisionModel
;
var
__webpack_exports__SpeechT5FeatureExtractor
=
__webpack_exports__
.
SpeechT5FeatureExtractor
;
var
__webpack_exports__SpeechT5ForSpeechToText
=
__webpack_exports__
.
SpeechT5ForSpeechToText
;
var
__webpack_exports__SpeechT5ForTextToSpeech
=
__webpack_exports__
.
SpeechT5ForTextToSpeech
;
var
__webpack_exports__SpeechT5HifiGan
=
__webpack_exports__
.
SpeechT5HifiGan
;
var
__webpack_exports__SpeechT5Model
=
__webpack_exports__
.
SpeechT5Model
;
var
__webpack_exports__SpeechT5PreTrainedModel
=
__webpack_exports__
.
SpeechT5PreTrainedModel
;
var
__webpack_exports__SpeechT5Processor
=
__webpack_exports__
.
SpeechT5Processor
;
var
__webpack_exports__SpeechT5Tokenizer
=
__webpack_exports__
.
SpeechT5Tokenizer
;
var
__webpack_exports__SqueezeBertForMaskedLM
=
__webpack_exports__
.
SqueezeBertForMaskedLM
;
var
__webpack_exports__SqueezeBertForQuestionAnswering
=
__webpack_exports__
.
SqueezeBertForQuestionAnswering
;
var
__webpack_exports__SqueezeBertForSequenceClassification
=
__webpack_exports__
.
SqueezeBertForSequenceClassification
;
var
__webpack_exports__SqueezeBertModel
=
__webpack_exports__
.
SqueezeBertModel
;
var
__webpack_exports__SqueezeBertPreTrainedModel
=
__webpack_exports__
.
SqueezeBertPreTrainedModel
;
var
__webpack_exports__SqueezeBertTokenizer
=
__webpack_exports__
.
SqueezeBertTokenizer
;
var
__webpack_exports__StableLmForCausalLM
=
__webpack_exports__
.
StableLmForCausalLM
;
var
__webpack_exports__StableLmModel
=
__webpack_exports__
.
StableLmModel
;
var
__webpack_exports__StableLmPreTrainedModel
=
__webpack_exports__
.
StableLmPreTrainedModel
;
var
__webpack_exports__Starcoder2ForCausalLM
=
__webpack_exports__
.
Starcoder2ForCausalLM
;
var
__webpack_exports__Starcoder2Model
=
__webpack_exports__
.
Starcoder2Model
;
var
__webpack_exports__Starcoder2PreTrainedModel
=
__webpack_exports__
.
Starcoder2PreTrainedModel
;
var
__webpack_exports__SummarizationPipeline
=
__webpack_exports__
.
SummarizationPipeline
;
var
__webpack_exports__Swin2SRForImageSuperResolution
=
__webpack_exports__
.
Swin2SRForImageSuperResolution
;
var
__webpack_exports__Swin2SRImageProcessor
=
__webpack_exports__
.
Swin2SRImageProcessor
;
var
__webpack_exports__Swin2SRModel
=
__webpack_exports__
.
Swin2SRModel
;
var
__webpack_exports__Swin2SRPreTrainedModel
=
__webpack_exports__
.
Swin2SRPreTrainedModel
;
var
__webpack_exports__SwinForImageClassification
=
__webpack_exports__
.
SwinForImageClassification
;
var
__webpack_exports__SwinModel
=
__webpack_exports__
.
SwinModel
;
var
__webpack_exports__SwinPreTrainedModel
=
__webpack_exports__
.
SwinPreTrainedModel
;
var
__webpack_exports__T5ForConditionalGeneration
=
__webpack_exports__
.
T5ForConditionalGeneration
;
var
__webpack_exports__T5Model
=
__webpack_exports__
.
T5Model
;
var
__webpack_exports__T5PreTrainedModel
=
__webpack_exports__
.
T5PreTrainedModel
;
var
__webpack_exports__T5Tokenizer
=
__webpack_exports__
.
T5Tokenizer
;
var
__webpack_exports__TableTransformerForObjectDetection
=
__webpack_exports__
.
TableTransformerForObjectDetection
;
var
__webpack_exports__TableTransformerModel
=
__webpack_exports__
.
TableTransformerModel
;
var
__webpack_exports__TableTransformerObjectDetectionOutput
=
__webpack_exports__
.
TableTransformerObjectDetectionOutput
;
var
__webpack_exports__TableTransformerPreTrainedModel
=
__webpack_exports__
.
TableTransformerPreTrainedModel
;
var
__webpack_exports__Tensor
=
__webpack_exports__
.
Tensor
;
var
__webpack_exports__Text2TextGenerationPipeline
=
__webpack_exports__
.
Text2TextGenerationPipeline
;
var
__webpack_exports__TextClassificationPipeline
=
__webpack_exports__
.
TextClassificationPipeline
;
var
__webpack_exports__TextGenerationPipeline
=
__webpack_exports__
.
TextGenerationPipeline
;
var
__webpack_exports__TextToAudioPipeline
=
__webpack_exports__
.
TextToAudioPipeline
;
var
__webpack_exports__TokenClassificationPipeline
=
__webpack_exports__
.
TokenClassificationPipeline
;
var
__webpack_exports__TokenClassifierOutput
=
__webpack_exports__
.
TokenClassifierOutput
;
var
__webpack_exports__TokenizerModel
=
__webpack_exports__
.
TokenizerModel
;
var
__webpack_exports__TrOCRForCausalLM
=
__webpack_exports__
.
TrOCRForCausalLM
;
var
__webpack_exports__TrOCRPreTrainedModel
=
__webpack_exports__
.
TrOCRPreTrainedModel
;
var
__webpack_exports__TranslationPipeline
=
__webpack_exports__
.
TranslationPipeline
;
var
__webpack_exports__UniSpeechForCTC
=
__webpack_exports__
.
UniSpeechForCTC
;
var
__webpack_exports__UniSpeechForSequenceClassification
=
__webpack_exports__
.
UniSpeechForSequenceClassification
;
var
__webpack_exports__UniSpeechModel
=
__webpack_exports__
.
UniSpeechModel
;
var
__webpack_exports__UniSpeechPreTrainedModel
=
__webpack_exports__
.
UniSpeechPreTrainedModel
;
var
__webpack_exports__UniSpeechSatForAudioFrameClassification
=
__webpack_exports__
.
UniSpeechSatForAudioFrameClassification
;
var
__webpack_exports__UniSpeechSatForCTC
=
__webpack_exports__
.
UniSpeechSatForCTC
;
var
__webpack_exports__UniSpeechSatForSequenceClassification
=
__webpack_exports__
.
UniSpeechSatForSequenceClassification
;
var
__webpack_exports__UniSpeechSatModel
=
__webpack_exports__
.
UniSpeechSatModel
;
var
__webpack_exports__UniSpeechSatPreTrainedModel
=
__webpack_exports__
.
UniSpeechSatPreTrainedModel
;
var
__webpack_exports__ViTFeatureExtractor
=
__webpack_exports__
.
ViTFeatureExtractor
;
var
__webpack_exports__ViTForImageClassification
=
__webpack_exports__
.
ViTForImageClassification
;
var
__webpack_exports__ViTImageProcessor
=
__webpack_exports__
.
ViTImageProcessor
;
var
__webpack_exports__ViTModel
=
__webpack_exports__
.
ViTModel
;
var
__webpack_exports__ViTPreTrainedModel
=
__webpack_exports__
.
ViTPreTrainedModel
;
var
__webpack_exports__VisionEncoderDecoderModel
=
__webpack_exports__
.
VisionEncoderDecoderModel
;
var
__webpack_exports__VitMatteForImageMatting
=
__webpack_exports__
.
VitMatteForImageMatting
;
var
__webpack_exports__VitMatteImageProcessor
=
__webpack_exports__
.
VitMatteImageProcessor
;
var
__webpack_exports__VitMattePreTrainedModel
=
__webpack_exports__
.
VitMattePreTrainedModel
;
var
__webpack_exports__VitsModel
=
__webpack_exports__
.
VitsModel
;
var
__webpack_exports__VitsModelOutput
=
__webpack_exports__
.
VitsModelOutput
;
var
__webpack_exports__VitsPreTrainedModel
=
__webpack_exports__
.
VitsPreTrainedModel
;
var
__webpack_exports__VitsTokenizer
=
__webpack_exports__
.
VitsTokenizer
;
var
__webpack_exports__Wav2Vec2BertForCTC
=
__webpack_exports__
.
Wav2Vec2BertForCTC
;
var
__webpack_exports__Wav2Vec2BertForSequenceClassification
=
__webpack_exports__
.
Wav2Vec2BertForSequenceClassification
;
var
__webpack_exports__Wav2Vec2BertModel
=
__webpack_exports__
.
Wav2Vec2BertModel
;
var
__webpack_exports__Wav2Vec2BertPreTrainedModel
=
__webpack_exports__
.
Wav2Vec2BertPreTrainedModel
;
var
__webpack_exports__Wav2Vec2CTCTokenizer
=
__webpack_exports__
.
Wav2Vec2CTCTokenizer
;
var
__webpack_exports__Wav2Vec2FeatureExtractor
=
__webpack_exports__
.
Wav2Vec2FeatureExtractor
;
var
__webpack_exports__Wav2Vec2ForAudioFrameClassification
=
__webpack_exports__
.
Wav2Vec2ForAudioFrameClassification
;
var
__webpack_exports__Wav2Vec2ForCTC
=
__webpack_exports__
.
Wav2Vec2ForCTC
;
var
__webpack_exports__Wav2Vec2ForSequenceClassification
=
__webpack_exports__
.
Wav2Vec2ForSequenceClassification
;
var
__webpack_exports__Wav2Vec2Model
=
__webpack_exports__
.
Wav2Vec2Model
;
var
__webpack_exports__Wav2Vec2PreTrainedModel
=
__webpack_exports__
.
Wav2Vec2PreTrainedModel
;
var
__webpack_exports__Wav2Vec2ProcessorWithLM
=
__webpack_exports__
.
Wav2Vec2ProcessorWithLM
;
var
__webpack_exports__WavLMForAudioFrameClassification
=
__webpack_exports__
.
WavLMForAudioFrameClassification
;
var
__webpack_exports__WavLMForCTC
=
__webpack_exports__
.
WavLMForCTC
;
var
__webpack_exports__WavLMForSequenceClassification
=
__webpack_exports__
.
WavLMForSequenceClassification
;
var
__webpack_exports__WavLMForXVector
=
__webpack_exports__
.
WavLMForXVector
;
var
__webpack_exports__WavLMModel
=
__webpack_exports__
.
WavLMModel
;
var
__webpack_exports__WavLMPreTrainedModel
=
__webpack_exports__
.
WavLMPreTrainedModel
;
var
__webpack_exports__WhisperFeatureExtractor
=
__webpack_exports__
.
WhisperFeatureExtractor
;
var
__webpack_exports__WhisperForConditionalGeneration
=
__webpack_exports__
.
WhisperForConditionalGeneration
;
var
__webpack_exports__WhisperModel
=
__webpack_exports__
.
WhisperModel
;
var
__webpack_exports__WhisperPreTrainedModel
=
__webpack_exports__
.
WhisperPreTrainedModel
;
var
__webpack_exports__WhisperProcessor
=
__webpack_exports__
.
WhisperProcessor
;
var
__webpack_exports__WhisperTokenizer
=
__webpack_exports__
.
WhisperTokenizer
;
var
__webpack_exports__XLMForQuestionAnswering
=
__webpack_exports__
.
XLMForQuestionAnswering
;
var
__webpack_exports__XLMForSequenceClassification
=
__webpack_exports__
.
XLMForSequenceClassification
;
var
__webpack_exports__XLMForTokenClassification
=
__webpack_exports__
.
XLMForTokenClassification
;
var
__webpack_exports__XLMModel
=
__webpack_exports__
.
XLMModel
;
var
__webpack_exports__XLMPreTrainedModel
=
__webpack_exports__
.
XLMPreTrainedModel
;
var
__webpack_exports__XLMRobertaForMaskedLM
=
__webpack_exports__
.
XLMRobertaForMaskedLM
;
var
__webpack_exports__XLMRobertaForQuestionAnswering
=
__webpack_exports__
.
XLMRobertaForQuestionAnswering
;
var
__webpack_exports__XLMRobertaForSequenceClassification
=
__webpack_exports__
.
XLMRobertaForSequenceClassification
;
var
__webpack_exports__XLMRobertaForTokenClassification
=
__webpack_exports__
.
XLMRobertaForTokenClassification
;
var
__webpack_exports__XLMRobertaModel
=
__webpack_exports__
.
XLMRobertaModel
;
var
__webpack_exports__XLMRobertaPreTrainedModel
=
__webpack_exports__
.
XLMRobertaPreTrainedModel
;
var
__webpack_exports__XLMRobertaTokenizer
=
__webpack_exports__
.
XLMRobertaTokenizer
;
var
__webpack_exports__XLMTokenizer
=
__webpack_exports__
.
XLMTokenizer
;
var
__webpack_exports__XLMWithLMHeadModel
=
__webpack_exports__
.
XLMWithLMHeadModel
;
var
__webpack_exports__XVectorOutput
=
__webpack_exports__
.
XVectorOutput
;
var
__webpack_exports__YolosFeatureExtractor
=
__webpack_exports__
.
YolosFeatureExtractor
;
var
__webpack_exports__YolosForObjectDetection
=
__webpack_exports__
.
YolosForObjectDetection
;
var
__webpack_exports__YolosModel
=
__webpack_exports__
.
YolosModel
;
var
__webpack_exports__YolosObjectDetectionOutput
=
__webpack_exports__
.
YolosObjectDetectionOutput
;
var
__webpack_exports__YolosPreTrainedModel
=
__webpack_exports__
.
YolosPreTrainedModel
;
var
__webpack_exports__ZeroShotAudioClassificationPipeline
=
__webpack_exports__
.
ZeroShotAudioClassificationPipeline
;
var
__webpack_exports__ZeroShotClassificationPipeline
=
__webpack_exports__
.
ZeroShotClassificationPipeline
;
var
__webpack_exports__ZeroShotImageClassificationPipeline
=
__webpack_exports__
.
ZeroShotImageClassificationPipeline
;
var
__webpack_exports__ZeroShotObjectDetectionPipeline
=
__webpack_exports__
.
ZeroShotObjectDetectionPipeline
;
var
__webpack_exports__bankers_round
=
__webpack_exports__
.
bankers_round
;
var
__webpack_exports__cat
=
__webpack_exports__
.
cat
;
var
__webpack_exports__cos_sim
=
__webpack_exports__
.
cos_sim
;
var
__webpack_exports__dot
=
__webpack_exports__
.
dot
;
var
__webpack_exports__dynamicTimeWarping
=
__webpack_exports__
.
dynamicTimeWarping
;
var
__webpack_exports__env
=
__webpack_exports__
.
env
;
var
__webpack_exports__getTopItems
=
__webpack_exports__
.
getTopItems
;
var
__webpack_exports__hanning
=
__webpack_exports__
.
hanning
;
var
__webpack_exports__interpolate
=
__webpack_exports__
.
interpolate
;
var
__webpack_exports__interpolate_data
=
__webpack_exports__
.
interpolate_data
;
var
__webpack_exports__layer_norm
=
__webpack_exports__
.
layer_norm
;
var
__webpack_exports__log_softmax
=
__webpack_exports__
.
log_softmax
;
var
__webpack_exports__magnitude
=
__webpack_exports__
.
magnitude
;
var
__webpack_exports__max
=
__webpack_exports__
.
max
;
var
__webpack_exports__mean
=
__webpack_exports__
.
mean
;
var
__webpack_exports__mean_pooling
=
__webpack_exports__
.
mean_pooling
;
var
__webpack_exports__medianFilter
=
__webpack_exports__
.
medianFilter
;
var
__webpack_exports__mel_filter_bank
=
__webpack_exports__
.
mel_filter_bank
;
var
__webpack_exports__min
=
__webpack_exports__
.
min
;
var
__webpack_exports__ones
=
__webpack_exports__
.
ones
;
var
__webpack_exports__ones_like
=
__webpack_exports__
.
ones_like
;
var
__webpack_exports__permute
=
__webpack_exports__
.
permute
;
var
__webpack_exports__permute_data
=
__webpack_exports__
.
permute_data
;
var
__webpack_exports__pipeline
=
__webpack_exports__
.
pipeline
;
var
__webpack_exports__read_audio
=
__webpack_exports__
.
read_audio
;
var
__webpack_exports__round
=
__webpack_exports__
.
round
;
var
__webpack_exports__softmax
=
__webpack_exports__
.
softmax
;
var
__webpack_exports__spectrogram
=
__webpack_exports__
.
spectrogram
;
var
__webpack_exports__stack
=
__webpack_exports__
.
stack
;
var
__webpack_exports__std_mean
=
__webpack_exports__
.
std_mean
;
var
__webpack_exports__window_function
=
__webpack_exports__
.
window_function
;
export
{
__webpack_exports__ASTFeatureExtractor
as
ASTFeatureExtractor
__webpack_exports__ASTForAudioClassification
as
ASTForAudioClassification
__webpack_exports__ASTModel
as
ASTModel
__webpack_exports__ASTPreTrainedModel
as
ASTPreTrainedModel
__webpack_exports__AlbertForMaskedLM
as
AlbertForMaskedLM
__webpack_exports__AlbertForQuestionAnswering
as
AlbertForQuestionAnswering
__webpack_exports__AlbertForSequenceClassification
as
AlbertForSequenceClassification
__webpack_exports__AlbertModel
as
AlbertModel
__webpack_exports__AlbertPreTrainedModel
as
AlbertPreTrainedModel
__webpack_exports__AlbertTokenizer
as
AlbertTokenizer
__webpack_exports__AudioClassificationPipeline
as
AudioClassificationPipeline
__webpack_exports__AutoConfig
as
AutoConfig
__webpack_exports__AutoModel
as
AutoModel
__webpack_exports__AutoModelForAudioClassification
as
AutoModelForAudioClassification
__webpack_exports__AutoModelForAudioFrameClassification
as
AutoModelForAudioFrameClassification
__webpack_exports__AutoModelForCTC
as
AutoModelForCTC
__webpack_exports__AutoModelForCausalLM
as
AutoModelForCausalLM
__webpack_exports__AutoModelForDepthEstimation
as
AutoModelForDepthEstimation
__webpack_exports__AutoModelForDocumentQuestionAnswering
as
AutoModelForDocumentQuestionAnswering
__webpack_exports__AutoModelForImageClassification
as
AutoModelForImageClassification
__webpack_exports__AutoModelForImageFeatureExtraction
as
AutoModelForImageFeatureExtraction
__webpack_exports__AutoModelForImageMatting
as
AutoModelForImageMatting
__webpack_exports__AutoModelForImageSegmentation
as
AutoModelForImageSegmentation
__webpack_exports__AutoModelForImageToImage
as
AutoModelForImageToImage
__webpack_exports__AutoModelForMaskGeneration
as
AutoModelForMaskGeneration
__webpack_exports__AutoModelForMaskedLM
as
AutoModelForMaskedLM
__webpack_exports__AutoModelForObjectDetection
as
AutoModelForObjectDetection
__webpack_exports__AutoModelForQuestionAnswering
as
AutoModelForQuestionAnswering
__webpack_exports__AutoModelForSemanticSegmentation
as
AutoModelForSemanticSegmentation
__webpack_exports__AutoModelForSeq2SeqLM
as
AutoModelForSeq2SeqLM
__webpack_exports__AutoModelForSequenceClassification
as
AutoModelForSequenceClassification
__webpack_exports__AutoModelForSpeechSeq2Seq
as
AutoModelForSpeechSeq2Seq
__webpack_exports__AutoModelForTextToSpectrogram
as
AutoModelForTextToSpectrogram
__webpack_exports__AutoModelForTextToWaveform
as
AutoModelForTextToWaveform
__webpack_exports__AutoModelForTokenClassification
as
AutoModelForTokenClassification
__webpack_exports__AutoModelForVision2Seq
as
AutoModelForVision2Seq
__webpack_exports__AutoModelForXVector
as
AutoModelForXVector
__webpack_exports__AutoModelForZeroShotObjectDetection
as
AutoModelForZeroShotObjectDetection
__webpack_exports__AutoProcessor
as
AutoProcessor
__webpack_exports__AutoTokenizer
as
AutoTokenizer
__webpack_exports__AutomaticSpeechRecognitionPipeline
as
AutomaticSpeechRecognitionPipeline
__webpack_exports__BartForConditionalGeneration
as
BartForConditionalGeneration
__webpack_exports__BartForSequenceClassification
as
BartForSequenceClassification
__webpack_exports__BartModel
as
BartModel
__webpack_exports__BartPretrainedModel
as
BartPretrainedModel
__webpack_exports__BartTokenizer
as
BartTokenizer
__webpack_exports__BaseModelOutput
as
BaseModelOutput
__webpack_exports__BeitFeatureExtractor
as
BeitFeatureExtractor
__webpack_exports__BeitForImageClassification
as
BeitForImageClassification
__webpack_exports__BeitModel
as
BeitModel
__webpack_exports__BeitPreTrainedModel
as
BeitPreTrainedModel
__webpack_exports__BertForMaskedLM
as
BertForMaskedLM
__webpack_exports__BertForQuestionAnswering
as
BertForQuestionAnswering
__webpack_exports__BertForSequenceClassification
as
BertForSequenceClassification
__webpack_exports__BertForTokenClassification
as
BertForTokenClassification
__webpack_exports__BertModel
as
BertModel
__webpack_exports__BertPreTrainedModel
as
BertPreTrainedModel
__webpack_exports__BertTokenizer
as
BertTokenizer
__webpack_exports__BitImageProcessor
as
BitImageProcessor
__webpack_exports__BlenderbotForConditionalGeneration
as
BlenderbotForConditionalGeneration
__webpack_exports__BlenderbotModel
as
BlenderbotModel
__webpack_exports__BlenderbotPreTrainedModel
as
BlenderbotPreTrainedModel
__webpack_exports__BlenderbotSmallForConditionalGeneration
as
BlenderbotSmallForConditionalGeneration
__webpack_exports__BlenderbotSmallModel
as
BlenderbotSmallModel
__webpack_exports__BlenderbotSmallPreTrainedModel
as
BlenderbotSmallPreTrainedModel
__webpack_exports__BlenderbotSmallTokenizer
as
BlenderbotSmallTokenizer
__webpack_exports__BlenderbotTokenizer
as
BlenderbotTokenizer
__webpack_exports__BloomForCausalLM
as
BloomForCausalLM
__webpack_exports__BloomModel
as
BloomModel
__webpack_exports__BloomPreTrainedModel
as
BloomPreTrainedModel
__webpack_exports__BloomTokenizer
as
BloomTokenizer
__webpack_exports__CLIPFeatureExtractor
as
CLIPFeatureExtractor
__webpack_exports__CLIPModel
as
CLIPModel
__webpack_exports__CLIPPreTrainedModel
as
CLIPPreTrainedModel
__webpack_exports__CLIPSegForImageSegmentation
as
CLIPSegForImageSegmentation
__webpack_exports__CLIPSegModel
as
CLIPSegModel
__webpack_exports__CLIPSegPreTrainedModel
as
CLIPSegPreTrainedModel
__webpack_exports__CLIPTextModelWithProjection
as
CLIPTextModelWithProjection
__webpack_exports__CLIPTokenizer
as
CLIPTokenizer
__webpack_exports__CLIPVisionModelWithProjection
as
CLIPVisionModelWithProjection
__webpack_exports__CamembertForMaskedLM
as
CamembertForMaskedLM
__webpack_exports__CamembertForQuestionAnswering
as
CamembertForQuestionAnswering
__webpack_exports__CamembertForSequenceClassification
as
CamembertForSequenceClassification
__webpack_exports__CamembertForTokenClassification
as
CamembertForTokenClassification
__webpack_exports__CamembertModel
as
CamembertModel
__webpack_exports__CamembertPreTrainedModel
as
CamembertPreTrainedModel
__webpack_exports__CamembertTokenizer
as
CamembertTokenizer
__webpack_exports__CausalLMOutput
as
CausalLMOutput
__webpack_exports__CausalLMOutputWithPast
as
CausalLMOutputWithPast
__webpack_exports__ChineseCLIPFeatureExtractor
as
ChineseCLIPFeatureExtractor
__webpack_exports__ChineseCLIPModel
as
ChineseCLIPModel
__webpack_exports__ChineseCLIPPreTrainedModel
as
ChineseCLIPPreTrainedModel
__webpack_exports__ClapAudioModelWithProjection
as
ClapAudioModelWithProjection
__webpack_exports__ClapFeatureExtractor
as
ClapFeatureExtractor
__webpack_exports__ClapModel
as
ClapModel
__webpack_exports__ClapPreTrainedModel
as
ClapPreTrainedModel
__webpack_exports__ClapTextModelWithProjection
as
ClapTextModelWithProjection
__webpack_exports__CodeGenForCausalLM
as
CodeGenForCausalLM
__webpack_exports__CodeGenModel
as
CodeGenModel
__webpack_exports__CodeGenPreTrainedModel
as
CodeGenPreTrainedModel
__webpack_exports__CodeGenTokenizer
as
CodeGenTokenizer
__webpack_exports__CodeLlamaTokenizer
as
CodeLlamaTokenizer
__webpack_exports__CohereTokenizer
as
CohereTokenizer
__webpack_exports__ConvBertForMaskedLM
as
ConvBertForMaskedLM
__webpack_exports__ConvBertForQuestionAnswering
as
ConvBertForQuestionAnswering
__webpack_exports__ConvBertForSequenceClassification
as
ConvBertForSequenceClassification
__webpack_exports__ConvBertForTokenClassification
as
ConvBertForTokenClassification
__webpack_exports__ConvBertModel
as
ConvBertModel
__webpack_exports__ConvBertPreTrainedModel
as
ConvBertPreTrainedModel
__webpack_exports__ConvBertTokenizer
as
ConvBertTokenizer
__webpack_exports__ConvNextFeatureExtractor
as
ConvNextFeatureExtractor
__webpack_exports__ConvNextForImageClassification
as
ConvNextForImageClassification
__webpack_exports__ConvNextImageProcessor
as
ConvNextImageProcessor
__webpack_exports__ConvNextModel
as
ConvNextModel
__webpack_exports__ConvNextPreTrainedModel
as
ConvNextPreTrainedModel
__webpack_exports__ConvNextV2ForImageClassification
as
ConvNextV2ForImageClassification
__webpack_exports__ConvNextV2Model
as
ConvNextV2Model
__webpack_exports__ConvNextV2PreTrainedModel
as
ConvNextV2PreTrainedModel
__webpack_exports__DPTFeatureExtractor
as
DPTFeatureExtractor
__webpack_exports__DPTForDepthEstimation
as
DPTForDepthEstimation
__webpack_exports__DPTImageProcessor
as
DPTImageProcessor
__webpack_exports__DPTModel
as
DPTModel
__webpack_exports__DPTPreTrainedModel
as
DPTPreTrainedModel
__webpack_exports__DebertaForMaskedLM
as
DebertaForMaskedLM
__webpack_exports__DebertaForQuestionAnswering
as
DebertaForQuestionAnswering
__webpack_exports__DebertaForSequenceClassification
as
DebertaForSequenceClassification
__webpack_exports__DebertaForTokenClassification
as
DebertaForTokenClassification
__webpack_exports__DebertaModel
as
DebertaModel
__webpack_exports__DebertaPreTrainedModel
as
DebertaPreTrainedModel
__webpack_exports__DebertaTokenizer
as
DebertaTokenizer
__webpack_exports__DebertaV2ForMaskedLM
as
DebertaV2ForMaskedLM
__webpack_exports__DebertaV2ForQuestionAnswering
as
DebertaV2ForQuestionAnswering
__webpack_exports__DebertaV2ForSequenceClassification
as
DebertaV2ForSequenceClassification
__webpack_exports__DebertaV2ForTokenClassification
as
DebertaV2ForTokenClassification
__webpack_exports__DebertaV2Model
as
DebertaV2Model
__webpack_exports__DebertaV2PreTrainedModel
as
DebertaV2PreTrainedModel
__webpack_exports__DebertaV2Tokenizer
as
DebertaV2Tokenizer
__webpack_exports__DeiTFeatureExtractor
as
DeiTFeatureExtractor
__webpack_exports__DeiTForImageClassification
as
DeiTForImageClassification
__webpack_exports__DeiTModel
as
DeiTModel
__webpack_exports__DeiTPreTrainedModel
as
DeiTPreTrainedModel
__webpack_exports__DepthAnythingForDepthEstimation
as
DepthAnythingForDepthEstimation
__webpack_exports__DepthAnythingPreTrainedModel
as
DepthAnythingPreTrainedModel
__webpack_exports__DepthEstimationPipeline
as
DepthEstimationPipeline
__webpack_exports__DetrFeatureExtractor
as
DetrFeatureExtractor
__webpack_exports__DetrForObjectDetection
as
DetrForObjectDetection
__webpack_exports__DetrForSegmentation
as
DetrForSegmentation
__webpack_exports__DetrModel
as
DetrModel
__webpack_exports__DetrObjectDetectionOutput
as
DetrObjectDetectionOutput
__webpack_exports__DetrPreTrainedModel
as
DetrPreTrainedModel
__webpack_exports__DetrSegmentationOutput
as
DetrSegmentationOutput
__webpack_exports__Dinov2ForImageClassification
as
Dinov2ForImageClassification
__webpack_exports__Dinov2Model
as
Dinov2Model
__webpack_exports__Dinov2PreTrainedModel
as
Dinov2PreTrainedModel
__webpack_exports__DistilBertForMaskedLM
as
DistilBertForMaskedLM
__webpack_exports__DistilBertForQuestionAnswering
as
DistilBertForQuestionAnswering
__webpack_exports__DistilBertForSequenceClassification
as
DistilBertForSequenceClassification
__webpack_exports__DistilBertForTokenClassification
as
DistilBertForTokenClassification
__webpack_exports__DistilBertModel
as
DistilBertModel
__webpack_exports__DistilBertPreTrainedModel
as
DistilBertPreTrainedModel
__webpack_exports__DistilBertTokenizer
as
DistilBertTokenizer
__webpack_exports__DocumentQuestionAnsweringPipeline
as
DocumentQuestionAnsweringPipeline
__webpack_exports__DonutFeatureExtractor
as
DonutFeatureExtractor
__webpack_exports__DonutSwinModel
as
DonutSwinModel
__webpack_exports__DonutSwinPreTrainedModel
as
DonutSwinPreTrainedModel
__webpack_exports__EfficientNetForImageClassification
as
EfficientNetForImageClassification
__webpack_exports__EfficientNetImageProcessor
as
EfficientNetImageProcessor
__webpack_exports__EfficientNetModel
as
EfficientNetModel
__webpack_exports__EfficientNetPreTrainedModel
as
EfficientNetPreTrainedModel
__webpack_exports__ElectraForMaskedLM
as
ElectraForMaskedLM
__webpack_exports__ElectraForQuestionAnswering
as
ElectraForQuestionAnswering
__webpack_exports__ElectraForSequenceClassification
as
ElectraForSequenceClassification
__webpack_exports__ElectraForTokenClassification
as
ElectraForTokenClassification
__webpack_exports__ElectraModel
as
ElectraModel
__webpack_exports__ElectraPreTrainedModel
as
ElectraPreTrainedModel
__webpack_exports__ElectraTokenizer
as
ElectraTokenizer
__webpack_exports__EsmForMaskedLM
as
EsmForMaskedLM
__webpack_exports__EsmForSequenceClassification
as
EsmForSequenceClassification
__webpack_exports__EsmForTokenClassification
as
EsmForTokenClassification
__webpack_exports__EsmModel
as
EsmModel
__webpack_exports__EsmPreTrainedModel
as
EsmPreTrainedModel
__webpack_exports__EsmTokenizer
as
EsmTokenizer
__webpack_exports__FFT
as
FFT
__webpack_exports__FalconForCausalLM
as
FalconForCausalLM
__webpack_exports__FalconModel
as
FalconModel
__webpack_exports__FalconPreTrainedModel
as
FalconPreTrainedModel
__webpack_exports__FalconTokenizer
as
FalconTokenizer
__webpack_exports__FeatureExtractionPipeline
as
FeatureExtractionPipeline
__webpack_exports__FeatureExtractor
as
FeatureExtractor
__webpack_exports__FillMaskPipeline
as
FillMaskPipeline
__webpack_exports__GLPNFeatureExtractor
as
GLPNFeatureExtractor
__webpack_exports__GLPNForDepthEstimation
as
GLPNForDepthEstimation
__webpack_exports__GLPNModel
as
GLPNModel
__webpack_exports__GLPNPreTrainedModel
as
GLPNPreTrainedModel
__webpack_exports__GPT2LMHeadModel
as
GPT2LMHeadModel
__webpack_exports__GPT2Model
as
GPT2Model
__webpack_exports__GPT2PreTrainedModel
as
GPT2PreTrainedModel
__webpack_exports__GPT2Tokenizer
as
GPT2Tokenizer
__webpack_exports__GPTBigCodeForCausalLM
as
GPTBigCodeForCausalLM
__webpack_exports__GPTBigCodeModel
as
GPTBigCodeModel
__webpack_exports__GPTBigCodePreTrainedModel
as
GPTBigCodePreTrainedModel
__webpack_exports__GPTJForCausalLM
as
GPTJForCausalLM
__webpack_exports__GPTJModel
as
GPTJModel
__webpack_exports__GPTJPreTrainedModel
as
GPTJPreTrainedModel
__webpack_exports__GPTNeoForCausalLM
as
GPTNeoForCausalLM
__webpack_exports__GPTNeoModel
as
GPTNeoModel
__webpack_exports__GPTNeoPreTrainedModel
as
GPTNeoPreTrainedModel
__webpack_exports__GPTNeoXForCausalLM
as
GPTNeoXForCausalLM
__webpack_exports__GPTNeoXModel
as
GPTNeoXModel
__webpack_exports__GPTNeoXPreTrainedModel
as
GPTNeoXPreTrainedModel
__webpack_exports__GPTNeoXTokenizer
as
GPTNeoXTokenizer
__webpack_exports__GemmaTokenizer
as
GemmaTokenizer
__webpack_exports__Grok1Tokenizer
as
Grok1Tokenizer
__webpack_exports__HerbertTokenizer
as
HerbertTokenizer
__webpack_exports__HubertForCTC
as
HubertForCTC
__webpack_exports__HubertForSequenceClassification
as
HubertForSequenceClassification
__webpack_exports__HubertModel
as
HubertModel
__webpack_exports__HubertPreTrainedModel
as
HubertPreTrainedModel
__webpack_exports__ImageClassificationPipeline
as
ImageClassificationPipeline
__webpack_exports__ImageFeatureExtractionPipeline
as
ImageFeatureExtractionPipeline
__webpack_exports__ImageFeatureExtractor
as
ImageFeatureExtractor
__webpack_exports__ImageMattingOutput
as
ImageMattingOutput
__webpack_exports__ImageSegmentationPipeline
as
ImageSegmentationPipeline
__webpack_exports__ImageToImagePipeline
as
ImageToImagePipeline
__webpack_exports__ImageToTextPipeline
as
ImageToTextPipeline
__webpack_exports__LlamaForCausalLM
as
LlamaForCausalLM
__webpack_exports__LlamaModel
as
LlamaModel
__webpack_exports__LlamaPreTrainedModel
as
LlamaPreTrainedModel
__webpack_exports__LlamaTokenizer
as
LlamaTokenizer
__webpack_exports__LongT5ForConditionalGeneration
as
LongT5ForConditionalGeneration
__webpack_exports__LongT5Model
as
LongT5Model
__webpack_exports__LongT5PreTrainedModel
as
LongT5PreTrainedModel
__webpack_exports__M2M100ForConditionalGeneration
as
M2M100ForConditionalGeneration
__webpack_exports__M2M100Model
as
M2M100Model
__webpack_exports__M2M100PreTrainedModel
as
M2M100PreTrainedModel
__webpack_exports__M2M100Tokenizer
as
M2M100Tokenizer
__webpack_exports__MBart50Tokenizer
as
MBart50Tokenizer
__webpack_exports__MBartForCausalLM
as
MBartForCausalLM
__webpack_exports__MBartForConditionalGeneration
as
MBartForConditionalGeneration
__webpack_exports__MBartForSequenceClassification
as
MBartForSequenceClassification
__webpack_exports__MBartModel
as
MBartModel
__webpack_exports__MBartPreTrainedModel
as
MBartPreTrainedModel
__webpack_exports__MBartTokenizer
as
MBartTokenizer
__webpack_exports__MPNetForMaskedLM
as
MPNetForMaskedLM
__webpack_exports__MPNetForQuestionAnswering
as
MPNetForQuestionAnswering
__webpack_exports__MPNetForSequenceClassification
as
MPNetForSequenceClassification
__webpack_exports__MPNetForTokenClassification
as
MPNetForTokenClassification
__webpack_exports__MPNetModel
as
MPNetModel
__webpack_exports__MPNetPreTrainedModel
as
MPNetPreTrainedModel
__webpack_exports__MPNetTokenizer
as
MPNetTokenizer
__webpack_exports__MT5ForConditionalGeneration
as
MT5ForConditionalGeneration
__webpack_exports__MT5Model
as
MT5Model
__webpack_exports__MT5PreTrainedModel
as
MT5PreTrainedModel
__webpack_exports__MarianMTModel
as
MarianMTModel
__webpack_exports__MarianModel
as
MarianModel
__webpack_exports__MarianPreTrainedModel
as
MarianPreTrainedModel
__webpack_exports__MarianTokenizer
as
MarianTokenizer
__webpack_exports__MaskedLMOutput
as
MaskedLMOutput
__webpack_exports__MistralForCausalLM
as
MistralForCausalLM
__webpack_exports__MistralModel
as
MistralModel
__webpack_exports__MistralPreTrainedModel
as
MistralPreTrainedModel
__webpack_exports__MobileBertForMaskedLM
as
MobileBertForMaskedLM
__webpack_exports__MobileBertForQuestionAnswering
as
MobileBertForQuestionAnswering
__webpack_exports__MobileBertForSequenceClassification
as
MobileBertForSequenceClassification
__webpack_exports__MobileBertModel
as
MobileBertModel
__webpack_exports__MobileBertPreTrainedModel
as
MobileBertPreTrainedModel
__webpack_exports__MobileBertTokenizer
as
MobileBertTokenizer
__webpack_exports__MobileViTFeatureExtractor
as
MobileViTFeatureExtractor
__webpack_exports__MobileViTForImageClassification
as
MobileViTForImageClassification
__webpack_exports__MobileViTModel
as
MobileViTModel
__webpack_exports__MobileViTPreTrainedModel
as
MobileViTPreTrainedModel
__webpack_exports__ModelOutput
as
ModelOutput
__webpack_exports__MptForCausalLM
as
MptForCausalLM
__webpack_exports__MptModel
as
MptModel
__webpack_exports__MptPreTrainedModel
as
MptPreTrainedModel
__webpack_exports__NllbTokenizer
as
NllbTokenizer
__webpack_exports__NomicBertModel
as
NomicBertModel
__webpack_exports__NomicBertPreTrainedModel
as
NomicBertPreTrainedModel
__webpack_exports__NougatImageProcessor
as
NougatImageProcessor
__webpack_exports__NougatTokenizer
as
NougatTokenizer
__webpack_exports__OPTForCausalLM
as
OPTForCausalLM
__webpack_exports__OPTModel
as
OPTModel
__webpack_exports__OPTPreTrainedModel
as
OPTPreTrainedModel
__webpack_exports__ObjectDetectionPipeline
as
ObjectDetectionPipeline
__webpack_exports__OwlViTFeatureExtractor
as
OwlViTFeatureExtractor
__webpack_exports__OwlViTForObjectDetection
as
OwlViTForObjectDetection
__webpack_exports__OwlViTModel
as
OwlViTModel
__webpack_exports__OwlViTPreTrainedModel
as
OwlViTPreTrainedModel
__webpack_exports__OwlViTProcessor
as
OwlViTProcessor
__webpack_exports__Owlv2ForObjectDetection
as
Owlv2ForObjectDetection
__webpack_exports__Owlv2ImageProcessor
as
Owlv2ImageProcessor
__webpack_exports__Owlv2Model
as
Owlv2Model
__webpack_exports__Owlv2PreTrainedModel
as
Owlv2PreTrainedModel
__webpack_exports__PhiForCausalLM
as
PhiForCausalLM
__webpack_exports__PhiModel
as
PhiModel
__webpack_exports__PhiPreTrainedModel
as
PhiPreTrainedModel
__webpack_exports__Pipeline
as
Pipeline
__webpack_exports__PreTrainedModel
as
PreTrainedModel
__webpack_exports__PreTrainedTokenizer
as
PreTrainedTokenizer
__webpack_exports__PretrainedConfig
as
PretrainedConfig
__webpack_exports__PretrainedMixin
as
PretrainedMixin
__webpack_exports__Processor
as
Processor
__webpack_exports__QuestionAnsweringModelOutput
as
QuestionAnsweringModelOutput
__webpack_exports__QuestionAnsweringPipeline
as
QuestionAnsweringPipeline
__webpack_exports__Qwen2ForCausalLM
as
Qwen2ForCausalLM
__webpack_exports__Qwen2Model
as
Qwen2Model
__webpack_exports__Qwen2PreTrainedModel
as
Qwen2PreTrainedModel
__webpack_exports__Qwen2Tokenizer
as
Qwen2Tokenizer
__webpack_exports__RawImage
as
RawImage
__webpack_exports__ResNetForImageClassification
as
ResNetForImageClassification
__webpack_exports__ResNetModel
as
ResNetModel
__webpack_exports__ResNetPreTrainedModel
as
ResNetPreTrainedModel
__webpack_exports__RoFormerForMaskedLM
as
RoFormerForMaskedLM
__webpack_exports__RoFormerForQuestionAnswering
as
RoFormerForQuestionAnswering
__webpack_exports__RoFormerForSequenceClassification
as
RoFormerForSequenceClassification
__webpack_exports__RoFormerForTokenClassification
as
RoFormerForTokenClassification
__webpack_exports__RoFormerModel
as
RoFormerModel
__webpack_exports__RoFormerPreTrainedModel
as
RoFormerPreTrainedModel
__webpack_exports__RoFormerTokenizer
as
RoFormerTokenizer
__webpack_exports__RobertaForMaskedLM
as
RobertaForMaskedLM
__webpack_exports__RobertaForQuestionAnswering
as
RobertaForQuestionAnswering
__webpack_exports__RobertaForSequenceClassification
as
RobertaForSequenceClassification
__webpack_exports__RobertaForTokenClassification
as
RobertaForTokenClassification
__webpack_exports__RobertaModel
as
RobertaModel
__webpack_exports__RobertaPreTrainedModel
as
RobertaPreTrainedModel
__webpack_exports__RobertaTokenizer
as
RobertaTokenizer
__webpack_exports__SamImageProcessor
as
SamImageProcessor
__webpack_exports__SamImageSegmentationOutput
as
SamImageSegmentationOutput
__webpack_exports__SamModel
as
SamModel
__webpack_exports__SamPreTrainedModel
as
SamPreTrainedModel
__webpack_exports__SamProcessor
as
SamProcessor
__webpack_exports__SeamlessM4TFeatureExtractor
as
SeamlessM4TFeatureExtractor
__webpack_exports__SegformerFeatureExtractor
as
SegformerFeatureExtractor
__webpack_exports__SegformerForImageClassification
as
SegformerForImageClassification
__webpack_exports__SegformerForSemanticSegmentation
as
SegformerForSemanticSegmentation
__webpack_exports__SegformerModel
as
SegformerModel
__webpack_exports__SegformerPreTrainedModel
as
SegformerPreTrainedModel
__webpack_exports__Seq2SeqLMOutput
as
Seq2SeqLMOutput
__webpack_exports__SequenceClassifierOutput
as
SequenceClassifierOutput
__webpack_exports__SiglipImageProcessor
as
SiglipImageProcessor
__webpack_exports__SiglipModel
as
SiglipModel
__webpack_exports__SiglipPreTrainedModel
as
SiglipPreTrainedModel
__webpack_exports__SiglipTextModel
as
SiglipTextModel
__webpack_exports__SiglipTokenizer
as
SiglipTokenizer
__webpack_exports__SiglipVisionModel
as
SiglipVisionModel
__webpack_exports__SpeechT5FeatureExtractor
as
SpeechT5FeatureExtractor
__webpack_exports__SpeechT5ForSpeechToText
as
SpeechT5ForSpeechToText
__webpack_exports__SpeechT5ForTextToSpeech
as
SpeechT5ForTextToSpeech
__webpack_exports__SpeechT5HifiGan
as
SpeechT5HifiGan
__webpack_exports__SpeechT5Model
as
SpeechT5Model
__webpack_exports__SpeechT5PreTrainedModel
as
SpeechT5PreTrainedModel
__webpack_exports__SpeechT5Processor
as
SpeechT5Processor
__webpack_exports__SpeechT5Tokenizer
as
SpeechT5Tokenizer
__webpack_exports__SqueezeBertForMaskedLM
as
SqueezeBertForMaskedLM
__webpack_exports__SqueezeBertForQuestionAnswering
as
SqueezeBertForQuestionAnswering
__webpack_exports__SqueezeBertForSequenceClassification
as
SqueezeBertForSequenceClassification
__webpack_exports__SqueezeBertModel
as
SqueezeBertModel
__webpack_exports__SqueezeBertPreTrainedModel
as
SqueezeBertPreTrainedModel
__webpack_exports__SqueezeBertTokenizer
as
SqueezeBertTokenizer
__webpack_exports__StableLmForCausalLM
as
StableLmForCausalLM
__webpack_exports__StableLmModel
as
StableLmModel
__webpack_exports__StableLmPreTrainedModel
as
StableLmPreTrainedModel
__webpack_exports__Starcoder2ForCausalLM
as
Starcoder2ForCausalLM
__webpack_exports__Starcoder2Model
as
Starcoder2Model
__webpack_exports__Starcoder2PreTrainedModel
as
Starcoder2PreTrainedModel
__webpack_exports__SummarizationPipeline
as
SummarizationPipeline
__webpack_exports__Swin2SRForImageSuperResolution
as
Swin2SRForImageSuperResolution
__webpack_exports__Swin2SRImageProcessor
as
Swin2SRImageProcessor
__webpack_exports__Swin2SRModel
as
Swin2SRModel
__webpack_exports__Swin2SRPreTrainedModel
as
Swin2SRPreTrainedModel
__webpack_exports__SwinForImageClassification
as
SwinForImageClassification
__webpack_exports__SwinModel
as
SwinModel
__webpack_exports__SwinPreTrainedModel
as
SwinPreTrainedModel
__webpack_exports__T5ForConditionalGeneration
as
T5ForConditionalGeneration
__webpack_exports__T5Model
as
T5Model
__webpack_exports__T5PreTrainedModel
as
T5PreTrainedModel
__webpack_exports__T5Tokenizer
as
T5Tokenizer
__webpack_exports__TableTransformerForObjectDetection
as
TableTransformerForObjectDetection
__webpack_exports__TableTransformerModel
as
TableTransformerModel
__webpack_exports__TableTransformerObjectDetectionOutput
as
TableTransformerObjectDetectionOutput
__webpack_exports__TableTransformerPreTrainedModel
as
TableTransformerPreTrainedModel
__webpack_exports__Tensor
as
Tensor
__webpack_exports__Text2TextGenerationPipeline
as
Text2TextGenerationPipeline
__webpack_exports__TextClassificationPipeline
as
TextClassificationPipeline
__webpack_exports__TextGenerationPipeline
as
TextGenerationPipeline
__webpack_exports__TextToAudioPipeline
as
TextToAudioPipeline
__webpack_exports__TokenClassificationPipeline
as
TokenClassificationPipeline
__webpack_exports__TokenClassifierOutput
as
TokenClassifierOutput
__webpack_exports__TokenizerModel
as
TokenizerModel
__webpack_exports__TrOCRForCausalLM
as
TrOCRForCausalLM
__webpack_exports__TrOCRPreTrainedModel
as
TrOCRPreTrainedModel
__webpack_exports__TranslationPipeline
as
TranslationPipeline
__webpack_exports__UniSpeechForCTC
as
UniSpeechForCTC
__webpack_exports__UniSpeechForSequenceClassification
as
UniSpeechForSequenceClassification
__webpack_exports__UniSpeechModel
as
UniSpeechModel
__webpack_exports__UniSpeechPreTrainedModel
as
UniSpeechPreTrainedModel
__webpack_exports__UniSpeechSatForAudioFrameClassification
as
UniSpeechSatForAudioFrameClassification
__webpack_exports__UniSpeechSatForCTC
as
UniSpeechSatForCTC
__webpack_exports__UniSpeechSatForSequenceClassification
as
UniSpeechSatForSequenceClassification
__webpack_exports__UniSpeechSatModel
as
UniSpeechSatModel
__webpack_exports__UniSpeechSatPreTrainedModel
as
UniSpeechSatPreTrainedModel
__webpack_exports__ViTFeatureExtractor
as
ViTFeatureExtractor
__webpack_exports__ViTForImageClassification
as
ViTForImageClassification
__webpack_exports__ViTImageProcessor
as
ViTImageProcessor
__webpack_exports__ViTModel
as
ViTModel
__webpack_exports__ViTPreTrainedModel
as
ViTPreTrainedModel
__webpack_exports__VisionEncoderDecoderModel
as
VisionEncoderDecoderModel
__webpack_exports__VitMatteForImageMatting
as
VitMatteForImageMatting
__webpack_exports__VitMatteImageProcessor
as
VitMatteImageProcessor
__webpack_exports__VitMattePreTrainedModel
as
VitMattePreTrainedModel
__webpack_exports__VitsModel
as
VitsModel
__webpack_exports__VitsModelOutput
as
VitsModelOutput
__webpack_exports__VitsPreTrainedModel
as
VitsPreTrainedModel
__webpack_exports__VitsTokenizer
as
VitsTokenizer
__webpack_exports__Wav2Vec2BertForCTC
as
Wav2Vec2BertForCTC
__webpack_exports__Wav2Vec2BertForSequenceClassification
as
Wav2Vec2BertForSequenceClassification
__webpack_exports__Wav2Vec2BertModel
as
Wav2Vec2BertModel
__webpack_exports__Wav2Vec2BertPreTrainedModel
as
Wav2Vec2BertPreTrainedModel
__webpack_exports__Wav2Vec2CTCTokenizer
as
Wav2Vec2CTCTokenizer
__webpack_exports__Wav2Vec2FeatureExtractor
as
Wav2Vec2FeatureExtractor
__webpack_exports__Wav2Vec2ForAudioFrameClassification
as
Wav2Vec2ForAudioFrameClassification
__webpack_exports__Wav2Vec2ForCTC
as
Wav2Vec2ForCTC
__webpack_exports__Wav2Vec2ForSequenceClassification
as
Wav2Vec2ForSequenceClassification
__webpack_exports__Wav2Vec2Model
as
Wav2Vec2Model
__webpack_exports__Wav2Vec2PreTrainedModel
as
Wav2Vec2PreTrainedModel
__webpack_exports__Wav2Vec2ProcessorWithLM
as
Wav2Vec2ProcessorWithLM
__webpack_exports__WavLMForAudioFrameClassification
as
WavLMForAudioFrameClassification
__webpack_exports__WavLMForCTC
as
WavLMForCTC
__webpack_exports__WavLMForSequenceClassification
as
WavLMForSequenceClassification
__webpack_exports__WavLMForXVector
as
WavLMForXVector
__webpack_exports__WavLMModel
as
WavLMModel
__webpack_exports__WavLMPreTrainedModel
as
WavLMPreTrainedModel
__webpack_exports__WhisperFeatureExtractor
as
WhisperFeatureExtractor
__webpack_exports__WhisperForConditionalGeneration
as
WhisperForConditionalGeneration
__webpack_exports__WhisperModel
as
WhisperModel
__webpack_exports__WhisperPreTrainedModel
as
WhisperPreTrainedModel
__webpack_exports__WhisperProcessor
as
WhisperProcessor
__webpack_exports__WhisperTokenizer
as
WhisperTokenizer
__webpack_exports__XLMForQuestionAnswering
as
XLMForQuestionAnswering
__webpack_exports__XLMForSequenceClassification
as
XLMForSequenceClassification
__webpack_exports__XLMForTokenClassification
as
XLMForTokenClassification
__webpack_exports__XLMModel
as
XLMModel
__webpack_exports__XLMPreTrainedModel
as
XLMPreTrainedModel
__webpack_exports__XLMRobertaForMaskedLM
as
XLMRobertaForMaskedLM
__webpack_exports__XLMRobertaForQuestionAnswering
as
XLMRobertaForQuestionAnswering
__webpack_exports__XLMRobertaForSequenceClassification
as
XLMRobertaForSequenceClassification
__webpack_exports__XLMRobertaForTokenClassification
as
XLMRobertaForTokenClassification
__webpack_exports__XLMRobertaModel
as
XLMRobertaModel
__webpack_exports__XLMRobertaPreTrainedModel
as
XLMRobertaPreTrainedModel
__webpack_exports__XLMRobertaTokenizer
as
XLMRobertaTokenizer
__webpack_exports__XLMTokenizer
as
XLMTokenizer
__webpack_exports__XLMWithLMHeadModel
as
XLMWithLMHeadModel
__webpack_exports__XVectorOutput
as
XVectorOutput
__webpack_exports__YolosFeatureExtractor
as
YolosFeatureExtractor
__webpack_exports__YolosForObjectDetection
as
YolosForObjectDetection
__webpack_exports__YolosModel
as
YolosModel
__webpack_exports__YolosObjectDetectionOutput
as
YolosObjectDetectionOutput
__webpack_exports__YolosPreTrainedModel
as
YolosPreTrainedModel
__webpack_exports__ZeroShotAudioClassificationPipeline
as
ZeroShotAudioClassificationPipeline
__webpack_exports__ZeroShotClassificationPipeline
as
ZeroShotClassificationPipeline
__webpack_exports__ZeroShotImageClassificationPipeline
as
ZeroShotImageClassificationPipeline
__webpack_exports__ZeroShotObjectDetectionPipeline
as
ZeroShotObjectDetectionPipeline
__webpack_exports__bankers_round
as
bankers_round
__webpack_exports__cat
as
cat
__webpack_exports__cos_sim
as
cos_sim
__webpack_exports__dot
as
dot
__webpack_exports__dynamicTimeWarping
as
dynamicTimeWarping
__webpack_exports__env
as
env
__webpack_exports__getTopItems
as
getTopItems
__webpack_exports__hanning
as
hanning
__webpack_exports__interpolate
as
interpolate
__webpack_exports__interpolate_data
as
interpolate_data
__webpack_exports__layer_norm
as
layer_norm
__webpack_exports__log_softmax
as
log_softmax
__webpack_exports__magnitude
as
magnitude
__webpack_exports__max
as
max
__webpack_exports__mean
as
mean
__webpack_exports__mean_pooling
as
mean_pooling
__webpack_exports__medianFilter
as
medianFilter
__webpack_exports__mel_filter_bank
as
mel_filter_bank
__webpack_exports__min
as
min
__webpack_exports__ones
as
ones
__webpack_exports__ones_like
as
ones_like
__webpack_exports__permute
as
permute
__webpack_exports__permute_data
as
permute_data
__webpack_exports__pipeline
as
pipeline
__webpack_exports__read_audio
as
read_audio
__webpack_exports__round
as
round
__webpack_exports__softmax
as
softmax
__webpack_exports__spectrogram
as
spectrogram
__webpack_exports__stack
as
stack
__webpack_exports__std_mean
as
std_mean
__webpack_exports__window_function
as
window_function
}
;
