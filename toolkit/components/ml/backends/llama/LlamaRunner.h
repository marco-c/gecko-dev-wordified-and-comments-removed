#
ifndef
mozilla_dom_LlamaRunner_h
#
define
mozilla_dom_LlamaRunner_h
#
include
"
mozilla
/
dom
/
LlamaRunnerBinding
.
h
"
#
include
"
mozilla
/
dom
/
ReadableStream
.
h
"
#
include
"
nsWrapperCache
.
h
"
#
include
"
mozilla
/
llama
/
LlamaBackend
.
h
"
#
include
"
prsystem
.
h
"
#
include
"
mozilla
/
Casting
.
h
"
#
include
"
mozilla
/
SPSCQueue
.
h
"
#
include
"
mozilla
/
Array
.
h
"
#
include
<
cstdint
>
#
include
"
mozilla
/
dom
/
Promise
.
h
"
#
include
"
mozilla
/
dom
/
ReadableStream
.
h
"
#
include
"
mozilla
/
dom
/
ReadableStreamDefaultController
.
h
"
#
include
"
mozilla
/
dom
/
UnderlyingSourceCallbackHelpers
.
h
"
#
include
"
nsThreadUtils
.
h
"
#
include
"
mozilla
/
TaskQueue
.
h
"
#
include
"
mozilla
/
ScopeExit
.
h
"
#
include
"
mozilla
/
RefPtr
.
h
"
#
include
"
nsIThread
.
h
"
#
include
"
nsError
.
h
"
#
include
"
mozilla
/
dom
/
ContentChild
.
h
"
#
include
"
mozilla
/
Atomics
.
h
"
#
include
"
mozilla
/
llama
/
LlamaBackend
.
h
"
#
include
"
mozilla
/
Logging
.
h
"
#
include
"
nsFmtString
.
h
"
#
include
"
nsThread
.
h
"
#
include
"
nsThreadManager
.
h
"
#
include
"
mozilla
/
ThreadEventQueue
.
h
"
#
include
"
mozilla
/
EventQueue
.
h
"
#
include
"
nsDeque
.
h
"
#
include
"
mozilla
/
dom
/
Blob
.
h
"
namespace
mozilla
:
:
dom
{
class
LlamaStreamSource
;
}
namespace
mozilla
:
:
llama
{
using
LlamaGenerateTaskPromise
=
MozPromise
<
mozilla
:
:
Maybe
<
LlamaChatResponse
>
nsCString
true
>
;
class
LlamaGenerateTask
final
:
public
mozilla
:
:
CancelableRunnable
{
public
:
using
LlamaChatResponse
=
mozilla
:
:
dom
:
:
LlamaChatResponse
;
enum
class
TaskState
:
uint32_t
{
Idle
Running
CompletedSuccess
CompletedFailure
Cancelled
}
;
LlamaGenerateTask
(
RefPtr
<
LlamaBackend
>
aBackend
const
LlamaChatOptions
&
aOptions
)
;
NS_IMETHOD
Run
(
)
override
;
nsresult
Cancel
(
)
override
;
RefPtr
<
LlamaGenerateTaskPromise
>
GetMessage
(
)
;
private
:
bool
MaybePushMessage
(
mozilla
:
:
Maybe
<
LlamaChatResponse
>
aMessage
)
;
bool
PushMessage
(
mozilla
:
:
Maybe
<
LlamaChatResponse
>
aMessage
)
;
mozilla
:
:
Atomic
<
TaskState
>
mState
{
TaskState
:
:
Idle
}
;
nsCString
mErrorMessage
;
RefPtr
<
LlamaBackend
>
mBackend
;
const
LlamaChatOptions
mChatOptions
;
int
mCurrentPromiseHolderIdx
{
0
}
;
Array
<
MozPromiseHolder
<
LlamaGenerateTaskPromise
>
2
>
mPromiseHolders
;
Atomic
<
bool
>
mHasPendingConsumer
{
false
}
;
SPSCQueue
<
mozilla
:
:
Maybe
<
LlamaChatResponse
>
>
mMessagesQueue
;
~
LlamaGenerateTask
(
)
;
}
;
}
namespace
mozilla
:
:
dom
{
using
LlamaBackend
=
:
:
mozilla
:
:
llama
:
:
LlamaBackend
;
class
LlamaStreamSource
final
:
public
UnderlyingSourceAlgorithmsWrapper
{
public
:
MOZ_DECLARE_REFCOUNTED_TYPENAME
(
LlamaStreamSource
)
NS_DECL_ISUPPORTS_INHERITED
NS_DECL_CYCLE_COLLECTION_CLASS_INHERITED
(
LlamaStreamSource
UnderlyingSourceAlgorithmsWrapper
)
LlamaStreamSource
(
RefPtr
<
LlamaBackend
>
aBackend
const
LlamaChatOptions
&
aOptions
)
;
MOZ_CAN_RUN_SCRIPT
already_AddRefed
<
Promise
>
CancelCallbackImpl
(
JSContext
*
aCx
const
Optional
<
JS
:
:
Handle
<
JS
:
:
Value
>
>
&
aReason
ErrorResult
&
aRv
)
override
;
MOZ_CAN_RUN_SCRIPT
already_AddRefed
<
Promise
>
PullCallbackImpl
(
JSContext
*
aCx
ReadableStreamControllerBase
&
aController
ErrorResult
&
aRv
)
override
;
void
SetControllerStream
(
RefPtr
<
ReadableStream
>
aStream
)
;
private
:
~
LlamaStreamSource
(
)
;
RefPtr
<
LlamaBackend
>
mBackend
;
const
LlamaChatOptions
mChatOptions
;
RefPtr
<
mozilla
:
:
llama
:
:
LlamaGenerateTask
>
mTask
;
nsCOMPtr
<
nsIThread
>
mGenerateThread
;
nsCOMPtr
<
nsISerialEventTarget
>
mOriginalEventTarget
;
RefPtr
<
ReadableStream
>
mControllerStream
;
}
;
class
LlamaRunner
final
:
public
nsISupports
public
nsWrapperCache
{
public
:
MOZ_DECLARE_REFCOUNTED_TYPENAME
(
LlamaRunner
)
NS_DECL_CYCLE_COLLECTING_ISUPPORTS
NS_DECL_CYCLE_COLLECTION_WRAPPERCACHE_CLASS
(
LlamaRunner
)
explicit
LlamaRunner
(
const
GlobalObject
&
aGlobal
)
;
nsISupports
*
GetParentObject
(
)
const
{
return
mGlobal
;
}
static
already_AddRefed
<
LlamaRunner
>
Constructor
(
const
GlobalObject
&
aGlobal
ErrorResult
&
aRv
)
;
already_AddRefed
<
Promise
>
Initialize
(
const
LlamaModelOptions
&
aOptions
const
Blob
&
aModelBlob
ErrorResult
&
aRv
)
;
virtual
JSObject
*
WrapObject
(
JSContext
*
aCx
JS
:
:
Handle
<
JSObject
*
>
aGivenProto
)
override
;
already_AddRefed
<
ReadableStream
>
CreateGenerationStream
(
const
LlamaChatOptions
&
aOptions
ErrorResult
&
aRv
)
;
already_AddRefed
<
Promise
>
FormatChat
(
const
LlamaFormatChatOptions
&
aOptions
ErrorResult
&
aRv
)
;
static
bool
InInferenceProcess
(
JSContext
*
JSObject
*
)
;
private
:
~
LlamaRunner
(
)
=
default
;
RefPtr
<
LlamaBackend
>
mBackend
;
RefPtr
<
LlamaStreamSource
>
mStreamSource
;
protected
:
nsCOMPtr
<
nsIGlobalObject
>
mGlobal
;
}
;
}
#
endif
