#
[
cfg
(
feature
=
"
nightly
"
)
]
use
std
:
:
sync
:
:
atomic
:
:
{
AtomicUsize
Ordering
}
;
use
std
:
:
cell
:
:
Cell
;
#
[
cfg
(
not
(
feature
=
"
nightly
"
)
)
]
use
stable
:
:
{
AtomicUsize
Ordering
}
;
use
std
:
:
time
:
:
{
Duration
Instant
}
;
use
parking_lot_core
:
:
{
self
ParkResult
UnparkResult
SpinWait
ParkToken
FilterOp
}
;
use
elision
:
:
{
have_elision
AtomicElisionExt
}
;
use
util
:
:
UncheckedOptionExt
;
use
raw_mutex
:
:
{
TOKEN_NORMAL
TOKEN_HANDOFF
}
;
const
TOKEN_SHARED
:
ParkToken
=
ParkToken
(
0
)
;
const
TOKEN_EXCLUSIVE
:
ParkToken
=
ParkToken
(
1
)
;
const
PARKED_BIT
:
usize
=
1
;
const
LOCKED_BIT
:
usize
=
2
;
const
SHARED_COUNT_MASK
:
usize
=
!
3
;
const
SHARED_COUNT_INC
:
usize
=
4
;
const
SHARED_COUNT_SHIFT
:
usize
=
2
;
pub
struct
RawRwLock
{
state
:
AtomicUsize
}
impl
RawRwLock
{
#
[
cfg
(
feature
=
"
nightly
"
)
]
#
[
inline
]
pub
const
fn
new
(
)
-
>
RawRwLock
{
RawRwLock
{
state
:
AtomicUsize
:
:
new
(
0
)
}
}
#
[
cfg
(
not
(
feature
=
"
nightly
"
)
)
]
#
[
inline
]
pub
fn
new
(
)
-
>
RawRwLock
{
RawRwLock
{
state
:
AtomicUsize
:
:
new
(
0
)
}
}
#
[
inline
]
pub
fn
lock_exclusive
(
&
self
)
{
if
self
.
state
.
compare_exchange_weak
(
0
LOCKED_BIT
Ordering
:
:
Acquire
Ordering
:
:
Relaxed
)
.
is_ok
(
)
{
return
;
}
self
.
lock_exclusive_slow
(
None
)
;
}
#
[
inline
]
pub
fn
try_lock_exclusive_until
(
&
self
timeout
:
Instant
)
-
>
bool
{
if
self
.
state
.
compare_exchange_weak
(
0
LOCKED_BIT
Ordering
:
:
Acquire
Ordering
:
:
Relaxed
)
.
is_ok
(
)
{
return
true
;
}
self
.
lock_exclusive_slow
(
Some
(
timeout
)
)
}
#
[
inline
]
pub
fn
try_lock_exclusive_for
(
&
self
timeout
:
Duration
)
-
>
bool
{
if
self
.
state
.
compare_exchange_weak
(
0
LOCKED_BIT
Ordering
:
:
Acquire
Ordering
:
:
Relaxed
)
.
is_ok
(
)
{
return
true
;
}
self
.
lock_exclusive_slow
(
Some
(
Instant
:
:
now
(
)
+
timeout
)
)
}
#
[
inline
]
pub
fn
try_lock_exclusive
(
&
self
)
-
>
bool
{
self
.
state
.
compare_exchange
(
0
LOCKED_BIT
Ordering
:
:
Acquire
Ordering
:
:
Relaxed
)
.
is_ok
(
)
}
#
[
inline
]
pub
fn
unlock_exclusive
(
&
self
force_fair
:
bool
)
{
if
self
.
state
.
compare_exchange_weak
(
LOCKED_BIT
0
Ordering
:
:
Release
Ordering
:
:
Relaxed
)
.
is_ok
(
)
{
return
;
}
self
.
unlock_exclusive_slow
(
force_fair
)
;
}
#
[
inline
]
pub
fn
downgrade
(
&
self
)
{
let
state
=
self
.
state
.
fetch_add
(
SHARED_COUNT_INC
-
LOCKED_BIT
Ordering
:
:
Release
)
;
if
state
&
PARKED_BIT
!
=
0
{
self
.
downgrade_slow
(
)
;
}
}
#
[
inline
(
always
)
]
fn
try_lock_shared_fast
(
&
self
recursive
:
bool
)
-
>
bool
{
let
state
=
self
.
state
.
load
(
Ordering
:
:
Relaxed
)
;
if
!
recursive
{
if
state
&
(
LOCKED_BIT
|
PARKED_BIT
)
!
=
0
{
return
false
;
}
}
else
{
if
state
&
LOCKED_BIT
!
=
0
{
return
false
;
}
}
if
have_elision
(
)
&
&
state
=
=
0
{
self
.
state
.
elision_acquire
(
0
SHARED_COUNT_INC
)
.
is_ok
(
)
}
else
if
let
Some
(
new_state
)
=
state
.
checked_add
(
SHARED_COUNT_INC
)
{
self
.
state
.
compare_exchange_weak
(
state
new_state
Ordering
:
:
Acquire
Ordering
:
:
Relaxed
)
.
is_ok
(
)
}
else
{
false
}
}
#
[
inline
]
pub
fn
lock_shared
(
&
self
recursive
:
bool
)
{
if
!
self
.
try_lock_shared_fast
(
recursive
)
{
self
.
lock_shared_slow
(
recursive
None
)
;
}
}
#
[
inline
]
pub
fn
try_lock_shared_until
(
&
self
recursive
:
bool
timeout
:
Instant
)
-
>
bool
{
if
self
.
try_lock_shared_fast
(
recursive
)
{
return
true
;
}
self
.
lock_shared_slow
(
recursive
Some
(
timeout
)
)
}
#
[
inline
]
pub
fn
try_lock_shared_for
(
&
self
recursive
:
bool
timeout
:
Duration
)
-
>
bool
{
if
self
.
try_lock_shared_fast
(
recursive
)
{
return
true
;
}
self
.
lock_shared_slow
(
recursive
Some
(
Instant
:
:
now
(
)
+
timeout
)
)
}
#
[
inline
]
pub
fn
try_lock_shared
(
&
self
recursive
:
bool
)
-
>
bool
{
if
self
.
try_lock_shared_fast
(
recursive
)
{
return
true
;
}
self
.
try_lock_shared_slow
(
recursive
)
}
#
[
inline
]
pub
fn
unlock_shared
(
&
self
force_fair
:
bool
)
{
let
state
=
self
.
state
.
load
(
Ordering
:
:
Relaxed
)
;
if
state
&
PARKED_BIT
=
=
0
|
|
state
&
SHARED_COUNT_MASK
!
=
SHARED_COUNT_INC
{
if
have_elision
(
)
{
if
self
.
state
.
elision_release
(
state
state
-
SHARED_COUNT_INC
)
.
is_ok
(
)
{
return
;
}
}
else
{
if
self
.
state
.
compare_exchange_weak
(
state
state
-
SHARED_COUNT_INC
Ordering
:
:
Release
Ordering
:
:
Relaxed
)
.
is_ok
(
)
{
return
;
}
}
}
self
.
unlock_shared_slow
(
force_fair
)
;
}
#
[
cold
]
#
[
inline
(
never
)
]
fn
lock_exclusive_slow
(
&
self
timeout
:
Option
<
Instant
>
)
-
>
bool
{
let
mut
spinwait
=
SpinWait
:
:
new
(
)
;
let
mut
state
=
self
.
state
.
load
(
Ordering
:
:
Relaxed
)
;
loop
{
if
state
&
(
LOCKED_BIT
|
SHARED_COUNT_MASK
)
=
=
0
{
match
self
.
state
.
compare_exchange_weak
(
state
state
|
LOCKED_BIT
Ordering
:
:
Acquire
Ordering
:
:
Relaxed
)
{
Ok
(
_
)
=
>
return
true
Err
(
x
)
=
>
state
=
x
}
continue
;
}
if
state
&
PARKED_BIT
=
=
0
&
&
(
state
&
LOCKED_BIT
!
=
0
|
|
state
&
SHARED_COUNT_MASK
=
=
SHARED_COUNT_INC
)
&
&
spinwait
.
spin
(
)
{
state
=
self
.
state
.
load
(
Ordering
:
:
Relaxed
)
;
continue
;
}
unsafe
{
let
addr
=
self
as
*
const
_
as
usize
;
let
validate
=
|
|
{
let
mut
state
=
self
.
state
.
load
(
Ordering
:
:
Relaxed
)
;
loop
{
if
state
&
(
LOCKED_BIT
|
SHARED_COUNT_MASK
)
=
=
0
{
return
false
;
}
if
state
&
PARKED_BIT
!
=
0
{
return
true
;
}
match
self
.
state
.
compare_exchange_weak
(
state
state
|
PARKED_BIT
Ordering
:
:
Relaxed
Ordering
:
:
Relaxed
)
{
Ok
(
_
)
=
>
return
true
Err
(
x
)
=
>
state
=
x
}
}
}
;
let
before_sleep
=
|
|
{
}
;
let
timed_out
=
|
_
was_last_thread
|
{
if
was_last_thread
{
self
.
state
.
fetch_and
(
!
PARKED_BIT
Ordering
:
:
Relaxed
)
;
}
}
;
match
parking_lot_core
:
:
park
(
addr
validate
before_sleep
timed_out
TOKEN_EXCLUSIVE
timeout
)
{
ParkResult
:
:
Unparked
(
TOKEN_HANDOFF
)
=
>
return
true
ParkResult
:
:
Unparked
(
_
)
=
>
(
)
ParkResult
:
:
Invalid
=
>
(
)
ParkResult
:
:
TimedOut
=
>
return
false
}
}
spinwait
.
reset
(
)
;
state
=
self
.
state
.
load
(
Ordering
:
:
Relaxed
)
;
}
}
#
[
cold
]
#
[
inline
(
never
)
]
fn
unlock_exclusive_slow
(
&
self
force_fair
:
bool
)
{
if
self
.
state
.
compare_exchange
(
LOCKED_BIT
0
Ordering
:
:
Release
Ordering
:
:
Relaxed
)
.
is_ok
(
)
{
return
;
}
let
first_token
=
Cell
:
:
new
(
None
)
;
unsafe
{
let
addr
=
self
as
*
const
_
as
usize
;
let
filter
=
|
token
|
-
>
FilterOp
{
if
let
Some
(
first_token
)
=
first_token
.
get
(
)
{
if
first_token
=
=
TOKEN_EXCLUSIVE
|
|
token
=
=
TOKEN_EXCLUSIVE
{
FilterOp
:
:
Stop
}
else
{
FilterOp
:
:
Unpark
}
}
else
{
first_token
.
set
(
Some
(
token
)
)
;
FilterOp
:
:
Unpark
}
}
;
let
callback
=
|
result
:
UnparkResult
|
{
if
result
.
unparked_threads
!
=
0
&
&
(
force_fair
|
|
result
.
be_fair
)
{
if
first_token
.
get
(
)
.
unchecked_unwrap
(
)
=
=
TOKEN_EXCLUSIVE
{
if
!
result
.
have_more_threads
{
self
.
state
.
store
(
LOCKED_BIT
Ordering
:
:
Relaxed
)
;
}
}
else
{
if
result
.
have_more_threads
{
self
.
state
.
store
(
(
result
.
unparked_threads
<
<
SHARED_COUNT_SHIFT
)
|
PARKED_BIT
Ordering
:
:
Release
)
;
}
else
{
self
.
state
.
store
(
result
.
unparked_threads
<
<
SHARED_COUNT_SHIFT
Ordering
:
:
Release
)
;
}
}
return
TOKEN_HANDOFF
;
}
if
result
.
have_more_threads
{
self
.
state
.
store
(
PARKED_BIT
Ordering
:
:
Release
)
;
}
else
{
self
.
state
.
store
(
0
Ordering
:
:
Release
)
;
}
TOKEN_NORMAL
}
;
parking_lot_core
:
:
unpark_filter
(
addr
filter
callback
)
;
}
}
#
[
cold
]
#
[
inline
(
never
)
]
fn
downgrade_slow
(
&
self
)
{
unsafe
{
let
addr
=
self
as
*
const
_
as
usize
;
let
filter
=
|
token
|
-
>
FilterOp
{
if
token
=
=
TOKEN_SHARED
{
FilterOp
:
:
Unpark
}
else
{
FilterOp
:
:
Stop
}
}
;
let
callback
=
|
result
:
UnparkResult
|
{
if
!
result
.
have_more_threads
{
self
.
state
.
fetch_and
(
!
PARKED_BIT
Ordering
:
:
Relaxed
)
;
}
TOKEN_NORMAL
}
;
parking_lot_core
:
:
unpark_filter
(
addr
filter
callback
)
;
}
}
#
[
cold
]
#
[
inline
(
never
)
]
fn
lock_shared_slow
(
&
self
recursive
:
bool
timeout
:
Option
<
Instant
>
)
-
>
bool
{
let
mut
spinwait
=
SpinWait
:
:
new
(
)
;
let
mut
spinwait_shared
=
SpinWait
:
:
new
(
)
;
let
mut
state
=
self
.
state
.
load
(
Ordering
:
:
Relaxed
)
;
let
mut
unparked
=
false
;
loop
{
if
have_elision
(
)
&
&
state
=
=
0
{
match
self
.
state
.
elision_acquire
(
0
SHARED_COUNT_INC
)
{
Ok
(
_
)
=
>
return
true
Err
(
x
)
=
>
state
=
x
}
}
if
state
&
LOCKED_BIT
=
=
0
&
&
(
unparked
|
|
recursive
|
|
state
&
PARKED_BIT
=
=
0
)
{
let
new
=
state
.
checked_add
(
SHARED_COUNT_INC
)
.
expect
(
"
RwLock
shared
count
overflow
"
)
;
if
self
.
state
.
compare_exchange_weak
(
state
new
Ordering
:
:
Acquire
Ordering
:
:
Relaxed
)
.
is_ok
(
)
{
return
true
;
}
spinwait_shared
.
spin_no_yield
(
)
;
state
=
self
.
state
.
load
(
Ordering
:
:
Relaxed
)
;
continue
;
}
if
state
&
PARKED_BIT
=
=
0
&
&
spinwait
.
spin
(
)
{
state
=
self
.
state
.
load
(
Ordering
:
:
Relaxed
)
;
continue
;
}
unsafe
{
let
addr
=
self
as
*
const
_
as
usize
;
let
validate
=
|
|
{
let
mut
state
=
self
.
state
.
load
(
Ordering
:
:
Relaxed
)
;
loop
{
if
state
&
PARKED_BIT
!
=
0
{
return
true
;
}
if
state
&
LOCKED_BIT
=
=
0
{
return
false
;
}
match
self
.
state
.
compare_exchange_weak
(
state
state
|
PARKED_BIT
Ordering
:
:
Relaxed
Ordering
:
:
Relaxed
)
{
Ok
(
_
)
=
>
return
true
Err
(
x
)
=
>
state
=
x
}
}
}
;
let
before_sleep
=
|
|
{
}
;
let
timed_out
=
|
_
was_last_thread
|
{
if
was_last_thread
{
self
.
state
.
fetch_and
(
!
PARKED_BIT
Ordering
:
:
Relaxed
)
;
}
}
;
match
parking_lot_core
:
:
park
(
addr
validate
before_sleep
timed_out
TOKEN_SHARED
timeout
)
{
ParkResult
:
:
Unparked
(
TOKEN_HANDOFF
)
=
>
return
true
ParkResult
:
:
Unparked
(
_
)
=
>
(
)
ParkResult
:
:
Invalid
=
>
(
)
ParkResult
:
:
TimedOut
=
>
return
false
}
}
spinwait
.
reset
(
)
;
spinwait_shared
.
reset
(
)
;
state
=
self
.
state
.
load
(
Ordering
:
:
Relaxed
)
;
unparked
=
true
;
}
}
#
[
cold
]
#
[
inline
(
never
)
]
pub
fn
try_lock_shared_slow
(
&
self
recursive
:
bool
)
-
>
bool
{
let
mut
state
=
self
.
state
.
load
(
Ordering
:
:
Relaxed
)
;
loop
{
let
mask
=
if
recursive
{
LOCKED_BIT
}
else
{
LOCKED_BIT
|
PARKED_BIT
}
;
if
state
&
mask
!
=
0
{
return
false
;
}
if
have_elision
(
)
&
&
state
=
=
0
{
match
self
.
state
.
elision_acquire
(
0
SHARED_COUNT_INC
)
{
Ok
(
_
)
=
>
return
true
Err
(
x
)
=
>
state
=
x
}
}
else
{
let
new
=
state
.
checked_add
(
SHARED_COUNT_INC
)
.
expect
(
"
RwLock
shared
count
overflow
"
)
;
match
self
.
state
.
compare_exchange_weak
(
state
new
Ordering
:
:
Acquire
Ordering
:
:
Relaxed
)
{
Ok
(
_
)
=
>
return
true
Err
(
x
)
=
>
state
=
x
}
}
}
}
#
[
cold
]
#
[
inline
(
never
)
]
fn
unlock_shared_slow
(
&
self
force_fair
:
bool
)
{
let
mut
state
=
self
.
state
.
load
(
Ordering
:
:
Relaxed
)
;
loop
{
if
state
&
PARKED_BIT
=
=
0
|
|
state
&
SHARED_COUNT_MASK
!
=
SHARED_COUNT_INC
{
match
self
.
state
.
compare_exchange_weak
(
state
state
-
SHARED_COUNT_INC
Ordering
:
:
Release
Ordering
:
:
Relaxed
)
{
Ok
(
_
)
=
>
return
Err
(
x
)
=
>
state
=
x
}
continue
;
}
let
first_token
=
Cell
:
:
new
(
None
)
;
unsafe
{
let
addr
=
self
as
*
const
_
as
usize
;
let
filter
=
|
token
|
-
>
FilterOp
{
if
let
Some
(
first_token
)
=
first_token
.
get
(
)
{
if
first_token
=
=
TOKEN_EXCLUSIVE
|
|
token
=
=
TOKEN_EXCLUSIVE
{
FilterOp
:
:
Stop
}
else
{
FilterOp
:
:
Unpark
}
}
else
{
first_token
.
set
(
Some
(
token
)
)
;
FilterOp
:
:
Unpark
}
}
;
let
callback
=
|
result
:
UnparkResult
|
{
let
mut
state
=
self
.
state
.
load
(
Ordering
:
:
Relaxed
)
;
loop
{
let
mut
new
=
state
-
SHARED_COUNT_INC
;
if
!
result
.
have_more_threads
{
new
&
=
!
PARKED_BIT
;
}
let
token
=
if
result
.
unparked_threads
!
=
0
&
&
new
&
SHARED_COUNT_MASK
=
=
0
&
&
first_token
.
get
(
)
.
unchecked_unwrap
(
)
=
=
TOKEN_EXCLUSIVE
&
&
(
force_fair
|
|
result
.
be_fair
)
{
new
|
=
LOCKED_BIT
;
TOKEN_HANDOFF
}
else
{
TOKEN_NORMAL
}
;
match
self
.
state
.
compare_exchange_weak
(
state
new
Ordering
:
:
Release
Ordering
:
:
Relaxed
)
{
Ok
(
_
)
=
>
return
token
Err
(
x
)
=
>
state
=
x
}
}
}
;
parking_lot_core
:
:
unpark_filter
(
addr
filter
callback
)
;
}
break
;
}
}
}
