use
std
:
:
ops
:
:
Range
;
use
std
:
:
cell
:
:
Cell
;
use
std
:
:
char
;
use
std
:
:
ascii
:
:
AsciiExt
;
use
std
:
:
i32
;
use
cow_rc_str
:
:
CowRcStr
;
use
self
:
:
Token
:
:
*
;
#
[
derive
(
PartialEq
Debug
Clone
)
]
pub
enum
Token
<
'
a
>
{
Ident
(
CowRcStr
<
'
a
>
)
AtKeyword
(
CowRcStr
<
'
a
>
)
Hash
(
CowRcStr
<
'
a
>
)
IDHash
(
CowRcStr
<
'
a
>
)
QuotedString
(
CowRcStr
<
'
a
>
)
UnquotedUrl
(
CowRcStr
<
'
a
>
)
Delim
(
char
)
Number
{
has_sign
:
bool
value
:
f32
int_value
:
Option
<
i32
>
}
Percentage
{
has_sign
:
bool
unit_value
:
f32
int_value
:
Option
<
i32
>
}
Dimension
{
has_sign
:
bool
value
:
f32
int_value
:
Option
<
i32
>
unit
:
CowRcStr
<
'
a
>
}
WhiteSpace
(
&
'
a
str
)
Comment
(
&
'
a
str
)
Colon
Semicolon
Comma
IncludeMatch
DashMatch
PrefixMatch
SuffixMatch
SubstringMatch
Column
CDO
CDC
Function
(
CowRcStr
<
'
a
>
)
ParenthesisBlock
SquareBracketBlock
CurlyBracketBlock
BadUrl
(
CowRcStr
<
'
a
>
)
BadString
(
CowRcStr
<
'
a
>
)
CloseParenthesis
CloseSquareBracket
CloseCurlyBracket
}
impl
<
'
a
>
Token
<
'
a
>
{
pub
fn
is_parse_error
(
&
self
)
-
>
bool
{
matches
!
(
*
self
BadUrl
(
_
)
|
BadString
(
_
)
|
CloseParenthesis
|
CloseSquareBracket
|
CloseCurlyBracket
)
}
}
#
[
derive
(
Clone
)
]
pub
struct
Tokenizer
<
'
a
>
{
input
:
&
'
a
str
position
:
usize
last_known_source_location
:
Cell
<
(
SourcePosition
SourceLocation
)
>
var_functions
:
SeenStatus
viewport_percentages
:
SeenStatus
}
#
[
derive
(
Copy
Clone
PartialEq
Eq
)
]
enum
SeenStatus
{
DontCare
LookingForThem
SeenAtLeastOne
}
impl
<
'
a
>
Tokenizer
<
'
a
>
{
#
[
inline
]
pub
fn
new
(
input
:
&
str
)
-
>
Tokenizer
{
Tokenizer
{
input
:
input
position
:
0
last_known_source_location
:
Cell
:
:
new
(
(
SourcePosition
(
0
)
SourceLocation
{
line
:
0
column
:
0
}
)
)
var_functions
:
SeenStatus
:
:
DontCare
viewport_percentages
:
SeenStatus
:
:
DontCare
}
}
#
[
inline
]
pub
fn
look_for_var_functions
(
&
mut
self
)
{
self
.
var_functions
=
SeenStatus
:
:
LookingForThem
;
}
#
[
inline
]
pub
fn
seen_var_functions
(
&
mut
self
)
-
>
bool
{
let
seen
=
self
.
var_functions
=
=
SeenStatus
:
:
SeenAtLeastOne
;
self
.
var_functions
=
SeenStatus
:
:
DontCare
;
seen
}
#
[
inline
]
pub
fn
see_function
(
&
mut
self
name
:
&
str
)
{
if
self
.
var_functions
=
=
SeenStatus
:
:
LookingForThem
{
if
name
.
eq_ignore_ascii_case
(
"
var
"
)
{
self
.
var_functions
=
SeenStatus
:
:
SeenAtLeastOne
;
}
}
}
#
[
inline
]
pub
fn
look_for_viewport_percentages
(
&
mut
self
)
{
self
.
viewport_percentages
=
SeenStatus
:
:
LookingForThem
;
}
#
[
inline
]
pub
fn
seen_viewport_percentages
(
&
mut
self
)
-
>
bool
{
let
seen
=
self
.
viewport_percentages
=
=
SeenStatus
:
:
SeenAtLeastOne
;
self
.
viewport_percentages
=
SeenStatus
:
:
DontCare
;
seen
}
#
[
inline
]
pub
fn
see_dimension
(
&
mut
self
unit
:
&
str
)
{
if
self
.
viewport_percentages
=
=
SeenStatus
:
:
LookingForThem
{
if
unit
.
eq_ignore_ascii_case
(
"
vh
"
)
|
|
unit
.
eq_ignore_ascii_case
(
"
vw
"
)
|
|
unit
.
eq_ignore_ascii_case
(
"
vmin
"
)
|
|
unit
.
eq_ignore_ascii_case
(
"
vmax
"
)
{
self
.
viewport_percentages
=
SeenStatus
:
:
SeenAtLeastOne
;
}
}
}
#
[
inline
]
pub
fn
next
(
&
mut
self
)
-
>
Result
<
Token
<
'
a
>
(
)
>
{
next_token
(
self
)
}
#
[
inline
]
pub
fn
position
(
&
self
)
-
>
SourcePosition
{
SourcePosition
(
self
.
position
)
}
#
[
inline
]
pub
fn
reset
(
&
mut
self
new_position
:
SourcePosition
)
{
self
.
position
=
new_position
.
0
;
}
#
[
inline
]
pub
fn
slice_from
(
&
self
start_pos
:
SourcePosition
)
-
>
&
'
a
str
{
&
self
.
input
[
start_pos
.
0
.
.
self
.
position
]
}
#
[
inline
]
pub
fn
slice
(
&
self
range
:
Range
<
SourcePosition
>
)
-
>
&
'
a
str
{
&
self
.
input
[
range
.
start
.
0
.
.
range
.
end
.
0
]
}
#
[
inline
]
pub
fn
current_source_location
(
&
self
)
-
>
SourceLocation
{
let
position
=
SourcePosition
(
self
.
position
)
;
self
.
source_location
(
position
)
}
pub
fn
current_source_line
(
&
self
)
-
>
&
'
a
str
{
let
current
=
self
.
position
;
let
start
=
self
.
input
[
0
.
.
current
]
.
rfind
(
|
c
|
matches
!
(
c
'
\
r
'
|
'
\
n
'
|
'
\
x0C
'
)
)
.
map_or
(
0
|
start
|
start
+
1
)
;
let
end
=
self
.
input
[
current
.
.
]
.
find
(
|
c
|
matches
!
(
c
'
\
r
'
|
'
\
n
'
|
'
\
x0C
'
)
)
.
map_or
(
self
.
input
.
len
(
)
|
end
|
current
+
end
)
;
&
self
.
input
[
start
.
.
end
]
}
pub
fn
source_location
(
&
self
position
:
SourcePosition
)
-
>
SourceLocation
{
let
target
=
position
.
0
;
let
mut
location
;
let
mut
position
;
let
(
SourcePosition
(
last_known_position
)
last_known_location
)
=
self
.
last_known_source_location
.
get
(
)
;
if
target
>
=
last_known_position
{
position
=
last_known_position
;
location
=
last_known_location
;
}
else
{
position
=
0
;
location
=
SourceLocation
{
line
:
0
column
:
0
}
;
}
let
mut
source
=
&
self
.
input
[
position
.
.
target
]
;
while
let
Some
(
newline_position
)
=
source
.
find
(
|
c
|
matches
!
(
c
'
\
n
'
|
'
\
r
'
|
'
\
x0C
'
)
)
{
let
offset
=
newline_position
+
if
source
[
newline_position
.
.
]
.
starts_with
(
"
\
r
\
n
"
)
{
2
}
else
{
1
}
;
source
=
&
source
[
offset
.
.
]
;
position
+
=
offset
;
location
.
line
+
=
1
;
location
.
column
=
0
;
}
debug_assert
!
(
position
<
=
target
)
;
location
.
column
+
=
(
target
-
position
)
as
u32
;
self
.
last_known_source_location
.
set
(
(
SourcePosition
(
target
)
location
)
)
;
location
}
#
[
inline
]
pub
fn
next_byte
(
&
self
)
-
>
Option
<
u8
>
{
if
self
.
is_eof
(
)
{
None
}
else
{
Some
(
self
.
input
.
as_bytes
(
)
[
self
.
position
]
)
}
}
#
[
inline
]
fn
is_eof
(
&
self
)
-
>
bool
{
!
self
.
has_at_least
(
0
)
}
#
[
inline
]
fn
has_at_least
(
&
self
n
:
usize
)
-
>
bool
{
self
.
position
+
n
<
self
.
input
.
len
(
)
}
#
[
inline
]
pub
fn
advance
(
&
mut
self
n
:
usize
)
{
self
.
position
+
=
n
}
#
[
inline
]
fn
next_byte_unchecked
(
&
self
)
-
>
u8
{
self
.
byte_at
(
0
)
}
#
[
inline
]
fn
byte_at
(
&
self
offset
:
usize
)
-
>
u8
{
self
.
input
.
as_bytes
(
)
[
self
.
position
+
offset
]
}
#
[
inline
]
fn
consume_byte
(
&
mut
self
)
-
>
u8
{
self
.
position
+
=
1
;
self
.
input
.
as_bytes
(
)
[
self
.
position
-
1
]
}
#
[
inline
]
fn
next_char
(
&
self
)
-
>
char
{
self
.
input
[
self
.
position
.
.
]
.
chars
(
)
.
next
(
)
.
unwrap
(
)
}
#
[
inline
]
fn
has_newline_at
(
&
self
offset
:
usize
)
-
>
bool
{
self
.
position
+
offset
<
self
.
input
.
len
(
)
&
&
matches
!
(
self
.
byte_at
(
offset
)
b
'
\
n
'
|
b
'
\
r
'
|
b
'
\
x0C
'
)
}
#
[
inline
]
fn
consume_char
(
&
mut
self
)
-
>
char
{
let
c
=
self
.
next_char
(
)
;
self
.
position
+
=
c
.
len_utf8
(
)
;
c
}
#
[
inline
]
fn
starts_with
(
&
self
needle
:
&
[
u8
]
)
-
>
bool
{
self
.
input
.
as_bytes
(
)
[
self
.
position
.
.
]
.
starts_with
(
needle
)
}
}
#
[
derive
(
PartialEq
Eq
PartialOrd
Ord
Debug
Clone
Copy
)
]
pub
struct
SourcePosition
(
usize
)
;
#
[
derive
(
PartialEq
Eq
Debug
Clone
Copy
)
]
pub
struct
SourceLocation
{
pub
line
:
u32
pub
column
:
u32
}
fn
next_token
<
'
a
>
(
tokenizer
:
&
mut
Tokenizer
<
'
a
>
)
-
>
Result
<
Token
<
'
a
>
(
)
>
{
if
tokenizer
.
is_eof
(
)
{
return
Err
(
(
)
)
}
let
b
=
tokenizer
.
next_byte_unchecked
(
)
;
let
token
=
match_byte
!
{
b
b
'
\
t
'
|
b
'
\
n
'
|
b
'
'
|
b
'
\
r
'
|
b
'
\
x0C
'
=
>
{
let
start_position
=
tokenizer
.
position
(
)
;
tokenizer
.
advance
(
1
)
;
while
!
tokenizer
.
is_eof
(
)
{
match
tokenizer
.
next_byte_unchecked
(
)
{
b
'
'
|
b
'
\
t
'
|
b
'
\
n
'
|
b
'
\
r
'
|
b
'
\
x0C
'
=
>
tokenizer
.
advance
(
1
)
_
=
>
break
}
}
WhiteSpace
(
tokenizer
.
slice_from
(
start_position
)
)
}
b
'
"
'
=
>
{
consume_string
(
tokenizer
false
)
}
b
'
#
'
=
>
{
tokenizer
.
advance
(
1
)
;
if
is_ident_start
(
tokenizer
)
{
IDHash
(
consume_name
(
tokenizer
)
)
}
else
if
!
tokenizer
.
is_eof
(
)
&
&
match
tokenizer
.
next_byte_unchecked
(
)
{
b
'
a
'
.
.
.
b
'
z
'
|
b
'
A
'
.
.
.
b
'
Z
'
|
b
'
0
'
.
.
.
b
'
9
'
|
b
'
-
'
|
b
'
_
'
=
>
true
b
'
\
\
'
=
>
!
tokenizer
.
has_newline_at
(
1
)
_
=
>
!
b
.
is_ascii
(
)
}
{
Hash
(
consume_name
(
tokenizer
)
)
}
else
{
Delim
(
'
#
'
)
}
}
b
'
'
=
>
{
if
tokenizer
.
starts_with
(
b
"
=
"
)
{
tokenizer
.
advance
(
2
)
;
SuffixMatch
}
else
{
tokenizer
.
advance
(
1
)
;
Delim
(
'
'
)
}
}
b
'
\
'
'
=
>
{
consume_string
(
tokenizer
true
)
}
b
'
(
'
=
>
{
tokenizer
.
advance
(
1
)
;
ParenthesisBlock
}
b
'
)
'
=
>
{
tokenizer
.
advance
(
1
)
;
CloseParenthesis
}
b
'
*
'
=
>
{
if
tokenizer
.
starts_with
(
b
"
*
=
"
)
{
tokenizer
.
advance
(
2
)
;
SubstringMatch
}
else
{
tokenizer
.
advance
(
1
)
;
Delim
(
'
*
'
)
}
}
b
'
+
'
=
>
{
if
(
tokenizer
.
has_at_least
(
1
)
&
&
matches
!
(
tokenizer
.
byte_at
(
1
)
b
'
0
'
.
.
.
b
'
9
'
)
)
|
|
(
tokenizer
.
has_at_least
(
2
)
&
&
tokenizer
.
byte_at
(
1
)
=
=
b
'
.
'
&
&
matches
!
(
tokenizer
.
byte_at
(
2
)
b
'
0
'
.
.
.
b
'
9
'
)
)
{
consume_numeric
(
tokenizer
)
}
else
{
tokenizer
.
advance
(
1
)
;
Delim
(
'
+
'
)
}
}
b
'
'
=
>
{
tokenizer
.
advance
(
1
)
;
Comma
}
b
'
-
'
=
>
{
if
(
tokenizer
.
has_at_least
(
1
)
&
&
matches
!
(
tokenizer
.
byte_at
(
1
)
b
'
0
'
.
.
.
b
'
9
'
)
)
|
|
(
tokenizer
.
has_at_least
(
2
)
&
&
tokenizer
.
byte_at
(
1
)
=
=
b
'
.
'
&
&
matches
!
(
tokenizer
.
byte_at
(
2
)
b
'
0
'
.
.
.
b
'
9
'
)
)
{
consume_numeric
(
tokenizer
)
}
else
if
tokenizer
.
starts_with
(
b
"
-
-
>
"
)
{
tokenizer
.
advance
(
3
)
;
CDC
}
else
if
is_ident_start
(
tokenizer
)
{
consume_ident_like
(
tokenizer
)
}
else
{
tokenizer
.
advance
(
1
)
;
Delim
(
'
-
'
)
}
}
b
'
.
'
=
>
{
if
tokenizer
.
has_at_least
(
1
)
&
&
matches
!
(
tokenizer
.
byte_at
(
1
)
b
'
0
'
.
.
.
b
'
9
'
)
{
consume_numeric
(
tokenizer
)
}
else
{
tokenizer
.
advance
(
1
)
;
Delim
(
'
.
'
)
}
}
b
'
/
'
=
>
{
if
tokenizer
.
starts_with
(
b
"
/
*
"
)
{
tokenizer
.
advance
(
2
)
;
/
/
consume
"
/
*
"
let
start_position
=
tokenizer
.
position
(
)
;
let
content
;
match
tokenizer
.
input
[
tokenizer
.
position
.
.
]
.
find
(
"
*
/
"
)
{
Some
(
offset
)
=
>
{
tokenizer
.
advance
(
offset
)
;
content
=
tokenizer
.
slice_from
(
start_position
)
;
tokenizer
.
advance
(
2
)
;
}
None
=
>
{
tokenizer
.
position
=
tokenizer
.
input
.
len
(
)
;
content
=
tokenizer
.
slice_from
(
start_position
)
;
}
}
Comment
(
content
)
}
else
{
tokenizer
.
advance
(
1
)
;
Delim
(
'
/
'
)
}
}
b
'
0
'
.
.
.
b
'
9
'
=
>
{
consume_numeric
(
tokenizer
)
}
b
'
:
'
=
>
{
tokenizer
.
advance
(
1
)
;
Colon
}
b
'
;
'
=
>
{
tokenizer
.
advance
(
1
)
;
Semicolon
}
b
'
<
'
=
>
{
if
tokenizer
.
starts_with
(
b
"
<
!
-
-
"
)
{
tokenizer
.
advance
(
4
)
;
CDO
}
else
{
tokenizer
.
advance
(
1
)
;
Delim
(
'
<
'
)
}
}
b
'
'
=
>
{
tokenizer
.
advance
(
1
)
;
if
is_ident_start
(
tokenizer
)
{
AtKeyword
(
consume_name
(
tokenizer
)
)
}
else
{
Delim
(
'
'
)
}
}
b
'
a
'
.
.
.
b
'
z
'
|
b
'
A
'
.
.
.
b
'
Z
'
|
b
'
_
'
|
b
'
\
0
'
=
>
{
consume_ident_like
(
tokenizer
)
}
b
'
[
'
=
>
{
tokenizer
.
advance
(
1
)
;
SquareBracketBlock
}
b
'
\
\
'
=
>
{
if
!
tokenizer
.
has_newline_at
(
1
)
{
consume_ident_like
(
tokenizer
)
}
else
{
tokenizer
.
advance
(
1
)
;
Delim
(
'
\
\
'
)
}
}
b
'
]
'
=
>
{
tokenizer
.
advance
(
1
)
;
CloseSquareBracket
}
b
'
^
'
=
>
{
if
tokenizer
.
starts_with
(
b
"
^
=
"
)
{
tokenizer
.
advance
(
2
)
;
PrefixMatch
}
else
{
tokenizer
.
advance
(
1
)
;
Delim
(
'
^
'
)
}
}
b
'
{
'
=
>
{
tokenizer
.
advance
(
1
)
;
CurlyBracketBlock
}
b
'
|
'
=
>
{
if
tokenizer
.
starts_with
(
b
"
|
=
"
)
{
tokenizer
.
advance
(
2
)
;
DashMatch
}
else
if
tokenizer
.
starts_with
(
b
"
|
|
"
)
{
tokenizer
.
advance
(
2
)
;
Column
}
else
{
tokenizer
.
advance
(
1
)
;
Delim
(
'
|
'
)
}
}
b
'
}
'
=
>
{
tokenizer
.
advance
(
1
)
;
CloseCurlyBracket
}
b
'
~
'
=
>
{
if
tokenizer
.
starts_with
(
b
"
~
=
"
)
{
tokenizer
.
advance
(
2
)
;
IncludeMatch
}
else
{
tokenizer
.
advance
(
1
)
;
Delim
(
'
~
'
)
}
}
_
=
>
{
if
!
b
.
is_ascii
(
)
{
consume_ident_like
(
tokenizer
)
}
else
{
tokenizer
.
advance
(
1
)
;
Delim
(
b
as
char
)
}
}
}
;
Ok
(
token
)
}
fn
consume_string
<
'
a
>
(
tokenizer
:
&
mut
Tokenizer
<
'
a
>
single_quote
:
bool
)
-
>
Token
<
'
a
>
{
match
consume_quoted_string
(
tokenizer
single_quote
)
{
Ok
(
value
)
=
>
QuotedString
(
value
)
Err
(
value
)
=
>
BadString
(
value
)
}
}
fn
consume_quoted_string
<
'
a
>
(
tokenizer
:
&
mut
Tokenizer
<
'
a
>
single_quote
:
bool
)
-
>
Result
<
CowRcStr
<
'
a
>
CowRcStr
<
'
a
>
>
{
tokenizer
.
advance
(
1
)
;
let
start_pos
=
tokenizer
.
position
(
)
;
let
mut
string_bytes
;
loop
{
if
tokenizer
.
is_eof
(
)
{
return
Ok
(
tokenizer
.
slice_from
(
start_pos
)
.
into
(
)
)
}
match_byte
!
{
tokenizer
.
next_byte_unchecked
(
)
b
'
"
'
=
>
{
if
!
single_quote
{
let
value
=
tokenizer
.
slice_from
(
start_pos
)
;
tokenizer
.
advance
(
1
)
;
return
Ok
(
value
.
into
(
)
)
}
}
b
'
\
'
'
=
>
{
if
single_quote
{
let
value
=
tokenizer
.
slice_from
(
start_pos
)
;
tokenizer
.
advance
(
1
)
;
return
Ok
(
value
.
into
(
)
)
}
}
b
'
\
\
'
|
b
'
\
0
'
=
>
{
/
/
*
The
tokenizer
s
input
is
UTF
-
8
since
it
s
&
str
.
/
/
*
start_pos
is
at
a
code
point
boundary
/
/
*
so
is
the
current
position
(
which
is
before
'
\
\
'
or
'
\
0
'
/
/
/
/
So
string_bytes
is
well
-
formed
UTF
-
8
.
string_bytes
=
tokenizer
.
slice_from
(
start_pos
)
.
as_bytes
(
)
.
to_owned
(
)
;
break
}
b
'
\
n
'
|
b
'
\
r
'
|
b
'
\
x0C
'
=
>
{
return
Err
(
tokenizer
.
slice_from
(
start_pos
)
.
into
(
)
)
}
_
=
>
{
}
}
tokenizer
.
consume_byte
(
)
;
}
while
!
tokenizer
.
is_eof
(
)
{
if
matches
!
(
tokenizer
.
next_byte_unchecked
(
)
b
'
\
n
'
|
b
'
\
r
'
|
b
'
\
x0C
'
)
{
return
Err
(
unsafe
{
from_utf8_release_unchecked
(
string_bytes
)
}
.
into
(
)
)
;
}
let
b
=
tokenizer
.
consume_byte
(
)
;
match_byte
!
{
b
b
'
"
'
=
>
{
if
!
single_quote
{
break
;
}
}
b
'
\
'
'
=
>
{
if
single_quote
{
break
;
}
}
b
'
\
\
'
=
>
{
if
!
tokenizer
.
is_eof
(
)
{
match
tokenizer
.
next_byte_unchecked
(
)
{
/
/
Escaped
newline
b
'
\
n
'
|
b
'
\
x0C
'
=
>
tokenizer
.
advance
(
1
)
b
'
\
r
'
=
>
{
tokenizer
.
advance
(
1
)
;
if
tokenizer
.
next_byte
(
)
=
=
Some
(
b
'
\
n
'
)
{
tokenizer
.
advance
(
1
)
;
}
}
/
/
This
pushes
one
well
-
formed
code
point
_
=
>
consume_escape_and_write
(
tokenizer
&
mut
string_bytes
)
}
}
/
/
else
:
escaped
EOF
do
nothing
.
continue
;
}
b
'
\
0
'
=
>
{
string_bytes
.
extend
(
"
\
u
{
FFFD
}
"
.
as_bytes
(
)
)
;
continue
;
}
_
=
>
{
}
}
string_bytes
.
push
(
b
)
;
}
Ok
(
unsafe
{
from_utf8_release_unchecked
(
string_bytes
)
}
.
into
(
)
)
}
#
[
inline
]
fn
is_ident_start
(
tokenizer
:
&
mut
Tokenizer
)
-
>
bool
{
!
tokenizer
.
is_eof
(
)
&
&
match_byte
!
{
tokenizer
.
next_byte_unchecked
(
)
b
'
a
'
.
.
.
b
'
z
'
|
b
'
A
'
.
.
.
b
'
Z
'
|
b
'
_
'
|
b
'
\
0
'
=
>
{
true
}
b
'
-
'
=
>
{
tokenizer
.
has_at_least
(
1
)
&
&
match_byte
!
{
tokenizer
.
byte_at
(
1
)
b
'
a
'
.
.
.
b
'
z
'
|
b
'
A
'
.
.
.
b
'
Z
'
|
b
'
-
'
|
b
'
_
'
|
b
'
\
0
'
=
>
{
true
}
b
'
\
\
'
=
>
{
!
tokenizer
.
has_newline_at
(
1
)
}
b
=
>
{
!
b
.
is_ascii
(
)
}
}
}
b
'
\
\
'
=
>
{
!
tokenizer
.
has_newline_at
(
1
)
}
b
=
>
{
!
b
.
is_ascii
(
)
}
}
}
fn
consume_ident_like
<
'
a
>
(
tokenizer
:
&
mut
Tokenizer
<
'
a
>
)
-
>
Token
<
'
a
>
{
let
value
=
consume_name
(
tokenizer
)
;
if
!
tokenizer
.
is_eof
(
)
&
&
tokenizer
.
next_byte_unchecked
(
)
=
=
b
'
(
'
{
tokenizer
.
advance
(
1
)
;
if
value
.
eq_ignore_ascii_case
(
"
url
"
)
{
consume_unquoted_url
(
tokenizer
)
.
unwrap_or
(
Function
(
value
)
)
}
else
{
tokenizer
.
see_function
(
&
value
)
;
Function
(
value
)
}
}
else
{
Ident
(
value
)
}
}
fn
consume_name
<
'
a
>
(
tokenizer
:
&
mut
Tokenizer
<
'
a
>
)
-
>
CowRcStr
<
'
a
>
{
let
start_pos
=
tokenizer
.
position
(
)
;
let
mut
value_bytes
;
loop
{
if
tokenizer
.
is_eof
(
)
{
return
tokenizer
.
slice_from
(
start_pos
)
.
into
(
)
}
match_byte
!
{
tokenizer
.
next_byte_unchecked
(
)
b
'
a
'
.
.
.
b
'
z
'
|
b
'
A
'
.
.
.
b
'
Z
'
|
b
'
0
'
.
.
.
b
'
9
'
|
b
'
_
'
|
b
'
-
'
=
>
{
tokenizer
.
advance
(
1
)
}
b
'
\
\
'
|
b
'
\
0
'
=
>
{
/
/
*
The
tokenizer
s
input
is
UTF
-
8
since
it
s
&
str
.
/
/
*
start_pos
is
at
a
code
point
boundary
/
/
*
so
is
the
current
position
(
which
is
before
'
\
\
'
or
'
\
0
'
/
/
/
/
So
value_bytes
is
well
-
formed
UTF
-
8
.
value_bytes
=
tokenizer
.
slice_from
(
start_pos
)
.
as_bytes
(
)
.
to_owned
(
)
;
break
}
b
=
>
{
if
b
.
is_ascii
(
)
{
return
tokenizer
.
slice_from
(
start_pos
)
.
into
(
)
;
}
tokenizer
.
advance
(
1
)
;
}
}
}
while
!
tokenizer
.
is_eof
(
)
{
let
b
=
tokenizer
.
next_byte_unchecked
(
)
;
match_byte
!
{
b
b
'
a
'
.
.
.
b
'
z
'
|
b
'
A
'
.
.
.
b
'
Z
'
|
b
'
0
'
.
.
.
b
'
9
'
|
b
'
_
'
|
b
'
-
'
=
>
{
tokenizer
.
advance
(
1
)
;
value_bytes
.
push
(
b
)
/
/
ASCII
}
b
'
\
\
'
=
>
{
if
tokenizer
.
has_newline_at
(
1
)
{
break
}
tokenizer
.
advance
(
1
)
;
/
/
This
pushes
one
well
-
formed
code
point
consume_escape_and_write
(
tokenizer
&
mut
value_bytes
)
}
b
'
\
0
'
=
>
{
tokenizer
.
advance
(
1
)
;
value_bytes
.
extend
(
"
\
u
{
FFFD
}
"
.
as_bytes
(
)
)
;
}
_
=
>
{
if
b
.
is_ascii
(
)
{
break
;
}
tokenizer
.
advance
(
1
)
;
/
/
This
byte
*
is
*
part
of
a
multi
-
byte
code
point
/
/
we
ll
end
up
copying
the
whole
code
point
before
this
loop
does
something
else
.
value_bytes
.
push
(
b
)
}
}
}
unsafe
{
from_utf8_release_unchecked
(
value_bytes
)
}
.
into
(
)
}
fn
byte_to_hex_digit
(
b
:
u8
)
-
>
Option
<
u32
>
{
Some
(
match_byte
!
{
b
b
'
0
'
.
.
.
b
'
9
'
=
>
{
b
-
b
'
0
'
}
b
'
a
'
.
.
.
b
'
f
'
=
>
{
b
-
b
'
a
'
+
10
}
b
'
A
'
.
.
.
b
'
F
'
=
>
{
b
-
b
'
A
'
+
10
}
_
=
>
{
return
None
}
}
as
u32
)
}
fn
byte_to_decimal_digit
(
b
:
u8
)
-
>
Option
<
u32
>
{
if
b
>
=
b
'
0
'
&
&
b
<
=
b
'
9
'
{
Some
(
(
b
-
b
'
0
'
)
as
u32
)
}
else
{
None
}
}
fn
consume_numeric
<
'
a
>
(
tokenizer
:
&
mut
Tokenizer
<
'
a
>
)
-
>
Token
<
'
a
>
{
let
(
has_sign
sign
)
=
match
tokenizer
.
next_byte_unchecked
(
)
{
b
'
-
'
=
>
(
true
-
1
.
)
b
'
+
'
=
>
(
true
1
.
)
_
=
>
(
false
1
.
)
}
;
if
has_sign
{
tokenizer
.
advance
(
1
)
;
}
let
mut
integral_part
:
f64
=
0
.
;
while
let
Some
(
digit
)
=
byte_to_decimal_digit
(
tokenizer
.
next_byte_unchecked
(
)
)
{
integral_part
=
integral_part
*
10
.
+
digit
as
f64
;
tokenizer
.
advance
(
1
)
;
if
tokenizer
.
is_eof
(
)
{
break
}
}
let
mut
is_integer
=
true
;
let
mut
fractional_part
:
f64
=
0
.
;
if
tokenizer
.
has_at_least
(
1
)
&
&
tokenizer
.
next_byte_unchecked
(
)
=
=
b
'
.
'
&
&
matches
!
(
tokenizer
.
byte_at
(
1
)
b
'
0
'
.
.
.
b
'
9
'
)
{
is_integer
=
false
;
tokenizer
.
advance
(
1
)
;
let
mut
factor
=
0
.
1
;
while
let
Some
(
digit
)
=
byte_to_decimal_digit
(
tokenizer
.
next_byte_unchecked
(
)
)
{
fractional_part
+
=
digit
as
f64
*
factor
;
factor
*
=
0
.
1
;
tokenizer
.
advance
(
1
)
;
if
tokenizer
.
is_eof
(
)
{
break
}
}
}
let
mut
value
=
sign
*
(
integral_part
+
fractional_part
)
;
if
tokenizer
.
has_at_least
(
1
)
&
&
matches
!
(
tokenizer
.
next_byte_unchecked
(
)
b
'
e
'
|
b
'
E
'
)
{
if
matches
!
(
tokenizer
.
byte_at
(
1
)
b
'
0
'
.
.
.
b
'
9
'
)
|
|
(
tokenizer
.
has_at_least
(
2
)
&
&
matches
!
(
tokenizer
.
byte_at
(
1
)
b
'
+
'
|
b
'
-
'
)
&
&
matches
!
(
tokenizer
.
byte_at
(
2
)
b
'
0
'
.
.
.
b
'
9
'
)
)
{
is_integer
=
false
;
tokenizer
.
advance
(
1
)
;
let
(
has_sign
sign
)
=
match
tokenizer
.
next_byte_unchecked
(
)
{
b
'
-
'
=
>
(
true
-
1
.
)
b
'
+
'
=
>
(
true
1
.
)
_
=
>
(
false
1
.
)
}
;
if
has_sign
{
tokenizer
.
advance
(
1
)
;
}
let
mut
exponent
:
f64
=
0
.
;
while
let
Some
(
digit
)
=
byte_to_decimal_digit
(
tokenizer
.
next_byte_unchecked
(
)
)
{
exponent
=
exponent
*
10
.
+
digit
as
f64
;
tokenizer
.
advance
(
1
)
;
if
tokenizer
.
is_eof
(
)
{
break
}
}
value
*
=
f64
:
:
powf
(
10
.
sign
*
exponent
)
;
}
}
let
int_value
=
if
is_integer
{
Some
(
if
value
>
=
i32
:
:
MAX
as
f64
{
i32
:
:
MAX
}
else
if
value
<
=
i32
:
:
MIN
as
f64
{
i32
:
:
MIN
}
else
{
value
as
i32
}
)
}
else
{
None
}
;
if
!
tokenizer
.
is_eof
(
)
&
&
tokenizer
.
next_byte_unchecked
(
)
=
=
b
'
%
'
{
tokenizer
.
advance
(
1
)
;
return
Percentage
{
unit_value
:
(
value
/
100
.
)
as
f32
int_value
:
int_value
has_sign
:
has_sign
}
}
let
value
=
value
as
f32
;
if
is_ident_start
(
tokenizer
)
{
let
unit
=
consume_name
(
tokenizer
)
;
tokenizer
.
see_dimension
(
&
unit
)
;
Dimension
{
value
:
value
int_value
:
int_value
has_sign
:
has_sign
unit
:
unit
}
}
else
{
Number
{
value
:
value
int_value
:
int_value
has_sign
:
has_sign
}
}
}
#
[
inline
]
unsafe
fn
from_utf8_release_unchecked
(
string_bytes
:
Vec
<
u8
>
)
-
>
String
{
if
cfg
!
(
debug_assertions
)
{
String
:
:
from_utf8
(
string_bytes
)
.
unwrap
(
)
}
else
{
String
:
:
from_utf8_unchecked
(
string_bytes
)
}
}
fn
consume_unquoted_url
<
'
a
>
(
tokenizer
:
&
mut
Tokenizer
<
'
a
>
)
-
>
Result
<
Token
<
'
a
>
(
)
>
{
for
(
offset
c
)
in
tokenizer
.
input
[
tokenizer
.
position
.
.
]
.
bytes
(
)
.
enumerate
(
)
{
match_byte
!
{
c
b
'
'
|
b
'
\
t
'
|
b
'
\
n
'
|
b
'
\
r
'
|
b
'
\
x0C
'
=
>
{
}
b
'
"
'
|
b
'
\
'
'
=
>
{
return
Err
(
(
)
)
}
/
/
Do
not
advance
b
'
)
'
=
>
{
tokenizer
.
advance
(
offset
+
1
)
;
return
Ok
(
UnquotedUrl
(
"
"
.
into
(
)
)
)
;
}
_
=
>
{
tokenizer
.
advance
(
offset
)
;
/
/
This
function
only
consumed
ASCII
(
whitespace
)
bytes
/
/
so
the
current
position
is
a
code
point
boundary
.
return
Ok
(
consume_unquoted_url_internal
(
tokenizer
)
)
}
}
}
tokenizer
.
position
=
tokenizer
.
input
.
len
(
)
;
return
Ok
(
UnquotedUrl
(
"
"
.
into
(
)
)
)
;
fn
consume_unquoted_url_internal
<
'
a
>
(
tokenizer
:
&
mut
Tokenizer
<
'
a
>
)
-
>
Token
<
'
a
>
{
let
start_pos
=
tokenizer
.
position
(
)
;
let
mut
string_bytes
:
Vec
<
u8
>
;
loop
{
if
tokenizer
.
is_eof
(
)
{
return
UnquotedUrl
(
tokenizer
.
slice_from
(
start_pos
)
.
into
(
)
)
}
match_byte
!
{
tokenizer
.
next_byte_unchecked
(
)
b
'
'
|
b
'
\
t
'
|
b
'
\
n
'
|
b
'
\
r
'
|
b
'
\
x0C
'
=
>
{
let
value
=
tokenizer
.
slice_from
(
start_pos
)
;
tokenizer
.
advance
(
1
)
;
return
consume_url_end
(
tokenizer
start_pos
value
.
into
(
)
)
}
b
'
)
'
=
>
{
let
value
=
tokenizer
.
slice_from
(
start_pos
)
;
tokenizer
.
advance
(
1
)
;
return
UnquotedUrl
(
value
.
into
(
)
)
}
b
'
\
x01
'
.
.
.
b
'
\
x08
'
|
b
'
\
x0B
'
|
b
'
\
x0E
'
.
.
.
b
'
\
x1F
'
|
b
'
\
x7F
'
/
/
non
-
printable
|
b
'
"
'
|
b
'
\
'
'
|
b
'
(
'
=
>
{
tokenizer
.
advance
(
1
)
;
return
consume_bad_url
(
tokenizer
start_pos
)
}
b
'
\
\
'
|
b
'
\
0
'
=
>
{
/
/
*
The
tokenizer
s
input
is
UTF
-
8
since
it
s
&
str
.
/
/
*
start_pos
is
at
a
code
point
boundary
/
/
*
so
is
the
current
position
(
which
is
before
'
\
\
'
or
'
\
0
'
/
/
/
/
So
string_bytes
is
well
-
formed
UTF
-
8
.
string_bytes
=
tokenizer
.
slice_from
(
start_pos
)
.
as_bytes
(
)
.
to_owned
(
)
;
break
}
_
=
>
{
tokenizer
.
consume_byte
(
)
;
}
}
}
while
!
tokenizer
.
is_eof
(
)
{
match_byte
!
{
tokenizer
.
consume_byte
(
)
b
'
'
|
b
'
\
t
'
|
b
'
\
n
'
|
b
'
\
r
'
|
b
'
\
x0C
'
=
>
{
/
/
string_bytes
is
well
-
formed
UTF
-
8
see
other
comments
.
let
string
=
unsafe
{
from_utf8_release_unchecked
(
string_bytes
)
}
.
into
(
)
;
return
consume_url_end
(
tokenizer
start_pos
string
)
}
b
'
)
'
=
>
{
break
;
}
b
'
\
x01
'
.
.
.
b
'
\
x08
'
|
b
'
\
x0B
'
|
b
'
\
x0E
'
.
.
.
b
'
\
x1F
'
|
b
'
\
x7F
'
/
/
non
-
printable
|
b
'
"
'
|
b
'
\
'
'
|
b
'
(
'
=
>
{
return
consume_bad_url
(
tokenizer
start_pos
)
;
}
b
'
\
\
'
=
>
{
if
tokenizer
.
has_newline_at
(
0
)
{
return
consume_bad_url
(
tokenizer
start_pos
)
}
/
/
This
pushes
one
well
-
formed
code
point
to
string_bytes
consume_escape_and_write
(
tokenizer
&
mut
string_bytes
)
}
b
'
\
0
'
=
>
{
string_bytes
.
extend
(
"
\
u
{
FFFD
}
"
.
as_bytes
(
)
)
;
}
/
/
If
this
byte
is
part
of
a
multi
-
byte
code
point
/
/
we
ll
end
up
copying
the
whole
code
point
before
this
loop
does
something
else
.
b
=
>
{
string_bytes
.
push
(
b
)
}
}
}
UnquotedUrl
(
unsafe
{
from_utf8_release_unchecked
(
string_bytes
)
}
.
into
(
)
)
}
fn
consume_url_end
<
'
a
>
(
tokenizer
:
&
mut
Tokenizer
<
'
a
>
start_pos
:
SourcePosition
string
:
CowRcStr
<
'
a
>
)
-
>
Token
<
'
a
>
{
while
!
tokenizer
.
is_eof
(
)
{
match_byte
!
{
tokenizer
.
consume_byte
(
)
b
'
'
|
b
'
\
t
'
|
b
'
\
n
'
|
b
'
\
r
'
|
b
'
\
x0C
'
=
>
{
}
b
'
)
'
=
>
{
break
}
_
=
>
{
return
consume_bad_url
(
tokenizer
start_pos
)
;
}
}
}
UnquotedUrl
(
string
)
}
fn
consume_bad_url
<
'
a
>
(
tokenizer
:
&
mut
Tokenizer
<
'
a
>
start_pos
:
SourcePosition
)
-
>
Token
<
'
a
>
{
while
!
tokenizer
.
is_eof
(
)
{
match_byte
!
{
tokenizer
.
consume_byte
(
)
b
'
)
'
=
>
{
break
}
b
'
\
\
'
=
>
{
if
matches
!
(
tokenizer
.
next_byte
(
)
Some
(
b
'
)
'
)
|
Some
(
b
'
\
\
'
)
)
{
tokenizer
.
advance
(
1
)
;
/
/
Skip
an
escaped
'
)
'
or
'
\
'
}
}
_
=
>
{
}
}
}
BadUrl
(
tokenizer
.
slice_from
(
start_pos
)
.
into
(
)
)
}
}
fn
consume_hex_digits
<
'
a
>
(
tokenizer
:
&
mut
Tokenizer
<
'
a
>
)
-
>
(
u32
u32
)
{
let
mut
value
=
0
;
let
mut
digits
=
0
;
while
digits
<
6
&
&
!
tokenizer
.
is_eof
(
)
{
match
byte_to_hex_digit
(
tokenizer
.
next_byte_unchecked
(
)
)
{
Some
(
digit
)
=
>
{
value
=
value
*
16
+
digit
;
digits
+
=
1
;
tokenizer
.
advance
(
1
)
;
}
None
=
>
break
}
}
(
value
digits
)
}
fn
consume_escape_and_write
(
tokenizer
:
&
mut
Tokenizer
bytes
:
&
mut
Vec
<
u8
>
)
{
bytes
.
extend
(
consume_escape
(
tokenizer
)
.
encode_utf8
(
&
mut
[
0
;
4
]
)
.
as_bytes
(
)
)
}
fn
consume_escape
(
tokenizer
:
&
mut
Tokenizer
)
-
>
char
{
if
tokenizer
.
is_eof
(
)
{
return
'
\
u
{
FFFD
}
'
}
match_byte
!
{
tokenizer
.
next_byte_unchecked
(
)
b
'
0
'
.
.
.
b
'
9
'
|
b
'
A
'
.
.
.
b
'
F
'
|
b
'
a
'
.
.
.
b
'
f
'
=
>
{
let
(
c
_
)
=
consume_hex_digits
(
tokenizer
)
;
if
!
tokenizer
.
is_eof
(
)
{
match
tokenizer
.
next_byte_unchecked
(
)
{
b
'
'
|
b
'
\
t
'
|
b
'
\
n
'
|
b
'
\
x0C
'
=
>
tokenizer
.
advance
(
1
)
b
'
\
r
'
=
>
{
tokenizer
.
advance
(
1
)
;
if
!
tokenizer
.
is_eof
(
)
&
&
tokenizer
.
next_byte_unchecked
(
)
=
=
b
'
\
n
'
{
tokenizer
.
advance
(
1
)
;
}
}
_
=
>
(
)
}
}
static
REPLACEMENT_CHAR
:
char
=
'
\
u
{
FFFD
}
'
;
if
c
!
=
0
{
let
c
=
char
:
:
from_u32
(
c
)
;
c
.
unwrap_or
(
REPLACEMENT_CHAR
)
}
else
{
REPLACEMENT_CHAR
}
}
b
'
\
0
'
=
>
{
tokenizer
.
advance
(
1
)
;
'
\
u
{
FFFD
}
'
}
_
=
>
{
tokenizer
.
consume_char
(
)
}
}
}
