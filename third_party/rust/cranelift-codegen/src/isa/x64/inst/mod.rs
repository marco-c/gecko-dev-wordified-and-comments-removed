#
!
[
allow
(
dead_code
)
]
#
!
[
allow
(
non_snake_case
)
]
#
!
[
allow
(
non_camel_case_types
)
]
use
crate
:
:
binemit
:
:
{
CodeOffset
StackMap
}
;
use
crate
:
:
ir
:
:
{
types
ExternalName
Opcode
SourceLoc
TrapCode
Type
}
;
use
crate
:
:
machinst
:
:
*
;
use
crate
:
:
{
settings
settings
:
:
Flags
CodegenError
CodegenResult
}
;
use
alloc
:
:
boxed
:
:
Box
;
use
alloc
:
:
vec
:
:
Vec
;
use
regalloc
:
:
{
RealRegUniverse
Reg
RegClass
RegUsageCollector
RegUsageMapper
SpillSlot
VirtualReg
Writable
}
;
use
smallvec
:
:
SmallVec
;
use
std
:
:
fmt
;
use
std
:
:
string
:
:
{
String
ToString
}
;
pub
mod
args
;
mod
emit
;
#
[
cfg
(
test
)
]
mod
emit_tests
;
pub
mod
regs
;
use
args
:
:
*
;
use
regs
:
:
{
create_reg_universe_systemv
show_ireg_sized
}
;
#
[
derive
(
Clone
)
]
pub
enum
Inst
{
Nop
{
len
:
u8
}
Alu_RMI_R
{
is_64
:
bool
op
:
AluRmiROpcode
src
:
RegMemImm
dst
:
Writable
<
Reg
>
}
UnaryRmR
{
size
:
u8
op
:
UnaryRmROpcode
src
:
RegMem
dst
:
Writable
<
Reg
>
}
Not
{
size
:
u8
src
:
Writable
<
Reg
>
}
Neg
{
size
:
u8
src
:
Writable
<
Reg
>
}
Div
{
size
:
u8
signed
:
bool
divisor
:
RegMem
loc
:
SourceLoc
}
MulHi
{
size
:
u8
signed
:
bool
rhs
:
RegMem
}
CheckedDivOrRemSeq
{
kind
:
DivOrRemKind
size
:
u8
divisor
:
Writable
<
Reg
>
tmp
:
Option
<
Writable
<
Reg
>
>
loc
:
SourceLoc
}
SignExtendData
{
size
:
u8
}
Imm
{
dst_is_64
:
bool
simm64
:
u64
dst
:
Writable
<
Reg
>
}
Mov_R_R
{
is_64
:
bool
src
:
Reg
dst
:
Writable
<
Reg
>
}
MovZX_RM_R
{
ext_mode
:
ExtMode
src
:
RegMem
dst
:
Writable
<
Reg
>
srcloc
:
Option
<
SourceLoc
>
}
Mov64_M_R
{
src
:
SyntheticAmode
dst
:
Writable
<
Reg
>
srcloc
:
Option
<
SourceLoc
>
}
LoadEffectiveAddress
{
addr
:
SyntheticAmode
dst
:
Writable
<
Reg
>
}
MovSX_RM_R
{
ext_mode
:
ExtMode
src
:
RegMem
dst
:
Writable
<
Reg
>
srcloc
:
Option
<
SourceLoc
>
}
Mov_R_M
{
size
:
u8
src
:
Reg
dst
:
SyntheticAmode
srcloc
:
Option
<
SourceLoc
>
}
Shift_R
{
size
:
u8
kind
:
ShiftKind
num_bits
:
Option
<
u8
>
dst
:
Writable
<
Reg
>
}
XmmRmiReg
{
opcode
:
SseOpcode
src
:
RegMemImm
dst
:
Writable
<
Reg
>
}
Cmp_RMI_R
{
size
:
u8
src
:
RegMemImm
dst
:
Reg
}
Setcc
{
cc
:
CC
dst
:
Writable
<
Reg
>
}
Cmove
{
size
:
u8
cc
:
CC
src
:
RegMem
dst
:
Writable
<
Reg
>
}
Push64
{
src
:
RegMemImm
}
Pop64
{
dst
:
Writable
<
Reg
>
}
XMM_RM_R
{
op
:
SseOpcode
src
:
RegMem
dst
:
Writable
<
Reg
>
}
XmmUnaryRmR
{
op
:
SseOpcode
src
:
RegMem
dst
:
Writable
<
Reg
>
srcloc
:
Option
<
SourceLoc
>
}
Xmm_Mov_R_M
{
op
:
SseOpcode
src
:
Reg
dst
:
SyntheticAmode
srcloc
:
Option
<
SourceLoc
>
}
XmmLoadConstSeq
{
val
:
Vec
<
u8
>
dst
:
Writable
<
Reg
>
ty
:
Type
}
XmmToGpr
{
op
:
SseOpcode
src
:
Reg
dst
:
Writable
<
Reg
>
dst_size
:
OperandSize
}
GprToXmm
{
op
:
SseOpcode
src
:
RegMem
dst
:
Writable
<
Reg
>
src_size
:
OperandSize
}
CvtUint64ToFloatSeq
{
to_f64
:
bool
src
:
Writable
<
Reg
>
dst
:
Writable
<
Reg
>
tmp_gpr1
:
Writable
<
Reg
>
tmp_gpr2
:
Writable
<
Reg
>
}
CvtFloatToSintSeq
{
dst_size
:
OperandSize
src_size
:
OperandSize
is_saturating
:
bool
src
:
Writable
<
Reg
>
dst
:
Writable
<
Reg
>
tmp_gpr
:
Writable
<
Reg
>
tmp_xmm
:
Writable
<
Reg
>
srcloc
:
SourceLoc
}
CvtFloatToUintSeq
{
src_size
:
OperandSize
dst_size
:
OperandSize
is_saturating
:
bool
src
:
Writable
<
Reg
>
dst
:
Writable
<
Reg
>
tmp_gpr
:
Writable
<
Reg
>
tmp_xmm
:
Writable
<
Reg
>
srcloc
:
SourceLoc
}
XmmMinMaxSeq
{
size
:
OperandSize
is_min
:
bool
lhs
:
Reg
rhs_dst
:
Writable
<
Reg
>
}
XmmCmove
{
is_64
:
bool
cc
:
CC
src
:
RegMem
dst
:
Writable
<
Reg
>
}
XMM_Cmp_RM_R
{
op
:
SseOpcode
src
:
RegMem
dst
:
Reg
}
XmmRmRImm
{
op
:
SseOpcode
src
:
RegMem
dst
:
Writable
<
Reg
>
imm
:
u8
}
CallKnown
{
dest
:
ExternalName
uses
:
Vec
<
Reg
>
defs
:
Vec
<
Writable
<
Reg
>
>
loc
:
SourceLoc
opcode
:
Opcode
}
CallUnknown
{
dest
:
RegMem
uses
:
Vec
<
Reg
>
defs
:
Vec
<
Writable
<
Reg
>
>
loc
:
SourceLoc
opcode
:
Opcode
}
Ret
EpiloguePlaceholder
JmpKnown
{
dst
:
BranchTarget
}
JmpIf
{
cc
:
CC
taken
:
BranchTarget
}
JmpCond
{
cc
:
CC
taken
:
BranchTarget
not_taken
:
BranchTarget
}
JmpTableSeq
{
idx
:
Reg
tmp1
:
Writable
<
Reg
>
tmp2
:
Writable
<
Reg
>
default_target
:
BranchTarget
targets
:
Vec
<
BranchTarget
>
targets_for_term
:
Vec
<
MachLabel
>
}
JmpUnknown
{
target
:
RegMem
}
TrapIf
{
cc
:
CC
trap_code
:
TrapCode
srcloc
:
SourceLoc
}
Hlt
Ud2
{
trap_info
:
(
SourceLoc
TrapCode
)
}
LoadExtName
{
dst
:
Writable
<
Reg
>
name
:
Box
<
ExternalName
>
srcloc
:
SourceLoc
offset
:
i64
}
LockCmpxchg
{
ty
:
Type
src
:
Reg
dst
:
SyntheticAmode
srcloc
:
Option
<
SourceLoc
>
}
AtomicRmwSeq
{
ty
:
Type
op
:
inst_common
:
:
AtomicRmwOp
srcloc
:
Option
<
SourceLoc
>
}
Fence
{
kind
:
FenceKind
}
VirtualSPOffsetAdj
{
offset
:
i64
}
}
pub
(
crate
)
fn
low32_will_sign_extend_to_64
(
x
:
u64
)
-
>
bool
{
let
xs
=
x
as
i64
;
xs
=
=
(
(
xs
<
<
32
)
>
>
32
)
}
impl
Inst
{
pub
(
crate
)
fn
nop
(
len
:
u8
)
-
>
Self
{
debug_assert
!
(
len
<
=
16
)
;
Self
:
:
Nop
{
len
}
}
pub
(
crate
)
fn
alu_rmi_r
(
is_64
:
bool
op
:
AluRmiROpcode
src
:
RegMemImm
dst
:
Writable
<
Reg
>
)
-
>
Self
{
src
.
assert_regclass_is
(
RegClass
:
:
I64
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Self
:
:
Alu_RMI_R
{
is_64
op
src
dst
}
}
pub
(
crate
)
fn
unary_rm_r
(
size
:
u8
op
:
UnaryRmROpcode
src
:
RegMem
dst
:
Writable
<
Reg
>
)
-
>
Self
{
src
.
assert_regclass_is
(
RegClass
:
:
I64
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
debug_assert
!
(
size
=
=
8
|
|
size
=
=
4
|
|
size
=
=
2
)
;
Self
:
:
UnaryRmR
{
size
op
src
dst
}
}
pub
(
crate
)
fn
not
(
size
:
u8
src
:
Writable
<
Reg
>
)
-
>
Inst
{
debug_assert_eq
!
(
src
.
to_reg
(
)
.
get_class
(
)
RegClass
:
:
I64
)
;
debug_assert
!
(
size
=
=
8
|
|
size
=
=
4
|
|
size
=
=
2
|
|
size
=
=
1
)
;
Inst
:
:
Not
{
size
src
}
}
pub
(
crate
)
fn
neg
(
size
:
u8
src
:
Writable
<
Reg
>
)
-
>
Inst
{
debug_assert_eq
!
(
src
.
to_reg
(
)
.
get_class
(
)
RegClass
:
:
I64
)
;
debug_assert
!
(
size
=
=
8
|
|
size
=
=
4
|
|
size
=
=
2
|
|
size
=
=
1
)
;
Inst
:
:
Neg
{
size
src
}
}
pub
(
crate
)
fn
div
(
size
:
u8
signed
:
bool
divisor
:
RegMem
loc
:
SourceLoc
)
-
>
Inst
{
divisor
.
assert_regclass_is
(
RegClass
:
:
I64
)
;
debug_assert
!
(
size
=
=
8
|
|
size
=
=
4
|
|
size
=
=
2
|
|
size
=
=
1
)
;
Inst
:
:
Div
{
size
signed
divisor
loc
}
}
pub
(
crate
)
fn
mul_hi
(
size
:
u8
signed
:
bool
rhs
:
RegMem
)
-
>
Inst
{
rhs
.
assert_regclass_is
(
RegClass
:
:
I64
)
;
debug_assert
!
(
size
=
=
8
|
|
size
=
=
4
|
|
size
=
=
2
|
|
size
=
=
1
)
;
Inst
:
:
MulHi
{
size
signed
rhs
}
}
pub
(
crate
)
fn
checked_div_or_rem_seq
(
kind
:
DivOrRemKind
size
:
u8
divisor
:
Writable
<
Reg
>
tmp
:
Option
<
Writable
<
Reg
>
>
loc
:
SourceLoc
)
-
>
Inst
{
debug_assert
!
(
size
=
=
8
|
|
size
=
=
4
|
|
size
=
=
2
|
|
size
=
=
1
)
;
debug_assert
!
(
divisor
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
debug_assert
!
(
tmp
.
map
(
|
tmp
|
tmp
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
.
unwrap_or
(
true
)
)
;
Inst
:
:
CheckedDivOrRemSeq
{
kind
size
divisor
tmp
loc
}
}
pub
(
crate
)
fn
sign_extend_data
(
size
:
u8
)
-
>
Inst
{
debug_assert
!
(
size
=
=
8
|
|
size
=
=
4
|
|
size
=
=
2
|
|
size
=
=
1
)
;
Inst
:
:
SignExtendData
{
size
}
}
pub
(
crate
)
fn
imm
(
size
:
OperandSize
simm64
:
u64
dst
:
Writable
<
Reg
>
)
-
>
Inst
{
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
let
dst_is_64
=
size
=
=
OperandSize
:
:
Size64
&
&
simm64
>
u32
:
:
max_value
(
)
as
u64
;
Inst
:
:
Imm
{
dst_is_64
simm64
dst
}
}
pub
(
crate
)
fn
mov_r_r
(
is_64
:
bool
src
:
Reg
dst
:
Writable
<
Reg
>
)
-
>
Inst
{
debug_assert
!
(
src
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Inst
:
:
Mov_R_R
{
is_64
src
dst
}
}
pub
(
crate
)
fn
xmm_mov
(
op
:
SseOpcode
src
:
RegMem
dst
:
Writable
<
Reg
>
srcloc
:
Option
<
SourceLoc
>
)
-
>
Inst
{
src
.
assert_regclass_is
(
RegClass
:
:
V128
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
Inst
:
:
XmmUnaryRmR
{
op
src
dst
srcloc
}
}
pub
(
crate
)
fn
xmm_load_const_seq
(
val
:
Vec
<
u8
>
dst
:
Writable
<
Reg
>
ty
:
Type
)
-
>
Inst
{
debug_assert
!
(
val
.
len
(
)
=
=
16
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
debug_assert
!
(
ty
.
is_vector
(
)
&
&
ty
.
bits
(
)
=
=
128
)
;
Inst
:
:
XmmLoadConstSeq
{
val
dst
ty
}
}
pub
(
crate
)
fn
xmm_unary_rm_r
(
op
:
SseOpcode
src
:
RegMem
dst
:
Writable
<
Reg
>
)
-
>
Inst
{
src
.
assert_regclass_is
(
RegClass
:
:
V128
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
Inst
:
:
XmmUnaryRmR
{
op
src
dst
srcloc
:
None
}
}
pub
(
crate
)
fn
xmm_rm_r
(
op
:
SseOpcode
src
:
RegMem
dst
:
Writable
<
Reg
>
)
-
>
Self
{
src
.
assert_regclass_is
(
RegClass
:
:
V128
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
Inst
:
:
XMM_RM_R
{
op
src
dst
}
}
pub
(
crate
)
fn
xmm_mov_r_m
(
op
:
SseOpcode
src
:
Reg
dst
:
impl
Into
<
SyntheticAmode
>
srcloc
:
Option
<
SourceLoc
>
)
-
>
Inst
{
debug_assert
!
(
src
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
Inst
:
:
Xmm_Mov_R_M
{
op
src
dst
:
dst
.
into
(
)
srcloc
}
}
pub
(
crate
)
fn
xmm_to_gpr
(
op
:
SseOpcode
src
:
Reg
dst
:
Writable
<
Reg
>
dst_size
:
OperandSize
)
-
>
Inst
{
debug_assert
!
(
src
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Inst
:
:
XmmToGpr
{
op
src
dst
dst_size
}
}
pub
(
crate
)
fn
gpr_to_xmm
(
op
:
SseOpcode
src
:
RegMem
src_size
:
OperandSize
dst
:
Writable
<
Reg
>
)
-
>
Inst
{
src
.
assert_regclass_is
(
RegClass
:
:
I64
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
Inst
:
:
GprToXmm
{
op
src
dst
src_size
}
}
pub
(
crate
)
fn
xmm_cmp_rm_r
(
op
:
SseOpcode
src
:
RegMem
dst
:
Reg
)
-
>
Inst
{
src
.
assert_regclass_is
(
RegClass
:
:
V128
)
;
debug_assert
!
(
dst
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
Inst
:
:
XMM_Cmp_RM_R
{
op
src
dst
}
}
pub
(
crate
)
fn
cvt_u64_to_float_seq
(
to_f64
:
bool
src
:
Writable
<
Reg
>
tmp_gpr1
:
Writable
<
Reg
>
tmp_gpr2
:
Writable
<
Reg
>
dst
:
Writable
<
Reg
>
)
-
>
Inst
{
debug_assert
!
(
src
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
debug_assert
!
(
tmp_gpr1
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
debug_assert
!
(
tmp_gpr2
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
Inst
:
:
CvtUint64ToFloatSeq
{
src
dst
tmp_gpr1
tmp_gpr2
to_f64
}
}
pub
(
crate
)
fn
cvt_float_to_sint_seq
(
src_size
:
OperandSize
dst_size
:
OperandSize
is_saturating
:
bool
src
:
Writable
<
Reg
>
dst
:
Writable
<
Reg
>
tmp_gpr
:
Writable
<
Reg
>
tmp_xmm
:
Writable
<
Reg
>
srcloc
:
SourceLoc
)
-
>
Inst
{
debug_assert
!
(
src
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
debug_assert
!
(
tmp_xmm
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
debug_assert
!
(
tmp_gpr
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Inst
:
:
CvtFloatToSintSeq
{
src_size
dst_size
is_saturating
src
dst
tmp_gpr
tmp_xmm
srcloc
}
}
pub
(
crate
)
fn
cvt_float_to_uint_seq
(
src_size
:
OperandSize
dst_size
:
OperandSize
is_saturating
:
bool
src
:
Writable
<
Reg
>
dst
:
Writable
<
Reg
>
tmp_gpr
:
Writable
<
Reg
>
tmp_xmm
:
Writable
<
Reg
>
srcloc
:
SourceLoc
)
-
>
Inst
{
debug_assert
!
(
src
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
debug_assert
!
(
tmp_xmm
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
debug_assert
!
(
tmp_gpr
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Inst
:
:
CvtFloatToUintSeq
{
src_size
dst_size
is_saturating
src
dst
tmp_gpr
tmp_xmm
srcloc
}
}
pub
(
crate
)
fn
xmm_min_max_seq
(
size
:
OperandSize
is_min
:
bool
lhs
:
Reg
rhs_dst
:
Writable
<
Reg
>
)
-
>
Inst
{
debug_assert_eq
!
(
lhs
.
get_class
(
)
RegClass
:
:
V128
)
;
debug_assert_eq
!
(
rhs_dst
.
to_reg
(
)
.
get_class
(
)
RegClass
:
:
V128
)
;
Inst
:
:
XmmMinMaxSeq
{
size
is_min
lhs
rhs_dst
}
}
pub
(
crate
)
fn
xmm_rm_r_imm
(
op
:
SseOpcode
src
:
RegMem
dst
:
Writable
<
Reg
>
imm
:
u8
)
-
>
Inst
{
src
.
assert_regclass_is
(
RegClass
:
:
V128
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
debug_assert
!
(
imm
<
8
)
;
Inst
:
:
XmmRmRImm
{
op
src
dst
imm
}
}
pub
(
crate
)
fn
movzx_rm_r
(
ext_mode
:
ExtMode
src
:
RegMem
dst
:
Writable
<
Reg
>
srcloc
:
Option
<
SourceLoc
>
)
-
>
Inst
{
src
.
assert_regclass_is
(
RegClass
:
:
I64
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Inst
:
:
MovZX_RM_R
{
ext_mode
src
dst
srcloc
}
}
pub
(
crate
)
fn
xmm_rmi_reg
(
opcode
:
SseOpcode
src
:
RegMemImm
dst
:
Writable
<
Reg
>
)
-
>
Inst
{
src
.
assert_regclass_is
(
RegClass
:
:
V128
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
Inst
:
:
XmmRmiReg
{
opcode
src
dst
}
}
pub
(
crate
)
fn
movsx_rm_r
(
ext_mode
:
ExtMode
src
:
RegMem
dst
:
Writable
<
Reg
>
srcloc
:
Option
<
SourceLoc
>
)
-
>
Inst
{
src
.
assert_regclass_is
(
RegClass
:
:
I64
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Inst
:
:
MovSX_RM_R
{
ext_mode
src
dst
srcloc
}
}
pub
(
crate
)
fn
mov64_m_r
(
src
:
impl
Into
<
SyntheticAmode
>
dst
:
Writable
<
Reg
>
srcloc
:
Option
<
SourceLoc
>
)
-
>
Inst
{
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Inst
:
:
Mov64_M_R
{
src
:
src
.
into
(
)
dst
srcloc
}
}
pub
(
crate
)
fn
mov64_rm_r
(
src
:
RegMem
dst
:
Writable
<
Reg
>
srcloc
:
Option
<
SourceLoc
>
)
-
>
Inst
{
src
.
assert_regclass_is
(
RegClass
:
:
I64
)
;
match
src
{
RegMem
:
:
Reg
{
reg
}
=
>
Self
:
:
mov_r_r
(
true
reg
dst
)
RegMem
:
:
Mem
{
addr
}
=
>
Self
:
:
mov64_m_r
(
addr
dst
srcloc
)
}
}
pub
(
crate
)
fn
mov_r_m
(
size
:
u8
src
:
Reg
dst
:
impl
Into
<
SyntheticAmode
>
srcloc
:
Option
<
SourceLoc
>
)
-
>
Inst
{
debug_assert
!
(
size
=
=
8
|
|
size
=
=
4
|
|
size
=
=
2
|
|
size
=
=
1
)
;
debug_assert
!
(
src
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Inst
:
:
Mov_R_M
{
size
src
dst
:
dst
.
into
(
)
srcloc
}
}
pub
(
crate
)
fn
lea
(
addr
:
impl
Into
<
SyntheticAmode
>
dst
:
Writable
<
Reg
>
)
-
>
Inst
{
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Inst
:
:
LoadEffectiveAddress
{
addr
:
addr
.
into
(
)
dst
}
}
pub
(
crate
)
fn
shift_r
(
size
:
u8
kind
:
ShiftKind
num_bits
:
Option
<
u8
>
dst
:
Writable
<
Reg
>
)
-
>
Inst
{
debug_assert
!
(
size
=
=
8
|
|
size
=
=
4
|
|
size
=
=
2
|
|
size
=
=
1
)
;
debug_assert
!
(
if
let
Some
(
num_bits
)
=
num_bits
{
num_bits
<
size
*
8
}
else
{
true
}
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Inst
:
:
Shift_R
{
size
kind
num_bits
dst
}
}
pub
(
crate
)
fn
cmp_rmi_r
(
size
:
u8
src
:
RegMemImm
dst
:
Reg
)
-
>
Inst
{
src
.
assert_regclass_is
(
RegClass
:
:
I64
)
;
debug_assert
!
(
size
=
=
8
|
|
size
=
=
4
|
|
size
=
=
2
|
|
size
=
=
1
)
;
debug_assert
!
(
dst
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Inst
:
:
Cmp_RMI_R
{
size
src
dst
}
}
pub
(
crate
)
fn
trap
(
srcloc
:
SourceLoc
trap_code
:
TrapCode
)
-
>
Inst
{
Inst
:
:
Ud2
{
trap_info
:
(
srcloc
trap_code
)
}
}
pub
(
crate
)
fn
setcc
(
cc
:
CC
dst
:
Writable
<
Reg
>
)
-
>
Inst
{
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Inst
:
:
Setcc
{
cc
dst
}
}
pub
(
crate
)
fn
cmove
(
size
:
u8
cc
:
CC
src
:
RegMem
dst
:
Writable
<
Reg
>
)
-
>
Inst
{
debug_assert
!
(
size
=
=
8
|
|
size
=
=
4
|
|
size
=
=
2
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Inst
:
:
Cmove
{
size
cc
src
dst
}
}
pub
(
crate
)
fn
xmm_cmove
(
is_64
:
bool
cc
:
CC
src
:
RegMem
dst
:
Writable
<
Reg
>
)
-
>
Inst
{
src
.
assert_regclass_is
(
RegClass
:
:
V128
)
;
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
V128
)
;
Inst
:
:
XmmCmove
{
is_64
cc
src
dst
}
}
pub
(
crate
)
fn
push64
(
src
:
RegMemImm
)
-
>
Inst
{
src
.
assert_regclass_is
(
RegClass
:
:
I64
)
;
Inst
:
:
Push64
{
src
}
}
pub
(
crate
)
fn
pop64
(
dst
:
Writable
<
Reg
>
)
-
>
Inst
{
debug_assert
!
(
dst
.
to_reg
(
)
.
get_class
(
)
=
=
RegClass
:
:
I64
)
;
Inst
:
:
Pop64
{
dst
}
}
pub
(
crate
)
fn
call_known
(
dest
:
ExternalName
uses
:
Vec
<
Reg
>
defs
:
Vec
<
Writable
<
Reg
>
>
loc
:
SourceLoc
opcode
:
Opcode
)
-
>
Inst
{
Inst
:
:
CallKnown
{
dest
uses
defs
loc
opcode
}
}
pub
(
crate
)
fn
call_unknown
(
dest
:
RegMem
uses
:
Vec
<
Reg
>
defs
:
Vec
<
Writable
<
Reg
>
>
loc
:
SourceLoc
opcode
:
Opcode
)
-
>
Inst
{
dest
.
assert_regclass_is
(
RegClass
:
:
I64
)
;
Inst
:
:
CallUnknown
{
dest
uses
defs
loc
opcode
}
}
pub
(
crate
)
fn
ret
(
)
-
>
Inst
{
Inst
:
:
Ret
}
pub
(
crate
)
fn
epilogue_placeholder
(
)
-
>
Inst
{
Inst
:
:
EpiloguePlaceholder
}
pub
(
crate
)
fn
jmp_known
(
dst
:
BranchTarget
)
-
>
Inst
{
Inst
:
:
JmpKnown
{
dst
}
}
pub
(
crate
)
fn
jmp_if
(
cc
:
CC
taken
:
BranchTarget
)
-
>
Inst
{
Inst
:
:
JmpIf
{
cc
taken
}
}
pub
(
crate
)
fn
jmp_cond
(
cc
:
CC
taken
:
BranchTarget
not_taken
:
BranchTarget
)
-
>
Inst
{
Inst
:
:
JmpCond
{
cc
taken
not_taken
}
}
pub
(
crate
)
fn
jmp_unknown
(
target
:
RegMem
)
-
>
Inst
{
target
.
assert_regclass_is
(
RegClass
:
:
I64
)
;
Inst
:
:
JmpUnknown
{
target
}
}
pub
(
crate
)
fn
trap_if
(
cc
:
CC
trap_code
:
TrapCode
srcloc
:
SourceLoc
)
-
>
Inst
{
Inst
:
:
TrapIf
{
cc
trap_code
srcloc
}
}
pub
(
crate
)
fn
load
(
ty
:
Type
from_addr
:
impl
Into
<
SyntheticAmode
>
to_reg
:
Writable
<
Reg
>
ext_kind
:
ExtKind
srcloc
:
Option
<
SourceLoc
>
)
-
>
Inst
{
let
rc
=
to_reg
.
to_reg
(
)
.
get_class
(
)
;
match
rc
{
RegClass
:
:
I64
=
>
{
let
ext_mode
=
match
ty
.
bytes
(
)
{
1
=
>
Some
(
ExtMode
:
:
BQ
)
2
=
>
Some
(
ExtMode
:
:
WQ
)
4
=
>
Some
(
ExtMode
:
:
LQ
)
8
=
>
None
_
=
>
unreachable
!
(
"
the
type
should
never
use
a
scalar
load
:
{
}
"
ty
)
}
;
if
let
Some
(
ext_mode
)
=
ext_mode
{
match
ext_kind
{
ExtKind
:
:
SignExtend
=
>
{
Inst
:
:
movsx_rm_r
(
ext_mode
RegMem
:
:
mem
(
from_addr
)
to_reg
srcloc
)
}
ExtKind
:
:
ZeroExtend
=
>
{
Inst
:
:
movzx_rm_r
(
ext_mode
RegMem
:
:
mem
(
from_addr
)
to_reg
srcloc
)
}
ExtKind
:
:
None
=
>
panic
!
(
"
expected
an
extension
kind
for
extension
mode
:
{
:
?
}
"
ext_mode
)
}
}
else
{
Inst
:
:
mov64_m_r
(
from_addr
to_reg
srcloc
)
}
}
RegClass
:
:
V128
=
>
{
let
opcode
=
match
ty
{
types
:
:
F32
=
>
SseOpcode
:
:
Movss
types
:
:
F64
=
>
SseOpcode
:
:
Movsd
types
:
:
F32X4
=
>
SseOpcode
:
:
Movups
types
:
:
F64X2
=
>
SseOpcode
:
:
Movupd
_
if
ty
.
is_vector
(
)
&
&
ty
.
bits
(
)
=
=
128
=
>
SseOpcode
:
:
Movdqu
_
=
>
unimplemented
!
(
"
unable
to
load
type
:
{
}
"
ty
)
}
;
Inst
:
:
xmm_unary_rm_r
(
opcode
RegMem
:
:
mem
(
from_addr
)
to_reg
)
}
_
=
>
panic
!
(
"
unable
to
generate
load
for
register
class
:
{
:
?
}
"
rc
)
}
}
pub
(
crate
)
fn
store
(
ty
:
Type
from_reg
:
Reg
to_addr
:
impl
Into
<
SyntheticAmode
>
srcloc
:
Option
<
SourceLoc
>
)
-
>
Inst
{
let
rc
=
from_reg
.
get_class
(
)
;
match
rc
{
RegClass
:
:
I64
=
>
Inst
:
:
mov_r_m
(
ty
.
bytes
(
)
as
u8
from_reg
to_addr
srcloc
)
RegClass
:
:
V128
=
>
{
let
opcode
=
match
ty
{
types
:
:
F32
=
>
SseOpcode
:
:
Movss
types
:
:
F64
=
>
SseOpcode
:
:
Movsd
types
:
:
F32X4
=
>
SseOpcode
:
:
Movups
types
:
:
F64X2
=
>
SseOpcode
:
:
Movupd
_
if
ty
.
is_vector
(
)
&
&
ty
.
bits
(
)
=
=
128
=
>
SseOpcode
:
:
Movdqu
_
=
>
unimplemented
!
(
"
unable
to
store
type
:
{
}
"
ty
)
}
;
Inst
:
:
xmm_mov_r_m
(
opcode
from_reg
to_addr
srcloc
)
}
_
=
>
panic
!
(
"
unable
to
generate
store
for
register
class
:
{
:
?
}
"
rc
)
}
}
}
impl
Inst
{
fn
produces_const
(
&
self
)
-
>
bool
{
match
self
{
Self
:
:
Alu_RMI_R
{
op
src
dst
.
.
}
=
>
{
src
.
to_reg
(
)
=
=
Some
(
dst
.
to_reg
(
)
)
&
&
(
*
op
=
=
AluRmiROpcode
:
:
Xor
|
|
*
op
=
=
AluRmiROpcode
:
:
Sub
)
}
Self
:
:
XMM_RM_R
{
op
src
dst
.
.
}
=
>
{
src
.
to_reg
(
)
=
=
Some
(
dst
.
to_reg
(
)
)
&
&
(
*
op
=
=
SseOpcode
:
:
Xorps
|
|
*
op
=
=
SseOpcode
:
:
Xorpd
|
|
*
op
=
=
SseOpcode
:
:
Pxor
)
}
Self
:
:
XmmRmRImm
{
op
src
dst
imm
}
=
>
{
src
.
to_reg
(
)
=
=
Some
(
dst
.
to_reg
(
)
)
&
&
(
*
op
=
=
SseOpcode
:
:
Cmppd
|
|
*
op
=
=
SseOpcode
:
:
Cmpps
)
&
&
*
imm
=
=
FcmpImm
:
:
Equal
.
encode
(
)
}
_
=
>
false
}
}
}
impl
ShowWithRRU
for
Inst
{
fn
show_rru
(
&
self
mb_rru
:
Option
<
&
RealRegUniverse
>
)
-
>
String
{
fn
ljustify
(
s
:
String
)
-
>
String
{
let
w
=
7
;
if
s
.
len
(
)
>
=
w
{
s
}
else
{
let
need
=
usize
:
:
min
(
w
w
-
s
.
len
(
)
)
;
s
+
&
format
!
(
"
{
nil
:
<
width
}
"
nil
=
"
"
width
=
need
)
}
}
fn
ljustify2
(
s1
:
String
s2
:
String
)
-
>
String
{
ljustify
(
s1
+
&
s2
)
}
fn
suffixLQ
(
is_64
:
bool
)
-
>
String
{
(
if
is_64
{
"
q
"
}
else
{
"
l
"
}
)
.
to_string
(
)
}
fn
sizeLQ
(
is_64
:
bool
)
-
>
u8
{
if
is_64
{
8
}
else
{
4
}
}
fn
suffixBWLQ
(
size
:
u8
)
-
>
String
{
match
size
{
1
=
>
"
b
"
.
to_string
(
)
2
=
>
"
w
"
.
to_string
(
)
4
=
>
"
l
"
.
to_string
(
)
8
=
>
"
q
"
.
to_string
(
)
_
=
>
panic
!
(
"
Inst
(
x64
)
.
show
.
suffixBWLQ
:
size
=
{
}
"
size
)
}
}
match
self
{
Inst
:
:
Nop
{
len
}
=
>
format
!
(
"
{
}
len
=
{
}
"
ljustify
(
"
nop
"
.
to_string
(
)
)
len
)
Inst
:
:
Alu_RMI_R
{
is_64
op
src
dst
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify2
(
op
.
to_string
(
)
suffixLQ
(
*
is_64
)
)
src
.
show_rru_sized
(
mb_rru
sizeLQ
(
*
is_64
)
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
sizeLQ
(
*
is_64
)
)
)
Inst
:
:
UnaryRmR
{
src
dst
op
size
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify2
(
op
.
to_string
(
)
suffixBWLQ
(
*
size
)
)
src
.
show_rru_sized
(
mb_rru
*
size
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
*
size
)
)
Inst
:
:
Not
{
size
src
}
=
>
format
!
(
"
{
}
{
}
"
ljustify2
(
"
not
"
.
to_string
(
)
suffixBWLQ
(
*
size
)
)
show_ireg_sized
(
src
.
to_reg
(
)
mb_rru
*
size
)
)
Inst
:
:
Neg
{
size
src
}
=
>
format
!
(
"
{
}
{
}
"
ljustify2
(
"
neg
"
.
to_string
(
)
suffixBWLQ
(
*
size
)
)
show_ireg_sized
(
src
.
to_reg
(
)
mb_rru
*
size
)
)
Inst
:
:
Div
{
size
signed
divisor
.
.
}
=
>
format
!
(
"
{
}
{
}
"
ljustify
(
if
*
signed
{
"
idiv
"
.
to_string
(
)
}
else
{
"
div
"
.
into
(
)
}
)
divisor
.
show_rru_sized
(
mb_rru
*
size
)
)
Inst
:
:
MulHi
{
size
signed
rhs
.
.
}
=
>
format
!
(
"
{
}
{
}
"
ljustify
(
if
*
signed
{
"
imul
"
.
to_string
(
)
}
else
{
"
mul
"
.
to_string
(
)
}
)
rhs
.
show_rru_sized
(
mb_rru
*
size
)
)
Inst
:
:
CheckedDivOrRemSeq
{
kind
size
divisor
.
.
}
=
>
format
!
(
"
{
}
rax
:
rdx
{
}
"
match
kind
{
DivOrRemKind
:
:
SignedDiv
=
>
"
sdiv
"
DivOrRemKind
:
:
UnsignedDiv
=
>
"
udiv
"
DivOrRemKind
:
:
SignedRem
=
>
"
srem
"
DivOrRemKind
:
:
UnsignedRem
=
>
"
urem
"
}
show_ireg_sized
(
divisor
.
to_reg
(
)
mb_rru
*
size
)
)
Inst
:
:
SignExtendData
{
size
}
=
>
match
size
{
1
=
>
"
cbw
"
2
=
>
"
cwd
"
4
=
>
"
cdq
"
8
=
>
"
cqo
"
_
=
>
unreachable
!
(
)
}
.
into
(
)
Inst
:
:
XmmUnaryRmR
{
op
src
dst
.
.
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
op
.
to_string
(
)
)
src
.
show_rru_sized
(
mb_rru
op
.
src_size
(
)
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
8
)
)
Inst
:
:
Xmm_Mov_R_M
{
op
src
dst
.
.
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
op
.
to_string
(
)
)
show_ireg_sized
(
*
src
mb_rru
8
)
dst
.
show_rru
(
mb_rru
)
)
Inst
:
:
XMM_RM_R
{
op
src
dst
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
op
.
to_string
(
)
)
src
.
show_rru_sized
(
mb_rru
8
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
8
)
)
Inst
:
:
XmmMinMaxSeq
{
lhs
rhs_dst
is_min
size
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify2
(
if
*
is_min
{
"
xmm
min
seq
"
.
to_string
(
)
}
else
{
"
xmm
max
seq
"
.
to_string
(
)
}
match
size
{
OperandSize
:
:
Size32
=
>
"
f32
"
OperandSize
:
:
Size64
=
>
"
f64
"
}
.
into
(
)
)
show_ireg_sized
(
*
lhs
mb_rru
8
)
show_ireg_sized
(
rhs_dst
.
to_reg
(
)
mb_rru
8
)
)
Inst
:
:
XmmRmRImm
{
op
src
dst
imm
}
=
>
format
!
(
"
{
}
{
}
{
}
{
}
"
ljustify
(
op
.
to_string
(
)
)
imm
src
.
show_rru
(
mb_rru
)
dst
.
show_rru
(
mb_rru
)
)
Inst
:
:
XmmLoadConstSeq
{
val
dst
.
.
}
=
>
{
format
!
(
"
load_const
{
:
?
}
{
}
"
val
dst
.
show_rru
(
mb_rru
)
)
}
Inst
:
:
XmmToGpr
{
op
src
dst
dst_size
}
=
>
{
let
dst_size
=
match
dst_size
{
OperandSize
:
:
Size32
=
>
4
OperandSize
:
:
Size64
=
>
8
}
;
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
op
.
to_string
(
)
)
src
.
show_rru
(
mb_rru
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
dst_size
)
)
}
Inst
:
:
GprToXmm
{
op
src
src_size
dst
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
op
.
to_string
(
)
)
src
.
show_rru_sized
(
mb_rru
src_size
.
to_bytes
(
)
)
dst
.
show_rru
(
mb_rru
)
)
Inst
:
:
XMM_Cmp_RM_R
{
op
src
dst
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
op
.
to_string
(
)
)
src
.
show_rru_sized
(
mb_rru
8
)
show_ireg_sized
(
*
dst
mb_rru
8
)
)
Inst
:
:
CvtUint64ToFloatSeq
{
src
dst
to_f64
.
.
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
format
!
(
"
u64_to_
{
}
_seq
"
if
*
to_f64
{
"
f64
"
}
else
{
"
f32
"
}
)
)
show_ireg_sized
(
src
.
to_reg
(
)
mb_rru
8
)
dst
.
show_rru
(
mb_rru
)
)
Inst
:
:
CvtFloatToSintSeq
{
src
dst
src_size
dst_size
.
.
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
format
!
(
"
cvt_float
{
}
_to_sint
{
}
_seq
"
if
*
src_size
=
=
OperandSize
:
:
Size64
{
"
64
"
}
else
{
"
32
"
}
if
*
dst_size
=
=
OperandSize
:
:
Size64
{
"
64
"
}
else
{
"
32
"
}
)
)
show_ireg_sized
(
src
.
to_reg
(
)
mb_rru
8
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
dst_size
.
to_bytes
(
)
)
)
Inst
:
:
CvtFloatToUintSeq
{
src
dst
src_size
dst_size
.
.
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
format
!
(
"
cvt_float
{
}
_to_uint
{
}
_seq
"
if
*
src_size
=
=
OperandSize
:
:
Size64
{
"
64
"
}
else
{
"
32
"
}
if
*
dst_size
=
=
OperandSize
:
:
Size64
{
"
64
"
}
else
{
"
32
"
}
)
)
show_ireg_sized
(
src
.
to_reg
(
)
mb_rru
8
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
dst_size
.
to_bytes
(
)
)
)
Inst
:
:
Imm
{
dst_is_64
simm64
dst
}
=
>
{
if
*
dst_is_64
{
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
"
movabsq
"
.
to_string
(
)
)
*
simm64
as
i64
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
8
)
)
}
else
{
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
"
movl
"
.
to_string
(
)
)
(
*
simm64
as
u32
)
as
i32
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
4
)
)
}
}
Inst
:
:
Mov_R_R
{
is_64
src
dst
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify2
(
"
mov
"
.
to_string
(
)
suffixLQ
(
*
is_64
)
)
show_ireg_sized
(
*
src
mb_rru
sizeLQ
(
*
is_64
)
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
sizeLQ
(
*
is_64
)
)
)
Inst
:
:
MovZX_RM_R
{
ext_mode
src
dst
.
.
}
=
>
{
if
*
ext_mode
=
=
ExtMode
:
:
LQ
{
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
"
movl
"
.
to_string
(
)
)
src
.
show_rru_sized
(
mb_rru
ext_mode
.
src_size
(
)
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
4
)
)
}
else
{
format
!
(
"
{
}
{
}
{
}
"
ljustify2
(
"
movz
"
.
to_string
(
)
ext_mode
.
to_string
(
)
)
src
.
show_rru_sized
(
mb_rru
ext_mode
.
src_size
(
)
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
ext_mode
.
dst_size
(
)
)
)
}
}
Inst
:
:
Mov64_M_R
{
src
dst
.
.
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
"
movq
"
.
to_string
(
)
)
src
.
show_rru
(
mb_rru
)
dst
.
show_rru
(
mb_rru
)
)
Inst
:
:
LoadEffectiveAddress
{
addr
dst
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
"
lea
"
.
to_string
(
)
)
addr
.
show_rru
(
mb_rru
)
dst
.
show_rru
(
mb_rru
)
)
Inst
:
:
MovSX_RM_R
{
ext_mode
src
dst
.
.
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify2
(
"
movs
"
.
to_string
(
)
ext_mode
.
to_string
(
)
)
src
.
show_rru_sized
(
mb_rru
ext_mode
.
src_size
(
)
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
ext_mode
.
dst_size
(
)
)
)
Inst
:
:
Mov_R_M
{
size
src
dst
.
.
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify2
(
"
mov
"
.
to_string
(
)
suffixBWLQ
(
*
size
)
)
show_ireg_sized
(
*
src
mb_rru
*
size
)
dst
.
show_rru
(
mb_rru
)
)
Inst
:
:
Shift_R
{
size
kind
num_bits
dst
}
=
>
match
num_bits
{
None
=
>
format
!
(
"
{
}
%
cl
{
}
"
ljustify2
(
kind
.
to_string
(
)
suffixBWLQ
(
*
size
)
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
*
size
)
)
Some
(
num_bits
)
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify2
(
kind
.
to_string
(
)
suffixBWLQ
(
*
size
)
)
num_bits
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
*
size
)
)
}
Inst
:
:
XmmRmiReg
{
opcode
src
dst
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
opcode
.
to_string
(
)
)
src
.
show_rru
(
mb_rru
)
dst
.
to_reg
(
)
.
show_rru
(
mb_rru
)
)
Inst
:
:
Cmp_RMI_R
{
size
src
dst
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify2
(
"
cmp
"
.
to_string
(
)
suffixBWLQ
(
*
size
)
)
src
.
show_rru_sized
(
mb_rru
*
size
)
show_ireg_sized
(
*
dst
mb_rru
*
size
)
)
Inst
:
:
Setcc
{
cc
dst
}
=
>
format
!
(
"
{
}
{
}
"
ljustify2
(
"
set
"
.
to_string
(
)
cc
.
to_string
(
)
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
1
)
)
Inst
:
:
Cmove
{
size
cc
src
dst
}
=
>
format
!
(
"
{
}
{
}
{
}
"
ljustify
(
format
!
(
"
cmov
{
}
{
}
"
cc
.
to_string
(
)
suffixBWLQ
(
*
size
)
)
)
src
.
show_rru_sized
(
mb_rru
*
size
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
*
size
)
)
Inst
:
:
XmmCmove
{
is_64
cc
src
dst
}
=
>
{
let
size
=
if
*
is_64
{
8
}
else
{
4
}
;
format
!
(
"
j
{
}
next
;
mov
{
}
{
}
{
}
;
next
:
"
cc
.
invert
(
)
.
to_string
(
)
if
*
is_64
{
"
sd
"
}
else
{
"
ss
"
}
src
.
show_rru_sized
(
mb_rru
size
)
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
size
)
)
}
Inst
:
:
Push64
{
src
}
=
>
{
format
!
(
"
{
}
{
}
"
ljustify
(
"
pushq
"
.
to_string
(
)
)
src
.
show_rru
(
mb_rru
)
)
}
Inst
:
:
Pop64
{
dst
}
=
>
{
format
!
(
"
{
}
{
}
"
ljustify
(
"
popq
"
.
to_string
(
)
)
dst
.
show_rru
(
mb_rru
)
)
}
Inst
:
:
CallKnown
{
dest
.
.
}
=
>
format
!
(
"
{
}
{
:
?
}
"
ljustify
(
"
call
"
.
to_string
(
)
)
dest
)
Inst
:
:
CallUnknown
{
dest
.
.
}
=
>
format
!
(
"
{
}
*
{
}
"
ljustify
(
"
call
"
.
to_string
(
)
)
dest
.
show_rru
(
mb_rru
)
)
Inst
:
:
Ret
=
>
"
ret
"
.
to_string
(
)
Inst
:
:
EpiloguePlaceholder
=
>
"
epilogue
placeholder
"
.
to_string
(
)
Inst
:
:
JmpKnown
{
dst
}
=
>
{
format
!
(
"
{
}
{
}
"
ljustify
(
"
jmp
"
.
to_string
(
)
)
dst
.
show_rru
(
mb_rru
)
)
}
Inst
:
:
JmpIf
{
cc
taken
}
=
>
format
!
(
"
{
}
{
}
"
ljustify2
(
"
j
"
.
to_string
(
)
cc
.
to_string
(
)
)
taken
.
show_rru
(
mb_rru
)
)
Inst
:
:
JmpCond
{
cc
taken
not_taken
}
=
>
format
!
(
"
{
}
{
}
;
j
{
}
"
ljustify2
(
"
j
"
.
to_string
(
)
cc
.
to_string
(
)
)
taken
.
show_rru
(
mb_rru
)
not_taken
.
show_rru
(
mb_rru
)
)
Inst
:
:
JmpTableSeq
{
idx
.
.
}
=
>
{
format
!
(
"
{
}
{
}
"
ljustify
(
"
br_table
"
.
into
(
)
)
idx
.
show_rru
(
mb_rru
)
)
}
Inst
:
:
JmpUnknown
{
target
}
=
>
format
!
(
"
{
}
*
{
}
"
ljustify
(
"
jmp
"
.
to_string
(
)
)
target
.
show_rru
(
mb_rru
)
)
Inst
:
:
TrapIf
{
cc
trap_code
.
.
}
=
>
{
format
!
(
"
j
{
}
;
ud2
{
}
;
"
cc
.
invert
(
)
.
to_string
(
)
trap_code
)
}
Inst
:
:
LoadExtName
{
dst
name
offset
.
.
}
=
>
format
!
(
"
{
}
{
}
+
{
}
{
}
"
ljustify
(
"
movaps
"
.
into
(
)
)
name
offset
show_ireg_sized
(
dst
.
to_reg
(
)
mb_rru
8
)
)
Inst
:
:
LockCmpxchg
{
ty
src
dst
.
.
}
=
>
{
let
size
=
ty
.
bytes
(
)
as
u8
;
format
!
(
"
lock
cmpxchg
{
}
{
}
{
}
"
suffixBWLQ
(
size
)
show_ireg_sized
(
*
src
mb_rru
size
)
dst
.
show_rru
(
mb_rru
)
)
}
Inst
:
:
AtomicRmwSeq
{
ty
op
.
.
}
=
>
{
format
!
(
"
atomically
{
{
{
}
_bits_at_
[
%
r9
]
)
{
:
?
}
=
%
r10
;
%
rax
=
old_value_at_
[
%
r9
]
;
%
r11
%
rflags
=
trash
}
}
"
ty
.
bits
(
)
op
)
}
Inst
:
:
Fence
{
kind
}
=
>
{
match
kind
{
FenceKind
:
:
MFence
=
>
"
mfence
"
.
to_string
(
)
FenceKind
:
:
LFence
=
>
"
lfence
"
.
to_string
(
)
FenceKind
:
:
SFence
=
>
"
sfence
"
.
to_string
(
)
}
}
Inst
:
:
VirtualSPOffsetAdj
{
offset
}
=
>
format
!
(
"
virtual_sp_offset_adjust
{
}
"
offset
)
Inst
:
:
Hlt
=
>
"
hlt
"
.
into
(
)
Inst
:
:
Ud2
{
trap_info
}
=
>
format
!
(
"
ud2
{
}
"
trap_info
.
1
)
}
}
}
impl
fmt
:
:
Debug
for
Inst
{
fn
fmt
(
&
self
fmt
:
&
mut
fmt
:
:
Formatter
)
-
>
fmt
:
:
Result
{
write
!
(
fmt
"
{
}
"
self
.
show_rru
(
None
)
)
}
}
fn
x64_get_regs
(
inst
:
&
Inst
collector
:
&
mut
RegUsageCollector
)
{
match
inst
{
Inst
:
:
Alu_RMI_R
{
src
dst
.
.
}
=
>
{
if
inst
.
produces_const
(
)
{
collector
.
add_def
(
*
dst
)
;
}
else
{
src
.
get_regs_as_uses
(
collector
)
;
collector
.
add_mod
(
*
dst
)
;
}
}
Inst
:
:
Not
{
src
.
.
}
=
>
{
collector
.
add_mod
(
*
src
)
;
}
Inst
:
:
Neg
{
src
.
.
}
=
>
{
collector
.
add_mod
(
*
src
)
;
}
Inst
:
:
Div
{
size
divisor
.
.
}
=
>
{
collector
.
add_mod
(
Writable
:
:
from_reg
(
regs
:
:
rax
(
)
)
)
;
if
*
size
=
=
1
{
collector
.
add_def
(
Writable
:
:
from_reg
(
regs
:
:
rdx
(
)
)
)
;
}
else
{
collector
.
add_mod
(
Writable
:
:
from_reg
(
regs
:
:
rdx
(
)
)
)
;
}
divisor
.
get_regs_as_uses
(
collector
)
;
}
Inst
:
:
MulHi
{
rhs
.
.
}
=
>
{
collector
.
add_mod
(
Writable
:
:
from_reg
(
regs
:
:
rax
(
)
)
)
;
collector
.
add_def
(
Writable
:
:
from_reg
(
regs
:
:
rdx
(
)
)
)
;
rhs
.
get_regs_as_uses
(
collector
)
;
}
Inst
:
:
CheckedDivOrRemSeq
{
divisor
tmp
.
.
}
=
>
{
collector
.
add_mod
(
Writable
:
:
from_reg
(
regs
:
:
rax
(
)
)
)
;
collector
.
add_mod
(
Writable
:
:
from_reg
(
regs
:
:
rdx
(
)
)
)
;
collector
.
add_mod
(
*
divisor
)
;
if
let
Some
(
tmp
)
=
tmp
{
collector
.
add_def
(
*
tmp
)
;
}
}
Inst
:
:
SignExtendData
{
size
}
=
>
match
size
{
1
=
>
collector
.
add_mod
(
Writable
:
:
from_reg
(
regs
:
:
rax
(
)
)
)
2
|
4
|
8
=
>
{
collector
.
add_use
(
regs
:
:
rax
(
)
)
;
collector
.
add_def
(
Writable
:
:
from_reg
(
regs
:
:
rdx
(
)
)
)
;
}
_
=
>
unreachable
!
(
)
}
Inst
:
:
UnaryRmR
{
src
dst
.
.
}
|
Inst
:
:
XmmUnaryRmR
{
src
dst
.
.
}
=
>
{
src
.
get_regs_as_uses
(
collector
)
;
collector
.
add_def
(
*
dst
)
;
}
Inst
:
:
XMM_RM_R
{
src
dst
.
.
}
=
>
{
if
inst
.
produces_const
(
)
{
collector
.
add_def
(
*
dst
)
;
}
else
{
src
.
get_regs_as_uses
(
collector
)
;
collector
.
add_mod
(
*
dst
)
;
}
}
Inst
:
:
XmmRmRImm
{
src
dst
.
.
}
=
>
{
if
inst
.
produces_const
(
)
{
collector
.
add_def
(
*
dst
)
;
}
else
{
src
.
get_regs_as_uses
(
collector
)
;
collector
.
add_mod
(
*
dst
)
;
}
}
Inst
:
:
XmmLoadConstSeq
{
dst
.
.
}
=
>
collector
.
add_def
(
*
dst
)
Inst
:
:
XmmMinMaxSeq
{
lhs
rhs_dst
.
.
}
=
>
{
collector
.
add_use
(
*
lhs
)
;
collector
.
add_mod
(
*
rhs_dst
)
;
}
Inst
:
:
XmmRmiReg
{
src
dst
.
.
}
=
>
{
src
.
get_regs_as_uses
(
collector
)
;
collector
.
add_mod
(
*
dst
)
;
}
Inst
:
:
Xmm_Mov_R_M
{
src
dst
.
.
}
=
>
{
collector
.
add_use
(
*
src
)
;
dst
.
get_regs_as_uses
(
collector
)
;
}
Inst
:
:
XMM_Cmp_RM_R
{
src
dst
.
.
}
=
>
{
src
.
get_regs_as_uses
(
collector
)
;
collector
.
add_use
(
*
dst
)
;
}
Inst
:
:
Imm
{
dst
.
.
}
=
>
{
collector
.
add_def
(
*
dst
)
;
}
Inst
:
:
Mov_R_R
{
src
dst
.
.
}
|
Inst
:
:
XmmToGpr
{
src
dst
.
.
}
=
>
{
collector
.
add_use
(
*
src
)
;
collector
.
add_def
(
*
dst
)
;
}
Inst
:
:
GprToXmm
{
src
dst
.
.
}
=
>
{
src
.
get_regs_as_uses
(
collector
)
;
collector
.
add_def
(
*
dst
)
;
}
Inst
:
:
CvtUint64ToFloatSeq
{
src
dst
tmp_gpr1
tmp_gpr2
.
.
}
=
>
{
collector
.
add_mod
(
*
src
)
;
collector
.
add_def
(
*
dst
)
;
collector
.
add_def
(
*
tmp_gpr1
)
;
collector
.
add_def
(
*
tmp_gpr2
)
;
}
Inst
:
:
CvtFloatToSintSeq
{
src
dst
tmp_xmm
tmp_gpr
.
.
}
|
Inst
:
:
CvtFloatToUintSeq
{
src
dst
tmp_gpr
tmp_xmm
.
.
}
=
>
{
collector
.
add_mod
(
*
src
)
;
collector
.
add_def
(
*
dst
)
;
collector
.
add_def
(
*
tmp_gpr
)
;
collector
.
add_def
(
*
tmp_xmm
)
;
}
Inst
:
:
MovZX_RM_R
{
src
dst
.
.
}
=
>
{
src
.
get_regs_as_uses
(
collector
)
;
collector
.
add_def
(
*
dst
)
;
}
Inst
:
:
Mov64_M_R
{
src
dst
.
.
}
|
Inst
:
:
LoadEffectiveAddress
{
addr
:
src
dst
}
=
>
{
src
.
get_regs_as_uses
(
collector
)
;
collector
.
add_def
(
*
dst
)
}
Inst
:
:
MovSX_RM_R
{
src
dst
.
.
}
=
>
{
src
.
get_regs_as_uses
(
collector
)
;
collector
.
add_def
(
*
dst
)
;
}
Inst
:
:
Mov_R_M
{
src
dst
.
.
}
=
>
{
collector
.
add_use
(
*
src
)
;
dst
.
get_regs_as_uses
(
collector
)
;
}
Inst
:
:
Shift_R
{
num_bits
dst
.
.
}
=
>
{
if
num_bits
.
is_none
(
)
{
collector
.
add_use
(
regs
:
:
rcx
(
)
)
;
}
collector
.
add_mod
(
*
dst
)
;
}
Inst
:
:
Cmp_RMI_R
{
src
dst
.
.
}
=
>
{
src
.
get_regs_as_uses
(
collector
)
;
collector
.
add_use
(
*
dst
)
;
}
Inst
:
:
Setcc
{
dst
.
.
}
=
>
{
collector
.
add_def
(
*
dst
)
;
}
Inst
:
:
Cmove
{
src
dst
.
.
}
|
Inst
:
:
XmmCmove
{
src
dst
.
.
}
=
>
{
src
.
get_regs_as_uses
(
collector
)
;
collector
.
add_mod
(
*
dst
)
;
}
Inst
:
:
Push64
{
src
}
=
>
{
src
.
get_regs_as_uses
(
collector
)
;
collector
.
add_mod
(
Writable
:
:
from_reg
(
regs
:
:
rsp
(
)
)
)
;
}
Inst
:
:
Pop64
{
dst
}
=
>
{
collector
.
add_def
(
*
dst
)
;
}
Inst
:
:
CallKnown
{
ref
uses
ref
defs
.
.
}
=
>
{
collector
.
add_uses
(
uses
)
;
collector
.
add_defs
(
defs
)
;
}
Inst
:
:
CallUnknown
{
ref
uses
ref
defs
dest
.
.
}
=
>
{
collector
.
add_uses
(
uses
)
;
collector
.
add_defs
(
defs
)
;
dest
.
get_regs_as_uses
(
collector
)
;
}
Inst
:
:
JmpTableSeq
{
ref
idx
ref
tmp1
ref
tmp2
.
.
}
=
>
{
collector
.
add_use
(
*
idx
)
;
collector
.
add_def
(
*
tmp1
)
;
collector
.
add_def
(
*
tmp2
)
;
}
Inst
:
:
JmpUnknown
{
target
}
=
>
{
target
.
get_regs_as_uses
(
collector
)
;
}
Inst
:
:
LoadExtName
{
dst
.
.
}
=
>
{
collector
.
add_def
(
*
dst
)
;
}
Inst
:
:
LockCmpxchg
{
src
dst
.
.
}
=
>
{
dst
.
get_regs_as_uses
(
collector
)
;
collector
.
add_use
(
*
src
)
;
collector
.
add_mod
(
Writable
:
:
from_reg
(
regs
:
:
rax
(
)
)
)
;
}
Inst
:
:
AtomicRmwSeq
{
.
.
}
=
>
{
collector
.
add_use
(
regs
:
:
r9
(
)
)
;
collector
.
add_use
(
regs
:
:
r10
(
)
)
;
collector
.
add_def
(
Writable
:
:
from_reg
(
regs
:
:
r11
(
)
)
)
;
collector
.
add_def
(
Writable
:
:
from_reg
(
regs
:
:
rax
(
)
)
)
;
}
Inst
:
:
Ret
|
Inst
:
:
EpiloguePlaceholder
|
Inst
:
:
JmpKnown
{
.
.
}
|
Inst
:
:
JmpIf
{
.
.
}
|
Inst
:
:
JmpCond
{
.
.
}
|
Inst
:
:
Nop
{
.
.
}
|
Inst
:
:
TrapIf
{
.
.
}
|
Inst
:
:
VirtualSPOffsetAdj
{
.
.
}
|
Inst
:
:
Hlt
|
Inst
:
:
Ud2
{
.
.
}
|
Inst
:
:
Fence
{
.
.
}
=
>
{
}
}
}
fn
map_use
<
RUM
:
RegUsageMapper
>
(
m
:
&
RUM
r
:
&
mut
Reg
)
{
if
let
Some
(
reg
)
=
r
.
as_virtual_reg
(
)
{
let
new
=
m
.
get_use
(
reg
)
.
unwrap
(
)
.
to_reg
(
)
;
*
r
=
new
;
}
}
fn
map_def
<
RUM
:
RegUsageMapper
>
(
m
:
&
RUM
r
:
&
mut
Writable
<
Reg
>
)
{
if
let
Some
(
reg
)
=
r
.
to_reg
(
)
.
as_virtual_reg
(
)
{
let
new
=
m
.
get_def
(
reg
)
.
unwrap
(
)
.
to_reg
(
)
;
*
r
=
Writable
:
:
from_reg
(
new
)
;
}
}
fn
map_mod
<
RUM
:
RegUsageMapper
>
(
m
:
&
RUM
r
:
&
mut
Writable
<
Reg
>
)
{
if
let
Some
(
reg
)
=
r
.
to_reg
(
)
.
as_virtual_reg
(
)
{
let
new
=
m
.
get_mod
(
reg
)
.
unwrap
(
)
.
to_reg
(
)
;
*
r
=
Writable
:
:
from_reg
(
new
)
;
}
}
impl
Amode
{
fn
map_uses
<
RUM
:
RegUsageMapper
>
(
&
mut
self
map
:
&
RUM
)
{
match
self
{
Amode
:
:
ImmReg
{
ref
mut
base
.
.
}
=
>
map_use
(
map
base
)
Amode
:
:
ImmRegRegShift
{
ref
mut
base
ref
mut
index
.
.
}
=
>
{
map_use
(
map
base
)
;
map_use
(
map
index
)
;
}
Amode
:
:
RipRelative
{
.
.
}
=
>
{
}
}
}
}
impl
RegMemImm
{
fn
map_uses
<
RUM
:
RegUsageMapper
>
(
&
mut
self
map
:
&
RUM
)
{
match
self
{
RegMemImm
:
:
Reg
{
ref
mut
reg
}
=
>
map_use
(
map
reg
)
RegMemImm
:
:
Mem
{
ref
mut
addr
}
=
>
addr
.
map_uses
(
map
)
RegMemImm
:
:
Imm
{
.
.
}
=
>
{
}
}
}
fn
map_as_def
<
RUM
:
RegUsageMapper
>
(
&
mut
self
mapper
:
&
RUM
)
{
match
self
{
Self
:
:
Reg
{
reg
}
=
>
{
let
mut
writable_src
=
Writable
:
:
from_reg
(
*
reg
)
;
map_def
(
mapper
&
mut
writable_src
)
;
*
self
=
Self
:
:
reg
(
writable_src
.
to_reg
(
)
)
;
}
_
=
>
panic
!
(
"
unexpected
RegMemImm
kind
in
map_src_reg_as_def
"
)
}
}
}
impl
RegMem
{
fn
map_uses
<
RUM
:
RegUsageMapper
>
(
&
mut
self
map
:
&
RUM
)
{
match
self
{
RegMem
:
:
Reg
{
ref
mut
reg
}
=
>
map_use
(
map
reg
)
RegMem
:
:
Mem
{
ref
mut
addr
.
.
}
=
>
addr
.
map_uses
(
map
)
}
}
fn
map_as_def
<
RUM
:
RegUsageMapper
>
(
&
mut
self
mapper
:
&
RUM
)
{
match
self
{
Self
:
:
Reg
{
reg
}
=
>
{
let
mut
writable_src
=
Writable
:
:
from_reg
(
*
reg
)
;
map_def
(
mapper
&
mut
writable_src
)
;
*
self
=
Self
:
:
reg
(
writable_src
.
to_reg
(
)
)
;
}
_
=
>
panic
!
(
"
unexpected
RegMem
kind
in
map_src_reg_as_def
"
)
}
}
}
fn
x64_map_regs
<
RUM
:
RegUsageMapper
>
(
inst
:
&
mut
Inst
mapper
:
&
RUM
)
{
let
produces_const
=
inst
.
produces_const
(
)
;
match
inst
{
Inst
:
:
Alu_RMI_R
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
if
produces_const
{
src
.
map_as_def
(
mapper
)
;
map_def
(
mapper
dst
)
;
}
else
{
src
.
map_uses
(
mapper
)
;
map_mod
(
mapper
dst
)
;
}
}
Inst
:
:
Not
{
src
.
.
}
|
Inst
:
:
Neg
{
src
.
.
}
=
>
map_mod
(
mapper
src
)
Inst
:
:
Div
{
divisor
.
.
}
=
>
divisor
.
map_uses
(
mapper
)
Inst
:
:
MulHi
{
rhs
.
.
}
=
>
rhs
.
map_uses
(
mapper
)
Inst
:
:
CheckedDivOrRemSeq
{
divisor
tmp
.
.
}
=
>
{
map_mod
(
mapper
divisor
)
;
if
let
Some
(
tmp
)
=
tmp
{
map_def
(
mapper
tmp
)
}
}
Inst
:
:
SignExtendData
{
.
.
}
=
>
{
}
Inst
:
:
XmmUnaryRmR
{
ref
mut
src
ref
mut
dst
.
.
}
|
Inst
:
:
UnaryRmR
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
src
.
map_uses
(
mapper
)
;
map_def
(
mapper
dst
)
;
}
Inst
:
:
XmmRmRImm
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
if
produces_const
{
src
.
map_as_def
(
mapper
)
;
map_def
(
mapper
dst
)
;
}
else
{
src
.
map_uses
(
mapper
)
;
map_mod
(
mapper
dst
)
;
}
}
Inst
:
:
XMM_RM_R
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
if
produces_const
{
src
.
map_as_def
(
mapper
)
;
map_def
(
mapper
dst
)
;
}
else
{
src
.
map_uses
(
mapper
)
;
map_mod
(
mapper
dst
)
;
}
}
Inst
:
:
XmmRmiReg
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
src
.
map_uses
(
mapper
)
;
map_mod
(
mapper
dst
)
;
}
Inst
:
:
XmmLoadConstSeq
{
ref
mut
dst
.
.
}
=
>
{
map_def
(
mapper
dst
)
;
}
Inst
:
:
XmmMinMaxSeq
{
ref
mut
lhs
ref
mut
rhs_dst
.
.
}
=
>
{
map_use
(
mapper
lhs
)
;
map_mod
(
mapper
rhs_dst
)
;
}
Inst
:
:
Xmm_Mov_R_M
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
map_use
(
mapper
src
)
;
dst
.
map_uses
(
mapper
)
;
}
Inst
:
:
XMM_Cmp_RM_R
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
src
.
map_uses
(
mapper
)
;
map_use
(
mapper
dst
)
;
}
Inst
:
:
Imm
{
ref
mut
dst
.
.
}
=
>
map_def
(
mapper
dst
)
Inst
:
:
Mov_R_R
{
ref
mut
src
ref
mut
dst
.
.
}
|
Inst
:
:
XmmToGpr
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
map_use
(
mapper
src
)
;
map_def
(
mapper
dst
)
;
}
Inst
:
:
GprToXmm
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
src
.
map_uses
(
mapper
)
;
map_def
(
mapper
dst
)
;
}
Inst
:
:
CvtUint64ToFloatSeq
{
ref
mut
src
ref
mut
dst
ref
mut
tmp_gpr1
ref
mut
tmp_gpr2
.
.
}
=
>
{
map_mod
(
mapper
src
)
;
map_def
(
mapper
dst
)
;
map_def
(
mapper
tmp_gpr1
)
;
map_def
(
mapper
tmp_gpr2
)
;
}
Inst
:
:
CvtFloatToSintSeq
{
ref
mut
src
ref
mut
dst
ref
mut
tmp_xmm
ref
mut
tmp_gpr
.
.
}
|
Inst
:
:
CvtFloatToUintSeq
{
ref
mut
src
ref
mut
dst
ref
mut
tmp_gpr
ref
mut
tmp_xmm
.
.
}
=
>
{
map_mod
(
mapper
src
)
;
map_def
(
mapper
dst
)
;
map_def
(
mapper
tmp_gpr
)
;
map_def
(
mapper
tmp_xmm
)
;
}
Inst
:
:
MovZX_RM_R
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
src
.
map_uses
(
mapper
)
;
map_def
(
mapper
dst
)
;
}
Inst
:
:
Mov64_M_R
{
src
dst
.
.
}
|
Inst
:
:
LoadEffectiveAddress
{
addr
:
src
dst
}
=
>
{
src
.
map_uses
(
mapper
)
;
map_def
(
mapper
dst
)
;
}
Inst
:
:
MovSX_RM_R
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
src
.
map_uses
(
mapper
)
;
map_def
(
mapper
dst
)
;
}
Inst
:
:
Mov_R_M
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
map_use
(
mapper
src
)
;
dst
.
map_uses
(
mapper
)
;
}
Inst
:
:
Shift_R
{
ref
mut
dst
.
.
}
=
>
{
map_mod
(
mapper
dst
)
;
}
Inst
:
:
Cmp_RMI_R
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
src
.
map_uses
(
mapper
)
;
map_use
(
mapper
dst
)
;
}
Inst
:
:
Setcc
{
ref
mut
dst
.
.
}
=
>
map_def
(
mapper
dst
)
Inst
:
:
Cmove
{
ref
mut
src
ref
mut
dst
.
.
}
|
Inst
:
:
XmmCmove
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
src
.
map_uses
(
mapper
)
;
map_mod
(
mapper
dst
)
}
Inst
:
:
Push64
{
ref
mut
src
}
=
>
src
.
map_uses
(
mapper
)
Inst
:
:
Pop64
{
ref
mut
dst
}
=
>
{
map_def
(
mapper
dst
)
;
}
Inst
:
:
CallKnown
{
ref
mut
uses
ref
mut
defs
.
.
}
=
>
{
for
r
in
uses
.
iter_mut
(
)
{
map_use
(
mapper
r
)
;
}
for
r
in
defs
.
iter_mut
(
)
{
map_def
(
mapper
r
)
;
}
}
Inst
:
:
CallUnknown
{
ref
mut
uses
ref
mut
defs
ref
mut
dest
.
.
}
=
>
{
for
r
in
uses
.
iter_mut
(
)
{
map_use
(
mapper
r
)
;
}
for
r
in
defs
.
iter_mut
(
)
{
map_def
(
mapper
r
)
;
}
dest
.
map_uses
(
mapper
)
;
}
Inst
:
:
JmpTableSeq
{
ref
mut
idx
ref
mut
tmp1
ref
mut
tmp2
.
.
}
=
>
{
map_use
(
mapper
idx
)
;
map_def
(
mapper
tmp1
)
;
map_def
(
mapper
tmp2
)
;
}
Inst
:
:
JmpUnknown
{
ref
mut
target
}
=
>
target
.
map_uses
(
mapper
)
Inst
:
:
LoadExtName
{
ref
mut
dst
.
.
}
=
>
map_def
(
mapper
dst
)
Inst
:
:
LockCmpxchg
{
ref
mut
src
ref
mut
dst
.
.
}
=
>
{
map_use
(
mapper
src
)
;
dst
.
map_uses
(
mapper
)
;
}
Inst
:
:
Ret
|
Inst
:
:
EpiloguePlaceholder
|
Inst
:
:
JmpKnown
{
.
.
}
|
Inst
:
:
JmpCond
{
.
.
}
|
Inst
:
:
JmpIf
{
.
.
}
|
Inst
:
:
Nop
{
.
.
}
|
Inst
:
:
TrapIf
{
.
.
}
|
Inst
:
:
VirtualSPOffsetAdj
{
.
.
}
|
Inst
:
:
Ud2
{
.
.
}
|
Inst
:
:
Hlt
|
Inst
:
:
AtomicRmwSeq
{
.
.
}
|
Inst
:
:
Fence
{
.
.
}
=
>
{
}
}
}
impl
MachInst
for
Inst
{
fn
get_regs
(
&
self
collector
:
&
mut
RegUsageCollector
)
{
x64_get_regs
(
&
self
collector
)
}
fn
map_regs
<
RUM
:
RegUsageMapper
>
(
&
mut
self
mapper
:
&
RUM
)
{
x64_map_regs
(
self
mapper
)
;
}
fn
is_move
(
&
self
)
-
>
Option
<
(
Writable
<
Reg
>
Reg
)
>
{
match
self
{
Self
:
:
Mov_R_R
{
is_64
src
dst
.
.
}
if
*
is_64
=
>
Some
(
(
*
dst
*
src
)
)
Self
:
:
XmmUnaryRmR
{
op
src
dst
.
.
}
if
*
op
=
=
SseOpcode
:
:
Movss
|
|
*
op
=
=
SseOpcode
:
:
Movsd
|
|
*
op
=
=
SseOpcode
:
:
Movaps
=
>
{
if
let
RegMem
:
:
Reg
{
reg
}
=
src
{
Some
(
(
*
dst
*
reg
)
)
}
else
{
None
}
}
_
=
>
None
}
}
fn
is_epilogue_placeholder
(
&
self
)
-
>
bool
{
if
let
Self
:
:
EpiloguePlaceholder
=
self
{
true
}
else
{
false
}
}
fn
is_term
<
'
a
>
(
&
'
a
self
)
-
>
MachTerminator
<
'
a
>
{
match
self
{
&
Self
:
:
Ret
|
&
Self
:
:
EpiloguePlaceholder
=
>
MachTerminator
:
:
Ret
&
Self
:
:
JmpKnown
{
dst
}
=
>
MachTerminator
:
:
Uncond
(
dst
.
as_label
(
)
.
unwrap
(
)
)
&
Self
:
:
JmpCond
{
taken
not_taken
.
.
}
=
>
MachTerminator
:
:
Cond
(
taken
.
as_label
(
)
.
unwrap
(
)
not_taken
.
as_label
(
)
.
unwrap
(
)
)
&
Self
:
:
JmpTableSeq
{
ref
targets_for_term
.
.
}
=
>
MachTerminator
:
:
Indirect
(
&
targets_for_term
[
.
.
]
)
_
=
>
MachTerminator
:
:
None
}
}
fn
gen_move
(
dst_reg
:
Writable
<
Reg
>
src_reg
:
Reg
ty
:
Type
)
-
>
Inst
{
let
rc_dst
=
dst_reg
.
to_reg
(
)
.
get_class
(
)
;
let
rc_src
=
src_reg
.
get_class
(
)
;
debug_assert
!
(
rc_dst
=
=
rc_src
)
;
match
rc_dst
{
RegClass
:
:
I64
=
>
Inst
:
:
mov_r_r
(
true
src_reg
dst_reg
)
RegClass
:
:
V128
=
>
{
let
opcode
=
match
ty
{
types
:
:
F32
=
>
SseOpcode
:
:
Movss
types
:
:
F64
=
>
SseOpcode
:
:
Movsd
types
:
:
F32X4
=
>
SseOpcode
:
:
Movaps
types
:
:
F64X2
=
>
SseOpcode
:
:
Movapd
_
if
ty
.
is_vector
(
)
&
&
ty
.
bits
(
)
=
=
128
=
>
SseOpcode
:
:
Movdqa
_
=
>
unimplemented
!
(
"
unable
to
move
type
:
{
}
"
ty
)
}
;
Inst
:
:
xmm_unary_rm_r
(
opcode
RegMem
:
:
reg
(
src_reg
)
dst_reg
)
}
_
=
>
panic
!
(
"
gen_move
(
x64
)
:
unhandled
regclass
{
:
?
}
"
rc_dst
)
}
}
fn
gen_zero_len_nop
(
)
-
>
Inst
{
Inst
:
:
Nop
{
len
:
0
}
}
fn
gen_nop
(
_preferred_size
:
usize
)
-
>
Inst
{
unimplemented
!
(
)
}
fn
maybe_direct_reload
(
&
self
_reg
:
VirtualReg
_slot
:
SpillSlot
)
-
>
Option
<
Inst
>
{
None
}
fn
rc_for_type
(
ty
:
Type
)
-
>
CodegenResult
<
RegClass
>
{
match
ty
{
types
:
:
I8
|
types
:
:
I16
|
types
:
:
I32
|
types
:
:
I64
|
types
:
:
B1
|
types
:
:
B8
|
types
:
:
B16
|
types
:
:
B32
|
types
:
:
B64
|
types
:
:
R32
|
types
:
:
R64
=
>
Ok
(
RegClass
:
:
I64
)
types
:
:
F32
|
types
:
:
F64
=
>
Ok
(
RegClass
:
:
V128
)
_
if
ty
.
bits
(
)
=
=
128
=
>
Ok
(
RegClass
:
:
V128
)
types
:
:
IFLAGS
|
types
:
:
FFLAGS
=
>
Ok
(
RegClass
:
:
I64
)
_
=
>
Err
(
CodegenError
:
:
Unsupported
(
format
!
(
"
Unexpected
SSA
-
value
type
:
{
}
"
ty
)
)
)
}
}
fn
gen_jump
(
label
:
MachLabel
)
-
>
Inst
{
Inst
:
:
jmp_known
(
BranchTarget
:
:
Label
(
label
)
)
}
fn
gen_constant
<
F
:
FnMut
(
RegClass
Type
)
-
>
Writable
<
Reg
>
>
(
to_reg
:
Writable
<
Reg
>
value
:
u64
ty
:
Type
mut
alloc_tmp
:
F
)
-
>
SmallVec
<
[
Self
;
4
]
>
{
let
mut
ret
=
SmallVec
:
:
new
(
)
;
if
ty
=
=
types
:
:
F32
{
if
value
=
=
0
{
ret
.
push
(
Inst
:
:
xmm_rm_r
(
SseOpcode
:
:
Xorps
RegMem
:
:
reg
(
to_reg
.
to_reg
(
)
)
to_reg
)
)
;
}
else
{
let
tmp
=
alloc_tmp
(
RegClass
:
:
I64
types
:
:
I32
)
;
ret
.
push
(
Inst
:
:
imm
(
OperandSize
:
:
Size32
value
tmp
)
)
;
ret
.
push
(
Inst
:
:
gpr_to_xmm
(
SseOpcode
:
:
Movd
RegMem
:
:
reg
(
tmp
.
to_reg
(
)
)
OperandSize
:
:
Size32
to_reg
)
)
;
}
}
else
if
ty
=
=
types
:
:
F64
{
if
value
=
=
0
{
ret
.
push
(
Inst
:
:
xmm_rm_r
(
SseOpcode
:
:
Xorpd
RegMem
:
:
reg
(
to_reg
.
to_reg
(
)
)
to_reg
)
)
;
}
else
{
let
tmp
=
alloc_tmp
(
RegClass
:
:
I64
types
:
:
I64
)
;
ret
.
push
(
Inst
:
:
imm
(
OperandSize
:
:
Size64
value
tmp
)
)
;
ret
.
push
(
Inst
:
:
gpr_to_xmm
(
SseOpcode
:
:
Movq
RegMem
:
:
reg
(
tmp
.
to_reg
(
)
)
OperandSize
:
:
Size64
to_reg
)
)
;
}
}
else
{
debug_assert
!
(
ty
=
=
types
:
:
B1
|
|
ty
=
=
types
:
:
I8
|
|
ty
=
=
types
:
:
B8
|
|
ty
=
=
types
:
:
I16
|
|
ty
=
=
types
:
:
B16
|
|
ty
=
=
types
:
:
I32
|
|
ty
=
=
types
:
:
B32
|
|
ty
=
=
types
:
:
I64
|
|
ty
=
=
types
:
:
B64
|
|
ty
=
=
types
:
:
R32
|
|
ty
=
=
types
:
:
R64
)
;
if
value
=
=
0
{
ret
.
push
(
Inst
:
:
alu_rmi_r
(
ty
=
=
types
:
:
I64
AluRmiROpcode
:
:
Xor
RegMemImm
:
:
reg
(
to_reg
.
to_reg
(
)
)
to_reg
)
)
;
}
else
{
ret
.
push
(
Inst
:
:
imm
(
OperandSize
:
:
from_bytes
(
ty
.
bytes
(
)
)
value
to_reg
)
)
;
}
}
ret
}
fn
reg_universe
(
flags
:
&
Flags
)
-
>
RealRegUniverse
{
create_reg_universe_systemv
(
flags
)
}
fn
worst_case_size
(
)
-
>
CodeOffset
{
15
}
fn
ref_type_regclass
(
_
:
&
settings
:
:
Flags
)
-
>
RegClass
{
RegClass
:
:
I64
}
type
LabelUse
=
LabelUse
;
}
#
[
derive
(
Default
Clone
Debug
)
]
pub
struct
EmitState
{
pub
(
crate
)
virtual_sp_offset
:
i64
pub
(
crate
)
nominal_sp_to_fp
:
i64
stack_map
:
Option
<
StackMap
>
}
impl
MachInstEmit
for
Inst
{
type
State
=
EmitState
;
fn
emit
(
&
self
sink
:
&
mut
MachBuffer
<
Inst
>
flags
:
&
settings
:
:
Flags
state
:
&
mut
Self
:
:
State
)
{
emit
:
:
emit
(
self
sink
flags
state
)
;
}
fn
pretty_print
(
&
self
mb_rru
:
Option
<
&
RealRegUniverse
>
_
:
&
mut
Self
:
:
State
)
-
>
String
{
self
.
show_rru
(
mb_rru
)
}
}
impl
MachInstEmitState
<
Inst
>
for
EmitState
{
fn
new
(
abi
:
&
dyn
ABICallee
<
I
=
Inst
>
)
-
>
Self
{
EmitState
{
virtual_sp_offset
:
0
nominal_sp_to_fp
:
abi
.
frame_size
(
)
as
i64
stack_map
:
None
}
}
fn
pre_safepoint
(
&
mut
self
stack_map
:
StackMap
)
{
self
.
stack_map
=
Some
(
stack_map
)
;
}
}
impl
EmitState
{
fn
take_stack_map
(
&
mut
self
)
-
>
Option
<
StackMap
>
{
self
.
stack_map
.
take
(
)
}
fn
clear_post_insn
(
&
mut
self
)
{
self
.
stack_map
=
None
;
}
}
#
[
derive
(
Clone
Copy
Debug
PartialEq
Eq
)
]
pub
enum
LabelUse
{
JmpRel32
PCRel32
}
impl
MachInstLabelUse
for
LabelUse
{
const
ALIGN
:
CodeOffset
=
1
;
fn
max_pos_range
(
self
)
-
>
CodeOffset
{
match
self
{
LabelUse
:
:
JmpRel32
|
LabelUse
:
:
PCRel32
=
>
0x7fff_ffff
}
}
fn
max_neg_range
(
self
)
-
>
CodeOffset
{
match
self
{
LabelUse
:
:
JmpRel32
|
LabelUse
:
:
PCRel32
=
>
0x8000_0000
}
}
fn
patch_size
(
self
)
-
>
CodeOffset
{
match
self
{
LabelUse
:
:
JmpRel32
|
LabelUse
:
:
PCRel32
=
>
4
}
}
fn
patch
(
self
buffer
:
&
mut
[
u8
]
use_offset
:
CodeOffset
label_offset
:
CodeOffset
)
{
let
pc_rel
=
(
label_offset
as
i64
)
-
(
use_offset
as
i64
)
;
debug_assert
!
(
pc_rel
<
=
self
.
max_pos_range
(
)
as
i64
)
;
debug_assert
!
(
pc_rel
>
=
-
(
self
.
max_neg_range
(
)
as
i64
)
)
;
let
pc_rel
=
pc_rel
as
u32
;
match
self
{
LabelUse
:
:
JmpRel32
=
>
{
let
addend
=
u32
:
:
from_le_bytes
(
[
buffer
[
0
]
buffer
[
1
]
buffer
[
2
]
buffer
[
3
]
]
)
;
let
value
=
pc_rel
.
wrapping_add
(
addend
)
.
wrapping_sub
(
4
)
;
buffer
.
copy_from_slice
(
&
value
.
to_le_bytes
(
)
[
.
.
]
)
;
}
LabelUse
:
:
PCRel32
=
>
{
let
addend
=
u32
:
:
from_le_bytes
(
[
buffer
[
0
]
buffer
[
1
]
buffer
[
2
]
buffer
[
3
]
]
)
;
let
value
=
pc_rel
.
wrapping_add
(
addend
)
;
buffer
.
copy_from_slice
(
&
value
.
to_le_bytes
(
)
[
.
.
]
)
;
}
}
}
fn
supports_veneer
(
self
)
-
>
bool
{
match
self
{
LabelUse
:
:
JmpRel32
|
LabelUse
:
:
PCRel32
=
>
false
}
}
fn
veneer_size
(
self
)
-
>
CodeOffset
{
match
self
{
LabelUse
:
:
JmpRel32
|
LabelUse
:
:
PCRel32
=
>
0
}
}
fn
generate_veneer
(
self
_
:
&
mut
[
u8
]
_
:
CodeOffset
)
-
>
(
CodeOffset
LabelUse
)
{
match
self
{
LabelUse
:
:
JmpRel32
|
LabelUse
:
:
PCRel32
=
>
{
panic
!
(
"
Veneer
not
supported
for
JumpRel32
label
-
use
.
"
)
;
}
}
}
}
