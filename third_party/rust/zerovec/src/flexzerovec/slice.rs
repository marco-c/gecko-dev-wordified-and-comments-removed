use
super
:
:
FlexZeroVec
;
use
crate
:
:
ZeroVecError
;
use
alloc
:
:
vec
:
:
Vec
;
use
core
:
:
cmp
:
:
Ordering
;
use
core
:
:
fmt
;
use
core
:
:
mem
;
use
core
:
:
ops
:
:
Range
;
const
USIZE_WIDTH
:
usize
=
mem
:
:
size_of
:
:
<
usize
>
(
)
;
#
[
repr
(
packed
)
]
pub
struct
FlexZeroSlice
{
width
:
u8
data
:
[
u8
]
}
impl
fmt
:
:
Debug
for
FlexZeroSlice
{
fn
fmt
(
&
self
f
:
&
mut
fmt
:
:
Formatter
)
-
>
fmt
:
:
Result
{
self
.
to_vec
(
)
.
fmt
(
f
)
}
}
impl
PartialEq
for
FlexZeroSlice
{
fn
eq
(
&
self
other
:
&
Self
)
-
>
bool
{
self
.
width
=
=
other
.
width
&
&
self
.
data
=
=
other
.
data
}
}
impl
Eq
for
FlexZeroSlice
{
}
#
[
inline
]
pub
(
crate
)
fn
chunk_to_usize
(
chunk
:
&
[
u8
]
width
:
usize
)
-
>
usize
{
debug_assert_eq
!
(
chunk
.
len
(
)
width
)
;
let
mut
bytes
=
[
0
;
USIZE_WIDTH
]
;
#
[
allow
(
clippy
:
:
indexing_slicing
)
]
bytes
[
0
.
.
width
]
.
copy_from_slice
(
chunk
)
;
usize
:
:
from_le_bytes
(
bytes
)
}
impl
FlexZeroSlice
{
#
[
inline
]
pub
const
fn
new_empty
(
)
-
>
&
'
static
Self
{
const
ARR
:
&
[
u8
]
=
&
[
1u8
]
;
unsafe
{
Self
:
:
from_byte_slice_unchecked
(
ARR
)
}
}
pub
const
fn
parse_byte_slice
(
bytes
:
&
[
u8
]
)
-
>
Result
<
&
Self
ZeroVecError
>
{
let
(
width_u8
data
)
=
match
bytes
.
split_first
(
)
{
Some
(
v
)
=
>
v
None
=
>
{
return
Err
(
ZeroVecError
:
:
InvalidLength
{
ty
:
"
FlexZeroSlice
"
len
:
0
}
)
}
}
;
let
width
=
*
width_u8
as
usize
;
if
width
<
1
|
|
width
>
USIZE_WIDTH
{
return
Err
(
ZeroVecError
:
:
ParseError
{
ty
:
"
FlexZeroSlice
"
}
)
;
}
if
data
.
len
(
)
%
width
!
=
0
{
return
Err
(
ZeroVecError
:
:
InvalidLength
{
ty
:
"
FlexZeroSlice
"
len
:
bytes
.
len
(
)
}
)
;
}
Ok
(
unsafe
{
Self
:
:
from_byte_slice_unchecked
(
bytes
)
}
)
}
#
[
inline
]
pub
const
unsafe
fn
from_byte_slice_unchecked
(
bytes
:
&
[
u8
]
)
-
>
&
Self
{
#
[
allow
(
clippy
:
:
panic
)
]
if
bytes
.
is_empty
(
)
{
panic
!
(
"
from_byte_slice_unchecked
called
with
empty
slice
"
)
}
let
slice
=
core
:
:
ptr
:
:
slice_from_raw_parts
(
bytes
.
as_ptr
(
)
bytes
.
len
(
)
-
1
)
;
&
*
(
slice
as
*
const
Self
)
}
#
[
inline
]
pub
(
crate
)
unsafe
fn
from_byte_slice_mut_unchecked
(
bytes
:
&
mut
[
u8
]
)
-
>
&
mut
Self
{
let
remainder
=
core
:
:
ptr
:
:
slice_from_raw_parts_mut
(
bytes
.
as_mut_ptr
(
)
bytes
.
len
(
)
-
1
)
;
&
mut
*
(
remainder
as
*
mut
Self
)
}
#
[
inline
]
pub
fn
as_bytes
(
&
self
)
-
>
&
[
u8
]
{
unsafe
{
core
:
:
slice
:
:
from_raw_parts
(
self
as
*
const
Self
as
*
const
u8
self
.
data
.
len
(
)
+
1
)
}
}
#
[
inline
]
pub
const
fn
as_flexzerovec
(
&
self
)
-
>
FlexZeroVec
{
FlexZeroVec
:
:
Borrowed
(
self
)
}
#
[
inline
]
pub
fn
len
(
&
self
)
-
>
usize
{
self
.
data
.
len
(
)
/
self
.
get_width
(
)
}
#
[
inline
]
pub
(
crate
)
fn
get_width
(
&
self
)
-
>
usize
{
usize
:
:
from
(
self
.
width
)
}
#
[
inline
]
pub
fn
is_empty
(
&
self
)
-
>
bool
{
self
.
data
.
len
(
)
=
=
0
}
#
[
inline
]
pub
fn
get
(
&
self
index
:
usize
)
-
>
Option
<
usize
>
{
if
index
>
=
self
.
len
(
)
{
None
}
else
{
Some
(
unsafe
{
self
.
get_unchecked
(
index
)
}
)
}
}
#
[
inline
]
pub
(
crate
)
fn
get_chunk
(
&
self
index
:
usize
)
-
>
Option
<
&
[
u8
]
>
{
let
w
=
self
.
get_width
(
)
;
self
.
data
.
get
(
index
*
w
.
.
index
*
w
+
w
)
}
#
[
inline
]
pub
unsafe
fn
get_unchecked
(
&
self
index
:
usize
)
-
>
usize
{
match
self
.
width
{
1
=
>
*
self
.
data
.
get_unchecked
(
index
)
as
usize
2
=
>
{
let
ptr
=
self
.
data
.
as_ptr
(
)
.
add
(
index
*
2
)
;
u16
:
:
from_le_bytes
(
core
:
:
ptr
:
:
read
(
ptr
as
*
const
[
u8
;
2
]
)
)
as
usize
}
_
=
>
{
let
mut
bytes
=
[
0
;
USIZE_WIDTH
]
;
let
w
=
self
.
get_width
(
)
;
assert
!
(
w
<
=
USIZE_WIDTH
)
;
let
ptr
=
self
.
data
.
as_ptr
(
)
.
add
(
index
*
w
)
;
core
:
:
ptr
:
:
copy_nonoverlapping
(
ptr
bytes
.
as_mut_ptr
(
)
w
)
;
usize
:
:
from_le_bytes
(
bytes
)
}
}
}
#
[
inline
]
pub
fn
first
(
&
self
)
-
>
Option
<
usize
>
{
let
w
=
self
.
get_width
(
)
;
self
.
data
.
get
(
0
.
.
w
)
.
map
(
|
chunk
|
chunk_to_usize
(
chunk
w
)
)
}
#
[
inline
]
pub
fn
last
(
&
self
)
-
>
Option
<
usize
>
{
let
l
=
self
.
data
.
len
(
)
;
if
l
=
=
0
{
None
}
else
{
let
w
=
self
.
get_width
(
)
;
self
.
data
.
get
(
l
-
w
.
.
l
)
.
map
(
|
chunk
|
chunk_to_usize
(
chunk
w
)
)
}
}
#
[
inline
]
pub
fn
iter
(
&
self
)
-
>
impl
DoubleEndedIterator
<
Item
=
usize
>
+
'
_
+
ExactSizeIterator
<
Item
=
usize
>
{
let
w
=
self
.
get_width
(
)
;
self
.
data
.
chunks_exact
(
w
)
.
map
(
move
|
chunk
|
chunk_to_usize
(
chunk
w
)
)
}
pub
fn
iter_pairs
(
&
self
)
-
>
impl
Iterator
<
Item
=
(
usize
Option
<
usize
>
)
>
+
'
_
{
self
.
iter
(
)
.
zip
(
self
.
iter
(
)
.
skip
(
1
)
.
map
(
Some
)
.
chain
(
[
None
]
)
)
}
#
[
inline
]
pub
fn
to_vec
(
&
self
)
-
>
Vec
<
usize
>
{
self
.
iter
(
)
.
collect
(
)
}
#
[
inline
]
pub
fn
binary_search
(
&
self
needle
:
usize
)
-
>
Result
<
usize
usize
>
{
self
.
binary_search_by
(
|
probe
|
probe
.
cmp
(
&
needle
)
)
}
#
[
inline
]
pub
fn
binary_search_in_range
(
&
self
needle
:
usize
range
:
Range
<
usize
>
)
-
>
Option
<
Result
<
usize
usize
>
>
{
self
.
binary_search_in_range_by
(
|
probe
|
probe
.
cmp
(
&
needle
)
range
)
}
#
[
inline
]
pub
fn
binary_search_by
(
&
self
predicate
:
impl
FnMut
(
usize
)
-
>
Ordering
)
-
>
Result
<
usize
usize
>
{
debug_assert
!
(
self
.
len
(
)
<
=
self
.
data
.
len
(
)
)
;
let
scaled_slice
=
unsafe
{
self
.
data
.
get_unchecked
(
0
.
.
self
.
len
(
)
)
}
;
self
.
binary_search_impl
(
predicate
scaled_slice
)
}
#
[
inline
]
pub
fn
binary_search_in_range_by
(
&
self
predicate
:
impl
FnMut
(
usize
)
-
>
Ordering
range
:
Range
<
usize
>
)
-
>
Option
<
Result
<
usize
usize
>
>
{
if
range
.
start
>
self
.
len
(
)
|
|
range
.
end
>
self
.
len
(
)
{
return
None
;
}
let
scaled_slice
=
self
.
data
.
get
(
range
)
?
;
Some
(
self
.
binary_search_impl
(
predicate
scaled_slice
)
)
}
#
[
inline
]
pub
fn
binary_search_with_index
(
&
self
predicate
:
impl
FnMut
(
usize
)
-
>
Ordering
)
-
>
Result
<
usize
usize
>
{
debug_assert
!
(
self
.
len
(
)
<
=
self
.
data
.
len
(
)
)
;
let
scaled_slice
=
unsafe
{
self
.
data
.
get_unchecked
(
0
.
.
self
.
len
(
)
)
}
;
self
.
binary_search_with_index_impl
(
predicate
scaled_slice
)
}
#
[
inline
]
pub
fn
binary_search_in_range_with_index
(
&
self
predicate
:
impl
FnMut
(
usize
)
-
>
Ordering
range
:
Range
<
usize
>
)
-
>
Option
<
Result
<
usize
usize
>
>
{
if
range
.
start
>
self
.
len
(
)
|
|
range
.
end
>
self
.
len
(
)
{
return
None
;
}
let
scaled_slice
=
self
.
data
.
get
(
range
)
?
;
Some
(
self
.
binary_search_with_index_impl
(
predicate
scaled_slice
)
)
}
#
[
inline
]
fn
binary_search_impl
(
&
self
mut
predicate
:
impl
FnMut
(
usize
)
-
>
Ordering
scaled_slice
:
&
[
u8
]
)
-
>
Result
<
usize
usize
>
{
self
.
binary_search_with_index_impl
(
|
index
|
{
let
actual_probe
=
unsafe
{
self
.
get_unchecked
(
index
)
}
;
predicate
(
actual_probe
)
}
scaled_slice
)
}
fn
binary_search_with_index_impl
(
&
self
mut
predicate
:
impl
FnMut
(
usize
)
-
>
Ordering
scaled_slice
:
&
[
u8
]
)
-
>
Result
<
usize
usize
>
{
let
zero_index
=
self
.
data
.
as_ptr
(
)
as
*
const
_
as
usize
;
scaled_slice
.
binary_search_by
(
|
probe
:
&
_
|
{
let
index
=
probe
as
*
const
_
as
usize
-
zero_index
;
predicate
(
index
)
}
)
}
}
#
[
inline
]
pub
(
crate
)
fn
get_item_width
(
item_bytes
:
&
[
u8
;
USIZE_WIDTH
]
)
-
>
usize
{
USIZE_WIDTH
-
item_bytes
.
iter
(
)
.
rev
(
)
.
take_while
(
|
b
|
*
*
b
=
=
0
)
.
count
(
)
}
pub
(
crate
)
struct
InsertInfo
{
pub
item_bytes
:
[
u8
;
USIZE_WIDTH
]
pub
new_width
:
usize
pub
new_count
:
usize
pub
new_bytes_len
:
usize
}
impl
FlexZeroSlice
{
pub
(
crate
)
fn
get_insert_info
(
&
self
new_item
:
usize
)
-
>
InsertInfo
{
let
item_bytes
=
new_item
.
to_le_bytes
(
)
;
let
item_width
=
get_item_width
(
&
item_bytes
)
;
let
old_width
=
self
.
get_width
(
)
;
let
new_width
=
core
:
:
cmp
:
:
max
(
old_width
item_width
)
;
let
new_count
=
1
+
(
self
.
data
.
len
(
)
/
old_width
)
;
#
[
allow
(
clippy
:
:
unwrap_used
)
]
let
new_bytes_len
=
new_count
.
checked_mul
(
new_width
)
.
unwrap
(
)
.
checked_add
(
1
)
.
unwrap
(
)
;
InsertInfo
{
item_bytes
new_width
new_count
new_bytes_len
}
}
pub
(
crate
)
fn
insert_impl
(
&
mut
self
insert_info
:
InsertInfo
insert_index
:
usize
)
{
let
InsertInfo
{
item_bytes
new_width
new_count
new_bytes_len
}
=
insert_info
;
debug_assert
!
(
new_width
<
=
USIZE_WIDTH
)
;
debug_assert
!
(
new_width
>
=
self
.
get_width
(
)
)
;
debug_assert
!
(
insert_index
<
new_count
)
;
debug_assert_eq
!
(
new_bytes_len
new_count
*
new_width
+
1
)
;
debug_assert_eq
!
(
new_bytes_len
self
.
data
.
len
(
)
+
1
)
;
let
lower_i
=
if
new_width
=
=
self
.
get_width
(
)
{
insert_index
}
else
{
0
}
;
for
i
in
(
lower_i
.
.
new_count
)
.
rev
(
)
{
let
bytes_to_write
=
if
i
=
=
insert_index
{
item_bytes
}
else
{
let
j
=
if
i
>
insert_index
{
i
-
1
}
else
{
i
}
;
debug_assert
!
(
j
<
new_count
-
1
)
;
unsafe
{
self
.
get_unchecked
(
j
)
.
to_le_bytes
(
)
}
}
;
unsafe
{
core
:
:
ptr
:
:
copy_nonoverlapping
(
bytes_to_write
.
as_ptr
(
)
self
.
data
.
as_mut_ptr
(
)
.
add
(
new_width
*
i
)
new_width
)
;
}
}
self
.
width
=
new_width
as
u8
;
}
}
pub
(
crate
)
struct
RemoveInfo
{
pub
remove_index
:
usize
pub
new_width
:
usize
pub
new_count
:
usize
pub
new_bytes_len
:
usize
}
impl
FlexZeroSlice
{
pub
(
crate
)
fn
get_remove_info
(
&
self
remove_index
:
usize
)
-
>
RemoveInfo
{
debug_assert
!
(
remove_index
<
self
.
len
(
)
)
;
let
item_bytes
=
unsafe
{
self
.
get_unchecked
(
remove_index
)
.
to_le_bytes
(
)
}
;
let
item_width
=
get_item_width
(
&
item_bytes
)
;
let
old_width
=
self
.
get_width
(
)
;
let
old_count
=
self
.
data
.
len
(
)
/
old_width
;
let
new_width
=
if
item_width
<
old_width
{
old_width
}
else
{
debug_assert_eq
!
(
old_width
item_width
)
;
let
mut
largest_width
=
1
;
for
i
in
0
.
.
old_count
{
if
i
=
=
remove_index
{
continue
;
}
let
curr_bytes
=
unsafe
{
self
.
get_unchecked
(
i
)
.
to_le_bytes
(
)
}
;
let
curr_width
=
get_item_width
(
&
curr_bytes
)
;
largest_width
=
core
:
:
cmp
:
:
max
(
curr_width
largest_width
)
;
}
largest_width
}
;
let
new_count
=
old_count
-
1
;
let
new_bytes_len
=
new_count
*
new_width
+
1
;
RemoveInfo
{
remove_index
new_width
new_count
new_bytes_len
}
}
pub
(
crate
)
fn
get_sorted_pop_info
(
&
self
)
-
>
RemoveInfo
{
debug_assert
!
(
!
self
.
is_empty
(
)
)
;
let
remove_index
=
self
.
len
(
)
-
1
;
let
old_count
=
self
.
len
(
)
;
let
new_width
=
if
old_count
=
=
1
{
1
}
else
{
let
largest_item
=
unsafe
{
self
.
get_unchecked
(
remove_index
-
1
)
.
to_le_bytes
(
)
}
;
get_item_width
(
&
largest_item
)
}
;
let
new_count
=
old_count
-
1
;
let
new_bytes_len
=
new_count
*
new_width
+
1
;
RemoveInfo
{
remove_index
new_width
new_count
new_bytes_len
}
}
pub
(
crate
)
fn
remove_impl
(
&
mut
self
remove_info
:
RemoveInfo
)
{
let
RemoveInfo
{
remove_index
new_width
new_count
.
.
}
=
remove_info
;
debug_assert
!
(
new_width
<
=
self
.
get_width
(
)
)
;
debug_assert
!
(
new_count
<
self
.
len
(
)
)
;
let
lower_i
=
if
new_width
=
=
self
.
get_width
(
)
{
remove_index
}
else
{
0
}
;
for
i
in
lower_i
.
.
new_count
{
let
j
=
if
i
<
remove_index
{
i
}
else
{
i
+
1
}
;
let
bytes_to_write
=
unsafe
{
self
.
get_unchecked
(
j
)
.
to_le_bytes
(
)
}
;
unsafe
{
core
:
:
ptr
:
:
copy_nonoverlapping
(
bytes_to_write
.
as_ptr
(
)
self
.
data
.
as_mut_ptr
(
)
.
add
(
new_width
*
i
)
new_width
)
;
}
}
self
.
width
=
new_width
as
u8
;
}
}
