use
{
ast
attr
}
;
use
syntax_pos
:
:
{
Span
DUMMY_SP
}
;
use
ext
:
:
base
:
:
{
DummyResult
ExtCtxt
MacEager
MacResult
SyntaxExtension
}
;
use
ext
:
:
base
:
:
{
IdentMacroExpander
NormalTT
TTMacroExpander
}
;
use
ext
:
:
expand
:
:
{
Expansion
ExpansionKind
}
;
use
ext
:
:
placeholders
;
use
ext
:
:
tt
:
:
macro_parser
:
:
{
Success
Error
Failure
}
;
use
ext
:
:
tt
:
:
macro_parser
:
:
{
MatchedSeq
MatchedNonterminal
}
;
use
ext
:
:
tt
:
:
macro_parser
:
:
{
parse
parse_failure_msg
}
;
use
parse
:
:
ParseSess
;
use
parse
:
:
lexer
:
:
new_tt_reader
;
use
parse
:
:
parser
:
:
{
Parser
Restrictions
}
;
use
parse
:
:
token
:
:
{
self
gensym_ident
NtTT
Token
}
;
use
parse
:
:
token
:
:
Token
:
:
*
;
use
print
;
use
tokenstream
:
:
{
self
TokenTree
}
;
use
std
:
:
collections
:
:
{
HashMap
}
;
use
std
:
:
collections
:
:
hash_map
:
:
{
Entry
}
;
use
std
:
:
rc
:
:
Rc
;
pub
struct
ParserAnyMacro
<
'
a
>
{
parser
:
Parser
<
'
a
>
site_span
:
Span
macro_ident
:
ast
:
:
Ident
}
impl
<
'
a
>
ParserAnyMacro
<
'
a
>
{
pub
fn
make
(
mut
self
:
Box
<
ParserAnyMacro
<
'
a
>
>
kind
:
ExpansionKind
)
-
>
Expansion
{
let
ParserAnyMacro
{
site_span
macro_ident
ref
mut
parser
}
=
*
self
;
let
expansion
=
panictry
!
(
parser
.
parse_expansion
(
kind
true
)
)
;
if
kind
=
=
ExpansionKind
:
:
Expr
&
&
parser
.
token
=
=
token
:
:
Semi
{
parser
.
bump
(
)
;
}
parser
.
ensure_complete_parse
(
macro_ident
.
name
kind
.
name
(
)
site_span
)
;
expansion
}
}
struct
MacroRulesMacroExpander
{
name
:
ast
:
:
Ident
lhses
:
Vec
<
TokenTree
>
rhses
:
Vec
<
TokenTree
>
valid
:
bool
}
impl
TTMacroExpander
for
MacroRulesMacroExpander
{
fn
expand
<
'
cx
>
(
&
self
cx
:
&
'
cx
mut
ExtCtxt
sp
:
Span
arg
:
&
[
TokenTree
]
)
-
>
Box
<
MacResult
+
'
cx
>
{
if
!
self
.
valid
{
return
DummyResult
:
:
any
(
sp
)
;
}
generic_extension
(
cx
sp
self
.
name
arg
&
self
.
lhses
&
self
.
rhses
)
}
}
fn
generic_extension
<
'
cx
>
(
cx
:
&
'
cx
ExtCtxt
sp
:
Span
name
:
ast
:
:
Ident
arg
:
&
[
TokenTree
]
lhses
:
&
[
TokenTree
]
rhses
:
&
[
TokenTree
]
)
-
>
Box
<
MacResult
+
'
cx
>
{
if
cx
.
trace_macros
(
)
{
println
!
(
"
{
}
!
{
{
{
}
}
}
"
name
print
:
:
pprust
:
:
tts_to_string
(
arg
)
)
;
}
let
mut
best_fail_spot
=
DUMMY_SP
;
let
mut
best_fail_tok
=
None
;
for
(
i
lhs
)
in
lhses
.
iter
(
)
.
enumerate
(
)
{
let
lhs_tt
=
match
*
lhs
{
TokenTree
:
:
Delimited
(
_
ref
delim
)
=
>
&
delim
.
tts
[
.
.
]
_
=
>
cx
.
span_bug
(
sp
"
malformed
macro
lhs
"
)
}
;
match
TokenTree
:
:
parse
(
cx
lhs_tt
arg
)
{
Success
(
named_matches
)
=
>
{
let
rhs
=
match
rhses
[
i
]
{
TokenTree
:
:
Delimited
(
_
ref
delimed
)
=
>
delimed
.
tts
.
clone
(
)
_
=
>
cx
.
span_bug
(
sp
"
malformed
macro
rhs
"
)
}
;
let
trncbr
=
new_tt_reader
(
&
cx
.
parse_sess
.
span_diagnostic
Some
(
named_matches
)
rhs
)
;
let
mut
p
=
Parser
:
:
new
(
cx
.
parse_sess
(
)
Box
:
:
new
(
trncbr
)
)
;
p
.
directory
=
cx
.
current_expansion
.
module
.
directory
.
clone
(
)
;
p
.
restrictions
=
match
cx
.
current_expansion
.
no_noninline_mod
{
true
=
>
Restrictions
:
:
no_noninline_mod
(
)
false
=
>
Restrictions
:
:
empty
(
)
}
;
p
.
check_unknown_macro_variable
(
)
;
return
Box
:
:
new
(
ParserAnyMacro
{
parser
:
p
site_span
:
sp
macro_ident
:
name
}
)
}
Failure
(
sp
tok
)
=
>
if
sp
.
lo
>
=
best_fail_spot
.
lo
{
best_fail_spot
=
sp
;
best_fail_tok
=
Some
(
tok
)
;
}
Error
(
err_sp
ref
msg
)
=
>
{
cx
.
span_fatal
(
err_sp
.
substitute_dummy
(
sp
)
&
msg
[
.
.
]
)
}
}
}
let
best_fail_msg
=
parse_failure_msg
(
best_fail_tok
.
expect
(
"
ran
no
matchers
"
)
)
;
cx
.
span_fatal
(
best_fail_spot
.
substitute_dummy
(
sp
)
&
best_fail_msg
)
;
}
pub
struct
MacroRulesExpander
;
impl
IdentMacroExpander
for
MacroRulesExpander
{
fn
expand
(
&
self
cx
:
&
mut
ExtCtxt
span
:
Span
ident
:
ast
:
:
Ident
tts
:
Vec
<
tokenstream
:
:
TokenTree
>
attrs
:
Vec
<
ast
:
:
Attribute
>
)
-
>
Box
<
MacResult
>
{
let
export
=
attr
:
:
contains_name
(
&
attrs
"
macro_export
"
)
;
let
def
=
ast
:
:
MacroDef
{
ident
:
ident
id
:
ast
:
:
DUMMY_NODE_ID
span
:
span
imported_from
:
None
body
:
tts
allow_internal_unstable
:
attr
:
:
contains_name
(
&
attrs
"
allow_internal_unstable
"
)
attrs
:
attrs
}
;
let
result
=
if
cx
.
ecfg
.
keep_macs
{
MacEager
:
:
items
(
placeholders
:
:
reconstructed_macro_rules
(
&
def
)
.
make_items
(
)
)
}
else
{
MacEager
:
:
items
(
placeholders
:
:
macro_scope_placeholder
(
)
.
make_items
(
)
)
}
;
cx
.
resolver
.
add_macro
(
cx
.
current_expansion
.
mark
def
export
)
;
result
}
}
pub
fn
compile
(
sess
:
&
ParseSess
def
:
&
ast
:
:
MacroDef
)
-
>
SyntaxExtension
{
let
lhs_nm
=
gensym_ident
(
"
lhs
"
)
;
let
rhs_nm
=
gensym_ident
(
"
rhs
"
)
;
let
match_lhs_tok
=
MatchNt
(
lhs_nm
token
:
:
str_to_ident
(
"
tt
"
)
)
;
let
match_rhs_tok
=
MatchNt
(
rhs_nm
token
:
:
str_to_ident
(
"
tt
"
)
)
;
let
argument_gram
=
vec
!
[
TokenTree
:
:
Sequence
(
DUMMY_SP
Rc
:
:
new
(
tokenstream
:
:
SequenceRepetition
{
tts
:
vec
!
[
TokenTree
:
:
Token
(
DUMMY_SP
match_lhs_tok
)
TokenTree
:
:
Token
(
DUMMY_SP
token
:
:
FatArrow
)
TokenTree
:
:
Token
(
DUMMY_SP
match_rhs_tok
)
]
separator
:
Some
(
token
:
:
Semi
)
op
:
tokenstream
:
:
KleeneOp
:
:
OneOrMore
num_captures
:
2
}
)
)
/
/
to
phase
into
semicolon
-
termination
instead
of
semicolon
-
separation
TokenTree
:
:
Sequence
(
DUMMY_SP
Rc
:
:
new
(
tokenstream
:
:
SequenceRepetition
{
tts
:
vec
!
[
TokenTree
:
:
Token
(
DUMMY_SP
token
:
:
Semi
)
]
separator
:
None
op
:
tokenstream
:
:
KleeneOp
:
:
ZeroOrMore
num_captures
:
0
}
)
)
]
;
let
arg_reader
=
new_tt_reader
(
&
sess
.
span_diagnostic
None
def
.
body
.
clone
(
)
)
;
let
argument_map
=
match
parse
(
sess
arg_reader
&
argument_gram
)
{
Success
(
m
)
=
>
m
Failure
(
sp
tok
)
=
>
{
let
s
=
parse_failure_msg
(
tok
)
;
panic
!
(
sess
.
span_diagnostic
.
span_fatal
(
sp
.
substitute_dummy
(
def
.
span
)
&
s
)
)
;
}
Error
(
sp
s
)
=
>
{
panic
!
(
sess
.
span_diagnostic
.
span_fatal
(
sp
.
substitute_dummy
(
def
.
span
)
&
s
)
)
;
}
}
;
let
mut
valid
=
true
;
let
lhses
=
match
*
*
argument_map
.
get
(
&
lhs_nm
)
.
unwrap
(
)
{
MatchedSeq
(
ref
s
_
)
=
>
{
s
.
iter
(
)
.
map
(
|
m
|
match
*
*
m
{
MatchedNonterminal
(
NtTT
(
ref
tt
)
)
=
>
{
valid
&
=
check_lhs_nt_follows
(
sess
tt
)
;
(
*
*
tt
)
.
clone
(
)
}
_
=
>
sess
.
span_diagnostic
.
span_bug
(
def
.
span
"
wrong
-
structured
lhs
"
)
}
)
.
collect
:
:
<
Vec
<
TokenTree
>
>
(
)
}
_
=
>
sess
.
span_diagnostic
.
span_bug
(
def
.
span
"
wrong
-
structured
lhs
"
)
}
;
let
rhses
=
match
*
*
argument_map
.
get
(
&
rhs_nm
)
.
unwrap
(
)
{
MatchedSeq
(
ref
s
_
)
=
>
{
s
.
iter
(
)
.
map
(
|
m
|
match
*
*
m
{
MatchedNonterminal
(
NtTT
(
ref
tt
)
)
=
>
(
*
*
tt
)
.
clone
(
)
_
=
>
sess
.
span_diagnostic
.
span_bug
(
def
.
span
"
wrong
-
structured
rhs
"
)
}
)
.
collect
(
)
}
_
=
>
sess
.
span_diagnostic
.
span_bug
(
def
.
span
"
wrong
-
structured
rhs
"
)
}
;
for
rhs
in
&
rhses
{
valid
&
=
check_rhs
(
sess
rhs
)
;
}
for
lhs
in
&
lhses
{
valid
&
=
check_lhs_no_empty_seq
(
sess
&
[
lhs
.
clone
(
)
]
)
}
let
exp
:
Box
<
_
>
=
Box
:
:
new
(
MacroRulesMacroExpander
{
name
:
def
.
ident
lhses
:
lhses
rhses
:
rhses
valid
:
valid
}
)
;
NormalTT
(
exp
Some
(
def
.
span
)
def
.
allow_internal_unstable
)
}
fn
check_lhs_nt_follows
(
sess
:
&
ParseSess
lhs
:
&
TokenTree
)
-
>
bool
{
match
lhs
{
&
TokenTree
:
:
Delimited
(
_
ref
tts
)
=
>
check_matcher
(
sess
&
tts
.
tts
)
_
=
>
{
let
msg
=
"
invalid
macro
matcher
;
matchers
must
be
contained
in
balanced
delimiters
"
;
sess
.
span_diagnostic
.
span_err
(
lhs
.
get_span
(
)
msg
)
;
false
}
}
}
fn
check_lhs_no_empty_seq
(
sess
:
&
ParseSess
tts
:
&
[
TokenTree
]
)
-
>
bool
{
for
tt
in
tts
{
match
*
tt
{
TokenTree
:
:
Token
(
_
_
)
=
>
(
)
TokenTree
:
:
Delimited
(
_
ref
del
)
=
>
if
!
check_lhs_no_empty_seq
(
sess
&
del
.
tts
)
{
return
false
;
}
TokenTree
:
:
Sequence
(
span
ref
seq
)
=
>
{
if
seq
.
separator
.
is_none
(
)
{
if
seq
.
tts
.
iter
(
)
.
all
(
|
seq_tt
|
{
match
*
seq_tt
{
TokenTree
:
:
Sequence
(
_
ref
sub_seq
)
=
>
sub_seq
.
op
=
=
tokenstream
:
:
KleeneOp
:
:
ZeroOrMore
_
=
>
false
}
}
)
{
sess
.
span_diagnostic
.
span_err
(
span
"
repetition
matches
empty
token
tree
"
)
;
return
false
;
}
}
if
!
check_lhs_no_empty_seq
(
sess
&
seq
.
tts
)
{
return
false
;
}
}
}
}
true
}
fn
check_rhs
(
sess
:
&
ParseSess
rhs
:
&
TokenTree
)
-
>
bool
{
match
*
rhs
{
TokenTree
:
:
Delimited
(
.
.
)
=
>
return
true
_
=
>
sess
.
span_diagnostic
.
span_err
(
rhs
.
get_span
(
)
"
macro
rhs
must
be
delimited
"
)
}
false
}
fn
check_matcher
(
sess
:
&
ParseSess
matcher
:
&
[
TokenTree
]
)
-
>
bool
{
let
first_sets
=
FirstSets
:
:
new
(
matcher
)
;
let
empty_suffix
=
TokenSet
:
:
empty
(
)
;
let
err
=
sess
.
span_diagnostic
.
err_count
(
)
;
check_matcher_core
(
sess
&
first_sets
matcher
&
empty_suffix
)
;
err
=
=
sess
.
span_diagnostic
.
err_count
(
)
}
struct
FirstSets
{
first
:
HashMap
<
Span
Option
<
TokenSet
>
>
}
impl
FirstSets
{
fn
new
(
tts
:
&
[
TokenTree
]
)
-
>
FirstSets
{
let
mut
sets
=
FirstSets
{
first
:
HashMap
:
:
new
(
)
}
;
build_recur
(
&
mut
sets
tts
)
;
return
sets
;
fn
build_recur
(
sets
:
&
mut
FirstSets
tts
:
&
[
TokenTree
]
)
-
>
TokenSet
{
let
mut
first
=
TokenSet
:
:
empty
(
)
;
for
tt
in
tts
.
iter
(
)
.
rev
(
)
{
match
*
tt
{
TokenTree
:
:
Token
(
sp
ref
tok
)
=
>
{
first
.
replace_with
(
(
sp
tok
.
clone
(
)
)
)
;
}
TokenTree
:
:
Delimited
(
_
ref
delimited
)
=
>
{
build_recur
(
sets
&
delimited
.
tts
[
.
.
]
)
;
first
.
replace_with
(
(
delimited
.
open_span
Token
:
:
OpenDelim
(
delimited
.
delim
)
)
)
;
}
TokenTree
:
:
Sequence
(
sp
ref
seq_rep
)
=
>
{
let
subfirst
=
build_recur
(
sets
&
seq_rep
.
tts
[
.
.
]
)
;
match
sets
.
first
.
entry
(
sp
)
{
Entry
:
:
Vacant
(
vac
)
=
>
{
vac
.
insert
(
Some
(
subfirst
.
clone
(
)
)
)
;
}
Entry
:
:
Occupied
(
mut
occ
)
=
>
{
occ
.
insert
(
None
)
;
}
}
if
let
(
Some
(
ref
sep
)
true
)
=
(
seq_rep
.
separator
.
clone
(
)
subfirst
.
maybe_empty
)
{
first
.
add_one_maybe
(
(
sp
sep
.
clone
(
)
)
)
;
}
if
subfirst
.
maybe_empty
|
|
seq_rep
.
op
=
=
tokenstream
:
:
KleeneOp
:
:
ZeroOrMore
{
first
.
add_all
(
&
TokenSet
{
maybe_empty
:
true
.
.
subfirst
}
)
;
}
else
{
first
=
subfirst
;
}
}
}
}
return
first
;
}
}
fn
first
(
&
self
tts
:
&
[
TokenTree
]
)
-
>
TokenSet
{
let
mut
first
=
TokenSet
:
:
empty
(
)
;
for
tt
in
tts
.
iter
(
)
{
assert
!
(
first
.
maybe_empty
)
;
match
*
tt
{
TokenTree
:
:
Token
(
sp
ref
tok
)
=
>
{
first
.
add_one
(
(
sp
tok
.
clone
(
)
)
)
;
return
first
;
}
TokenTree
:
:
Delimited
(
_
ref
delimited
)
=
>
{
first
.
add_one
(
(
delimited
.
open_span
Token
:
:
OpenDelim
(
delimited
.
delim
)
)
)
;
return
first
;
}
TokenTree
:
:
Sequence
(
sp
ref
seq_rep
)
=
>
{
match
self
.
first
.
get
(
&
sp
)
{
Some
(
&
Some
(
ref
subfirst
)
)
=
>
{
if
let
(
Some
(
ref
sep
)
true
)
=
(
seq_rep
.
separator
.
clone
(
)
subfirst
.
maybe_empty
)
{
first
.
add_one_maybe
(
(
sp
sep
.
clone
(
)
)
)
;
}
assert
!
(
first
.
maybe_empty
)
;
first
.
add_all
(
subfirst
)
;
if
subfirst
.
maybe_empty
|
|
seq_rep
.
op
=
=
tokenstream
:
:
KleeneOp
:
:
ZeroOrMore
{
first
.
maybe_empty
=
true
;
continue
;
}
else
{
return
first
;
}
}
Some
(
&
None
)
=
>
{
panic
!
(
"
assume
all
sequences
have
(
unique
)
spans
for
now
"
)
;
}
None
=
>
{
panic
!
(
"
We
missed
a
sequence
during
FirstSets
construction
"
)
;
}
}
}
}
}
assert
!
(
first
.
maybe_empty
)
;
return
first
;
}
}
#
[
derive
(
Clone
Debug
)
]
struct
TokenSet
{
tokens
:
Vec
<
(
Span
Token
)
>
maybe_empty
:
bool
}
impl
TokenSet
{
fn
empty
(
)
-
>
Self
{
TokenSet
{
tokens
:
Vec
:
:
new
(
)
maybe_empty
:
true
}
}
fn
singleton
(
tok
:
(
Span
Token
)
)
-
>
Self
{
TokenSet
{
tokens
:
vec
!
[
tok
]
maybe_empty
:
false
}
}
fn
replace_with
(
&
mut
self
tok
:
(
Span
Token
)
)
{
self
.
tokens
.
clear
(
)
;
self
.
tokens
.
push
(
tok
)
;
self
.
maybe_empty
=
false
;
}
fn
replace_with_irrelevant
(
&
mut
self
)
{
self
.
tokens
.
clear
(
)
;
self
.
maybe_empty
=
false
;
}
fn
add_one
(
&
mut
self
tok
:
(
Span
Token
)
)
{
if
!
self
.
tokens
.
contains
(
&
tok
)
{
self
.
tokens
.
push
(
tok
)
;
}
self
.
maybe_empty
=
false
;
}
fn
add_one_maybe
(
&
mut
self
tok
:
(
Span
Token
)
)
{
if
!
self
.
tokens
.
contains
(
&
tok
)
{
self
.
tokens
.
push
(
tok
)
;
}
}
fn
add_all
(
&
mut
self
other
:
&
Self
)
{
for
tok
in
&
other
.
tokens
{
if
!
self
.
tokens
.
contains
(
tok
)
{
self
.
tokens
.
push
(
tok
.
clone
(
)
)
;
}
}
if
!
other
.
maybe_empty
{
self
.
maybe_empty
=
false
;
}
}
}
fn
check_matcher_core
(
sess
:
&
ParseSess
first_sets
:
&
FirstSets
matcher
:
&
[
TokenTree
]
follow
:
&
TokenSet
)
-
>
TokenSet
{
use
print
:
:
pprust
:
:
token_to_string
;
let
mut
last
=
TokenSet
:
:
empty
(
)
;
'
each_token
:
for
i
in
0
.
.
matcher
.
len
(
)
{
let
token
=
&
matcher
[
i
]
;
let
suffix
=
&
matcher
[
i
+
1
.
.
]
;
let
build_suffix_first
=
|
|
{
let
mut
s
=
first_sets
.
first
(
suffix
)
;
if
s
.
maybe_empty
{
s
.
add_all
(
follow
)
;
}
return
s
;
}
;
let
suffix_first
;
match
*
token
{
TokenTree
:
:
Token
(
sp
ref
tok
)
=
>
{
let
can_be_followed_by_any
;
if
let
Err
(
bad_frag
)
=
has_legal_fragment_specifier
(
tok
)
{
let
msg
=
format
!
(
"
invalid
fragment
specifier
{
}
"
bad_frag
)
;
sess
.
span_diagnostic
.
struct_span_err
(
sp
&
msg
)
.
help
(
"
valid
fragment
specifiers
are
ident
block
\
stmt
expr
pat
ty
path
meta
tt
\
and
item
"
)
.
emit
(
)
;
can_be_followed_by_any
=
true
;
}
else
{
can_be_followed_by_any
=
token_can_be_followed_by_any
(
tok
)
;
}
if
can_be_followed_by_any
{
last
.
replace_with_irrelevant
(
)
;
continue
'
each_token
;
}
else
{
last
.
replace_with
(
(
sp
tok
.
clone
(
)
)
)
;
suffix_first
=
build_suffix_first
(
)
;
}
}
TokenTree
:
:
Delimited
(
_
ref
d
)
=
>
{
let
my_suffix
=
TokenSet
:
:
singleton
(
(
d
.
close_span
Token
:
:
CloseDelim
(
d
.
delim
)
)
)
;
check_matcher_core
(
sess
first_sets
&
d
.
tts
&
my_suffix
)
;
last
.
replace_with_irrelevant
(
)
;
continue
'
each_token
;
}
TokenTree
:
:
Sequence
(
sp
ref
seq_rep
)
=
>
{
suffix_first
=
build_suffix_first
(
)
;
let
mut
new
;
let
my_suffix
=
if
let
Some
(
ref
u
)
=
seq_rep
.
separator
{
new
=
suffix_first
.
clone
(
)
;
new
.
add_one_maybe
(
(
sp
u
.
clone
(
)
)
)
;
&
new
}
else
{
&
suffix_first
}
;
let
next
=
check_matcher_core
(
sess
first_sets
&
seq_rep
.
tts
my_suffix
)
;
if
next
.
maybe_empty
{
last
.
add_all
(
&
next
)
;
}
else
{
last
=
next
;
}
continue
'
each_token
;
}
}
'
each_last
:
for
&
(
_sp
ref
t
)
in
&
last
.
tokens
{
if
let
MatchNt
(
ref
name
ref
frag_spec
)
=
*
t
{
for
&
(
sp
ref
next_token
)
in
&
suffix_first
.
tokens
{
match
is_in_follow
(
next_token
&
frag_spec
.
name
.
as_str
(
)
)
{
Err
(
(
msg
help
)
)
=
>
{
sess
.
span_diagnostic
.
struct_span_err
(
sp
&
msg
)
.
help
(
help
)
.
emit
(
)
;
continue
'
each_last
;
}
Ok
(
true
)
=
>
{
}
Ok
(
false
)
=
>
{
let
may_be
=
if
last
.
tokens
.
len
(
)
=
=
1
&
&
suffix_first
.
tokens
.
len
(
)
=
=
1
{
"
is
"
}
else
{
"
may
be
"
}
;
sess
.
span_diagnostic
.
span_err
(
sp
&
format
!
(
"
{
name
}
:
{
frag
}
{
may_be
}
followed
by
{
next
}
which
\
is
not
allowed
for
{
frag
}
fragments
"
name
=
name
frag
=
frag_spec
next
=
token_to_string
(
next_token
)
may_be
=
may_be
)
)
;
}
}
}
}
}
}
last
}
fn
token_can_be_followed_by_any
(
tok
:
&
Token
)
-
>
bool
{
if
let
&
MatchNt
(
_
ref
frag_spec
)
=
tok
{
frag_can_be_followed_by_any
(
&
frag_spec
.
name
.
as_str
(
)
)
}
else
{
true
}
}
fn
frag_can_be_followed_by_any
(
frag
:
&
str
)
-
>
bool
{
match
frag
{
"
item
"
|
"
block
"
|
"
ident
"
|
"
meta
"
|
"
tt
"
=
>
true
_
=
>
false
}
}
fn
is_in_follow
(
tok
:
&
Token
frag
:
&
str
)
-
>
Result
<
bool
(
String
&
'
static
str
)
>
{
if
let
&
CloseDelim
(
_
)
=
tok
{
Ok
(
true
)
}
else
{
match
frag
{
"
item
"
=
>
{
Ok
(
true
)
}
"
block
"
=
>
{
Ok
(
true
)
}
"
stmt
"
|
"
expr
"
=
>
{
match
*
tok
{
FatArrow
|
Comma
|
Semi
=
>
Ok
(
true
)
_
=
>
Ok
(
false
)
}
}
"
pat
"
=
>
{
match
*
tok
{
FatArrow
|
Comma
|
Eq
|
BinOp
(
token
:
:
Or
)
=
>
Ok
(
true
)
Ident
(
i
)
if
(
i
.
name
.
as_str
(
)
=
=
"
if
"
|
|
i
.
name
.
as_str
(
)
=
=
"
in
"
)
=
>
Ok
(
true
)
_
=
>
Ok
(
false
)
}
}
"
path
"
|
"
ty
"
=
>
{
match
*
tok
{
OpenDelim
(
token
:
:
DelimToken
:
:
Brace
)
|
OpenDelim
(
token
:
:
DelimToken
:
:
Bracket
)
|
Comma
|
FatArrow
|
Colon
|
Eq
|
Gt
|
Semi
|
BinOp
(
token
:
:
Or
)
=
>
Ok
(
true
)
MatchNt
(
_
ref
frag
)
if
frag
.
name
.
as_str
(
)
=
=
"
block
"
=
>
Ok
(
true
)
Ident
(
i
)
if
i
.
name
.
as_str
(
)
=
=
"
as
"
|
|
i
.
name
.
as_str
(
)
=
=
"
where
"
=
>
Ok
(
true
)
_
=
>
Ok
(
false
)
}
}
"
ident
"
=
>
{
Ok
(
true
)
}
"
meta
"
|
"
tt
"
=
>
{
Ok
(
true
)
}
_
=
>
Err
(
(
format
!
(
"
invalid
fragment
specifier
{
}
"
frag
)
"
valid
fragment
specifiers
are
ident
block
\
stmt
expr
pat
ty
path
meta
tt
\
and
item
"
)
)
}
}
}
fn
has_legal_fragment_specifier
(
tok
:
&
Token
)
-
>
Result
<
(
)
String
>
{
debug
!
(
"
has_legal_fragment_specifier
(
{
:
?
}
)
"
tok
)
;
if
let
&
MatchNt
(
_
ref
frag_spec
)
=
tok
{
let
s
=
&
frag_spec
.
name
.
as_str
(
)
;
if
!
is_legal_fragment_specifier
(
s
)
{
return
Err
(
s
.
to_string
(
)
)
;
}
}
Ok
(
(
)
)
}
fn
is_legal_fragment_specifier
(
frag
:
&
str
)
-
>
bool
{
match
frag
{
"
item
"
|
"
block
"
|
"
stmt
"
|
"
expr
"
|
"
pat
"
|
"
path
"
|
"
ty
"
|
"
ident
"
|
"
meta
"
|
"
tt
"
=
>
true
_
=
>
false
}
}
