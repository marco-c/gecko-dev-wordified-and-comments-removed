use
crate
:
:
thread_parker
:
:
{
ThreadParker
ThreadParkerT
UnparkHandleT
}
;
use
crate
:
:
util
:
:
UncheckedOptionExt
;
use
crate
:
:
word_lock
:
:
WordLock
;
use
core
:
:
{
cell
:
:
{
Cell
UnsafeCell
}
ptr
sync
:
:
atomic
:
:
{
AtomicPtr
AtomicUsize
Ordering
}
}
;
use
instant
:
:
Instant
;
use
smallvec
:
:
SmallVec
;
use
std
:
:
time
:
:
Duration
;
static
NUM_THREADS
:
AtomicUsize
=
AtomicUsize
:
:
new
(
0
)
;
static
HASHTABLE
:
AtomicPtr
<
HashTable
>
=
AtomicPtr
:
:
new
(
ptr
:
:
null_mut
(
)
)
;
const
LOAD_FACTOR
:
usize
=
3
;
struct
HashTable
{
entries
:
Box
<
[
Bucket
]
>
hash_bits
:
u32
_prev
:
*
const
HashTable
}
impl
HashTable
{
#
[
inline
]
fn
new
(
num_threads
:
usize
prev
:
*
const
HashTable
)
-
>
Box
<
HashTable
>
{
let
new_size
=
(
num_threads
*
LOAD_FACTOR
)
.
next_power_of_two
(
)
;
let
hash_bits
=
0usize
.
leading_zeros
(
)
-
new_size
.
leading_zeros
(
)
-
1
;
let
now
=
Instant
:
:
now
(
)
;
let
mut
entries
=
Vec
:
:
with_capacity
(
new_size
)
;
for
i
in
0
.
.
new_size
{
entries
.
push
(
Bucket
:
:
new
(
now
i
as
u32
+
1
)
)
;
}
Box
:
:
new
(
HashTable
{
entries
:
entries
.
into_boxed_slice
(
)
hash_bits
_prev
:
prev
}
)
}
}
#
[
repr
(
align
(
64
)
)
]
struct
Bucket
{
mutex
:
WordLock
queue_head
:
Cell
<
*
const
ThreadData
>
queue_tail
:
Cell
<
*
const
ThreadData
>
fair_timeout
:
UnsafeCell
<
FairTimeout
>
}
impl
Bucket
{
#
[
inline
]
pub
fn
new
(
timeout
:
Instant
seed
:
u32
)
-
>
Self
{
Self
{
mutex
:
WordLock
:
:
new
(
)
queue_head
:
Cell
:
:
new
(
ptr
:
:
null
(
)
)
queue_tail
:
Cell
:
:
new
(
ptr
:
:
null
(
)
)
fair_timeout
:
UnsafeCell
:
:
new
(
FairTimeout
:
:
new
(
timeout
seed
)
)
}
}
}
struct
FairTimeout
{
timeout
:
Instant
seed
:
u32
}
impl
FairTimeout
{
#
[
inline
]
fn
new
(
timeout
:
Instant
seed
:
u32
)
-
>
FairTimeout
{
FairTimeout
{
timeout
seed
}
}
#
[
inline
]
fn
should_timeout
(
&
mut
self
)
-
>
bool
{
let
now
=
Instant
:
:
now
(
)
;
if
now
>
self
.
timeout
{
let
nanos
=
self
.
gen_u32
(
)
%
1_000_000
;
self
.
timeout
=
now
+
Duration
:
:
new
(
0
nanos
)
;
true
}
else
{
false
}
}
fn
gen_u32
(
&
mut
self
)
-
>
u32
{
self
.
seed
^
=
self
.
seed
<
<
13
;
self
.
seed
^
=
self
.
seed
>
>
17
;
self
.
seed
^
=
self
.
seed
<
<
5
;
self
.
seed
}
}
struct
ThreadData
{
parker
:
ThreadParker
key
:
AtomicUsize
next_in_queue
:
Cell
<
*
const
ThreadData
>
unpark_token
:
Cell
<
UnparkToken
>
park_token
:
Cell
<
ParkToken
>
parked_with_timeout
:
Cell
<
bool
>
#
[
cfg
(
feature
=
"
deadlock_detection
"
)
]
deadlock_data
:
deadlock
:
:
DeadlockData
}
impl
ThreadData
{
fn
new
(
)
-
>
ThreadData
{
let
num_threads
=
NUM_THREADS
.
fetch_add
(
1
Ordering
:
:
Relaxed
)
+
1
;
grow_hashtable
(
num_threads
)
;
ThreadData
{
parker
:
ThreadParker
:
:
new
(
)
key
:
AtomicUsize
:
:
new
(
0
)
next_in_queue
:
Cell
:
:
new
(
ptr
:
:
null
(
)
)
unpark_token
:
Cell
:
:
new
(
DEFAULT_UNPARK_TOKEN
)
park_token
:
Cell
:
:
new
(
DEFAULT_PARK_TOKEN
)
parked_with_timeout
:
Cell
:
:
new
(
false
)
#
[
cfg
(
feature
=
"
deadlock_detection
"
)
]
deadlock_data
:
deadlock
:
:
DeadlockData
:
:
new
(
)
}
}
}
#
[
inline
(
always
)
]
fn
with_thread_data
<
T
>
(
f
:
impl
FnOnce
(
&
ThreadData
)
-
>
T
)
-
>
T
{
let
mut
thread_data_storage
=
None
;
thread_local
!
(
static
THREAD_DATA
:
ThreadData
=
ThreadData
:
:
new
(
)
)
;
let
thread_data_ptr
=
THREAD_DATA
.
try_with
(
|
x
|
x
as
*
const
ThreadData
)
.
unwrap_or_else
(
|
_
|
thread_data_storage
.
get_or_insert_with
(
ThreadData
:
:
new
)
)
;
f
(
unsafe
{
&
*
thread_data_ptr
}
)
}
impl
Drop
for
ThreadData
{
fn
drop
(
&
mut
self
)
{
NUM_THREADS
.
fetch_sub
(
1
Ordering
:
:
Relaxed
)
;
}
}
#
[
inline
]
fn
get_hashtable
(
)
-
>
&
'
static
HashTable
{
let
table
=
HASHTABLE
.
load
(
Ordering
:
:
Acquire
)
;
if
table
.
is_null
(
)
{
create_hashtable
(
)
}
else
{
unsafe
{
&
*
table
}
}
}
#
[
cold
]
fn
create_hashtable
(
)
-
>
&
'
static
HashTable
{
let
new_table
=
Box
:
:
into_raw
(
HashTable
:
:
new
(
LOAD_FACTOR
ptr
:
:
null
(
)
)
)
;
let
table
=
match
HASHTABLE
.
compare_exchange
(
ptr
:
:
null_mut
(
)
new_table
Ordering
:
:
AcqRel
Ordering
:
:
Acquire
)
{
Ok
(
_
)
=
>
new_table
Err
(
old_table
)
=
>
{
unsafe
{
Box
:
:
from_raw
(
new_table
)
;
}
old_table
}
}
;
unsafe
{
&
*
table
}
}
fn
grow_hashtable
(
num_threads
:
usize
)
{
let
old_table
=
loop
{
let
table
=
get_hashtable
(
)
;
if
table
.
entries
.
len
(
)
>
=
LOAD_FACTOR
*
num_threads
{
return
;
}
for
bucket
in
&
table
.
entries
[
.
.
]
{
bucket
.
mutex
.
lock
(
)
;
}
if
HASHTABLE
.
load
(
Ordering
:
:
Relaxed
)
=
=
table
as
*
const
_
as
*
mut
_
{
break
table
;
}
for
bucket
in
&
table
.
entries
[
.
.
]
{
unsafe
{
bucket
.
mutex
.
unlock
(
)
}
;
}
}
;
let
mut
new_table
=
HashTable
:
:
new
(
num_threads
old_table
)
;
for
bucket
in
&
old_table
.
entries
[
.
.
]
{
unsafe
{
rehash_bucket_into
(
bucket
&
mut
new_table
)
}
;
}
HASHTABLE
.
store
(
Box
:
:
into_raw
(
new_table
)
Ordering
:
:
Release
)
;
for
bucket
in
&
old_table
.
entries
[
.
.
]
{
unsafe
{
bucket
.
mutex
.
unlock
(
)
}
;
}
}
unsafe
fn
rehash_bucket_into
(
bucket
:
&
'
static
Bucket
table
:
&
mut
HashTable
)
{
let
mut
current
:
*
const
ThreadData
=
bucket
.
queue_head
.
get
(
)
;
while
!
current
.
is_null
(
)
{
let
next
=
(
*
current
)
.
next_in_queue
.
get
(
)
;
let
hash
=
hash
(
(
*
current
)
.
key
.
load
(
Ordering
:
:
Relaxed
)
table
.
hash_bits
)
;
if
table
.
entries
[
hash
]
.
queue_tail
.
get
(
)
.
is_null
(
)
{
table
.
entries
[
hash
]
.
queue_head
.
set
(
current
)
;
}
else
{
(
*
table
.
entries
[
hash
]
.
queue_tail
.
get
(
)
)
.
next_in_queue
.
set
(
current
)
;
}
table
.
entries
[
hash
]
.
queue_tail
.
set
(
current
)
;
(
*
current
)
.
next_in_queue
.
set
(
ptr
:
:
null
(
)
)
;
current
=
next
;
}
}
#
[
cfg
(
target_pointer_width
=
"
32
"
)
]
#
[
inline
]
fn
hash
(
key
:
usize
bits
:
u32
)
-
>
usize
{
key
.
wrapping_mul
(
0x9E3779B9
)
>
>
(
32
-
bits
)
}
#
[
cfg
(
target_pointer_width
=
"
64
"
)
]
#
[
inline
]
fn
hash
(
key
:
usize
bits
:
u32
)
-
>
usize
{
key
.
wrapping_mul
(
0x9E3779B97F4A7C15
)
>
>
(
64
-
bits
)
}
#
[
inline
]
fn
lock_bucket
(
key
:
usize
)
-
>
&
'
static
Bucket
{
loop
{
let
hashtable
=
get_hashtable
(
)
;
let
hash
=
hash
(
key
hashtable
.
hash_bits
)
;
let
bucket
=
&
hashtable
.
entries
[
hash
]
;
bucket
.
mutex
.
lock
(
)
;
if
HASHTABLE
.
load
(
Ordering
:
:
Relaxed
)
=
=
hashtable
as
*
const
_
as
*
mut
_
{
return
bucket
;
}
unsafe
{
bucket
.
mutex
.
unlock
(
)
}
;
}
}
#
[
inline
]
fn
lock_bucket_checked
(
key
:
&
AtomicUsize
)
-
>
(
usize
&
'
static
Bucket
)
{
loop
{
let
hashtable
=
get_hashtable
(
)
;
let
current_key
=
key
.
load
(
Ordering
:
:
Relaxed
)
;
let
hash
=
hash
(
current_key
hashtable
.
hash_bits
)
;
let
bucket
=
&
hashtable
.
entries
[
hash
]
;
bucket
.
mutex
.
lock
(
)
;
if
HASHTABLE
.
load
(
Ordering
:
:
Relaxed
)
=
=
hashtable
as
*
const
_
as
*
mut
_
&
&
key
.
load
(
Ordering
:
:
Relaxed
)
=
=
current_key
{
return
(
current_key
bucket
)
;
}
unsafe
{
bucket
.
mutex
.
unlock
(
)
}
;
}
}
#
[
inline
]
fn
lock_bucket_pair
(
key1
:
usize
key2
:
usize
)
-
>
(
&
'
static
Bucket
&
'
static
Bucket
)
{
loop
{
let
hashtable
=
get_hashtable
(
)
;
let
hash1
=
hash
(
key1
hashtable
.
hash_bits
)
;
let
hash2
=
hash
(
key2
hashtable
.
hash_bits
)
;
let
bucket1
=
if
hash1
<
=
hash2
{
&
hashtable
.
entries
[
hash1
]
}
else
{
&
hashtable
.
entries
[
hash2
]
}
;
bucket1
.
mutex
.
lock
(
)
;
if
HASHTABLE
.
load
(
Ordering
:
:
Relaxed
)
=
=
hashtable
as
*
const
_
as
*
mut
_
{
if
hash1
=
=
hash2
{
return
(
bucket1
bucket1
)
;
}
else
if
hash1
<
hash2
{
let
bucket2
=
&
hashtable
.
entries
[
hash2
]
;
bucket2
.
mutex
.
lock
(
)
;
return
(
bucket1
bucket2
)
;
}
else
{
let
bucket2
=
&
hashtable
.
entries
[
hash1
]
;
bucket2
.
mutex
.
lock
(
)
;
return
(
bucket2
bucket1
)
;
}
}
unsafe
{
bucket1
.
mutex
.
unlock
(
)
}
;
}
}
#
[
inline
]
unsafe
fn
unlock_bucket_pair
(
bucket1
:
&
Bucket
bucket2
:
&
Bucket
)
{
bucket1
.
mutex
.
unlock
(
)
;
if
!
ptr
:
:
eq
(
bucket1
bucket2
)
{
bucket2
.
mutex
.
unlock
(
)
;
}
}
#
[
derive
(
Copy
Clone
Eq
PartialEq
Debug
)
]
pub
enum
ParkResult
{
Unparked
(
UnparkToken
)
Invalid
TimedOut
}
impl
ParkResult
{
#
[
inline
]
pub
fn
is_unparked
(
self
)
-
>
bool
{
if
let
ParkResult
:
:
Unparked
(
_
)
=
self
{
true
}
else
{
false
}
}
}
#
[
derive
(
Copy
Clone
Default
Eq
PartialEq
Debug
)
]
pub
struct
UnparkResult
{
pub
unparked_threads
:
usize
pub
requeued_threads
:
usize
pub
have_more_threads
:
bool
pub
be_fair
:
bool
_sealed
:
(
)
}
#
[
derive
(
Copy
Clone
Eq
PartialEq
Debug
)
]
pub
enum
RequeueOp
{
Abort
UnparkOneRequeueRest
RequeueAll
UnparkOne
RequeueOne
}
#
[
derive
(
Copy
Clone
Eq
PartialEq
Debug
)
]
pub
enum
FilterOp
{
Unpark
Skip
Stop
}
#
[
derive
(
Copy
Clone
Eq
PartialEq
Debug
)
]
pub
struct
UnparkToken
(
pub
usize
)
;
#
[
derive
(
Copy
Clone
Eq
PartialEq
Debug
)
]
pub
struct
ParkToken
(
pub
usize
)
;
pub
const
DEFAULT_UNPARK_TOKEN
:
UnparkToken
=
UnparkToken
(
0
)
;
pub
const
DEFAULT_PARK_TOKEN
:
ParkToken
=
ParkToken
(
0
)
;
#
[
inline
]
pub
unsafe
fn
park
(
key
:
usize
validate
:
impl
FnOnce
(
)
-
>
bool
before_sleep
:
impl
FnOnce
(
)
timed_out
:
impl
FnOnce
(
usize
bool
)
park_token
:
ParkToken
timeout
:
Option
<
Instant
>
)
-
>
ParkResult
{
with_thread_data
(
|
thread_data
|
{
let
bucket
=
lock_bucket
(
key
)
;
if
!
validate
(
)
{
bucket
.
mutex
.
unlock
(
)
;
return
ParkResult
:
:
Invalid
;
}
thread_data
.
parked_with_timeout
.
set
(
timeout
.
is_some
(
)
)
;
thread_data
.
next_in_queue
.
set
(
ptr
:
:
null
(
)
)
;
thread_data
.
key
.
store
(
key
Ordering
:
:
Relaxed
)
;
thread_data
.
park_token
.
set
(
park_token
)
;
thread_data
.
parker
.
prepare_park
(
)
;
if
!
bucket
.
queue_head
.
get
(
)
.
is_null
(
)
{
(
*
bucket
.
queue_tail
.
get
(
)
)
.
next_in_queue
.
set
(
thread_data
)
;
}
else
{
bucket
.
queue_head
.
set
(
thread_data
)
;
}
bucket
.
queue_tail
.
set
(
thread_data
)
;
bucket
.
mutex
.
unlock
(
)
;
before_sleep
(
)
;
let
unparked
=
match
timeout
{
Some
(
timeout
)
=
>
thread_data
.
parker
.
park_until
(
timeout
)
None
=
>
{
thread_data
.
parker
.
park
(
)
;
deadlock
:
:
on_unpark
(
thread_data
)
;
true
}
}
;
if
unparked
{
return
ParkResult
:
:
Unparked
(
thread_data
.
unpark_token
.
get
(
)
)
;
}
let
(
key
bucket
)
=
lock_bucket_checked
(
&
thread_data
.
key
)
;
if
!
thread_data
.
parker
.
timed_out
(
)
{
bucket
.
mutex
.
unlock
(
)
;
return
ParkResult
:
:
Unparked
(
thread_data
.
unpark_token
.
get
(
)
)
;
}
let
mut
link
=
&
bucket
.
queue_head
;
let
mut
current
=
bucket
.
queue_head
.
get
(
)
;
let
mut
previous
=
ptr
:
:
null
(
)
;
let
mut
was_last_thread
=
true
;
while
!
current
.
is_null
(
)
{
if
current
=
=
thread_data
{
let
next
=
(
*
current
)
.
next_in_queue
.
get
(
)
;
link
.
set
(
next
)
;
if
bucket
.
queue_tail
.
get
(
)
=
=
current
{
bucket
.
queue_tail
.
set
(
previous
)
;
}
else
{
let
mut
scan
=
next
;
while
!
scan
.
is_null
(
)
{
if
(
*
scan
)
.
key
.
load
(
Ordering
:
:
Relaxed
)
=
=
key
{
was_last_thread
=
false
;
break
;
}
scan
=
(
*
scan
)
.
next_in_queue
.
get
(
)
;
}
}
timed_out
(
key
was_last_thread
)
;
break
;
}
else
{
if
(
*
current
)
.
key
.
load
(
Ordering
:
:
Relaxed
)
=
=
key
{
was_last_thread
=
false
;
}
link
=
&
(
*
current
)
.
next_in_queue
;
previous
=
current
;
current
=
link
.
get
(
)
;
}
}
debug_assert
!
(
!
current
.
is_null
(
)
)
;
bucket
.
mutex
.
unlock
(
)
;
ParkResult
:
:
TimedOut
}
)
}
#
[
inline
]
pub
unsafe
fn
unpark_one
(
key
:
usize
callback
:
impl
FnOnce
(
UnparkResult
)
-
>
UnparkToken
)
-
>
UnparkResult
{
let
bucket
=
lock_bucket
(
key
)
;
let
mut
link
=
&
bucket
.
queue_head
;
let
mut
current
=
bucket
.
queue_head
.
get
(
)
;
let
mut
previous
=
ptr
:
:
null
(
)
;
let
mut
result
=
UnparkResult
:
:
default
(
)
;
while
!
current
.
is_null
(
)
{
if
(
*
current
)
.
key
.
load
(
Ordering
:
:
Relaxed
)
=
=
key
{
let
next
=
(
*
current
)
.
next_in_queue
.
get
(
)
;
link
.
set
(
next
)
;
if
bucket
.
queue_tail
.
get
(
)
=
=
current
{
bucket
.
queue_tail
.
set
(
previous
)
;
}
else
{
let
mut
scan
=
next
;
while
!
scan
.
is_null
(
)
{
if
(
*
scan
)
.
key
.
load
(
Ordering
:
:
Relaxed
)
=
=
key
{
result
.
have_more_threads
=
true
;
break
;
}
scan
=
(
*
scan
)
.
next_in_queue
.
get
(
)
;
}
}
result
.
unparked_threads
=
1
;
result
.
be_fair
=
(
*
bucket
.
fair_timeout
.
get
(
)
)
.
should_timeout
(
)
;
let
token
=
callback
(
result
)
;
(
*
current
)
.
unpark_token
.
set
(
token
)
;
let
handle
=
(
*
current
)
.
parker
.
unpark_lock
(
)
;
bucket
.
mutex
.
unlock
(
)
;
handle
.
unpark
(
)
;
return
result
;
}
else
{
link
=
&
(
*
current
)
.
next_in_queue
;
previous
=
current
;
current
=
link
.
get
(
)
;
}
}
callback
(
result
)
;
bucket
.
mutex
.
unlock
(
)
;
result
}
#
[
inline
]
pub
unsafe
fn
unpark_all
(
key
:
usize
unpark_token
:
UnparkToken
)
-
>
usize
{
let
bucket
=
lock_bucket
(
key
)
;
let
mut
link
=
&
bucket
.
queue_head
;
let
mut
current
=
bucket
.
queue_head
.
get
(
)
;
let
mut
previous
=
ptr
:
:
null
(
)
;
let
mut
threads
=
SmallVec
:
:
<
[
_
;
8
]
>
:
:
new
(
)
;
while
!
current
.
is_null
(
)
{
if
(
*
current
)
.
key
.
load
(
Ordering
:
:
Relaxed
)
=
=
key
{
let
next
=
(
*
current
)
.
next_in_queue
.
get
(
)
;
link
.
set
(
next
)
;
if
bucket
.
queue_tail
.
get
(
)
=
=
current
{
bucket
.
queue_tail
.
set
(
previous
)
;
}
(
*
current
)
.
unpark_token
.
set
(
unpark_token
)
;
threads
.
push
(
(
*
current
)
.
parker
.
unpark_lock
(
)
)
;
current
=
next
;
}
else
{
link
=
&
(
*
current
)
.
next_in_queue
;
previous
=
current
;
current
=
link
.
get
(
)
;
}
}
bucket
.
mutex
.
unlock
(
)
;
let
num_threads
=
threads
.
len
(
)
;
for
handle
in
threads
.
into_iter
(
)
{
handle
.
unpark
(
)
;
}
num_threads
}
#
[
inline
]
pub
unsafe
fn
unpark_requeue
(
key_from
:
usize
key_to
:
usize
validate
:
impl
FnOnce
(
)
-
>
RequeueOp
callback
:
impl
FnOnce
(
RequeueOp
UnparkResult
)
-
>
UnparkToken
)
-
>
UnparkResult
{
let
(
bucket_from
bucket_to
)
=
lock_bucket_pair
(
key_from
key_to
)
;
let
mut
result
=
UnparkResult
:
:
default
(
)
;
let
op
=
validate
(
)
;
if
op
=
=
RequeueOp
:
:
Abort
{
unlock_bucket_pair
(
bucket_from
bucket_to
)
;
return
result
;
}
let
mut
link
=
&
bucket_from
.
queue_head
;
let
mut
current
=
bucket_from
.
queue_head
.
get
(
)
;
let
mut
previous
=
ptr
:
:
null
(
)
;
let
mut
requeue_threads
:
*
const
ThreadData
=
ptr
:
:
null
(
)
;
let
mut
requeue_threads_tail
:
*
const
ThreadData
=
ptr
:
:
null
(
)
;
let
mut
wakeup_thread
=
None
;
while
!
current
.
is_null
(
)
{
if
(
*
current
)
.
key
.
load
(
Ordering
:
:
Relaxed
)
=
=
key_from
{
let
next
=
(
*
current
)
.
next_in_queue
.
get
(
)
;
link
.
set
(
next
)
;
if
bucket_from
.
queue_tail
.
get
(
)
=
=
current
{
bucket_from
.
queue_tail
.
set
(
previous
)
;
}
if
(
op
=
=
RequeueOp
:
:
UnparkOneRequeueRest
|
|
op
=
=
RequeueOp
:
:
UnparkOne
)
&
&
wakeup_thread
.
is_none
(
)
{
wakeup_thread
=
Some
(
current
)
;
result
.
unparked_threads
=
1
;
}
else
{
if
!
requeue_threads
.
is_null
(
)
{
(
*
requeue_threads_tail
)
.
next_in_queue
.
set
(
current
)
;
}
else
{
requeue_threads
=
current
;
}
requeue_threads_tail
=
current
;
(
*
current
)
.
key
.
store
(
key_to
Ordering
:
:
Relaxed
)
;
result
.
requeued_threads
+
=
1
;
}
if
op
=
=
RequeueOp
:
:
UnparkOne
|
|
op
=
=
RequeueOp
:
:
RequeueOne
{
let
mut
scan
=
next
;
while
!
scan
.
is_null
(
)
{
if
(
*
scan
)
.
key
.
load
(
Ordering
:
:
Relaxed
)
=
=
key_from
{
result
.
have_more_threads
=
true
;
break
;
}
scan
=
(
*
scan
)
.
next_in_queue
.
get
(
)
;
}
break
;
}
current
=
next
;
}
else
{
link
=
&
(
*
current
)
.
next_in_queue
;
previous
=
current
;
current
=
link
.
get
(
)
;
}
}
if
!
requeue_threads
.
is_null
(
)
{
(
*
requeue_threads_tail
)
.
next_in_queue
.
set
(
ptr
:
:
null
(
)
)
;
if
!
bucket_to
.
queue_head
.
get
(
)
.
is_null
(
)
{
(
*
bucket_to
.
queue_tail
.
get
(
)
)
.
next_in_queue
.
set
(
requeue_threads
)
;
}
else
{
bucket_to
.
queue_head
.
set
(
requeue_threads
)
;
}
bucket_to
.
queue_tail
.
set
(
requeue_threads_tail
)
;
}
if
result
.
unparked_threads
!
=
0
{
result
.
be_fair
=
(
*
bucket_from
.
fair_timeout
.
get
(
)
)
.
should_timeout
(
)
;
}
let
token
=
callback
(
op
result
)
;
if
let
Some
(
wakeup_thread
)
=
wakeup_thread
{
(
*
wakeup_thread
)
.
unpark_token
.
set
(
token
)
;
let
handle
=
(
*
wakeup_thread
)
.
parker
.
unpark_lock
(
)
;
unlock_bucket_pair
(
bucket_from
bucket_to
)
;
handle
.
unpark
(
)
;
}
else
{
unlock_bucket_pair
(
bucket_from
bucket_to
)
;
}
result
}
#
[
inline
]
pub
unsafe
fn
unpark_filter
(
key
:
usize
mut
filter
:
impl
FnMut
(
ParkToken
)
-
>
FilterOp
callback
:
impl
FnOnce
(
UnparkResult
)
-
>
UnparkToken
)
-
>
UnparkResult
{
let
bucket
=
lock_bucket
(
key
)
;
let
mut
link
=
&
bucket
.
queue_head
;
let
mut
current
=
bucket
.
queue_head
.
get
(
)
;
let
mut
previous
=
ptr
:
:
null
(
)
;
let
mut
threads
=
SmallVec
:
:
<
[
_
;
8
]
>
:
:
new
(
)
;
let
mut
result
=
UnparkResult
:
:
default
(
)
;
while
!
current
.
is_null
(
)
{
if
(
*
current
)
.
key
.
load
(
Ordering
:
:
Relaxed
)
=
=
key
{
let
next
=
(
*
current
)
.
next_in_queue
.
get
(
)
;
match
filter
(
(
*
current
)
.
park_token
.
get
(
)
)
{
FilterOp
:
:
Unpark
=
>
{
link
.
set
(
next
)
;
if
bucket
.
queue_tail
.
get
(
)
=
=
current
{
bucket
.
queue_tail
.
set
(
previous
)
;
}
threads
.
push
(
(
current
None
)
)
;
current
=
next
;
}
FilterOp
:
:
Skip
=
>
{
result
.
have_more_threads
=
true
;
link
=
&
(
*
current
)
.
next_in_queue
;
previous
=
current
;
current
=
link
.
get
(
)
;
}
FilterOp
:
:
Stop
=
>
{
result
.
have_more_threads
=
true
;
break
;
}
}
}
else
{
link
=
&
(
*
current
)
.
next_in_queue
;
previous
=
current
;
current
=
link
.
get
(
)
;
}
}
result
.
unparked_threads
=
threads
.
len
(
)
;
if
result
.
unparked_threads
!
=
0
{
result
.
be_fair
=
(
*
bucket
.
fair_timeout
.
get
(
)
)
.
should_timeout
(
)
;
}
let
token
=
callback
(
result
)
;
for
t
in
threads
.
iter_mut
(
)
{
(
*
t
.
0
)
.
unpark_token
.
set
(
token
)
;
t
.
1
=
Some
(
(
*
t
.
0
)
.
parker
.
unpark_lock
(
)
)
;
}
bucket
.
mutex
.
unlock
(
)
;
for
(
_
handle
)
in
threads
.
into_iter
(
)
{
handle
.
unchecked_unwrap
(
)
.
unpark
(
)
;
}
result
}
pub
mod
deadlock
{
#
[
cfg
(
feature
=
"
deadlock_detection
"
)
]
use
super
:
:
deadlock_impl
;
#
[
cfg
(
feature
=
"
deadlock_detection
"
)
]
pub
(
super
)
use
super
:
:
deadlock_impl
:
:
DeadlockData
;
#
[
inline
]
pub
unsafe
fn
acquire_resource
(
_key
:
usize
)
{
#
[
cfg
(
feature
=
"
deadlock_detection
"
)
]
deadlock_impl
:
:
acquire_resource
(
_key
)
;
}
#
[
inline
]
pub
unsafe
fn
release_resource
(
_key
:
usize
)
{
#
[
cfg
(
feature
=
"
deadlock_detection
"
)
]
deadlock_impl
:
:
release_resource
(
_key
)
;
}
#
[
cfg
(
feature
=
"
deadlock_detection
"
)
]
#
[
inline
]
pub
fn
check_deadlock
(
)
-
>
Vec
<
Vec
<
deadlock_impl
:
:
DeadlockedThread
>
>
{
deadlock_impl
:
:
check_deadlock
(
)
}
#
[
inline
]
pub
(
super
)
unsafe
fn
on_unpark
(
_td
:
&
super
:
:
ThreadData
)
{
#
[
cfg
(
feature
=
"
deadlock_detection
"
)
]
deadlock_impl
:
:
on_unpark
(
_td
)
;
}
}
#
[
cfg
(
feature
=
"
deadlock_detection
"
)
]
mod
deadlock_impl
{
use
super
:
:
{
get_hashtable
lock_bucket
with_thread_data
ThreadData
NUM_THREADS
}
;
use
crate
:
:
thread_parker
:
:
{
ThreadParkerT
UnparkHandleT
}
;
use
crate
:
:
word_lock
:
:
WordLock
;
use
backtrace
:
:
Backtrace
;
use
petgraph
;
use
petgraph
:
:
graphmap
:
:
DiGraphMap
;
use
std
:
:
cell
:
:
{
Cell
UnsafeCell
}
;
use
std
:
:
collections
:
:
HashSet
;
use
std
:
:
sync
:
:
atomic
:
:
Ordering
;
use
std
:
:
sync
:
:
mpsc
;
use
thread_id
;
pub
struct
DeadlockedThread
{
thread_id
:
usize
backtrace
:
Backtrace
}
impl
DeadlockedThread
{
pub
fn
thread_id
(
&
self
)
-
>
usize
{
self
.
thread_id
}
pub
fn
backtrace
(
&
self
)
-
>
&
Backtrace
{
&
self
.
backtrace
}
}
pub
struct
DeadlockData
{
resources
:
UnsafeCell
<
Vec
<
usize
>
>
deadlocked
:
Cell
<
bool
>
backtrace_sender
:
UnsafeCell
<
Option
<
mpsc
:
:
Sender
<
DeadlockedThread
>
>
>
thread_id
:
usize
}
impl
DeadlockData
{
pub
fn
new
(
)
-
>
Self
{
DeadlockData
{
resources
:
UnsafeCell
:
:
new
(
Vec
:
:
new
(
)
)
deadlocked
:
Cell
:
:
new
(
false
)
backtrace_sender
:
UnsafeCell
:
:
new
(
None
)
thread_id
:
thread_id
:
:
get
(
)
}
}
}
pub
(
super
)
unsafe
fn
on_unpark
(
td
:
&
ThreadData
)
{
if
td
.
deadlock_data
.
deadlocked
.
get
(
)
{
let
sender
=
(
*
td
.
deadlock_data
.
backtrace_sender
.
get
(
)
)
.
take
(
)
.
unwrap
(
)
;
sender
.
send
(
DeadlockedThread
{
thread_id
:
td
.
deadlock_data
.
thread_id
backtrace
:
Backtrace
:
:
new
(
)
}
)
.
unwrap
(
)
;
drop
(
sender
)
;
td
.
parker
.
prepare_park
(
)
;
td
.
parker
.
park
(
)
;
unreachable
!
(
"
unparked
deadlocked
thread
!
"
)
;
}
}
pub
unsafe
fn
acquire_resource
(
key
:
usize
)
{
with_thread_data
(
|
thread_data
|
{
(
*
thread_data
.
deadlock_data
.
resources
.
get
(
)
)
.
push
(
key
)
;
}
)
;
}
pub
unsafe
fn
release_resource
(
key
:
usize
)
{
with_thread_data
(
|
thread_data
|
{
let
resources
=
&
mut
(
*
thread_data
.
deadlock_data
.
resources
.
get
(
)
)
;
if
let
Some
(
p
)
=
resources
.
iter
(
)
.
rposition
(
|
x
|
*
x
=
=
key
)
{
resources
.
swap_remove
(
p
)
;
}
}
)
;
}
pub
fn
check_deadlock
(
)
-
>
Vec
<
Vec
<
DeadlockedThread
>
>
{
unsafe
{
if
check_wait_graph_fast
(
)
{
check_wait_graph_slow
(
)
}
else
{
Vec
:
:
new
(
)
}
}
}
unsafe
fn
check_wait_graph_fast
(
)
-
>
bool
{
let
table
=
get_hashtable
(
)
;
let
thread_count
=
NUM_THREADS
.
load
(
Ordering
:
:
Relaxed
)
;
let
mut
graph
=
DiGraphMap
:
:
<
usize
(
)
>
:
:
with_capacity
(
thread_count
*
2
thread_count
*
2
)
;
for
b
in
&
(
*
table
)
.
entries
[
.
.
]
{
b
.
mutex
.
lock
(
)
;
let
mut
current
=
b
.
queue_head
.
get
(
)
;
while
!
current
.
is_null
(
)
{
if
!
(
*
current
)
.
parked_with_timeout
.
get
(
)
&
&
!
(
*
current
)
.
deadlock_data
.
deadlocked
.
get
(
)
{
for
&
resource
in
&
(
*
(
*
current
)
.
deadlock_data
.
resources
.
get
(
)
)
{
graph
.
add_edge
(
resource
current
as
usize
(
)
)
;
}
graph
.
add_edge
(
current
as
usize
(
*
current
)
.
key
.
load
(
Ordering
:
:
Relaxed
)
(
)
)
;
}
current
=
(
*
current
)
.
next_in_queue
.
get
(
)
;
}
b
.
mutex
.
unlock
(
)
;
}
petgraph
:
:
algo
:
:
is_cyclic_directed
(
&
graph
)
}
#
[
derive
(
Hash
PartialEq
Eq
PartialOrd
Ord
Copy
Clone
)
]
enum
WaitGraphNode
{
Thread
(
*
const
ThreadData
)
Resource
(
usize
)
}
use
self
:
:
WaitGraphNode
:
:
*
;
unsafe
fn
check_wait_graph_slow
(
)
-
>
Vec
<
Vec
<
DeadlockedThread
>
>
{
static
DEADLOCK_DETECTION_LOCK
:
WordLock
=
WordLock
:
:
new
(
)
;
DEADLOCK_DETECTION_LOCK
.
lock
(
)
;
let
mut
table
=
get_hashtable
(
)
;
loop
{
for
b
in
&
table
.
entries
[
.
.
]
{
b
.
mutex
.
lock
(
)
;
}
let
new_table
=
get_hashtable
(
)
;
if
new_table
as
*
const
_
=
=
table
as
*
const
_
{
break
;
}
for
b
in
&
table
.
entries
[
.
.
]
{
b
.
mutex
.
unlock
(
)
;
}
table
=
new_table
;
}
let
thread_count
=
NUM_THREADS
.
load
(
Ordering
:
:
Relaxed
)
;
let
mut
graph
=
DiGraphMap
:
:
<
WaitGraphNode
(
)
>
:
:
with_capacity
(
thread_count
*
2
thread_count
*
2
)
;
for
b
in
&
table
.
entries
[
.
.
]
{
let
mut
current
=
b
.
queue_head
.
get
(
)
;
while
!
current
.
is_null
(
)
{
if
!
(
*
current
)
.
parked_with_timeout
.
get
(
)
&
&
!
(
*
current
)
.
deadlock_data
.
deadlocked
.
get
(
)
{
for
&
resource
in
&
(
*
(
*
current
)
.
deadlock_data
.
resources
.
get
(
)
)
{
graph
.
add_edge
(
Resource
(
resource
)
Thread
(
current
)
(
)
)
;
}
graph
.
add_edge
(
Thread
(
current
)
Resource
(
(
*
current
)
.
key
.
load
(
Ordering
:
:
Relaxed
)
)
(
)
)
;
}
current
=
(
*
current
)
.
next_in_queue
.
get
(
)
;
}
}
for
b
in
&
table
.
entries
[
.
.
]
{
b
.
mutex
.
unlock
(
)
;
}
let
cycles
=
graph_cycles
(
&
graph
)
;
let
mut
results
=
Vec
:
:
with_capacity
(
cycles
.
len
(
)
)
;
for
cycle
in
cycles
{
let
(
sender
receiver
)
=
mpsc
:
:
channel
(
)
;
for
td
in
cycle
{
let
bucket
=
lock_bucket
(
(
*
td
)
.
key
.
load
(
Ordering
:
:
Relaxed
)
)
;
(
*
td
)
.
deadlock_data
.
deadlocked
.
set
(
true
)
;
*
(
*
td
)
.
deadlock_data
.
backtrace_sender
.
get
(
)
=
Some
(
sender
.
clone
(
)
)
;
let
handle
=
(
*
td
)
.
parker
.
unpark_lock
(
)
;
bucket
.
mutex
.
unlock
(
)
;
handle
.
unpark
(
)
;
}
drop
(
sender
)
;
results
.
push
(
receiver
.
iter
(
)
.
collect
(
)
)
;
}
DEADLOCK_DETECTION_LOCK
.
unlock
(
)
;
results
}
fn
normalize_cycle
<
T
:
Ord
+
Copy
+
Clone
>
(
input
:
&
[
T
]
)
-
>
Vec
<
T
>
{
let
min_pos
=
input
.
iter
(
)
.
enumerate
(
)
.
min_by_key
(
|
&
(
_
&
t
)
|
t
)
.
map
(
|
(
p
_
)
|
p
)
.
unwrap_or
(
0
)
;
input
.
iter
(
)
.
cycle
(
)
.
skip
(
min_pos
)
.
take
(
input
.
len
(
)
)
.
cloned
(
)
.
collect
(
)
}
fn
graph_cycles
(
g
:
&
DiGraphMap
<
WaitGraphNode
(
)
>
)
-
>
Vec
<
Vec
<
*
const
ThreadData
>
>
{
use
petgraph
:
:
visit
:
:
depth_first_search
;
use
petgraph
:
:
visit
:
:
DfsEvent
;
use
petgraph
:
:
visit
:
:
NodeIndexable
;
let
mut
cycles
=
HashSet
:
:
new
(
)
;
let
mut
path
=
Vec
:
:
with_capacity
(
g
.
node_bound
(
)
)
;
let
threads
=
g
.
nodes
(
)
.
filter
(
|
n
|
if
let
&
Thread
(
_
)
=
n
{
true
}
else
{
false
}
)
;
depth_first_search
(
g
threads
|
e
|
match
e
{
DfsEvent
:
:
Discover
(
Thread
(
n
)
_
)
=
>
path
.
push
(
n
)
DfsEvent
:
:
Finish
(
Thread
(
_
)
_
)
=
>
{
path
.
pop
(
)
;
}
DfsEvent
:
:
BackEdge
(
_
Thread
(
n
)
)
=
>
{
let
from
=
path
.
iter
(
)
.
rposition
(
|
&
i
|
i
=
=
n
)
.
unwrap
(
)
;
cycles
.
insert
(
normalize_cycle
(
&
path
[
from
.
.
]
)
)
;
}
_
=
>
(
)
}
)
;
cycles
.
iter
(
)
.
cloned
(
)
.
collect
(
)
}
}
#
[
cfg
(
test
)
]
mod
tests
{
use
super
:
:
{
ThreadData
DEFAULT_PARK_TOKEN
DEFAULT_UNPARK_TOKEN
}
;
use
std
:
:
{
ptr
sync
:
:
{
atomic
:
:
{
AtomicIsize
AtomicPtr
AtomicUsize
Ordering
}
Arc
}
thread
time
:
:
Duration
}
;
fn
for_each
(
key
:
usize
mut
f
:
impl
FnMut
(
&
ThreadData
)
)
{
let
bucket
=
super
:
:
lock_bucket
(
key
)
;
let
mut
current
:
*
const
ThreadData
=
bucket
.
queue_head
.
get
(
)
;
while
!
current
.
is_null
(
)
{
let
current_ref
=
unsafe
{
&
*
current
}
;
if
current_ref
.
key
.
load
(
Ordering
:
:
Relaxed
)
=
=
key
{
f
(
current_ref
)
;
}
current
=
current_ref
.
next_in_queue
.
get
(
)
;
}
unsafe
{
bucket
.
mutex
.
unlock
(
)
}
;
}
macro_rules
!
test
{
(
(
name
:
ident
(
repeats
:
repeats
:
expr
latches
:
latches
:
expr
delay
:
delay
:
expr
threads
:
threads
:
expr
single_unparks
:
single_unparks
:
expr
)
;
)
*
)
=
>
{
(
#
[
test
]
fn
name
(
)
{
let
delay
=
Duration
:
:
from_micros
(
delay
)
;
for
_
in
0
.
.
repeats
{
run_parking_test
(
latches
delay
threads
single_unparks
)
;
}
}
)
*
}
;
}
test
!
{
unpark_all_one_fast
(
repeats
:
10000
latches
:
1
delay
:
0
threads
:
1
single_unparks
:
0
)
;
unpark_all_hundred_fast
(
repeats
:
100
latches
:
1
delay
:
0
threads
:
100
single_unparks
:
0
)
;
unpark_one_one_fast
(
repeats
:
1000
latches
:
1
delay
:
0
threads
:
1
single_unparks
:
1
)
;
unpark_one_hundred_fast
(
repeats
:
20
latches
:
1
delay
:
0
threads
:
100
single_unparks
:
100
)
;
unpark_one_fifty_then_fifty_all_fast
(
repeats
:
50
latches
:
1
delay
:
0
threads
:
100
single_unparks
:
50
)
;
unpark_all_one
(
repeats
:
100
latches
:
1
delay
:
10000
threads
:
1
single_unparks
:
0
)
;
unpark_all_hundred
(
repeats
:
100
latches
:
1
delay
:
10000
threads
:
100
single_unparks
:
0
)
;
unpark_one_one
(
repeats
:
10
latches
:
1
delay
:
10000
threads
:
1
single_unparks
:
1
)
;
unpark_one_fifty
(
repeats
:
1
latches
:
1
delay
:
10000
threads
:
50
single_unparks
:
50
)
;
unpark_one_fifty_then_fifty_all
(
repeats
:
2
latches
:
1
delay
:
10000
threads
:
100
single_unparks
:
50
)
;
hundred_unpark_all_one_fast
(
repeats
:
100
latches
:
100
delay
:
0
threads
:
1
single_unparks
:
0
)
;
hundred_unpark_all_one
(
repeats
:
1
latches
:
100
delay
:
10000
threads
:
1
single_unparks
:
0
)
;
}
fn
run_parking_test
(
num_latches
:
usize
delay
:
Duration
num_threads
:
usize
num_single_unparks
:
usize
)
{
let
mut
tests
=
Vec
:
:
with_capacity
(
num_latches
)
;
for
_
in
0
.
.
num_latches
{
let
test
=
Arc
:
:
new
(
SingleLatchTest
:
:
new
(
num_threads
)
)
;
let
mut
threads
=
Vec
:
:
with_capacity
(
num_threads
)
;
for
_
in
0
.
.
num_threads
{
let
test
=
test
.
clone
(
)
;
threads
.
push
(
thread
:
:
spawn
(
move
|
|
test
.
run
(
)
)
)
;
}
tests
.
push
(
(
test
threads
)
)
;
}
for
unpark_index
in
0
.
.
num_single_unparks
{
thread
:
:
sleep
(
delay
)
;
for
(
test
_
)
in
&
tests
{
test
.
unpark_one
(
unpark_index
)
;
}
}
for
(
test
threads
)
in
tests
{
test
.
finish
(
num_single_unparks
)
;
for
thread
in
threads
{
thread
.
join
(
)
.
expect
(
"
Test
thread
panic
"
)
;
}
}
}
struct
SingleLatchTest
{
semaphore
:
AtomicIsize
num_awake
:
AtomicUsize
last_awoken
:
AtomicPtr
<
ThreadData
>
num_threads
:
usize
}
impl
SingleLatchTest
{
pub
fn
new
(
num_threads
:
usize
)
-
>
Self
{
Self
{
semaphore
:
AtomicIsize
:
:
new
(
0
)
num_awake
:
AtomicUsize
:
:
new
(
0
)
last_awoken
:
AtomicPtr
:
:
new
(
ptr
:
:
null_mut
(
)
)
num_threads
}
}
pub
fn
run
(
&
self
)
{
self
.
down
(
)
;
let
this_thread_ptr
=
super
:
:
with_thread_data
(
|
t
|
t
as
*
const
_
as
*
mut
_
)
;
self
.
last_awoken
.
store
(
this_thread_ptr
Ordering
:
:
SeqCst
)
;
self
.
num_awake
.
fetch_add
(
1
Ordering
:
:
SeqCst
)
;
}
pub
fn
unpark_one
(
&
self
single_unpark_index
:
usize
)
{
assert
!
(
self
.
last_awoken
.
load
(
Ordering
:
:
SeqCst
)
.
is_null
(
)
)
;
let
mut
queue
:
Vec
<
*
mut
ThreadData
>
=
Vec
:
:
with_capacity
(
self
.
num_threads
)
;
for_each
(
self
.
semaphore_addr
(
)
|
thread_data
|
{
queue
.
push
(
thread_data
as
*
const
_
as
*
mut
_
)
;
}
)
;
assert
!
(
queue
.
len
(
)
<
=
self
.
num_threads
-
single_unpark_index
)
;
let
num_awake_before_up
=
self
.
num_awake
.
load
(
Ordering
:
:
SeqCst
)
;
self
.
up
(
)
;
while
self
.
num_awake
.
load
(
Ordering
:
:
SeqCst
)
!
=
num_awake_before_up
+
1
{
thread
:
:
yield_now
(
)
;
}
let
last_awoken
=
self
.
last_awoken
.
load
(
Ordering
:
:
SeqCst
)
;
assert
!
(
!
last_awoken
.
is_null
(
)
)
;
if
!
queue
.
is_empty
(
)
&
&
queue
[
0
]
!
=
last_awoken
{
panic
!
(
"
Woke
up
wrong
thread
:
\
n
\
tqueue
:
{
:
?
}
\
n
\
tlast
awoken
:
{
:
?
}
"
queue
last_awoken
)
;
}
self
.
last_awoken
.
store
(
ptr
:
:
null_mut
(
)
Ordering
:
:
SeqCst
)
;
}
pub
fn
finish
(
&
self
num_single_unparks
:
usize
)
{
let
mut
num_threads_left
=
self
.
num_threads
.
checked_sub
(
num_single_unparks
)
.
unwrap
(
)
;
while
num_threads_left
>
0
{
let
mut
num_waiting_on_address
=
0
;
for_each
(
self
.
semaphore_addr
(
)
|
_thread_data
|
{
num_waiting_on_address
+
=
1
;
}
)
;
assert
!
(
num_waiting_on_address
<
=
num_threads_left
)
;
let
num_awake_before_unpark
=
self
.
num_awake
.
load
(
Ordering
:
:
SeqCst
)
;
let
num_unparked
=
unsafe
{
super
:
:
unpark_all
(
self
.
semaphore_addr
(
)
DEFAULT_UNPARK_TOKEN
)
}
;
assert
!
(
num_unparked
>
=
num_waiting_on_address
)
;
assert
!
(
num_unparked
<
=
num_threads_left
)
;
while
self
.
num_awake
.
load
(
Ordering
:
:
SeqCst
)
!
=
num_awake_before_unpark
+
num_unparked
{
thread
:
:
yield_now
(
)
}
num_threads_left
=
num_threads_left
.
checked_sub
(
num_unparked
)
.
unwrap
(
)
;
}
assert_eq
!
(
self
.
num_awake
.
load
(
Ordering
:
:
SeqCst
)
self
.
num_threads
)
;
let
mut
num_waiting_on_address
=
0
;
for_each
(
self
.
semaphore_addr
(
)
|
_thread_data
|
{
num_waiting_on_address
+
=
1
;
}
)
;
assert_eq
!
(
num_waiting_on_address
0
)
;
}
pub
fn
down
(
&
self
)
{
let
old_semaphore_value
=
self
.
semaphore
.
fetch_sub
(
1
Ordering
:
:
SeqCst
)
;
if
old_semaphore_value
>
0
{
return
;
}
let
validate
=
|
|
true
;
let
before_sleep
=
|
|
{
}
;
let
timed_out
=
|
_
_
|
{
}
;
unsafe
{
super
:
:
park
(
self
.
semaphore_addr
(
)
validate
before_sleep
timed_out
DEFAULT_PARK_TOKEN
None
)
;
}
}
pub
fn
up
(
&
self
)
{
let
old_semaphore_value
=
self
.
semaphore
.
fetch_add
(
1
Ordering
:
:
SeqCst
)
;
if
old_semaphore_value
<
0
{
loop
{
match
unsafe
{
super
:
:
unpark_one
(
self
.
semaphore_addr
(
)
|
_
|
DEFAULT_UNPARK_TOKEN
)
.
unparked_threads
}
{
1
=
>
break
0
=
>
(
)
i
=
>
panic
!
(
"
Should
not
wake
up
{
}
threads
"
i
)
}
}
}
}
fn
semaphore_addr
(
&
self
)
-
>
usize
{
&
self
.
semaphore
as
*
const
_
as
usize
}
}
}
