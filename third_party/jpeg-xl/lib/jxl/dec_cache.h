#
ifndef
LIB_JXL_DEC_CACHE_H_
#
define
LIB_JXL_DEC_CACHE_H_
#
include
<
stdint
.
h
>
#
include
<
atomic
>
#
include
<
hwy
/
base
.
h
>
#
include
"
jxl
/
decode
.
h
"
#
include
"
lib
/
jxl
/
ac_strategy
.
h
"
#
include
"
lib
/
jxl
/
base
/
profiler
.
h
"
#
include
"
lib
/
jxl
/
coeff_order
.
h
"
#
include
"
lib
/
jxl
/
common
.
h
"
#
include
"
lib
/
jxl
/
convolve
.
h
"
#
include
"
lib
/
jxl
/
dec_group_border
.
h
"
#
include
"
lib
/
jxl
/
dec_noise
.
h
"
#
include
"
lib
/
jxl
/
image
.
h
"
#
include
"
lib
/
jxl
/
passes_state
.
h
"
#
include
"
lib
/
jxl
/
quant_weights
.
h
"
#
include
"
lib
/
jxl
/
render_pipeline
/
render_pipeline
.
h
"
#
include
"
lib
/
jxl
/
render_pipeline
/
stage_upsampling
.
h
"
#
include
"
lib
/
jxl
/
sanitizers
.
h
"
namespace
jxl
{
constexpr
size_t
kSigmaBorder
=
1
;
constexpr
size_t
kSigmaPadding
=
2
;
struct
PixelCallback
{
PixelCallback
(
)
=
default
;
PixelCallback
(
JxlImageOutInitCallback
init
JxlImageOutRunCallback
run
JxlImageOutDestroyCallback
destroy
void
*
init_opaque
)
:
init
(
init
)
run
(
run
)
destroy
(
destroy
)
init_opaque
(
init_opaque
)
{
#
if
JXL_ENABLE_ASSERT
const
bool
has_init
=
init
!
=
nullptr
;
const
bool
has_run
=
run
!
=
nullptr
;
const
bool
has_destroy
=
destroy
!
=
nullptr
;
JXL_ASSERT
(
has_init
=
=
has_run
&
&
has_run
=
=
has_destroy
)
;
#
endif
}
bool
IsPresent
(
)
const
{
return
run
!
=
nullptr
;
}
void
*
Init
(
size_t
num_threads
size_t
num_pixels
)
const
{
return
init
(
init_opaque
num_threads
num_pixels
)
;
}
JxlImageOutInitCallback
init
=
nullptr
;
JxlImageOutRunCallback
run
=
nullptr
;
JxlImageOutDestroyCallback
destroy
=
nullptr
;
void
*
init_opaque
=
nullptr
;
}
;
struct
PassesDecoderState
{
PassesSharedState
shared_storage
;
const
PassesSharedState
*
JXL_RESTRICT
shared
=
&
shared_storage
;
std
:
:
unique_ptr
<
RenderPipelineStage
>
upsampler8x
;
std
:
:
vector
<
ANSCode
>
code
;
std
:
:
vector
<
std
:
:
vector
<
uint8_t
>
>
context_map
;
float
x_dm_multiplier
;
float
b_dm_multiplier
;
ImageF
sigma
;
uint8_t
*
rgb_output
;
size_t
rgb_stride
=
0
;
bool
fast_xyb_srgb8_conversion
;
bool
rgb_output_is_rgba
;
bool
unpremul_alpha
;
PixelCallback
pixel_callback
;
std
:
:
vector
<
float
>
opaque_alpha
;
std
:
:
vector
<
std
:
:
vector
<
float
>
>
pixel_callback_rows
;
size_t
visible_frame_index
=
0
;
size_t
nonvisible_frame_index
=
0
;
std
:
:
atomic
<
uint32_t
>
used_acs
{
0
}
;
std
:
:
unique_ptr
<
ACImage
>
coefficients
=
make_unique
<
ACImageT
<
int32_t
>
>
(
0
0
)
;
std
:
:
unique_ptr
<
RenderPipeline
>
render_pipeline
;
ImageBundle
frame_storage_for_referencing
;
struct
PipelineOptions
{
bool
use_slow_render_pipeline
;
bool
coalescing
;
bool
render_spotcolors
;
}
;
Status
PreparePipeline
(
ImageBundle
*
decoded
PipelineOptions
options
)
;
OutputEncodingInfo
output_encoding_info
;
Status
Init
(
)
{
x_dm_multiplier
=
std
:
:
pow
(
1
/
(
1
.
25f
)
shared
-
>
frame_header
.
x_qm_scale
-
2
.
0f
)
;
b_dm_multiplier
=
std
:
:
pow
(
1
/
(
1
.
25f
)
shared
-
>
frame_header
.
b_qm_scale
-
2
.
0f
)
;
rgb_output
=
nullptr
;
rgb_output_is_rgba
=
false
;
unpremul_alpha
=
false
;
fast_xyb_srgb8_conversion
=
false
;
pixel_callback
=
PixelCallback
(
)
;
used_acs
=
0
;
upsampler8x
=
GetUpsamplingStage
(
shared
-
>
metadata
-
>
transform_data
0
3
)
;
if
(
shared
-
>
frame_header
.
loop_filter
.
epf_iters
>
0
)
{
sigma
=
ImageF
(
shared
-
>
frame_dim
.
xsize_blocks
+
2
*
kSigmaPadding
shared
-
>
frame_dim
.
ysize_blocks
+
2
*
kSigmaPadding
)
;
}
return
true
;
}
Status
InitForAC
(
ThreadPool
*
pool
)
{
shared_storage
.
coeff_order_size
=
0
;
for
(
uint8_t
o
=
0
;
o
<
AcStrategy
:
:
kNumValidStrategies
;
+
+
o
)
{
if
(
(
(
1
<
<
o
)
&
used_acs
)
=
=
0
)
continue
;
uint8_t
ord
=
kStrategyOrder
[
o
]
;
shared_storage
.
coeff_order_size
=
std
:
:
max
(
kCoeffOrderOffset
[
3
*
(
ord
+
1
)
]
*
kDCTBlockSize
shared_storage
.
coeff_order_size
)
;
}
size_t
sz
=
shared_storage
.
frame_header
.
passes
.
num_passes
*
shared_storage
.
coeff_order_size
;
if
(
sz
>
shared_storage
.
coeff_orders
.
size
(
)
)
{
shared_storage
.
coeff_orders
.
resize
(
sz
)
;
}
return
true
;
}
void
ComputeSigma
(
const
Rect
&
block_rect
PassesDecoderState
*
state
)
;
}
;
struct
GroupDecCache
{
void
InitOnce
(
size_t
num_passes
size_t
used_acs
)
{
PROFILER_FUNC
;
for
(
size_t
i
=
0
;
i
<
num_passes
;
i
+
+
)
{
if
(
num_nzeroes
[
i
]
.
xsize
(
)
=
=
0
)
{
num_nzeroes
[
i
]
=
Image3I
(
kGroupDimInBlocks
kGroupDimInBlocks
)
;
}
}
size_t
max_block_area
=
0
;
for
(
uint8_t
o
=
0
;
o
<
AcStrategy
:
:
kNumValidStrategies
;
+
+
o
)
{
AcStrategy
acs
=
AcStrategy
:
:
FromRawStrategy
(
o
)
;
if
(
(
used_acs
&
(
1
<
<
o
)
)
=
=
0
)
continue
;
size_t
area
=
acs
.
covered_blocks_x
(
)
*
acs
.
covered_blocks_y
(
)
*
kDCTBlockSize
;
max_block_area
=
std
:
:
max
(
area
max_block_area
)
;
}
if
(
max_block_area
>
max_block_area_
)
{
max_block_area_
=
max_block_area
;
float_memory_
=
hwy
:
:
AllocateAligned
<
float
>
(
max_block_area_
*
4
)
;
int32_memory_
=
hwy
:
:
AllocateAligned
<
int32_t
>
(
max_block_area_
*
3
)
;
int16_memory_
=
hwy
:
:
AllocateAligned
<
int16_t
>
(
max_block_area_
*
3
)
;
}
dec_group_block
=
float_memory_
.
get
(
)
;
scratch_space
=
dec_group_block
+
max_block_area_
*
3
;
dec_group_qblock
=
int32_memory_
.
get
(
)
;
dec_group_qblock16
=
int16_memory_
.
get
(
)
;
}
void
InitDCBufferOnce
(
)
{
if
(
dc_buffer
.
xsize
(
)
=
=
0
)
{
dc_buffer
=
ImageF
(
kGroupDimInBlocks
+
kRenderPipelineXOffset
*
2
kGroupDimInBlocks
+
4
)
;
}
}
float
*
dec_group_block
;
int32_t
*
dec_group_qblock
;
int16_t
*
dec_group_qblock16
;
float
*
scratch_space
;
Image3I
num_nzeroes
[
kMaxNumPasses
]
;
ImageF
dc_buffer
;
private
:
hwy
:
:
AlignedFreeUniquePtr
<
float
[
]
>
float_memory_
;
hwy
:
:
AlignedFreeUniquePtr
<
int32_t
[
]
>
int32_memory_
;
hwy
:
:
AlignedFreeUniquePtr
<
int16_t
[
]
>
int16_memory_
;
size_t
max_block_area_
=
0
;
}
;
}
#
endif
