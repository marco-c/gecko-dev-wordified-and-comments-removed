#
include
"
lib
/
jpegli
/
adaptive_quantization
.
h
"
#
include
<
stddef
.
h
>
#
include
<
stdio
.
h
>
#
include
<
stdlib
.
h
>
#
include
<
algorithm
>
#
include
<
cmath
>
#
include
<
string
>
#
include
<
vector
>
#
undef
HWY_TARGET_INCLUDE
#
define
HWY_TARGET_INCLUDE
"
lib
/
jpegli
/
adaptive_quantization
.
cc
"
#
include
<
hwy
/
foreach_target
.
h
>
#
include
<
hwy
/
highway
.
h
>
#
include
"
lib
/
jpegli
/
encode_internal
.
h
"
#
include
"
lib
/
jxl
/
base
/
compiler_specific
.
h
"
#
include
"
lib
/
jxl
/
base
/
data_parallel
.
h
"
#
include
"
lib
/
jxl
/
base
/
status
.
h
"
#
include
"
lib
/
jxl
/
image
.
h
"
#
include
"
lib
/
jxl
/
image_ops
.
h
"
HWY_BEFORE_NAMESPACE
(
)
;
namespace
jpegli
{
namespace
HWY_NAMESPACE
{
namespace
{
using
hwy
:
:
HWY_NAMESPACE
:
:
AbsDiff
;
using
hwy
:
:
HWY_NAMESPACE
:
:
Add
;
using
hwy
:
:
HWY_NAMESPACE
:
:
And
;
using
hwy
:
:
HWY_NAMESPACE
:
:
Div
;
using
hwy
:
:
HWY_NAMESPACE
:
:
Floor
;
using
hwy
:
:
HWY_NAMESPACE
:
:
GetLane
;
using
hwy
:
:
HWY_NAMESPACE
:
:
Max
;
using
hwy
:
:
HWY_NAMESPACE
:
:
Min
;
using
hwy
:
:
HWY_NAMESPACE
:
:
Mul
;
using
hwy
:
:
HWY_NAMESPACE
:
:
MulAdd
;
using
hwy
:
:
HWY_NAMESPACE
:
:
NegMulAdd
;
using
hwy
:
:
HWY_NAMESPACE
:
:
Rebind
;
using
hwy
:
:
HWY_NAMESPACE
:
:
ShiftLeft
;
using
hwy
:
:
HWY_NAMESPACE
:
:
ShiftRight
;
using
hwy
:
:
HWY_NAMESPACE
:
:
Sqrt
;
using
hwy
:
:
HWY_NAMESPACE
:
:
Sub
;
using
hwy
:
:
HWY_NAMESPACE
:
:
ZeroIfNegative
;
static
constexpr
size_t
kEncTileDim
=
64
;
;
static
constexpr
size_t
kEncTileDimInBlocks
=
kEncTileDim
/
jxl
:
:
kBlockDim
;
template
<
typename
T
class
V
>
struct
FastDivision
{
HWY_INLINE
V
operator
(
)
(
const
V
n
const
V
d
)
const
{
return
n
/
d
;
}
}
;
template
<
class
V
>
struct
FastDivision
<
float
V
>
{
static
HWY_INLINE
V
ReciprocalNR
(
const
V
x
)
{
const
auto
rcp
=
ApproximateReciprocal
(
x
)
;
const
auto
sum
=
Add
(
rcp
rcp
)
;
const
auto
x_rcp
=
Mul
(
x
rcp
)
;
return
NegMulAdd
(
x_rcp
rcp
sum
)
;
}
V
operator
(
)
(
const
V
n
const
V
d
)
const
{
#
if
1
return
Div
(
n
d
)
;
#
else
return
n
*
ReciprocalNR
(
d
)
;
#
endif
}
}
;
template
<
size_t
NP
size_t
NQ
class
D
class
V
typename
T
>
HWY_INLINE
HWY_MAYBE_UNUSED
V
EvalRationalPolynomial
(
const
D
d
const
V
x
const
T
(
&
p
)
[
NP
]
const
T
(
&
q
)
[
NQ
]
)
{
constexpr
size_t
kDegP
=
NP
/
4
-
1
;
constexpr
size_t
kDegQ
=
NQ
/
4
-
1
;
auto
yp
=
LoadDup128
(
d
&
p
[
kDegP
*
4
]
)
;
auto
yq
=
LoadDup128
(
d
&
q
[
kDegQ
*
4
]
)
;
HWY_FENCE
;
if
(
kDegP
>
=
1
)
yp
=
MulAdd
(
yp
x
LoadDup128
(
d
p
+
(
(
kDegP
-
1
)
*
4
)
)
)
;
if
(
kDegQ
>
=
1
)
yq
=
MulAdd
(
yq
x
LoadDup128
(
d
q
+
(
(
kDegQ
-
1
)
*
4
)
)
)
;
HWY_FENCE
;
if
(
kDegP
>
=
2
)
yp
=
MulAdd
(
yp
x
LoadDup128
(
d
p
+
(
(
kDegP
-
2
)
*
4
)
)
)
;
if
(
kDegQ
>
=
2
)
yq
=
MulAdd
(
yq
x
LoadDup128
(
d
q
+
(
(
kDegQ
-
2
)
*
4
)
)
)
;
HWY_FENCE
;
if
(
kDegP
>
=
3
)
yp
=
MulAdd
(
yp
x
LoadDup128
(
d
p
+
(
(
kDegP
-
3
)
*
4
)
)
)
;
if
(
kDegQ
>
=
3
)
yq
=
MulAdd
(
yq
x
LoadDup128
(
d
q
+
(
(
kDegQ
-
3
)
*
4
)
)
)
;
HWY_FENCE
;
if
(
kDegP
>
=
4
)
yp
=
MulAdd
(
yp
x
LoadDup128
(
d
p
+
(
(
kDegP
-
4
)
*
4
)
)
)
;
if
(
kDegQ
>
=
4
)
yq
=
MulAdd
(
yq
x
LoadDup128
(
d
q
+
(
(
kDegQ
-
4
)
*
4
)
)
)
;
HWY_FENCE
;
if
(
kDegP
>
=
5
)
yp
=
MulAdd
(
yp
x
LoadDup128
(
d
p
+
(
(
kDegP
-
5
)
*
4
)
)
)
;
if
(
kDegQ
>
=
5
)
yq
=
MulAdd
(
yq
x
LoadDup128
(
d
q
+
(
(
kDegQ
-
5
)
*
4
)
)
)
;
HWY_FENCE
;
if
(
kDegP
>
=
6
)
yp
=
MulAdd
(
yp
x
LoadDup128
(
d
p
+
(
(
kDegP
-
6
)
*
4
)
)
)
;
if
(
kDegQ
>
=
6
)
yq
=
MulAdd
(
yq
x
LoadDup128
(
d
q
+
(
(
kDegQ
-
6
)
*
4
)
)
)
;
HWY_FENCE
;
if
(
kDegP
>
=
7
)
yp
=
MulAdd
(
yp
x
LoadDup128
(
d
p
+
(
(
kDegP
-
7
)
*
4
)
)
)
;
if
(
kDegQ
>
=
7
)
yq
=
MulAdd
(
yq
x
LoadDup128
(
d
q
+
(
(
kDegQ
-
7
)
*
4
)
)
)
;
return
FastDivision
<
T
V
>
(
)
(
yp
yq
)
;
}
template
<
class
DF
class
V
>
V
FastLog2f
(
const
DF
df
V
x
)
{
HWY_ALIGN
const
float
p
[
4
*
(
2
+
1
)
]
=
{
HWY_REP4
(
-
1
.
8503833400518310E
-
06f
)
HWY_REP4
(
1
.
4287160470083755E
+
00f
)
HWY_REP4
(
7
.
4245873327820566E
-
01f
)
}
;
HWY_ALIGN
const
float
q
[
4
*
(
2
+
1
)
]
=
{
HWY_REP4
(
9
.
9032814277590719E
-
01f
)
HWY_REP4
(
1
.
0096718572241148E
+
00f
)
HWY_REP4
(
1
.
7409343003366853E
-
01f
)
}
;
const
Rebind
<
int32_t
DF
>
di
;
const
auto
x_bits
=
BitCast
(
di
x
)
;
const
auto
exp_bits
=
Sub
(
x_bits
Set
(
di
0x3f2aaaab
)
)
;
const
auto
exp_shifted
=
ShiftRight
<
23
>
(
exp_bits
)
;
const
auto
mantissa
=
BitCast
(
df
Sub
(
x_bits
ShiftLeft
<
23
>
(
exp_shifted
)
)
)
;
const
auto
exp_val
=
ConvertTo
(
df
exp_shifted
)
;
return
Add
(
EvalRationalPolynomial
(
df
Sub
(
mantissa
Set
(
df
1
.
0f
)
)
p
q
)
exp_val
)
;
}
template
<
class
DF
class
V
>
V
FastPow2f
(
const
DF
df
V
x
)
{
const
Rebind
<
int32_t
DF
>
di
;
auto
floorx
=
Floor
(
x
)
;
auto
exp
=
BitCast
(
df
ShiftLeft
<
23
>
(
Add
(
ConvertTo
(
di
floorx
)
Set
(
di
127
)
)
)
)
;
auto
frac
=
Sub
(
x
floorx
)
;
auto
num
=
Add
(
frac
Set
(
df
1
.
01749063e
+
01
)
)
;
num
=
MulAdd
(
num
frac
Set
(
df
4
.
88687798e
+
01
)
)
;
num
=
MulAdd
(
num
frac
Set
(
df
9
.
85506591e
+
01
)
)
;
num
=
Mul
(
num
exp
)
;
auto
den
=
MulAdd
(
frac
Set
(
df
2
.
10242958e
-
01
)
Set
(
df
-
2
.
22328856e
-
02
)
)
;
den
=
MulAdd
(
den
frac
Set
(
df
-
1
.
94414990e
+
01
)
)
;
den
=
MulAdd
(
den
frac
Set
(
df
9
.
85506633e
+
01
)
)
;
return
Div
(
num
den
)
;
}
inline
float
FastPow2f
(
float
f
)
{
HWY_CAPPED
(
float
1
)
D
;
return
GetLane
(
FastPow2f
(
D
Set
(
D
f
)
)
)
;
}
template
<
class
D
class
V
>
V
ComputeMask
(
const
D
d
const
V
out_val
)
{
const
auto
kBase
=
Set
(
d
-
0
.
74174993f
)
;
const
auto
kMul4
=
Set
(
d
3
.
2353257320940401f
)
;
const
auto
kMul2
=
Set
(
d
12
.
906028311180409f
)
;
const
auto
kOffset2
=
Set
(
d
305
.
04035728311436f
)
;
const
auto
kMul3
=
Set
(
d
5
.
0220313103171232f
)
;
const
auto
kOffset3
=
Set
(
d
2
.
1925739705298404f
)
;
const
auto
kOffset4
=
Mul
(
Set
(
d
0
.
25f
)
kOffset3
)
;
const
auto
kMul0
=
Set
(
d
0
.
74760422233706747f
)
;
const
auto
k1
=
Set
(
d
1
.
0f
)
;
const
auto
v1
=
Max
(
Mul
(
out_val
kMul0
)
Set
(
d
1e
-
3f
)
)
;
const
auto
v2
=
Div
(
k1
Add
(
v1
kOffset2
)
)
;
const
auto
v3
=
Div
(
k1
MulAdd
(
v1
v1
kOffset3
)
)
;
const
auto
v4
=
Div
(
k1
MulAdd
(
v1
v1
kOffset4
)
)
;
return
Add
(
kBase
MulAdd
(
kMul4
v4
MulAdd
(
kMul2
v2
Mul
(
kMul3
v3
)
)
)
)
;
}
static
const
float
kSGmul
=
226
.
0480446705883f
;
static
const
float
kSGmul2
=
1
.
0f
/
73
.
377132366608819f
;
static
const
float
kLog2
=
0
.
693147181f
;
static
const
float
kSGRetMul
=
kSGmul2
*
18
.
6580932135f
*
kLog2
;
static
const
float
kSGVOffset
=
7
.
14672470003f
;
template
<
bool
invert
typename
D
typename
V
>
V
RatioOfDerivativesOfCubicRootToSimpleGamma
(
const
D
d
V
v
)
{
float
kEpsilon
=
1e
-
2
;
v
=
ZeroIfNegative
(
v
)
;
const
auto
kNumMul
=
Set
(
d
kSGRetMul
*
3
*
kSGmul
)
;
const
auto
kVOffset
=
Set
(
d
kSGVOffset
*
kLog2
+
kEpsilon
)
;
const
auto
kDenMul
=
Set
(
d
kLog2
*
kSGmul
)
;
const
auto
v2
=
Mul
(
v
v
)
;
const
auto
num
=
MulAdd
(
kNumMul
v2
Set
(
d
kEpsilon
)
)
;
const
auto
den
=
MulAdd
(
Mul
(
kDenMul
v
)
v2
kVOffset
)
;
return
invert
?
Div
(
num
den
)
:
Div
(
den
num
)
;
}
template
<
bool
invert
=
false
>
static
float
RatioOfDerivativesOfCubicRootToSimpleGamma
(
float
v
)
{
using
DScalar
=
HWY_CAPPED
(
float
1
)
;
auto
vscalar
=
Load
(
DScalar
(
)
&
v
)
;
return
GetLane
(
RatioOfDerivativesOfCubicRootToSimpleGamma
<
invert
>
(
DScalar
(
)
vscalar
)
)
;
}
template
<
class
D
class
V
>
V
GammaModulation
(
const
D
d
const
size_t
x
const
size_t
y
const
jxl
:
:
ImageF
&
xyb_y
const
V
out_val
)
{
const
float
kBias
=
0
.
16f
;
auto
overall_ratio
=
Zero
(
d
)
;
auto
bias
=
Set
(
d
kBias
)
;
for
(
size_t
dy
=
0
;
dy
<
8
;
+
+
dy
)
{
const
float
*
const
JXL_RESTRICT
row_in_y
=
xyb_y
.
Row
(
y
+
dy
)
;
for
(
size_t
dx
=
0
;
dx
<
8
;
dx
+
=
Lanes
(
d
)
)
{
const
auto
iny
=
Add
(
Load
(
d
row_in_y
+
x
+
dx
)
bias
)
;
const
auto
ratio_g
=
RatioOfDerivativesOfCubicRootToSimpleGamma
<
true
>
(
d
iny
)
;
overall_ratio
=
Add
(
overall_ratio
ratio_g
)
;
}
}
overall_ratio
=
Mul
(
SumOfLanes
(
d
overall_ratio
)
Set
(
d
1
.
0f
/
64
)
)
;
const
auto
kGam
=
Set
(
d
-
0
.
15526878023684174f
*
0
.
693147180559945f
)
;
return
MulAdd
(
kGam
FastLog2f
(
d
overall_ratio
)
out_val
)
;
}
template
<
class
D
class
V
>
V
HfModulation
(
const
D
d
const
size_t
x
const
size_t
y
const
jxl
:
:
ImageF
&
xyb
const
V
out_val
)
{
const
Rebind
<
uint32_t
D
>
du
;
HWY_ALIGN
constexpr
uint32_t
kMaskRight
[
jxl
:
:
kBlockDim
]
=
{
~
0u
~
0u
~
0u
~
0u
~
0u
~
0u
~
0u
0
}
;
auto
sum
=
Zero
(
d
)
;
for
(
size_t
dy
=
0
;
dy
<
8
;
+
+
dy
)
{
const
float
*
JXL_RESTRICT
row_in
=
xyb
.
Row
(
y
+
dy
)
+
x
;
const
float
*
JXL_RESTRICT
row_in_next
=
dy
=
=
7
?
row_in
:
xyb
.
Row
(
y
+
dy
+
1
)
+
x
;
#
if
HWY_TARGET
!
=
HWY_SCALAR
for
(
size_t
dx
=
0
;
dx
<
8
;
dx
+
=
Lanes
(
d
)
)
{
#
else
for
(
size_t
dx
=
0
;
dx
<
7
;
dx
+
=
Lanes
(
d
)
)
{
#
endif
const
auto
p
=
Load
(
d
row_in
+
dx
)
;
const
auto
pr
=
LoadU
(
d
row_in
+
dx
+
1
)
;
const
auto
mask
=
BitCast
(
d
Load
(
du
kMaskRight
+
dx
)
)
;
sum
=
Add
(
sum
And
(
mask
AbsDiff
(
p
pr
)
)
)
;
const
auto
pd
=
Load
(
d
row_in_next
+
dx
)
;
sum
=
Add
(
sum
AbsDiff
(
p
pd
)
)
;
}
#
if
HWY_TARGET
=
=
HWY_SCALAR
const
auto
p
=
Load
(
d
row_in
+
7
)
;
const
auto
pd
=
Load
(
d
row_in_next
+
7
)
;
sum
=
Add
(
sum
AbsDiff
(
p
pd
)
)
;
#
endif
}
sum
=
SumOfLanes
(
d
sum
)
;
return
MulAdd
(
sum
Set
(
d
-
2
.
0052193233688884f
/
112
)
out_val
)
;
}
void
PerBlockModulations
(
const
float
butteraugli_target
const
jxl
:
:
ImageF
&
xyb_y
const
float
scale
const
jxl
:
:
Rect
&
rect
jxl
:
:
ImageF
*
out
)
{
JXL_ASSERT
(
DivCeil
(
xyb_y
.
xsize
(
)
jxl
:
:
kBlockDim
)
=
=
out
-
>
xsize
(
)
)
;
JXL_ASSERT
(
DivCeil
(
xyb_y
.
ysize
(
)
jxl
:
:
kBlockDim
)
=
=
out
-
>
ysize
(
)
)
;
float
base_level
=
0
.
48f
*
scale
;
float
kDampenRampStart
=
2
.
0f
;
float
kDampenRampEnd
=
14
.
0f
;
float
dampen
=
1
.
0f
;
if
(
butteraugli_target
>
=
kDampenRampStart
)
{
dampen
=
1
.
0f
-
(
(
butteraugli_target
-
kDampenRampStart
)
/
(
kDampenRampEnd
-
kDampenRampStart
)
)
;
if
(
dampen
<
0
)
{
dampen
=
0
;
}
}
const
float
mul
=
scale
*
dampen
;
const
float
add
=
(
1
.
0f
-
dampen
)
*
base_level
;
for
(
size_t
iy
=
rect
.
y0
(
)
;
iy
<
rect
.
y0
(
)
+
rect
.
ysize
(
)
;
iy
+
+
)
{
const
size_t
y
=
iy
*
8
;
float
*
const
JXL_RESTRICT
row_out
=
out
-
>
Row
(
iy
)
;
const
HWY_CAPPED
(
float
jxl
:
:
kBlockDim
)
df
;
for
(
size_t
ix
=
rect
.
x0
(
)
;
ix
<
rect
.
x0
(
)
+
rect
.
xsize
(
)
;
ix
+
+
)
{
size_t
x
=
ix
*
8
;
auto
out_val
=
Set
(
df
row_out
[
ix
]
)
;
out_val
=
ComputeMask
(
df
out_val
)
;
out_val
=
HfModulation
(
df
x
y
xyb_y
out_val
)
;
out_val
=
GammaModulation
(
df
x
y
xyb_y
out_val
)
;
row_out
[
ix
]
=
FastPow2f
(
GetLane
(
out_val
)
*
1
.
442695041f
)
*
mul
+
add
;
}
}
}
template
<
typename
D
typename
V
>
V
MaskingSqrt
(
const
D
d
V
v
)
{
static
const
float
kLogOffset
=
28
;
static
const
float
kMul
=
211
.
50759899638012f
;
const
auto
mul_v
=
Set
(
d
kMul
*
1e8
)
;
const
auto
offset_v
=
Set
(
d
kLogOffset
)
;
return
Mul
(
Set
(
d
0
.
25f
)
Sqrt
(
MulAdd
(
v
Sqrt
(
mul_v
)
offset_v
)
)
)
;
}
float
MaskingSqrt
(
const
float
v
)
{
using
DScalar
=
HWY_CAPPED
(
float
1
)
;
auto
vscalar
=
Load
(
DScalar
(
)
&
v
)
;
return
GetLane
(
MaskingSqrt
(
DScalar
(
)
vscalar
)
)
;
}
void
StoreMin4
(
const
float
v
float
&
min0
float
&
min1
float
&
min2
float
&
min3
)
{
if
(
v
<
min3
)
{
if
(
v
<
min0
)
{
min3
=
min2
;
min2
=
min1
;
min1
=
min0
;
min0
=
v
;
}
else
if
(
v
<
min1
)
{
min3
=
min2
;
min2
=
min1
;
min1
=
v
;
}
else
if
(
v
<
min2
)
{
min3
=
min2
;
min2
=
v
;
}
else
{
min3
=
v
;
}
}
}
void
FuzzyErosion
(
const
jxl
:
:
Rect
&
from_rect
const
jxl
:
:
ImageF
&
from
const
jxl
:
:
Rect
&
to_rect
jxl
:
:
ImageF
*
to
)
{
const
size_t
xsize
=
from
.
xsize
(
)
;
const
size_t
ysize
=
from
.
ysize
(
)
;
constexpr
int
kStep
=
1
;
static_assert
(
kStep
=
=
1
"
Step
must
be
1
"
)
;
JXL_ASSERT
(
to_rect
.
xsize
(
)
*
2
=
=
from_rect
.
xsize
(
)
)
;
JXL_ASSERT
(
to_rect
.
ysize
(
)
*
2
=
=
from_rect
.
ysize
(
)
)
;
for
(
size_t
fy
=
0
;
fy
<
from_rect
.
ysize
(
)
;
+
+
fy
)
{
size_t
y
=
fy
+
from_rect
.
y0
(
)
;
size_t
ym1
=
y
>
=
kStep
?
y
-
kStep
:
y
;
size_t
yp1
=
y
+
kStep
<
ysize
?
y
+
kStep
:
y
;
const
float
*
rowt
=
from
.
Row
(
ym1
)
;
const
float
*
row
=
from
.
Row
(
y
)
;
const
float
*
rowb
=
from
.
Row
(
yp1
)
;
float
*
row_out
=
to_rect
.
Row
(
to
fy
/
2
)
;
for
(
size_t
fx
=
0
;
fx
<
from_rect
.
xsize
(
)
;
+
+
fx
)
{
size_t
x
=
fx
+
from_rect
.
x0
(
)
;
size_t
xm1
=
x
>
=
kStep
?
x
-
kStep
:
x
;
size_t
xp1
=
x
+
kStep
<
xsize
?
x
+
kStep
:
x
;
float
min0
=
row
[
x
]
;
float
min1
=
row
[
xm1
]
;
float
min2
=
row
[
xp1
]
;
float
min3
=
rowt
[
xm1
]
;
if
(
min0
>
min1
)
std
:
:
swap
(
min0
min1
)
;
if
(
min0
>
min2
)
std
:
:
swap
(
min0
min2
)
;
if
(
min0
>
min3
)
std
:
:
swap
(
min0
min3
)
;
if
(
min1
>
min2
)
std
:
:
swap
(
min1
min2
)
;
if
(
min1
>
min3
)
std
:
:
swap
(
min1
min3
)
;
if
(
min2
>
min3
)
std
:
:
swap
(
min2
min3
)
;
StoreMin4
(
rowt
[
x
]
min0
min1
min2
min3
)
;
StoreMin4
(
rowt
[
xp1
]
min0
min1
min2
min3
)
;
StoreMin4
(
rowb
[
xm1
]
min0
min1
min2
min3
)
;
StoreMin4
(
rowb
[
x
]
min0
min1
min2
min3
)
;
StoreMin4
(
rowb
[
xp1
]
min0
min1
min2
min3
)
;
static
const
float
kMul0
=
0
.
125f
;
static
const
float
kMul1
=
0
.
075f
;
static
const
float
kMul2
=
0
.
06f
;
static
const
float
kMul3
=
0
.
05f
;
float
v
=
kMul0
*
min0
+
kMul1
*
min1
+
kMul2
*
min2
+
kMul3
*
min3
;
if
(
fx
%
2
=
=
0
&
&
fy
%
2
=
=
0
)
{
row_out
[
fx
/
2
]
=
v
;
}
else
{
row_out
[
fx
/
2
]
+
=
v
;
}
}
}
}
struct
AdaptiveQuantizationImpl
{
void
Init
(
const
jxl
:
:
ImageF
&
xyb_y
)
{
JXL_DASSERT
(
xyb_y
.
xsize
(
)
%
jxl
:
:
kBlockDim
=
=
0
)
;
JXL_DASSERT
(
xyb_y
.
ysize
(
)
%
jxl
:
:
kBlockDim
=
=
0
)
;
const
size_t
xsize
=
xyb_y
.
xsize
(
)
;
const
size_t
ysize
=
xyb_y
.
ysize
(
)
;
aq_map
=
jxl
:
:
ImageF
(
xsize
/
jxl
:
:
kBlockDim
ysize
/
jxl
:
:
kBlockDim
)
;
}
void
PrepareBuffers
(
size_t
num_threads
)
{
diff_buffer
=
jxl
:
:
ImageF
(
kEncTileDim
+
8
num_threads
)
;
for
(
size_t
i
=
pre_erosion
.
size
(
)
;
i
<
num_threads
;
i
+
+
)
{
pre_erosion
.
emplace_back
(
kEncTileDimInBlocks
*
2
+
2
kEncTileDimInBlocks
*
2
+
2
)
;
}
}
void
ComputeTile
(
float
butteraugli_target
float
scale
const
jxl
:
:
ImageF
&
xyb_y
const
jxl
:
:
Rect
&
rect
const
int
thread
)
{
const
size_t
xsize
=
xyb_y
.
xsize
(
)
;
const
size_t
ysize
=
xyb_y
.
ysize
(
)
;
const
float
match_gamma_offset
=
0
.
019
;
const
HWY_FULL
(
float
)
df
;
size_t
y_start
=
rect
.
y0
(
)
*
8
;
size_t
y_end
=
y_start
+
rect
.
ysize
(
)
*
8
;
size_t
x0
=
rect
.
x0
(
)
*
8
;
size_t
x1
=
x0
+
rect
.
xsize
(
)
*
8
;
if
(
x0
!
=
0
)
x0
-
=
4
;
if
(
x1
!
=
xyb_y
.
xsize
(
)
)
x1
+
=
4
;
if
(
y_start
!
=
0
)
y_start
-
=
4
;
if
(
y_end
!
=
xyb_y
.
ysize
(
)
)
y_end
+
=
4
;
pre_erosion
[
thread
]
.
ShrinkTo
(
(
x1
-
x0
)
/
4
(
y_end
-
y_start
)
/
4
)
;
static
const
float
limit
=
0
.
2f
;
for
(
size_t
y
=
y_start
;
y
<
y_end
;
+
+
y
)
{
size_t
y2
=
y
+
1
<
ysize
?
y
+
1
:
y
;
size_t
y1
=
y
>
0
?
y
-
1
:
y
;
const
float
*
row_in
=
xyb_y
.
Row
(
y
)
;
const
float
*
row_in1
=
xyb_y
.
Row
(
y1
)
;
const
float
*
row_in2
=
xyb_y
.
Row
(
y2
)
;
float
*
JXL_RESTRICT
row_out
=
diff_buffer
.
Row
(
thread
)
;
auto
scalar_pixel
=
[
&
]
(
size_t
x
)
{
const
size_t
x2
=
x
+
1
<
xsize
?
x
+
1
:
x
;
const
size_t
x1
=
x
>
0
?
x
-
1
:
x
;
const
float
base
=
0
.
25f
*
(
row_in2
[
x
]
+
row_in1
[
x
]
+
row_in
[
x1
]
+
row_in
[
x2
]
)
;
const
float
gammac
=
RatioOfDerivativesOfCubicRootToSimpleGamma
(
row_in
[
x
]
+
match_gamma_offset
)
;
float
diff
=
gammac
*
(
row_in
[
x
]
-
base
)
;
diff
*
=
diff
;
if
(
diff
>
=
limit
)
{
diff
=
limit
;
}
diff
=
MaskingSqrt
(
diff
)
;
if
(
(
y
%
4
)
!
=
0
)
{
row_out
[
x
-
x0
]
+
=
diff
;
}
else
{
row_out
[
x
-
x0
]
=
diff
;
}
}
;
size_t
x
=
x0
;
if
(
x0
=
=
0
)
{
scalar_pixel
(
x0
)
;
+
+
x
;
}
const
auto
match_gamma_offset_v
=
Set
(
df
match_gamma_offset
)
;
const
auto
quarter
=
Set
(
df
0
.
25f
)
;
for
(
;
x
+
1
+
Lanes
(
df
)
<
x1
;
x
+
=
Lanes
(
df
)
)
{
const
auto
in
=
LoadU
(
df
row_in
+
x
)
;
const
auto
in_r
=
LoadU
(
df
row_in
+
x
+
1
)
;
const
auto
in_l
=
LoadU
(
df
row_in
+
x
-
1
)
;
const
auto
in_t
=
LoadU
(
df
row_in2
+
x
)
;
const
auto
in_b
=
LoadU
(
df
row_in1
+
x
)
;
auto
base
=
Mul
(
quarter
Add
(
Add
(
in_r
in_l
)
Add
(
in_t
in_b
)
)
)
;
auto
gammacv
=
RatioOfDerivativesOfCubicRootToSimpleGamma
<
false
>
(
df
Add
(
in
match_gamma_offset_v
)
)
;
auto
diff
=
Mul
(
gammacv
Sub
(
in
base
)
)
;
diff
=
Mul
(
diff
diff
)
;
diff
=
Min
(
diff
Set
(
df
limit
)
)
;
diff
=
MaskingSqrt
(
df
diff
)
;
if
(
(
y
&
3
)
!
=
0
)
{
diff
=
Add
(
diff
LoadU
(
df
row_out
+
x
-
x0
)
)
;
}
StoreU
(
diff
df
row_out
+
x
-
x0
)
;
}
for
(
;
x
<
x1
;
+
+
x
)
{
scalar_pixel
(
x
)
;
}
if
(
y
%
4
=
=
3
)
{
float
*
row_dout
=
pre_erosion
[
thread
]
.
Row
(
(
y
-
y_start
)
/
4
)
;
for
(
size_t
x
=
0
;
x
<
(
x1
-
x0
)
/
4
;
x
+
+
)
{
row_dout
[
x
]
=
(
row_out
[
x
*
4
]
+
row_out
[
x
*
4
+
1
]
+
row_out
[
x
*
4
+
2
]
+
row_out
[
x
*
4
+
3
]
)
*
0
.
25f
;
}
}
}
jxl
:
:
Rect
from_rect
(
x0
%
8
=
=
0
?
0
:
1
y_start
%
8
=
=
0
?
0
:
1
rect
.
xsize
(
)
*
2
rect
.
ysize
(
)
*
2
)
;
FuzzyErosion
(
from_rect
pre_erosion
[
thread
]
rect
&
aq_map
)
;
PerBlockModulations
(
butteraugli_target
xyb_y
scale
rect
&
aq_map
)
;
}
std
:
:
vector
<
jxl
:
:
ImageF
>
pre_erosion
;
jxl
:
:
ImageF
aq_map
;
jxl
:
:
ImageF
diff_buffer
;
}
;
jxl
:
:
ImageF
AdaptiveQuantizationMap
(
const
float
butteraugli_target
const
jxl
:
:
ImageF
&
xyb_y
float
scale
jxl
:
:
ThreadPool
*
pool
)
{
AdaptiveQuantizationImpl
impl
;
impl
.
Init
(
xyb_y
)
;
const
size_t
xsize_blocks
=
xyb_y
.
xsize
(
)
/
jxl
:
:
kBlockDim
;
const
size_t
ysize_blocks
=
xyb_y
.
ysize
(
)
/
jxl
:
:
kBlockDim
;
JXL_CHECK
(
RunOnPool
(
pool
0
DivCeil
(
xsize_blocks
kEncTileDimInBlocks
)
*
DivCeil
(
ysize_blocks
kEncTileDimInBlocks
)
[
&
]
(
const
size_t
num_threads
)
{
impl
.
PrepareBuffers
(
num_threads
)
;
return
true
;
}
[
&
]
(
const
uint32_t
tid
const
size_t
thread
)
{
size_t
n_enc_tiles
=
DivCeil
(
xsize_blocks
kEncTileDimInBlocks
)
;
size_t
tx
=
tid
%
n_enc_tiles
;
size_t
ty
=
tid
/
n_enc_tiles
;
size_t
by0
=
ty
*
kEncTileDimInBlocks
;
size_t
by1
=
std
:
:
min
(
(
ty
+
1
)
*
kEncTileDimInBlocks
ysize_blocks
)
;
size_t
bx0
=
tx
*
kEncTileDimInBlocks
;
size_t
bx1
=
std
:
:
min
(
(
tx
+
1
)
*
kEncTileDimInBlocks
xsize_blocks
)
;
jxl
:
:
Rect
r
(
bx0
by0
bx1
-
bx0
by1
-
by0
)
;
impl
.
ComputeTile
(
butteraugli_target
scale
xyb_y
r
thread
)
;
}
"
AQ
DiffPrecompute
"
)
)
;
return
std
:
:
move
(
impl
)
.
aq_map
;
}
}
}
}
HWY_AFTER_NAMESPACE
(
)
;
#
if
HWY_ONCE
namespace
jpegli
{
HWY_EXPORT
(
AdaptiveQuantizationMap
)
;
namespace
{
constexpr
float
kDcQuantPow
=
0
.
66f
;
static
const
float
kDcQuant
=
1
.
1f
;
static
const
float
kAcQuant
=
0
.
841f
;
}
float
InitialQuantDC
(
float
butteraugli_target
)
{
const
float
kDcMul
=
1
.
5
;
const
float
butteraugli_target_dc
=
std
:
:
max
<
float
>
(
0
.
5f
*
butteraugli_target
std
:
:
min
<
float
>
(
butteraugli_target
kDcMul
*
std
:
:
pow
(
(
1
.
0f
/
kDcMul
)
*
butteraugli_target
kDcQuantPow
)
)
)
;
return
std
:
:
min
(
kDcQuant
/
butteraugli_target_dc
50
.
f
)
;
}
jxl
:
:
ImageF
InitialQuantField
(
const
float
butteraugli_target
const
jxl
:
:
ImageF
&
opsin_y
jxl
:
:
ThreadPool
*
pool
float
rescale
)
{
const
float
quant_ac
=
kAcQuant
/
butteraugli_target
;
return
HWY_DYNAMIC_DISPATCH
(
AdaptiveQuantizationMap
)
(
butteraugli_target
opsin_y
quant_ac
*
rescale
pool
)
;
}
void
ComputeAdaptiveQuantField
(
j_compress_ptr
cinfo
)
{
jpeg_comp_master
*
m
=
cinfo
-
>
master
;
const
size_t
xsize_blocks
=
DivCeil
(
cinfo
-
>
image_width
DCTSIZE
)
;
const
size_t
ysize_blocks
=
DivCeil
(
cinfo
-
>
image_height
DCTSIZE
)
;
int
y_channel
=
cinfo
-
>
jpeg_color_space
=
=
JCS_RGB
?
1
:
0
;
jpeg_component_info
*
y_comp
=
&
cinfo
-
>
comp_info
[
y_channel
]
;
m
-
>
quant_field
.
Allocate
(
ysize_blocks
xsize_blocks
)
;
if
(
m
-
>
use_adaptive_quantization
&
&
y_comp
-
>
h_samp_factor
=
=
cinfo
-
>
max_h_samp_factor
&
&
y_comp
-
>
v_samp_factor
=
=
cinfo
-
>
max_v_samp_factor
)
{
JXL_ASSERT
(
y_comp
-
>
width_in_blocks
=
=
xsize_blocks
)
;
JXL_ASSERT
(
y_comp
-
>
height_in_blocks
=
=
ysize_blocks
)
;
jxl
:
:
ImageF
input
(
y_comp
-
>
width_in_blocks
*
DCTSIZE
y_comp
-
>
height_in_blocks
*
DCTSIZE
)
;
for
(
size_t
y
=
0
;
y
<
input
.
ysize
(
)
;
+
+
y
)
{
memcpy
(
input
.
Row
(
y
)
m
-
>
input_buffer
[
y_channel
]
.
Row
(
y
)
input
.
xsize
(
)
*
sizeof
(
float
)
)
;
}
jxl
:
:
ImageF
qf
=
jpegli
:
:
InitialQuantField
(
m
-
>
distance
input
nullptr
m
-
>
distance
)
;
float
qfmin
qfmax
;
ImageMinMax
(
qf
&
qfmin
&
qfmax
)
;
m
-
>
quant_field_max
=
qfmax
;
for
(
size_t
y
=
0
;
y
<
y_comp
-
>
height_in_blocks
;
+
+
y
)
{
const
float
*
row_in
=
qf
.
Row
(
y
)
;
float
*
row_out
=
m
-
>
quant_field
.
Row
(
y
)
;
for
(
size_t
x
=
0
;
x
<
y_comp
-
>
width_in_blocks
;
+
+
x
)
{
row_out
[
x
]
=
(
qfmax
/
row_in
[
x
]
)
-
1
.
0f
;
}
}
}
else
{
m
-
>
quant_field_max
=
kDefaultQuantFieldMax
;
for
(
size_t
y
=
0
;
y
<
ysize_blocks
;
+
+
y
)
{
m
-
>
quant_field
.
FillRow
(
y
0
.
0f
xsize_blocks
)
;
}
}
}
}
#
endif
