#
include
<
assert
.
h
>
#
include
<
immintrin
.
h
>
#
include
<
math
.
h
>
#
include
"
aom_dsp
/
aom_dsp_common
.
h
"
#
include
"
av1
/
common
/
av1_common_int
.
h
"
#
include
"
av1
/
encoder
/
cnn
.
h
"
DECLARE_ALIGNED
(
32
static
const
uint32_t
shuffle_src_layer0
[
2
]
[
8
]
)
=
{
{
0
1
2
3
4
4
5
6
}
{
0
1
1
2
3
4
5
0
}
}
;
DECLARE_ALIGNED
(
32
static
const
uint32_t
shuffle_weight_layer0
[
2
]
[
8
]
)
=
{
{
0
1
2
3
4
0
1
2
}
{
3
4
0
1
2
3
4
0
}
}
;
DECLARE_ALIGNED
(
32
static
const
uint32_t
shuffle_weight_layer_1_and_2
[
2
]
[
8
]
)
=
{
{
0
1
0
1
0
1
0
1
}
{
2
3
2
3
2
3
2
3
}
}
;
DECLARE_ALIGNED
(
32
static
const
uint32_t
shuffle_output_layer_1_and_2
[
8
]
)
=
{
0
1
4
5
2
3
6
7
}
;
static
inline
void
prepare_weights_for_5x5_convolve
(
const
float
*
layer_config_weights
int
off
float
weight
[
5
]
[
8
]
const
int
cstep
__m256
*
shuffle_weight
const
__m256i
weight_mask_0
const
__m256i
weight_mask_1
)
{
for
(
int
row
=
0
;
row
<
5
;
+
+
row
)
{
for
(
int
col
=
0
;
col
<
5
;
+
+
col
)
{
weight
[
row
]
[
col
]
=
layer_config_weights
[
off
]
;
off
+
=
cstep
;
}
}
shuffle_weight
[
0
]
=
_mm256_loadu_ps
(
weight
[
0
]
)
;
shuffle_weight
[
1
]
=
_mm256_loadu_ps
(
weight
[
1
]
)
;
shuffle_weight
[
2
]
=
_mm256_loadu_ps
(
weight
[
2
]
)
;
shuffle_weight
[
3
]
=
_mm256_loadu_ps
(
weight
[
3
]
)
;
shuffle_weight
[
4
]
=
_mm256_loadu_ps
(
weight
[
4
]
)
;
shuffle_weight
[
0
]
=
_mm256_permutevar8x32_ps
(
shuffle_weight
[
0
]
weight_mask_0
)
;
shuffle_weight
[
1
]
=
_mm256_permutevar8x32_ps
(
shuffle_weight
[
1
]
weight_mask_0
)
;
shuffle_weight
[
2
]
=
_mm256_permutevar8x32_ps
(
shuffle_weight
[
2
]
weight_mask_0
)
;
shuffle_weight
[
3
]
=
_mm256_permutevar8x32_ps
(
shuffle_weight
[
3
]
weight_mask_0
)
;
shuffle_weight
[
4
]
=
_mm256_permutevar8x32_ps
(
shuffle_weight
[
4
]
weight_mask_0
)
;
shuffle_weight
[
5
]
=
_mm256_permutevar8x32_ps
(
shuffle_weight
[
0
]
weight_mask_1
)
;
shuffle_weight
[
6
]
=
_mm256_permutevar8x32_ps
(
shuffle_weight
[
1
]
weight_mask_1
)
;
shuffle_weight
[
7
]
=
_mm256_permutevar8x32_ps
(
shuffle_weight
[
2
]
weight_mask_1
)
;
shuffle_weight
[
8
]
=
_mm256_permutevar8x32_ps
(
shuffle_weight
[
3
]
weight_mask_1
)
;
shuffle_weight
[
9
]
=
_mm256_permutevar8x32_ps
(
shuffle_weight
[
4
]
weight_mask_1
)
;
}
#
define
PERFORM_CONVOLVE_FOR_3_5X5_BLOCKS
(
)
\
do
{
\
for
(
int
row
=
0
;
row
<
5
;
row
+
+
)
{
\
load_src_0
=
_mm256_loadu_ps
(
input_ptr
)
;
\
load_src_1
=
_mm256_loadu_ps
(
input_ptr
+
7
)
;
\
load_src_0
=
_mm256_permutevar8x32_ps
(
load_src_0
block0_1
)
;
\
load_src_1
=
_mm256_permutevar8x32_ps
(
load_src_1
block1_2
)
;
\
load_src_0
=
_mm256_mul_ps
(
load_src_0
shuffle_weight
[
0
+
row
]
)
;
\
load_src_1
=
_mm256_mul_ps
(
load_src_1
shuffle_weight
[
5
+
row
]
)
;
\
accum_src_0
=
_mm256_add_ps
(
load_src_0
accum_src_0
)
;
\
accum_src_1
=
_mm256_add_ps
(
load_src_1
accum_src_1
)
;
\
input_ptr
+
=
in_stride
;
\
}
\
}
while
(
0
)
static
inline
void
load_shuffle_masks_for_2x2_convolve
(
__m256i
*
output_mask
__m256i
*
weight_mask
)
{
*
output_mask
=
_mm256_load_si256
(
(
const
__m256i
*
)
shuffle_output_layer_1_and_2
)
;
weight_mask
[
0
]
=
_mm256_load_si256
(
(
const
__m256i
*
)
shuffle_weight_layer_1_and_2
[
0
]
)
;
weight_mask
[
1
]
=
_mm256_load_si256
(
(
const
__m256i
*
)
shuffle_weight_layer_1_and_2
[
1
]
)
;
}
static
inline
void
prepare_weights_for_2x2_convolve
(
const
float
*
layer_config_weights
int
off
const
int
cstep
__m256
*
shuffle_weight
__m256i
*
weight_mask
)
{
float
weight
[
4
]
=
{
0
}
;
for
(
int
i
=
0
;
i
<
4
;
+
+
i
)
{
weight
[
i
]
=
layer_config_weights
[
off
]
;
off
+
=
cstep
;
}
const
__m256
weight_vec
=
_mm256_castps128_ps256
(
_mm_loadu_ps
(
weight
)
)
;
shuffle_weight
[
0
]
=
_mm256_permutevar8x32_ps
(
weight_vec
weight_mask
[
0
]
)
;
shuffle_weight
[
1
]
=
_mm256_permutevar8x32_ps
(
weight_vec
weight_mask
[
1
]
)
;
}
#
define
PERFORM_CONVOLVE_FOR_1_5X5_BLOCK
(
w
accum0
in_stride
)
\
do
{
\
__m128
load_src
[
5
]
;
\
load_src
[
0
]
=
_mm_loadu_ps
(
input_ptr
)
;
\
last_column_sum
+
=
input_ptr
[
4
]
*
weight
[
0
]
[
4
]
;
\
input_ptr
+
=
in_stride
;
\
load_src
[
1
]
=
_mm_loadu_ps
(
input_ptr
)
;
\
last_column_sum
+
=
input_ptr
[
4
]
*
weight
[
1
]
[
4
]
;
\
input_ptr
+
=
in_stride
;
\
load_src
[
2
]
=
_mm_loadu_ps
(
input_ptr
)
;
\
last_column_sum
+
=
input_ptr
[
4
]
*
weight
[
2
]
[
4
]
;
\
input_ptr
+
=
in_stride
;
\
load_src
[
3
]
=
_mm_loadu_ps
(
input_ptr
)
;
\
last_column_sum
+
=
input_ptr
[
4
]
*
weight
[
3
]
[
4
]
;
\
input_ptr
+
=
in_stride
;
\
load_src
[
4
]
=
_mm_loadu_ps
(
input_ptr
)
;
\
last_column_sum
+
=
input_ptr
[
4
]
*
weight
[
4
]
[
4
]
;
\
\
load_src
[
0
]
=
_mm_mul_ps
(
load_src
[
0
]
_mm256_castps256_ps128
(
w
[
0
]
)
)
;
\
load_src
[
1
]
=
_mm_mul_ps
(
load_src
[
1
]
_mm256_castps256_ps128
(
w
[
1
]
)
)
;
\
load_src
[
2
]
=
_mm_mul_ps
(
load_src
[
2
]
_mm256_castps256_ps128
(
w
[
2
]
)
)
;
\
load_src
[
3
]
=
_mm_mul_ps
(
load_src
[
3
]
_mm256_castps256_ps128
(
w
[
3
]
)
)
;
\
load_src
[
4
]
=
_mm_mul_ps
(
load_src
[
4
]
_mm256_castps256_ps128
(
w
[
4
]
)
)
;
\
\
accum0
=
_mm_add_ps
(
load_src
[
0
]
accum0
)
;
\
load_src
[
1
]
=
_mm_add_ps
(
load_src
[
1
]
load_src
[
2
]
)
;
\
load_src
[
3
]
=
_mm_add_ps
(
load_src
[
3
]
load_src
[
4
]
)
;
\
load_src
[
1
]
=
_mm_add_ps
(
load_src
[
1
]
load_src
[
3
]
)
;
\
accum0
=
_mm_add_ps
(
accum0
load_src
[
1
]
)
;
\
}
while
(
0
)
static
inline
void
perform_convolve_for_8h_2x2_blocks
(
const
float
*
input_ptr
int
in_stride
__m256
*
weight
__m256
*
out_accum
__m256i
shuffle_output_mask
)
{
__m256
load_src
[
4
]
;
load_src
[
0
]
=
_mm256_loadu_ps
(
input_ptr
)
;
load_src
[
1
]
=
_mm256_loadu_ps
(
input_ptr
+
8
)
;
load_src
[
2
]
=
_mm256_loadu_ps
(
input_ptr
+
in_stride
)
;
load_src
[
3
]
=
_mm256_loadu_ps
(
input_ptr
+
in_stride
+
8
)
;
load_src
[
0
]
=
_mm256_mul_ps
(
load_src
[
0
]
weight
[
0
]
)
;
load_src
[
1
]
=
_mm256_mul_ps
(
load_src
[
1
]
weight
[
0
]
)
;
load_src
[
2
]
=
_mm256_mul_ps
(
load_src
[
2
]
weight
[
1
]
)
;
load_src
[
3
]
=
_mm256_mul_ps
(
load_src
[
3
]
weight
[
1
]
)
;
load_src
[
0
]
=
_mm256_add_ps
(
load_src
[
0
]
load_src
[
2
]
)
;
load_src
[
1
]
=
_mm256_add_ps
(
load_src
[
1
]
load_src
[
3
]
)
;
load_src
[
0
]
=
_mm256_hadd_ps
(
load_src
[
0
]
load_src
[
1
]
)
;
load_src
[
0
]
=
_mm256_permutevar8x32_ps
(
load_src
[
0
]
shuffle_output_mask
)
;
*
out_accum
=
_mm256_add_ps
(
*
out_accum
load_src
[
0
]
)
;
}
static
inline
void
perform_convolve_for_4hx2v_2x2_blocks
(
const
float
*
input_ptr
int
in_stride
__m256
*
weight
__m256
*
out_accum
__m256i
shuffle_output_mask
)
{
__m256
load_src
[
4
]
;
load_src
[
0
]
=
_mm256_loadu_ps
(
input_ptr
)
;
load_src
[
1
]
=
_mm256_loadu_ps
(
input_ptr
+
in_stride
)
;
load_src
[
2
]
=
_mm256_loadu_ps
(
input_ptr
+
(
in_stride
*
2
)
)
;
load_src
[
3
]
=
_mm256_loadu_ps
(
input_ptr
+
(
in_stride
*
3
)
)
;
load_src
[
0
]
=
_mm256_mul_ps
(
load_src
[
0
]
weight
[
0
]
)
;
load_src
[
1
]
=
_mm256_mul_ps
(
load_src
[
1
]
weight
[
1
]
)
;
load_src
[
2
]
=
_mm256_mul_ps
(
load_src
[
2
]
weight
[
0
]
)
;
load_src
[
3
]
=
_mm256_mul_ps
(
load_src
[
3
]
weight
[
1
]
)
;
load_src
[
0
]
=
_mm256_add_ps
(
load_src
[
0
]
load_src
[
1
]
)
;
load_src
[
2
]
=
_mm256_add_ps
(
load_src
[
2
]
load_src
[
3
]
)
;
load_src
[
0
]
=
_mm256_hadd_ps
(
load_src
[
0
]
load_src
[
2
]
)
;
load_src
[
0
]
=
_mm256_permutevar8x32_ps
(
load_src
[
0
]
shuffle_output_mask
)
;
*
out_accum
=
_mm256_add_ps
(
*
out_accum
load_src
[
0
]
)
;
}
static
void
cnn_convolve_no_maxpool_padding_valid_5x5_avx2
(
const
float
*
*
input
int
in_width
int
in_height
int
in_stride
const
CNN_LAYER_CONFIG
*
const
layer_config
float
*
*
output
int
out_stride
int
start_idx
const
int
cstep
const
int
channel_step
)
{
const
int
kFilterWidth
=
5
;
const
int
kFilterHeight
=
5
;
const
int
kSkipWidth
=
4
;
const
int
kSkipHeight
=
4
;
assert
(
layer_config
-
>
filter_width
=
=
kFilterWidth
&
&
layer_config
-
>
filter_height
=
=
kFilterHeight
)
;
assert
(
layer_config
-
>
skip_width
=
=
kSkipWidth
&
&
layer_config
-
>
skip_height
=
=
kSkipHeight
)
;
const
__m256i
block0_1
=
_mm256_load_si256
(
(
const
__m256i
*
)
shuffle_src_layer0
[
0
]
)
;
const
__m256i
block1_2
=
_mm256_load_si256
(
(
const
__m256i
*
)
shuffle_src_layer0
[
1
]
)
;
const
__m256i
weight_mask_0
=
_mm256_load_si256
(
(
const
__m256i
*
)
shuffle_weight_layer0
[
0
]
)
;
const
__m256i
weight_mask_1
=
_mm256_load_si256
(
(
const
__m256i
*
)
shuffle_weight_layer0
[
1
]
)
;
const
int
kSkipWidthForNextIter
=
kSkipWidth
*
3
;
const
int
kMinWidthFor3_5x5Blocks
=
(
kSkipWidth
*
2
)
+
kFilterWidth
;
for
(
int
i
=
start_idx
;
i
<
layer_config
-
>
out_channels
;
i
+
=
channel_step
)
{
const
float
out_ch_bias
=
layer_config
-
>
bias
[
i
]
;
for
(
int
k
=
0
;
k
<
layer_config
-
>
in_channels
;
+
+
k
)
{
__m256
shuffle_weight
[
10
]
;
float
weight
[
5
]
[
8
]
=
{
{
0
}
}
;
int
off
=
k
*
layer_config
-
>
out_channels
+
i
;
prepare_weights_for_5x5_convolve
(
layer_config
-
>
weights
off
weight
cstep
shuffle_weight
weight_mask_0
weight_mask_1
)
;
for
(
int
h
=
0
u
=
0
;
h
<
in_height
-
kFilterHeight
+
1
;
h
+
=
kSkipHeight
+
+
u
)
{
const
int
out_h
=
u
*
out_stride
;
int
v
=
0
;
int
w
=
0
;
int
rem_width
=
in_width
;
while
(
rem_width
>
=
kMinWidthFor3_5x5Blocks
)
{
__m256
load_src_0
load_src_1
;
__m256
accum_src_0
=
_mm256_setzero_ps
(
)
;
__m256
accum_src_1
=
_mm256_setzero_ps
(
)
;
const
float
*
input_ptr
=
&
input
[
k
]
[
h
*
in_stride
+
w
]
;
PERFORM_CONVOLVE_FOR_3_5X5_BLOCKS
(
)
;
__m256
accum
=
_mm256_hadd_ps
(
accum_src_0
accum_src_1
)
;
__m128
tmp_reg_0
=
_mm256_extractf128_ps
(
accum_src_0
1
)
;
__m128
tmp_reg_1
=
_mm256_extractf128_ps
(
accum_src_1
1
)
;
__m128
accum_l
=
_mm256_castps256_ps128
(
accum
)
;
__m128
accum_h
=
_mm256_extractf128_ps
(
accum
1
)
;
__m128
tmp_reg_2
=
_mm_add_ps
(
accum_l
tmp_reg_0
)
;
__m128
tmp_reg_3
=
_mm_add_ps
(
tmp_reg_0
accum_h
)
;
__m128
tmp_reg_4
=
_mm_add_ps
(
tmp_reg_1
accum_h
)
;
output
[
i
]
[
out_h
+
v
]
=
out_ch_bias
+
_mm_cvtss_f32
(
tmp_reg_2
)
+
_mm_cvtss_f32
(
_mm_shuffle_ps
(
accum_l
accum_l
1
)
)
;
output
[
i
]
[
out_h
+
v
+
1
]
=
out_ch_bias
+
_mm_cvtss_f32
(
_mm_shuffle_ps
(
tmp_reg_3
tmp_reg_3
1
)
)
+
_mm_cvtss_f32
(
_mm_shuffle_ps
(
accum_l
accum_l
2
)
)
;
output
[
i
]
[
out_h
+
v
+
2
]
=
out_ch_bias
+
_mm_cvtss_f32
(
_mm_shuffle_ps
(
tmp_reg_4
tmp_reg_4
2
)
)
+
_mm_cvtss_f32
(
_mm_shuffle_ps
(
accum_l
accum_l
3
)
)
;
v
+
=
3
;
w
+
=
kSkipWidthForNextIter
;
rem_width
-
=
kSkipWidthForNextIter
;
}
while
(
rem_width
>
=
kFilterWidth
)
{
float
last_column_sum
=
0
;
__m128
accum
=
_mm_setzero_ps
(
)
;
const
float
*
input_ptr
=
&
input
[
k
]
[
h
*
in_stride
+
w
]
;
PERFORM_CONVOLVE_FOR_1_5X5_BLOCK
(
shuffle_weight
accum
in_stride
)
;
accum
=
_mm_hadd_ps
(
accum
accum
)
;
output
[
i
]
[
out_h
+
v
]
=
out_ch_bias
+
last_column_sum
+
_mm_cvtss_f32
(
accum
)
+
_mm_cvtss_f32
(
_mm_shuffle_ps
(
accum
accum
1
)
)
;
v
+
=
1
;
w
+
=
kSkipWidth
;
rem_width
-
=
kSkipWidth
;
}
}
}
}
}
static
inline
void
cnn_convolve_no_maxpool_padding_valid_layer1_avx2
(
const
float
*
*
input
int
in_stride
const
CNN_LAYER_CONFIG
*
const
layer_config
float
*
*
output
int
out_stride
int
start_idx
const
int
cstep
const
int
channel_step
)
{
__m256i
weight_mask
[
2
]
;
__m256i
shuffle_output_mask
;
load_shuffle_masks_for_2x2_convolve
(
&
shuffle_output_mask
weight_mask
)
;
const
int
kInHeight
=
16
;
const
int
kFilterHeight
=
2
;
const
int
kSkipHeight
=
2
;
for
(
int
i
=
start_idx
;
i
<
layer_config
-
>
out_channels
;
i
+
=
channel_step
)
{
__m256
bias_reg
=
_mm256_set1_ps
(
layer_config
-
>
bias
[
i
]
)
;
__m256
out_accum
[
8
]
;
for
(
int
j
=
0
;
j
<
8
;
+
+
j
)
out_accum
[
j
]
=
bias_reg
;
for
(
int
k
=
0
;
k
<
layer_config
-
>
in_channels
;
+
+
k
)
{
__m256
shuffle_weight
[
2
]
;
int
off
=
k
*
layer_config
-
>
out_channels
+
i
;
prepare_weights_for_2x2_convolve
(
layer_config
-
>
weights
off
cstep
shuffle_weight
weight_mask
)
;
for
(
int
h
=
0
u
=
0
;
h
<
kInHeight
-
kFilterHeight
+
1
;
h
+
=
kSkipHeight
+
+
u
)
{
const
float
*
input_ptr
=
&
input
[
k
]
[
h
*
in_stride
]
;
perform_convolve_for_8h_2x2_blocks
(
input_ptr
in_stride
shuffle_weight
&
out_accum
[
u
]
shuffle_output_mask
)
;
}
}
for
(
int
j
=
0
;
j
<
8
;
+
+
j
)
{
_mm256_storeu_ps
(
&
output
[
i
]
[
j
*
out_stride
]
out_accum
[
j
]
)
;
}
}
}
static
inline
void
cnn_convolve_no_maxpool_padding_valid_layer2_avx2
(
const
float
*
*
input
int
in_stride
const
CNN_LAYER_CONFIG
*
const
layer_config
float
*
*
output
int
out_stride
int
start_idx
const
int
cstep
const
int
channel_step
)
{
__m256i
weight_mask
[
2
]
;
__m256i
shuffle_output_mask
;
load_shuffle_masks_for_2x2_convolve
(
&
shuffle_output_mask
weight_mask
)
;
const
int
kInHeight
=
8
;
const
int
kFilterHeight
=
2
;
const
int
kSkipHeight
=
2
;
for
(
int
i
=
start_idx
;
i
<
layer_config
-
>
out_channels
;
i
+
=
channel_step
)
{
__m256
bias_reg
=
_mm256_set1_ps
(
layer_config
-
>
bias
[
i
]
)
;
__m256
out_accum
[
2
]
;
const
int
kSkipHeightForNextIter
=
kSkipHeight
*
2
;
for
(
int
j
=
0
;
j
<
2
;
+
+
j
)
out_accum
[
j
]
=
bias_reg
;
for
(
int
k
=
0
;
k
<
layer_config
-
>
in_channels
;
+
+
k
)
{
__m256
shuffle_weight
[
2
]
;
int
off
=
k
*
layer_config
-
>
out_channels
+
i
;
prepare_weights_for_2x2_convolve
(
layer_config
-
>
weights
off
cstep
shuffle_weight
weight_mask
)
;
for
(
int
h
=
0
u
=
0
;
h
<
kInHeight
-
kFilterHeight
+
1
;
h
+
=
kSkipHeightForNextIter
+
+
u
)
{
const
float
*
input_ptr
=
&
input
[
k
]
[
h
*
in_stride
]
;
perform_convolve_for_4hx2v_2x2_blocks
(
input_ptr
in_stride
shuffle_weight
&
out_accum
[
u
]
shuffle_output_mask
)
;
}
}
for
(
int
j
=
0
;
j
<
2
;
+
+
j
)
{
_mm256_storeu_ps
(
&
output
[
i
]
[
j
*
out_stride
*
2
]
out_accum
[
j
]
)
;
}
}
}
static
void
cnn_convolve_no_maxpool_padding_valid_2x2_avx2
(
const
float
*
*
input
int
in_width
int
in_height
int
in_stride
const
CNN_LAYER_CONFIG
*
const
layer_config
float
*
*
output
int
out_stride
int
start_idx
const
int
cstep
const
int
channel_step
)
{
assert
(
layer_config
-
>
filter_width
=
=
2
&
&
layer_config
-
>
filter_height
=
=
2
)
;
assert
(
layer_config
-
>
skip_width
=
=
2
&
&
layer_config
-
>
skip_height
=
=
2
)
;
if
(
in_width
=
=
16
&
&
in_height
=
=
16
)
{
cnn_convolve_no_maxpool_padding_valid_layer1_avx2
(
input
in_stride
layer_config
output
out_stride
start_idx
cstep
channel_step
)
;
}
else
if
(
in_width
=
=
8
&
&
in_height
=
=
8
)
{
cnn_convolve_no_maxpool_padding_valid_layer2_avx2
(
input
in_stride
layer_config
output
out_stride
start_idx
cstep
channel_step
)
;
}
else
{
av1_cnn_convolve_no_maxpool_padding_valid_c
(
input
in_width
in_height
in_stride
layer_config
output
out_stride
start_idx
cstep
channel_step
)
;
}
}
void
av1_cnn_convolve_no_maxpool_padding_valid_avx2
(
const
float
*
*
input
int
in_width
int
in_height
int
in_stride
const
CNN_LAYER_CONFIG
*
layer_config
float
*
*
output
int
out_stride
int
start_idx
int
cstep
int
channel_step
)
{
if
(
layer_config
-
>
filter_width
=
=
5
&
&
layer_config
-
>
filter_height
=
=
5
&
&
layer_config
-
>
skip_width
=
=
4
&
&
layer_config
-
>
skip_height
=
=
4
)
{
cnn_convolve_no_maxpool_padding_valid_5x5_avx2
(
input
in_width
in_height
in_stride
layer_config
output
out_stride
start_idx
cstep
channel_step
)
;
}
else
if
(
layer_config
-
>
filter_width
=
=
2
&
&
layer_config
-
>
filter_height
=
=
2
&
&
layer_config
-
>
skip_width
=
=
2
&
&
layer_config
-
>
skip_height
=
=
2
)
{
cnn_convolve_no_maxpool_padding_valid_2x2_avx2
(
input
in_width
in_height
in_stride
layer_config
output
out_stride
start_idx
cstep
channel_step
)
;
}
else
{
av1_cnn_convolve_no_maxpool_padding_valid_c
(
input
in_width
in_height
in_stride
layer_config
output
out_stride
start_idx
cstep
channel_step
)
;
}
}
