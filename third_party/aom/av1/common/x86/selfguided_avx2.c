#
include
<
immintrin
.
h
>
#
include
"
config
/
aom_config
.
h
"
#
include
"
config
/
av1_rtcd
.
h
"
#
include
"
av1
/
common
/
restoration
.
h
"
#
include
"
aom_dsp
/
x86
/
synonyms
.
h
"
#
include
"
aom_dsp
/
x86
/
synonyms_avx2
.
h
"
static
__m256i
yy256_load_extend_8_32
(
const
void
*
p
)
{
return
_mm256_cvtepu8_epi32
(
xx_loadl_64
(
p
)
)
;
}
static
__m256i
yy256_load_extend_16_32
(
const
void
*
p
)
{
return
_mm256_cvtepu16_epi32
(
xx_loadu_128
(
p
)
)
;
}
static
__m256i
scan_32
(
__m256i
x
)
{
const
__m256i
x01
=
_mm256_slli_si256
(
x
4
)
;
const
__m256i
x02
=
_mm256_add_epi32
(
x
x01
)
;
const
__m256i
x03
=
_mm256_slli_si256
(
x02
8
)
;
const
__m256i
x04
=
_mm256_add_epi32
(
x02
x03
)
;
const
int32_t
s
=
_mm256_extract_epi32
(
x04
3
)
;
const
__m128i
s01
=
_mm_set1_epi32
(
s
)
;
const
__m256i
s02
=
_mm256_insertf128_si256
(
_mm256_setzero_si256
(
)
s01
1
)
;
return
_mm256_add_epi32
(
x04
s02
)
;
}
static
void
*
memset_zero_avx
(
int32_t
*
dest
const
__m256i
*
zero
size_t
count
)
{
unsigned
int
i
=
0
;
for
(
i
=
0
;
i
<
(
count
&
0xffffffe0
)
;
i
+
=
32
)
{
_mm256_storeu_si256
(
(
__m256i
*
)
(
dest
+
i
)
*
zero
)
;
_mm256_storeu_si256
(
(
__m256i
*
)
(
dest
+
i
+
8
)
*
zero
)
;
_mm256_storeu_si256
(
(
__m256i
*
)
(
dest
+
i
+
16
)
*
zero
)
;
_mm256_storeu_si256
(
(
__m256i
*
)
(
dest
+
i
+
24
)
*
zero
)
;
}
for
(
;
i
<
(
count
&
0xfffffff8
)
;
i
+
=
8
)
{
_mm256_storeu_si256
(
(
__m256i
*
)
(
dest
+
i
)
*
zero
)
;
}
for
(
;
i
<
count
;
i
+
+
)
{
dest
[
i
]
=
0
;
}
return
dest
;
}
static
void
integral_images
(
const
uint8_t
*
src
int
src_stride
int
width
int
height
int32_t
*
A
int32_t
*
B
int
buf_stride
)
{
const
__m256i
zero
=
_mm256_setzero_si256
(
)
;
memset_zero_avx
(
A
&
zero
(
width
+
8
)
)
;
memset_zero_avx
(
B
&
zero
(
width
+
8
)
)
;
for
(
int
i
=
0
;
i
<
height
;
+
+
i
)
{
A
[
(
i
+
1
)
*
buf_stride
]
=
B
[
(
i
+
1
)
*
buf_stride
]
=
0
;
__m256i
ldiff1
=
zero
ldiff2
=
zero
;
for
(
int
j
=
0
;
j
<
width
;
j
+
=
8
)
{
const
int
ABj
=
1
+
j
;
const
__m256i
above1
=
yy_load_256
(
B
+
ABj
+
i
*
buf_stride
)
;
const
__m256i
above2
=
yy_load_256
(
A
+
ABj
+
i
*
buf_stride
)
;
const
__m256i
x1
=
yy256_load_extend_8_32
(
src
+
j
+
i
*
src_stride
)
;
const
__m256i
x2
=
_mm256_madd_epi16
(
x1
x1
)
;
const
__m256i
sc1
=
scan_32
(
x1
)
;
const
__m256i
sc2
=
scan_32
(
x2
)
;
const
__m256i
row1
=
_mm256_add_epi32
(
_mm256_add_epi32
(
sc1
above1
)
ldiff1
)
;
const
__m256i
row2
=
_mm256_add_epi32
(
_mm256_add_epi32
(
sc2
above2
)
ldiff2
)
;
yy_store_256
(
B
+
ABj
+
(
i
+
1
)
*
buf_stride
row1
)
;
yy_store_256
(
A
+
ABj
+
(
i
+
1
)
*
buf_stride
row2
)
;
ldiff1
=
_mm256_set1_epi32
(
_mm256_extract_epi32
(
_mm256_sub_epi32
(
row1
above1
)
7
)
)
;
ldiff2
=
_mm256_set1_epi32
(
_mm256_extract_epi32
(
_mm256_sub_epi32
(
row2
above2
)
7
)
)
;
}
}
}
static
void
integral_images_highbd
(
const
uint16_t
*
src
int
src_stride
int
width
int
height
int32_t
*
A
int32_t
*
B
int
buf_stride
)
{
const
__m256i
zero
=
_mm256_setzero_si256
(
)
;
memset_zero_avx
(
A
&
zero
(
width
+
8
)
)
;
memset_zero_avx
(
B
&
zero
(
width
+
8
)
)
;
for
(
int
i
=
0
;
i
<
height
;
+
+
i
)
{
A
[
(
i
+
1
)
*
buf_stride
]
=
B
[
(
i
+
1
)
*
buf_stride
]
=
0
;
__m256i
ldiff1
=
zero
ldiff2
=
zero
;
for
(
int
j
=
0
;
j
<
width
;
j
+
=
8
)
{
const
int
ABj
=
1
+
j
;
const
__m256i
above1
=
yy_load_256
(
B
+
ABj
+
i
*
buf_stride
)
;
const
__m256i
above2
=
yy_load_256
(
A
+
ABj
+
i
*
buf_stride
)
;
const
__m256i
x1
=
yy256_load_extend_16_32
(
src
+
j
+
i
*
src_stride
)
;
const
__m256i
x2
=
_mm256_madd_epi16
(
x1
x1
)
;
const
__m256i
sc1
=
scan_32
(
x1
)
;
const
__m256i
sc2
=
scan_32
(
x2
)
;
const
__m256i
row1
=
_mm256_add_epi32
(
_mm256_add_epi32
(
sc1
above1
)
ldiff1
)
;
const
__m256i
row2
=
_mm256_add_epi32
(
_mm256_add_epi32
(
sc2
above2
)
ldiff2
)
;
yy_store_256
(
B
+
ABj
+
(
i
+
1
)
*
buf_stride
row1
)
;
yy_store_256
(
A
+
ABj
+
(
i
+
1
)
*
buf_stride
row2
)
;
ldiff1
=
_mm256_set1_epi32
(
_mm256_extract_epi32
(
_mm256_sub_epi32
(
row1
above1
)
7
)
)
;
ldiff2
=
_mm256_set1_epi32
(
_mm256_extract_epi32
(
_mm256_sub_epi32
(
row2
above2
)
7
)
)
;
}
}
}
static
INLINE
__m256i
boxsum_from_ii
(
const
int32_t
*
ii
int
stride
int
r
)
{
const
__m256i
tl
=
yy_loadu_256
(
ii
-
(
r
+
1
)
-
(
r
+
1
)
*
stride
)
;
const
__m256i
tr
=
yy_loadu_256
(
ii
+
(
r
+
0
)
-
(
r
+
1
)
*
stride
)
;
const
__m256i
bl
=
yy_loadu_256
(
ii
-
(
r
+
1
)
+
r
*
stride
)
;
const
__m256i
br
=
yy_loadu_256
(
ii
+
(
r
+
0
)
+
r
*
stride
)
;
const
__m256i
u
=
_mm256_sub_epi32
(
tr
tl
)
;
const
__m256i
v
=
_mm256_sub_epi32
(
br
bl
)
;
return
_mm256_sub_epi32
(
v
u
)
;
}
static
__m256i
round_for_shift
(
unsigned
shift
)
{
return
_mm256_set1_epi32
(
(
1
<
<
shift
)
>
>
1
)
;
}
static
__m256i
compute_p
(
__m256i
sum1
__m256i
sum2
int
bit_depth
int
n
)
{
__m256i
an
bb
;
if
(
bit_depth
>
8
)
{
const
__m256i
rounding_a
=
round_for_shift
(
2
*
(
bit_depth
-
8
)
)
;
const
__m256i
rounding_b
=
round_for_shift
(
bit_depth
-
8
)
;
const
__m128i
shift_a
=
_mm_cvtsi32_si128
(
2
*
(
bit_depth
-
8
)
)
;
const
__m128i
shift_b
=
_mm_cvtsi32_si128
(
bit_depth
-
8
)
;
const
__m256i
a
=
_mm256_srl_epi32
(
_mm256_add_epi32
(
sum2
rounding_a
)
shift_a
)
;
const
__m256i
b
=
_mm256_srl_epi32
(
_mm256_add_epi32
(
sum1
rounding_b
)
shift_b
)
;
bb
=
_mm256_madd_epi16
(
b
b
)
;
an
=
_mm256_max_epi32
(
_mm256_mullo_epi32
(
a
_mm256_set1_epi32
(
n
)
)
bb
)
;
}
else
{
bb
=
_mm256_madd_epi16
(
sum1
sum1
)
;
an
=
_mm256_mullo_epi32
(
sum2
_mm256_set1_epi32
(
n
)
)
;
}
return
_mm256_sub_epi32
(
an
bb
)
;
}
static
void
calc_ab
(
int32_t
*
A
int32_t
*
B
const
int32_t
*
C
const
int32_t
*
D
int
width
int
height
int
buf_stride
int
bit_depth
int
sgr_params_idx
int
radius_idx
)
{
const
sgr_params_type
*
const
params
=
&
sgr_params
[
sgr_params_idx
]
;
const
int
r
=
params
-
>
r
[
radius_idx
]
;
const
int
n
=
(
2
*
r
+
1
)
*
(
2
*
r
+
1
)
;
const
__m256i
s
=
_mm256_set1_epi32
(
params
-
>
s
[
radius_idx
]
)
;
const
__m256i
one_over_n
=
_mm256_set1_epi32
(
one_by_x
[
n
-
1
]
)
;
const
__m256i
rnd_z
=
round_for_shift
(
SGRPROJ_MTABLE_BITS
)
;
const
__m256i
rnd_res
=
round_for_shift
(
SGRPROJ_RECIP_BITS
)
;
const
__m128i
ones32
=
_mm_set_epi32
(
0
0
0xffffffff
0xffffffff
)
;
__m256i
mask
[
8
]
;
for
(
int
idx
=
0
;
idx
<
8
;
idx
+
+
)
{
const
__m128i
shift
=
_mm_cvtsi32_si128
(
8
*
(
8
-
idx
)
)
;
mask
[
idx
]
=
_mm256_cvtepi8_epi32
(
_mm_srl_epi64
(
ones32
shift
)
)
;
}
for
(
int
i
=
-
1
;
i
<
height
+
1
;
+
+
i
)
{
for
(
int
j
=
-
1
;
j
<
width
+
1
;
j
+
=
8
)
{
const
int32_t
*
Cij
=
C
+
i
*
buf_stride
+
j
;
const
int32_t
*
Dij
=
D
+
i
*
buf_stride
+
j
;
__m256i
sum1
=
boxsum_from_ii
(
Dij
buf_stride
r
)
;
__m256i
sum2
=
boxsum_from_ii
(
Cij
buf_stride
r
)
;
int
idx
=
AOMMIN
(
8
width
+
1
-
j
)
;
assert
(
idx
>
=
1
)
;
if
(
idx
<
8
)
{
sum1
=
_mm256_and_si256
(
mask
[
idx
]
sum1
)
;
sum2
=
_mm256_and_si256
(
mask
[
idx
]
sum2
)
;
}
const
__m256i
p
=
compute_p
(
sum1
sum2
bit_depth
n
)
;
const
__m256i
z
=
_mm256_min_epi32
(
_mm256_srli_epi32
(
_mm256_add_epi32
(
_mm256_mullo_epi32
(
p
s
)
rnd_z
)
SGRPROJ_MTABLE_BITS
)
_mm256_set1_epi32
(
255
)
)
;
const
__m256i
a_res
=
_mm256_i32gather_epi32
(
x_by_xplus1
z
4
)
;
yy_storeu_256
(
A
+
i
*
buf_stride
+
j
a_res
)
;
const
__m256i
a_complement
=
_mm256_sub_epi32
(
_mm256_set1_epi32
(
SGRPROJ_SGR
)
a_res
)
;
const
__m256i
a_comp_over_n
=
_mm256_madd_epi16
(
a_complement
one_over_n
)
;
const
__m256i
b_int
=
_mm256_mullo_epi32
(
a_comp_over_n
sum1
)
;
const
__m256i
b_res
=
_mm256_srli_epi32
(
_mm256_add_epi32
(
b_int
rnd_res
)
SGRPROJ_RECIP_BITS
)
;
yy_storeu_256
(
B
+
i
*
buf_stride
+
j
b_res
)
;
}
}
}
static
INLINE
__m256i
cross_sum
(
const
int32_t
*
buf
int
stride
)
{
const
__m256i
xtl
=
yy_loadu_256
(
buf
-
1
-
stride
)
;
const
__m256i
xt
=
yy_loadu_256
(
buf
-
stride
)
;
const
__m256i
xtr
=
yy_loadu_256
(
buf
+
1
-
stride
)
;
const
__m256i
xl
=
yy_loadu_256
(
buf
-
1
)
;
const
__m256i
x
=
yy_loadu_256
(
buf
)
;
const
__m256i
xr
=
yy_loadu_256
(
buf
+
1
)
;
const
__m256i
xbl
=
yy_loadu_256
(
buf
-
1
+
stride
)
;
const
__m256i
xb
=
yy_loadu_256
(
buf
+
stride
)
;
const
__m256i
xbr
=
yy_loadu_256
(
buf
+
1
+
stride
)
;
const
__m256i
fours
=
_mm256_add_epi32
(
xl
_mm256_add_epi32
(
xt
_mm256_add_epi32
(
xr
_mm256_add_epi32
(
xb
x
)
)
)
)
;
const
__m256i
threes
=
_mm256_add_epi32
(
xtl
_mm256_add_epi32
(
xtr
_mm256_add_epi32
(
xbr
xbl
)
)
)
;
return
_mm256_sub_epi32
(
_mm256_slli_epi32
(
_mm256_add_epi32
(
fours
threes
)
2
)
threes
)
;
}
static
void
final_filter
(
int32_t
*
dst
int
dst_stride
const
int32_t
*
A
const
int32_t
*
B
int
buf_stride
const
void
*
dgd8
int
dgd_stride
int
width
int
height
int
highbd
)
{
const
int
nb
=
5
;
const
__m256i
rounding
=
round_for_shift
(
SGRPROJ_SGR_BITS
+
nb
-
SGRPROJ_RST_BITS
)
;
const
uint8_t
*
dgd_real
=
highbd
?
(
const
uint8_t
*
)
CONVERT_TO_SHORTPTR
(
dgd8
)
:
dgd8
;
for
(
int
i
=
0
;
i
<
height
;
+
+
i
)
{
for
(
int
j
=
0
;
j
<
width
;
j
+
=
8
)
{
const
__m256i
a
=
cross_sum
(
A
+
i
*
buf_stride
+
j
buf_stride
)
;
const
__m256i
b
=
cross_sum
(
B
+
i
*
buf_stride
+
j
buf_stride
)
;
const
__m128i
raw
=
xx_loadu_128
(
dgd_real
+
(
(
i
*
dgd_stride
+
j
)
<
<
highbd
)
)
;
const
__m256i
src
=
highbd
?
_mm256_cvtepu16_epi32
(
raw
)
:
_mm256_cvtepu8_epi32
(
raw
)
;
__m256i
v
=
_mm256_add_epi32
(
_mm256_madd_epi16
(
a
src
)
b
)
;
__m256i
w
=
_mm256_srai_epi32
(
_mm256_add_epi32
(
v
rounding
)
SGRPROJ_SGR_BITS
+
nb
-
SGRPROJ_RST_BITS
)
;
yy_storeu_256
(
dst
+
i
*
dst_stride
+
j
w
)
;
}
}
}
static
void
calc_ab_fast
(
int32_t
*
A
int32_t
*
B
const
int32_t
*
C
const
int32_t
*
D
int
width
int
height
int
buf_stride
int
bit_depth
int
sgr_params_idx
int
radius_idx
)
{
const
sgr_params_type
*
const
params
=
&
sgr_params
[
sgr_params_idx
]
;
const
int
r
=
params
-
>
r
[
radius_idx
]
;
const
int
n
=
(
2
*
r
+
1
)
*
(
2
*
r
+
1
)
;
const
__m256i
s
=
_mm256_set1_epi32
(
params
-
>
s
[
radius_idx
]
)
;
const
__m256i
one_over_n
=
_mm256_set1_epi32
(
one_by_x
[
n
-
1
]
)
;
const
__m256i
rnd_z
=
round_for_shift
(
SGRPROJ_MTABLE_BITS
)
;
const
__m256i
rnd_res
=
round_for_shift
(
SGRPROJ_RECIP_BITS
)
;
const
__m128i
ones32
=
_mm_set_epi32
(
0
0
0xffffffff
0xffffffff
)
;
__m256i
mask
[
8
]
;
for
(
int
idx
=
0
;
idx
<
8
;
idx
+
+
)
{
const
__m128i
shift
=
_mm_cvtsi32_si128
(
8
*
(
8
-
idx
)
)
;
mask
[
idx
]
=
_mm256_cvtepi8_epi32
(
_mm_srl_epi64
(
ones32
shift
)
)
;
}
for
(
int
i
=
-
1
;
i
<
height
+
1
;
i
+
=
2
)
{
for
(
int
j
=
-
1
;
j
<
width
+
1
;
j
+
=
8
)
{
const
int32_t
*
Cij
=
C
+
i
*
buf_stride
+
j
;
const
int32_t
*
Dij
=
D
+
i
*
buf_stride
+
j
;
__m256i
sum1
=
boxsum_from_ii
(
Dij
buf_stride
r
)
;
__m256i
sum2
=
boxsum_from_ii
(
Cij
buf_stride
r
)
;
int
idx
=
AOMMIN
(
8
width
+
1
-
j
)
;
assert
(
idx
>
=
1
)
;
if
(
idx
<
8
)
{
sum1
=
_mm256_and_si256
(
mask
[
idx
]
sum1
)
;
sum2
=
_mm256_and_si256
(
mask
[
idx
]
sum2
)
;
}
const
__m256i
p
=
compute_p
(
sum1
sum2
bit_depth
n
)
;
const
__m256i
z
=
_mm256_min_epi32
(
_mm256_srli_epi32
(
_mm256_add_epi32
(
_mm256_mullo_epi32
(
p
s
)
rnd_z
)
SGRPROJ_MTABLE_BITS
)
_mm256_set1_epi32
(
255
)
)
;
const
__m256i
a_res
=
_mm256_i32gather_epi32
(
x_by_xplus1
z
4
)
;
yy_storeu_256
(
A
+
i
*
buf_stride
+
j
a_res
)
;
const
__m256i
a_complement
=
_mm256_sub_epi32
(
_mm256_set1_epi32
(
SGRPROJ_SGR
)
a_res
)
;
const
__m256i
a_comp_over_n
=
_mm256_madd_epi16
(
a_complement
one_over_n
)
;
const
__m256i
b_int
=
_mm256_mullo_epi32
(
a_comp_over_n
sum1
)
;
const
__m256i
b_res
=
_mm256_srli_epi32
(
_mm256_add_epi32
(
b_int
rnd_res
)
SGRPROJ_RECIP_BITS
)
;
yy_storeu_256
(
B
+
i
*
buf_stride
+
j
b_res
)
;
}
}
}
static
INLINE
__m256i
cross_sum_fast_even_row
(
const
int32_t
*
buf
int
stride
)
{
const
__m256i
xtl
=
yy_loadu_256
(
buf
-
1
-
stride
)
;
const
__m256i
xt
=
yy_loadu_256
(
buf
-
stride
)
;
const
__m256i
xtr
=
yy_loadu_256
(
buf
+
1
-
stride
)
;
const
__m256i
xbl
=
yy_loadu_256
(
buf
-
1
+
stride
)
;
const
__m256i
xb
=
yy_loadu_256
(
buf
+
stride
)
;
const
__m256i
xbr
=
yy_loadu_256
(
buf
+
1
+
stride
)
;
const
__m256i
fives
=
_mm256_add_epi32
(
xtl
_mm256_add_epi32
(
xtr
_mm256_add_epi32
(
xbr
xbl
)
)
)
;
const
__m256i
sixes
=
_mm256_add_epi32
(
xt
xb
)
;
const
__m256i
fives_plus_sixes
=
_mm256_add_epi32
(
fives
sixes
)
;
return
_mm256_add_epi32
(
_mm256_add_epi32
(
_mm256_slli_epi32
(
fives_plus_sixes
2
)
fives_plus_sixes
)
sixes
)
;
}
static
INLINE
__m256i
cross_sum_fast_odd_row
(
const
int32_t
*
buf
)
{
const
__m256i
xl
=
yy_loadu_256
(
buf
-
1
)
;
const
__m256i
x
=
yy_loadu_256
(
buf
)
;
const
__m256i
xr
=
yy_loadu_256
(
buf
+
1
)
;
const
__m256i
fives
=
_mm256_add_epi32
(
xl
xr
)
;
const
__m256i
sixes
=
x
;
const
__m256i
fives_plus_sixes
=
_mm256_add_epi32
(
fives
sixes
)
;
return
_mm256_add_epi32
(
_mm256_add_epi32
(
_mm256_slli_epi32
(
fives_plus_sixes
2
)
fives_plus_sixes
)
sixes
)
;
}
static
void
final_filter_fast
(
int32_t
*
dst
int
dst_stride
const
int32_t
*
A
const
int32_t
*
B
int
buf_stride
const
void
*
dgd8
int
dgd_stride
int
width
int
height
int
highbd
)
{
const
int
nb0
=
5
;
const
int
nb1
=
4
;
const
__m256i
rounding0
=
round_for_shift
(
SGRPROJ_SGR_BITS
+
nb0
-
SGRPROJ_RST_BITS
)
;
const
__m256i
rounding1
=
round_for_shift
(
SGRPROJ_SGR_BITS
+
nb1
-
SGRPROJ_RST_BITS
)
;
const
uint8_t
*
dgd_real
=
highbd
?
(
const
uint8_t
*
)
CONVERT_TO_SHORTPTR
(
dgd8
)
:
dgd8
;
for
(
int
i
=
0
;
i
<
height
;
+
+
i
)
{
if
(
!
(
i
&
1
)
)
{
for
(
int
j
=
0
;
j
<
width
;
j
+
=
8
)
{
const
__m256i
a
=
cross_sum_fast_even_row
(
A
+
i
*
buf_stride
+
j
buf_stride
)
;
const
__m256i
b
=
cross_sum_fast_even_row
(
B
+
i
*
buf_stride
+
j
buf_stride
)
;
const
__m128i
raw
=
xx_loadu_128
(
dgd_real
+
(
(
i
*
dgd_stride
+
j
)
<
<
highbd
)
)
;
const
__m256i
src
=
highbd
?
_mm256_cvtepu16_epi32
(
raw
)
:
_mm256_cvtepu8_epi32
(
raw
)
;
__m256i
v
=
_mm256_add_epi32
(
_mm256_madd_epi16
(
a
src
)
b
)
;
__m256i
w
=
_mm256_srai_epi32
(
_mm256_add_epi32
(
v
rounding0
)
SGRPROJ_SGR_BITS
+
nb0
-
SGRPROJ_RST_BITS
)
;
yy_storeu_256
(
dst
+
i
*
dst_stride
+
j
w
)
;
}
}
else
{
for
(
int
j
=
0
;
j
<
width
;
j
+
=
8
)
{
const
__m256i
a
=
cross_sum_fast_odd_row
(
A
+
i
*
buf_stride
+
j
)
;
const
__m256i
b
=
cross_sum_fast_odd_row
(
B
+
i
*
buf_stride
+
j
)
;
const
__m128i
raw
=
xx_loadu_128
(
dgd_real
+
(
(
i
*
dgd_stride
+
j
)
<
<
highbd
)
)
;
const
__m256i
src
=
highbd
?
_mm256_cvtepu16_epi32
(
raw
)
:
_mm256_cvtepu8_epi32
(
raw
)
;
__m256i
v
=
_mm256_add_epi32
(
_mm256_madd_epi16
(
a
src
)
b
)
;
__m256i
w
=
_mm256_srai_epi32
(
_mm256_add_epi32
(
v
rounding1
)
SGRPROJ_SGR_BITS
+
nb1
-
SGRPROJ_RST_BITS
)
;
yy_storeu_256
(
dst
+
i
*
dst_stride
+
j
w
)
;
}
}
}
}
void
av1_selfguided_restoration_avx2
(
const
uint8_t
*
dgd8
int
width
int
height
int
dgd_stride
int32_t
*
flt0
int32_t
*
flt1
int
flt_stride
int
sgr_params_idx
int
bit_depth
int
highbd
)
{
const
int
buf_elts
=
ALIGN_POWER_OF_TWO
(
RESTORATION_PROC_UNIT_PELS
3
)
;
DECLARE_ALIGNED
(
32
int32_t
buf
[
4
*
ALIGN_POWER_OF_TWO
(
RESTORATION_PROC_UNIT_PELS
3
)
]
)
;
const
int
width_ext
=
width
+
2
*
SGRPROJ_BORDER_HORZ
;
const
int
height_ext
=
height
+
2
*
SGRPROJ_BORDER_VERT
;
int
buf_stride
=
ALIGN_POWER_OF_TWO
(
width_ext
+
16
3
)
;
int32_t
*
Atl
=
buf
+
0
*
buf_elts
+
7
;
int32_t
*
Btl
=
buf
+
1
*
buf_elts
+
7
;
int32_t
*
Ctl
=
buf
+
2
*
buf_elts
+
7
;
int32_t
*
Dtl
=
buf
+
3
*
buf_elts
+
7
;
const
int
buf_diag_border
=
SGRPROJ_BORDER_HORZ
+
buf_stride
*
SGRPROJ_BORDER_VERT
;
int32_t
*
A0
=
Atl
+
1
+
buf_stride
;
int32_t
*
B0
=
Btl
+
1
+
buf_stride
;
int32_t
*
C0
=
Ctl
+
1
+
buf_stride
;
int32_t
*
D0
=
Dtl
+
1
+
buf_stride
;
int32_t
*
A
=
A0
+
buf_diag_border
;
int32_t
*
B
=
B0
+
buf_diag_border
;
int32_t
*
C
=
C0
+
buf_diag_border
;
int32_t
*
D
=
D0
+
buf_diag_border
;
const
int
dgd_diag_border
=
SGRPROJ_BORDER_HORZ
+
dgd_stride
*
SGRPROJ_BORDER_VERT
;
const
uint8_t
*
dgd0
=
dgd8
-
dgd_diag_border
;
if
(
highbd
)
integral_images_highbd
(
CONVERT_TO_SHORTPTR
(
dgd0
)
dgd_stride
width_ext
height_ext
Ctl
Dtl
buf_stride
)
;
else
integral_images
(
dgd0
dgd_stride
width_ext
height_ext
Ctl
Dtl
buf_stride
)
;
const
sgr_params_type
*
const
params
=
&
sgr_params
[
sgr_params_idx
]
;
assert
(
!
(
params
-
>
r
[
0
]
=
=
0
&
&
params
-
>
r
[
1
]
=
=
0
)
)
;
assert
(
params
-
>
r
[
0
]
<
AOMMIN
(
SGRPROJ_BORDER_VERT
SGRPROJ_BORDER_HORZ
)
)
;
assert
(
params
-
>
r
[
1
]
<
AOMMIN
(
SGRPROJ_BORDER_VERT
SGRPROJ_BORDER_HORZ
)
)
;
if
(
params
-
>
r
[
0
]
>
0
)
{
calc_ab_fast
(
A
B
C
D
width
height
buf_stride
bit_depth
sgr_params_idx
0
)
;
final_filter_fast
(
flt0
flt_stride
A
B
buf_stride
dgd8
dgd_stride
width
height
highbd
)
;
}
if
(
params
-
>
r
[
1
]
>
0
)
{
calc_ab
(
A
B
C
D
width
height
buf_stride
bit_depth
sgr_params_idx
1
)
;
final_filter
(
flt1
flt_stride
A
B
buf_stride
dgd8
dgd_stride
width
height
highbd
)
;
}
}
void
apply_selfguided_restoration_avx2
(
const
uint8_t
*
dat8
int
width
int
height
int
stride
int
eps
const
int
*
xqd
uint8_t
*
dst8
int
dst_stride
int32_t
*
tmpbuf
int
bit_depth
int
highbd
)
{
int32_t
*
flt0
=
tmpbuf
;
int32_t
*
flt1
=
flt0
+
RESTORATION_UNITPELS_MAX
;
assert
(
width
*
height
<
=
RESTORATION_UNITPELS_MAX
)
;
av1_selfguided_restoration_avx2
(
dat8
width
height
stride
flt0
flt1
width
eps
bit_depth
highbd
)
;
const
sgr_params_type
*
const
params
=
&
sgr_params
[
eps
]
;
int
xq
[
2
]
;
decode_xq
(
xqd
xq
params
)
;
__m256i
xq0
=
_mm256_set1_epi32
(
xq
[
0
]
)
;
__m256i
xq1
=
_mm256_set1_epi32
(
xq
[
1
]
)
;
for
(
int
i
=
0
;
i
<
height
;
+
+
i
)
{
for
(
int
j
=
0
;
j
<
width
;
j
+
=
16
)
{
const
int
k
=
i
*
width
+
j
;
const
int
m
=
i
*
dst_stride
+
j
;
const
uint8_t
*
dat8ij
=
dat8
+
i
*
stride
+
j
;
__m256i
ep_0
ep_1
;
__m128i
src_0
src_1
;
if
(
highbd
)
{
src_0
=
xx_loadu_128
(
CONVERT_TO_SHORTPTR
(
dat8ij
)
)
;
src_1
=
xx_loadu_128
(
CONVERT_TO_SHORTPTR
(
dat8ij
+
8
)
)
;
ep_0
=
_mm256_cvtepu16_epi32
(
src_0
)
;
ep_1
=
_mm256_cvtepu16_epi32
(
src_1
)
;
}
else
{
src_0
=
xx_loadu_128
(
dat8ij
)
;
ep_0
=
_mm256_cvtepu8_epi32
(
src_0
)
;
ep_1
=
_mm256_cvtepu8_epi32
(
_mm_srli_si128
(
src_0
8
)
)
;
}
const
__m256i
u_0
=
_mm256_slli_epi32
(
ep_0
SGRPROJ_RST_BITS
)
;
const
__m256i
u_1
=
_mm256_slli_epi32
(
ep_1
SGRPROJ_RST_BITS
)
;
__m256i
v_0
=
_mm256_slli_epi32
(
u_0
SGRPROJ_PRJ_BITS
)
;
__m256i
v_1
=
_mm256_slli_epi32
(
u_1
SGRPROJ_PRJ_BITS
)
;
if
(
params
-
>
r
[
0
]
>
0
)
{
const
__m256i
f1_0
=
_mm256_sub_epi32
(
yy_loadu_256
(
&
flt0
[
k
]
)
u_0
)
;
v_0
=
_mm256_add_epi32
(
v_0
_mm256_mullo_epi32
(
xq0
f1_0
)
)
;
const
__m256i
f1_1
=
_mm256_sub_epi32
(
yy_loadu_256
(
&
flt0
[
k
+
8
]
)
u_1
)
;
v_1
=
_mm256_add_epi32
(
v_1
_mm256_mullo_epi32
(
xq0
f1_1
)
)
;
}
if
(
params
-
>
r
[
1
]
>
0
)
{
const
__m256i
f2_0
=
_mm256_sub_epi32
(
yy_loadu_256
(
&
flt1
[
k
]
)
u_0
)
;
v_0
=
_mm256_add_epi32
(
v_0
_mm256_mullo_epi32
(
xq1
f2_0
)
)
;
const
__m256i
f2_1
=
_mm256_sub_epi32
(
yy_loadu_256
(
&
flt1
[
k
+
8
]
)
u_1
)
;
v_1
=
_mm256_add_epi32
(
v_1
_mm256_mullo_epi32
(
xq1
f2_1
)
)
;
}
const
__m256i
rounding
=
round_for_shift
(
SGRPROJ_PRJ_BITS
+
SGRPROJ_RST_BITS
)
;
const
__m256i
w_0
=
_mm256_srai_epi32
(
_mm256_add_epi32
(
v_0
rounding
)
SGRPROJ_PRJ_BITS
+
SGRPROJ_RST_BITS
)
;
const
__m256i
w_1
=
_mm256_srai_epi32
(
_mm256_add_epi32
(
v_1
rounding
)
SGRPROJ_PRJ_BITS
+
SGRPROJ_RST_BITS
)
;
if
(
highbd
)
{
const
__m256i
tmp
=
_mm256_packus_epi32
(
w_0
w_1
)
;
const
__m256i
tmp2
=
_mm256_permute4x64_epi64
(
tmp
0xd8
)
;
const
__m256i
max
=
_mm256_set1_epi16
(
(
1
<
<
bit_depth
)
-
1
)
;
const
__m256i
res
=
_mm256_min_epi16
(
tmp2
max
)
;
yy_storeu_256
(
CONVERT_TO_SHORTPTR
(
dst8
+
m
)
res
)
;
}
else
{
const
__m256i
tmp
=
_mm256_packs_epi32
(
w_0
w_1
)
;
const
__m256i
tmp2
=
_mm256_permute4x64_epi64
(
tmp
0xd8
)
;
const
__m256i
res
=
_mm256_packus_epi16
(
tmp2
tmp2
)
;
const
__m128i
res2
=
_mm256_castsi256_si128
(
_mm256_permute4x64_epi64
(
res
0xd8
)
)
;
xx_storeu_128
(
dst8
+
m
res2
)
;
}
}
}
}
