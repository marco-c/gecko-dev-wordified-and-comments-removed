#
include
<
smmintrin
.
h
>
#
include
<
emmintrin
.
h
>
#
include
<
tmmintrin
.
h
>
#
include
<
float
.
h
>
#
include
"
.
/
av1_rtcd
.
h
"
#
include
"
av1
/
common
/
x86
/
pvq_sse4
.
h
"
#
include
"
.
.
/
odintrin
.
h
"
#
include
"
av1
/
common
/
pvq
.
h
"
#
define
EPSILON
1e
-
15f
static
__m128
horizontal_sum_ps
(
__m128
x
)
{
x
=
_mm_add_ps
(
x
_mm_shuffle_ps
(
x
x
_MM_SHUFFLE
(
1
0
3
2
)
)
)
;
x
=
_mm_add_ps
(
x
_mm_shuffle_ps
(
x
x
_MM_SHUFFLE
(
2
3
0
1
)
)
)
;
return
x
;
}
static
__m128i
horizontal_sum_epi32
(
__m128i
x
)
{
x
=
_mm_add_epi32
(
x
_mm_shuffle_epi32
(
x
_MM_SHUFFLE
(
1
0
3
2
)
)
)
;
x
=
_mm_add_epi32
(
x
_mm_shuffle_epi32
(
x
_MM_SHUFFLE
(
2
3
0
1
)
)
)
;
return
x
;
}
static
INLINE
float
rsqrtf
(
float
x
)
{
float
y
;
_mm_store_ss
(
&
y
_mm_rsqrt_ss
(
_mm_load_ss
(
&
x
)
)
)
;
return
y
;
}
double
pvq_search_rdo_double_sse4_1
(
const
od_val16
*
xcoeff
int
n
int
k
int
*
ypulse
double
g2
double
pvq_norm_lambda
int
prev_k
)
{
int
i
j
;
int
reuse_pulses
=
prev_k
>
0
&
&
prev_k
<
=
k
;
float
xx
=
0
xy
=
0
yy
=
0
;
float
x
[
MAXN
+
3
]
;
float
y
[
MAXN
+
3
]
;
float
sign_y
[
MAXN
+
3
]
;
for
(
i
=
0
;
i
<
n
;
i
+
+
)
{
float
tmp
=
(
float
)
xcoeff
[
i
]
;
xx
+
=
tmp
*
tmp
;
x
[
i
]
=
xcoeff
[
i
]
;
}
x
[
n
]
=
x
[
n
+
1
]
=
x
[
n
+
2
]
=
0
;
ypulse
[
n
]
=
ypulse
[
n
+
1
]
=
ypulse
[
n
+
2
]
=
0
;
__m128
sums
=
_mm_setzero_ps
(
)
;
for
(
i
=
0
;
i
<
n
;
i
+
=
4
)
{
__m128
x4
=
_mm_loadu_ps
(
&
x
[
i
]
)
;
__m128
s4
=
_mm_cmplt_ps
(
x4
_mm_setzero_ps
(
)
)
;
_mm_storeu_ps
(
&
sign_y
[
i
]
s4
)
;
x4
=
_mm_andnot_ps
(
_mm_set_ps1
(
-
0
.
f
)
x4
)
;
sums
=
_mm_add_ps
(
sums
x4
)
;
if
(
!
reuse_pulses
)
{
_mm_storeu_ps
(
&
y
[
i
]
_mm_setzero_ps
(
)
)
;
_mm_storeu_si128
(
(
__m128i
*
)
&
ypulse
[
i
]
_mm_setzero_si128
(
)
)
;
}
_mm_storeu_ps
(
&
x
[
i
]
x4
)
;
}
sums
=
horizontal_sum_ps
(
sums
)
;
int
pulses_left
=
k
;
{
__m128i
pulses_sum
;
__m128
yy4
xy4
;
xy4
=
yy4
=
_mm_setzero_ps
(
)
;
pulses_sum
=
_mm_setzero_si128
(
)
;
if
(
reuse_pulses
)
{
for
(
j
=
0
;
j
<
n
;
j
+
=
4
)
{
__m128
x4
y4
;
__m128i
iy4
;
iy4
=
_mm_abs_epi32
(
_mm_loadu_si128
(
(
__m128i
*
)
&
ypulse
[
j
]
)
)
;
pulses_sum
=
_mm_add_epi32
(
pulses_sum
iy4
)
;
_mm_storeu_si128
(
(
__m128i
*
)
&
ypulse
[
j
]
iy4
)
;
y4
=
_mm_cvtepi32_ps
(
iy4
)
;
x4
=
_mm_loadu_ps
(
&
x
[
j
]
)
;
xy4
=
_mm_add_ps
(
xy4
_mm_mul_ps
(
x4
y4
)
)
;
yy4
=
_mm_add_ps
(
yy4
_mm_mul_ps
(
y4
y4
)
)
;
_mm_storeu_ps
(
&
y
[
j
]
_mm_add_ps
(
y4
y4
)
)
;
}
pulses_left
-
=
_mm_cvtsi128_si32
(
horizontal_sum_epi32
(
pulses_sum
)
)
;
xy4
=
horizontal_sum_ps
(
xy4
)
;
xy
=
_mm_cvtss_f32
(
xy4
)
;
yy4
=
horizontal_sum_ps
(
yy4
)
;
yy
=
_mm_cvtss_f32
(
yy4
)
;
}
else
if
(
k
>
(
n
>
>
1
)
)
{
__m128
rcp4
;
float
sum
=
_mm_cvtss_f32
(
sums
)
;
if
(
sum
<
=
EPSILON
)
{
x
[
0
]
=
1
.
f
;
for
(
i
=
1
;
i
<
n
;
i
+
+
)
{
x
[
i
]
=
0
;
}
sums
=
_mm_set_ps1
(
1
.
f
)
;
}
rcp4
=
_mm_mul_ps
(
_mm_set_ps1
(
(
float
)
k
+
.
8f
)
_mm_rcp_ps
(
sums
)
)
;
xy4
=
yy4
=
_mm_setzero_ps
(
)
;
pulses_sum
=
_mm_setzero_si128
(
)
;
for
(
j
=
0
;
j
<
n
;
j
+
=
4
)
{
__m128
rx4
x4
y4
;
__m128i
iy4
;
x4
=
_mm_loadu_ps
(
&
x
[
j
]
)
;
rx4
=
_mm_mul_ps
(
x4
rcp4
)
;
iy4
=
_mm_cvttps_epi32
(
rx4
)
;
pulses_sum
=
_mm_add_epi32
(
pulses_sum
iy4
)
;
_mm_storeu_si128
(
(
__m128i
*
)
&
ypulse
[
j
]
iy4
)
;
y4
=
_mm_cvtepi32_ps
(
iy4
)
;
xy4
=
_mm_add_ps
(
xy4
_mm_mul_ps
(
x4
y4
)
)
;
yy4
=
_mm_add_ps
(
yy4
_mm_mul_ps
(
y4
y4
)
)
;
_mm_storeu_ps
(
&
y
[
j
]
_mm_add_ps
(
y4
y4
)
)
;
}
pulses_left
-
=
_mm_cvtsi128_si32
(
horizontal_sum_epi32
(
pulses_sum
)
)
;
xy
=
_mm_cvtss_f32
(
horizontal_sum_ps
(
xy4
)
)
;
yy
=
_mm_cvtss_f32
(
horizontal_sum_ps
(
yy4
)
)
;
}
x
[
n
]
=
x
[
n
+
1
]
=
x
[
n
+
2
]
=
-
100
;
y
[
n
]
=
y
[
n
+
1
]
=
y
[
n
+
2
]
=
100
;
}
OD_ASSERT
(
pulses_left
<
=
n
+
3
)
;
float
lambda_delta_rate
[
MAXN
+
3
]
;
if
(
pulses_left
)
{
float
lambda
=
0
.
5f
*
sqrtf
(
xx
)
*
(
float
)
pvq_norm_lambda
/
(
FLT_MIN
+
(
float
)
g2
)
;
float
delta_rate
=
3
.
f
/
n
;
__m128
count
=
_mm_set_ps
(
3
2
1
0
)
;
for
(
i
=
0
;
i
<
n
;
i
+
=
4
)
{
_mm_storeu_ps
(
&
lambda_delta_rate
[
i
]
_mm_mul_ps
(
count
_mm_set_ps1
(
lambda
*
delta_rate
)
)
)
;
count
=
_mm_add_ps
(
count
_mm_set_ps
(
4
4
4
4
)
)
;
}
}
lambda_delta_rate
[
n
]
=
lambda_delta_rate
[
n
+
1
]
=
lambda_delta_rate
[
n
+
2
]
=
1e30f
;
for
(
i
=
0
;
i
<
pulses_left
;
i
+
+
)
{
int
best_id
=
0
;
__m128
xy4
yy4
;
__m128
max
max2
;
__m128i
count
;
__m128i
pos
;
yy
=
yy
+
1
;
xy4
=
_mm_load1_ps
(
&
xy
)
;
yy4
=
_mm_load1_ps
(
&
yy
)
;
max
=
_mm_setzero_ps
(
)
;
pos
=
_mm_setzero_si128
(
)
;
count
=
_mm_set_epi32
(
3
2
1
0
)
;
for
(
j
=
0
;
j
<
n
;
j
+
=
4
)
{
__m128
x4
y4
r4
;
x4
=
_mm_loadu_ps
(
&
x
[
j
]
)
;
y4
=
_mm_loadu_ps
(
&
y
[
j
]
)
;
x4
=
_mm_add_ps
(
x4
xy4
)
;
y4
=
_mm_add_ps
(
y4
yy4
)
;
y4
=
_mm_rsqrt_ps
(
y4
)
;
r4
=
_mm_mul_ps
(
x4
y4
)
;
r4
=
_mm_sub_ps
(
r4
_mm_loadu_ps
(
&
lambda_delta_rate
[
j
]
)
)
;
pos
=
_mm_max_epi16
(
pos
_mm_and_si128
(
count
_mm_castps_si128
(
_mm_cmpgt_ps
(
r4
max
)
)
)
)
;
max
=
_mm_max_ps
(
max
r4
)
;
count
=
_mm_add_epi32
(
count
_mm_set_epi32
(
4
4
4
4
)
)
;
}
max2
=
_mm_max_ps
(
max
_mm_shuffle_ps
(
max
max
_MM_SHUFFLE
(
1
0
3
2
)
)
)
;
max2
=
_mm_max_ps
(
max2
_mm_shuffle_ps
(
max2
max2
_MM_SHUFFLE
(
2
3
0
1
)
)
)
;
pos
=
_mm_and_si128
(
pos
_mm_castps_si128
(
_mm_cmpeq_ps
(
max
max2
)
)
)
;
pos
=
_mm_max_epi16
(
pos
_mm_unpackhi_epi64
(
pos
pos
)
)
;
pos
=
_mm_max_epi16
(
pos
_mm_shufflelo_epi16
(
pos
_MM_SHUFFLE
(
1
0
3
2
)
)
)
;
best_id
=
_mm_cvtsi128_si32
(
pos
)
;
OD_ASSERT
(
best_id
<
n
)
;
xy
=
xy
+
x
[
best_id
]
;
yy
=
yy
+
y
[
best_id
]
;
y
[
best_id
]
+
=
2
;
ypulse
[
best_id
]
+
+
;
}
for
(
i
=
0
;
i
<
n
;
i
+
=
4
)
{
__m128i
y4
;
__m128i
s4
;
y4
=
_mm_loadu_si128
(
(
__m128i
*
)
&
ypulse
[
i
]
)
;
s4
=
_mm_castps_si128
(
_mm_loadu_ps
(
&
sign_y
[
i
]
)
)
;
y4
=
_mm_xor_si128
(
_mm_add_epi32
(
y4
s4
)
s4
)
;
_mm_storeu_si128
(
(
__m128i
*
)
&
ypulse
[
i
]
y4
)
;
}
return
xy
*
rsqrtf
(
xx
*
yy
+
FLT_MIN
)
;
}
