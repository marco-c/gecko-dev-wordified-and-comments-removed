#
include
<
immintrin
.
h
>
#
include
<
string
.
h
>
#
include
"
config
/
av1_rtcd
.
h
"
#
include
"
av1
/
common
/
resize
.
h
"
#
include
"
aom_dsp
/
x86
/
synonyms
.
h
"
#
define
CAST_HI
(
x
)
_mm256_castsi128_si256
(
x
)
#
define
CAST_LOW
(
x
)
_mm256_castsi256_si128
(
x
)
#
define
PROCESS_RESIZE_Y_WD16
\
const
int
idx1
=
AOMMIN
(
height
-
1
i
+
5
)
;
\
const
int
idx2
=
AOMMIN
(
height
-
1
i
+
6
)
;
\
l6
=
l10
;
\
l7
=
l11
;
\
l8
=
_mm_loadu_si128
(
(
__m128i
*
)
(
data
+
idx1
*
stride
)
)
;
\
l9
=
_mm_loadu_si128
(
(
__m128i
*
)
(
data
+
idx2
*
stride
)
)
;
\
\
/
*
g0
.
.
.
g15
|
i0
.
.
.
i15
*
/
\
const
__m256i
s68
=
\
_mm256_permute2x128_si256
(
CAST_HI
(
l6
)
CAST_HI
(
l8
)
0x20
)
;
\
/
*
h0
.
.
.
h15
|
j0
.
.
.
j15
*
/
\
const
__m256i
s79
=
\
_mm256_permute2x128_si256
(
CAST_HI
(
l7
)
CAST_HI
(
l9
)
0x20
)
;
\
\
/
*
g0h0
.
.
.
g7g7
|
i0j0
.
.
.
i7j
*
/
\
s
[
3
]
=
_mm256_unpacklo_epi8
(
s68
s79
)
;
\
/
*
g8h8
.
.
.
g15g15
|
i8j8
.
.
.
i15j15
*
/
\
s
[
8
]
=
_mm256_unpackhi_epi8
(
s68
s79
)
;
\
\
__m256i
res_out
[
2
]
=
{
0
}
;
\
resize_y_convolve
(
s
coeffs_y
res_out
)
;
\
\
/
*
r00
.
.
.
r07
*
/
\
__m256i
res_a_round_1
=
_mm256_add_epi32
(
res_out
[
0
]
round_const_bits
)
;
\
/
*
r20
.
.
.
r27
*
/
\
__m256i
res_a_round_2
=
_mm256_add_epi32
(
res_out
[
1
]
round_const_bits
)
;
\
\
res_a_round_1
=
_mm256_sra_epi32
(
res_a_round_1
round_shift_bits
)
;
\
res_a_round_2
=
_mm256_sra_epi32
(
res_a_round_2
round_shift_bits
)
;
\
\
__m256i
res_out_b
[
2
]
=
{
0
}
;
\
resize_y_convolve
(
s
+
5
coeffs_y
res_out_b
)
;
\
\
/
*
r08
.
.
.
r015
*
/
\
__m256i
res_b_round_1
=
_mm256_add_epi32
(
res_out_b
[
0
]
round_const_bits
)
;
\
/
*
r28
.
.
.
r215
*
/
\
__m256i
res_b_round_2
=
_mm256_add_epi32
(
res_out_b
[
1
]
round_const_bits
)
;
\
res_b_round_1
=
_mm256_sra_epi32
(
res_b_round_1
round_shift_bits
)
;
\
res_b_round_2
=
_mm256_sra_epi32
(
res_b_round_2
round_shift_bits
)
;
\
\
/
*
r00
.
.
.
r03
r20
.
.
.
r23
|
r04
.
.
.
r07
r24
.
.
.
r27
*
/
\
__m256i
res_8bit0
=
_mm256_packus_epi32
(
res_a_round_1
res_a_round_2
)
;
\
/
*
r08
.
.
.
r012
r28
.
.
.
r212
|
r013
.
.
.
r015
r213
.
.
.
r215
*
/
\
__m256i
res_8bit1
=
_mm256_packus_epi32
(
res_b_round_1
res_b_round_2
)
;
\
/
*
r00
.
.
.
r07
|
r20
.
.
.
r27
*
/
\
res_8bit0
=
_mm256_permute4x64_epi64
(
res_8bit0
0xd8
)
;
\
/
*
r08
.
.
.
r015
|
r28
.
.
.
r215
*
/
\
res_8bit1
=
_mm256_permute4x64_epi64
(
res_8bit1
0xd8
)
;
\
/
*
r00
.
.
.
r015
|
r20
.
.
.
r215
*
/
\
res_8bit1
=
_mm256_packus_epi16
(
res_8bit0
res_8bit1
)
;
\
res_8bit0
=
_mm256_min_epu8
(
res_8bit1
clip_pixel
)
;
\
res_8bit0
=
_mm256_max_epu8
(
res_8bit0
zero
)
;
#
define
PROCESS_RESIZE_Y_WD8
\
const
int
idx1
=
AOMMIN
(
height
-
1
i
+
5
)
;
\
const
int
idx2
=
AOMMIN
(
height
-
1
i
+
6
)
;
\
l6
=
l10
;
\
l7
=
l11
;
\
l8
=
_mm_loadl_epi64
(
(
__m128i
*
)
(
data
+
idx1
*
stride
)
)
;
\
l9
=
_mm_loadl_epi64
(
(
__m128i
*
)
(
data
+
idx2
*
stride
)
)
;
\
\
/
*
g0h0
.
.
.
g7h7
*
/
\
s67
=
_mm_unpacklo_epi8
(
l6
l7
)
;
\
/
*
i0j0
.
.
.
i7j7
*
/
\
__m128i
s89
=
_mm_unpacklo_epi8
(
l8
l9
)
;
\
\
/
*
g0h0
.
.
.
g7g7
|
i0j0
.
.
.
i7j7
*
/
\
s
[
3
]
=
_mm256_permute2x128_si256
(
CAST_HI
(
s67
)
CAST_HI
(
s89
)
0x20
)
;
\
\
__m256i
res_out
[
2
]
=
{
0
}
;
\
resize_y_convolve
(
s
coeffs_y
res_out
)
;
\
\
/
*
r00
.
.
.
r07
*
/
\
__m256i
res_a_round_1
=
_mm256_add_epi32
(
res_out
[
0
]
round_const_bits
)
;
\
/
*
r20
.
.
.
r27
*
/
\
__m256i
res_a_round_2
=
_mm256_add_epi32
(
res_out
[
1
]
round_const_bits
)
;
\
res_a_round_1
=
_mm256_sra_epi32
(
res_a_round_1
round_shift_bits
)
;
\
res_a_round_2
=
_mm256_sra_epi32
(
res_a_round_2
round_shift_bits
)
;
\
\
/
*
r00
.
.
.
r03
r20
.
.
.
r23
|
r04
.
.
.
r07
r24
.
.
.
r27
*
/
\
res_a_round_1
=
_mm256_packus_epi32
(
res_a_round_1
res_a_round_2
)
;
\
/
*
r00
.
.
.
r07
|
r20
.
.
.
r27
*
/
\
res_a_round_1
=
_mm256_permute4x64_epi64
(
res_a_round_1
0xd8
)
;
\
res_a_round_1
=
_mm256_packus_epi16
(
res_a_round_1
res_a_round_1
)
;
\
res_a_round_1
=
_mm256_min_epu8
(
res_a_round_1
clip_pixel
)
;
\
res_a_round_1
=
_mm256_max_epu8
(
res_a_round_1
zero
)
;
static
INLINE
void
resize_y_convolve
(
const
__m256i
*
const
s
const
__m256i
*
const
coeffs
__m256i
*
res_out
)
{
const
__m256i
res_0
=
_mm256_maddubs_epi16
(
s
[
0
]
coeffs
[
0
]
)
;
const
__m256i
res_1
=
_mm256_maddubs_epi16
(
s
[
1
]
coeffs
[
1
]
)
;
const
__m256i
res_2
=
_mm256_maddubs_epi16
(
s
[
2
]
coeffs
[
2
]
)
;
const
__m256i
res_3
=
_mm256_maddubs_epi16
(
s
[
3
]
coeffs
[
3
]
)
;
const
__m256i
dst_0
=
_mm256_add_epi16
(
res_0
res_1
)
;
const
__m256i
dst_1
=
_mm256_add_epi16
(
res_2
res_3
)
;
const
__m256i
dst_00
=
_mm256_cvtepi16_epi32
(
CAST_LOW
(
dst_0
)
)
;
const
__m256i
dst_01
=
_mm256_cvtepi16_epi32
(
_mm256_extracti128_si256
(
dst_0
1
)
)
;
const
__m256i
dst_10
=
_mm256_cvtepi16_epi32
(
CAST_LOW
(
dst_1
)
)
;
const
__m256i
dst_11
=
_mm256_cvtepi16_epi32
(
_mm256_extracti128_si256
(
dst_1
1
)
)
;
res_out
[
0
]
=
_mm256_add_epi32
(
dst_00
dst_10
)
;
res_out
[
1
]
=
_mm256_add_epi32
(
dst_01
dst_11
)
;
}
static
INLINE
void
prepare_filter_coeffs
(
const
int16_t
*
filter
__m256i
*
const
coeffs
)
{
const
__m128i
sym_even_filter
=
_mm_loadl_epi64
(
(
__m128i
*
)
filter
)
;
const
__m128i
tmp0
=
_mm_shuffle_epi32
(
sym_even_filter
0x44
)
;
const
__m128i
tmp1
=
_mm_shufflehi_epi16
(
tmp0
0xb1
)
;
const
__m128i
filter_8bit
=
_mm_packs_epi16
(
tmp1
tmp1
)
;
coeffs
[
2
]
=
_mm256_broadcastw_epi16
(
filter_8bit
)
;
coeffs
[
3
]
=
_mm256_broadcastw_epi16
(
_mm_bsrli_si128
(
filter_8bit
2
)
)
;
coeffs
[
0
]
=
_mm256_broadcastw_epi16
(
_mm_bsrli_si128
(
filter_8bit
6
)
)
;
coeffs
[
1
]
=
_mm256_broadcastw_epi16
(
_mm_bsrli_si128
(
filter_8bit
4
)
)
;
}
bool
resize_vert_dir_avx2
(
uint8_t
*
intbuf
uint8_t
*
output
int
out_stride
int
height
int
height2
int
stride
int
start_col
)
{
assert
(
start_col
<
=
stride
)
;
if
(
height
&
1
|
|
height
<
8
)
{
return
resize_vert_dir_c
(
intbuf
output
out_stride
height
height2
stride
start_col
)
;
}
__m256i
s
[
10
]
coeffs_y
[
4
]
;
const
int
bits
=
FILTER_BITS
;
const
__m128i
round_shift_bits
=
_mm_cvtsi32_si128
(
bits
)
;
const
__m256i
round_const_bits
=
_mm256_set1_epi32
(
(
1
<
<
bits
)
>
>
1
)
;
const
uint8_t
max_pixel
=
255
;
const
__m256i
clip_pixel
=
_mm256_set1_epi8
(
max_pixel
)
;
const
__m256i
zero
=
_mm256_setzero_si256
(
)
;
prepare_filter_coeffs
(
av1_down2_symeven_half_filter
coeffs_y
)
;
const
int
num_col16
=
stride
/
16
;
int
remain_col
=
stride
%
16
;
const
int
remain_row
=
(
height
%
4
=
=
0
)
?
4
:
6
;
for
(
int
j
=
start_col
;
j
<
stride
-
remain_col
;
j
+
=
16
)
{
const
uint8_t
*
data
=
&
intbuf
[
j
]
;
const
__m128i
l3
=
_mm_loadu_si128
(
(
__m128i
*
)
(
data
+
0
*
stride
)
)
;
const
__m128i
l0
=
l3
;
const
__m128i
l1
=
l3
;
const
__m128i
l2
=
l3
;
const
__m128i
l4
=
_mm_loadu_si128
(
(
__m128i
*
)
(
data
+
1
*
stride
)
)
;
__m128i
l6
l7
l8
l9
;
__m128i
l5
=
_mm_loadu_si128
(
(
__m128i
*
)
(
data
+
2
*
stride
)
)
;
__m128i
l10
=
_mm_loadu_si128
(
(
__m128i
*
)
(
data
+
3
*
stride
)
)
;
__m128i
l11
=
_mm_loadu_si128
(
(
__m128i
*
)
(
data
+
4
*
stride
)
)
;
const
__m256i
s02
=
_mm256_permute2x128_si256
(
CAST_HI
(
l0
)
CAST_HI
(
l2
)
0x20
)
;
const
__m256i
s13
=
_mm256_permute2x128_si256
(
CAST_HI
(
l1
)
CAST_HI
(
l3
)
0x20
)
;
const
__m256i
s24
=
_mm256_permute2x128_si256
(
CAST_HI
(
l2
)
CAST_HI
(
l4
)
0x20
)
;
const
__m256i
s35
=
_mm256_permute2x128_si256
(
CAST_HI
(
l3
)
CAST_HI
(
l5
)
0x20
)
;
const
__m256i
s46
=
_mm256_permute2x128_si256
(
CAST_HI
(
l4
)
CAST_HI
(
l10
)
0x20
)
;
const
__m256i
s57
=
_mm256_permute2x128_si256
(
CAST_HI
(
l5
)
CAST_HI
(
l11
)
0x20
)
;
s
[
0
]
=
_mm256_unpacklo_epi8
(
s02
s13
)
;
s
[
1
]
=
_mm256_unpacklo_epi8
(
s24
s35
)
;
s
[
2
]
=
_mm256_unpacklo_epi8
(
s46
s57
)
;
s
[
5
]
=
_mm256_unpackhi_epi8
(
s02
s13
)
;
s
[
6
]
=
_mm256_unpackhi_epi8
(
s24
s35
)
;
s
[
7
]
=
_mm256_unpackhi_epi8
(
s46
s57
)
;
const
int
process_ht
=
height
-
remain_row
;
for
(
int
i
=
0
;
i
<
process_ht
;
i
+
=
4
)
{
PROCESS_RESIZE_Y_WD16
_mm_storeu_si128
(
(
__m128i
*
)
&
output
[
(
i
/
2
)
*
out_stride
+
j
]
CAST_LOW
(
res_8bit0
)
)
;
_mm_storeu_si128
(
(
__m128i
*
)
&
output
[
(
i
/
2
)
*
out_stride
+
j
+
out_stride
]
_mm256_extracti128_si256
(
res_8bit0
1
)
)
;
const
int
idx7
=
AOMMIN
(
height
-
1
i
+
7
)
;
const
int
idx8
=
AOMMIN
(
height
-
1
i
+
8
)
;
l10
=
_mm_loadu_si128
(
(
__m128i
*
)
(
data
+
idx7
*
stride
)
)
;
l11
=
_mm_loadu_si128
(
(
__m128i
*
)
(
data
+
idx8
*
stride
)
)
;
const
__m256i
s810
=
_mm256_permute2x128_si256
(
CAST_HI
(
l8
)
CAST_HI
(
l10
)
0x20
)
;
const
__m256i
s911
=
_mm256_permute2x128_si256
(
CAST_HI
(
l9
)
CAST_HI
(
l11
)
0x20
)
;
s
[
4
]
=
_mm256_unpacklo_epi8
(
s810
s911
)
;
s
[
9
]
=
_mm256_unpackhi_epi8
(
s810
s911
)
;
s
[
0
]
=
s
[
2
]
;
s
[
1
]
=
s
[
3
]
;
s
[
2
]
=
s
[
4
]
;
s
[
5
]
=
s
[
7
]
;
s
[
6
]
=
s
[
8
]
;
s
[
7
]
=
s
[
9
]
;
}
int
i
=
process_ht
;
while
(
i
<
height
-
1
)
{
PROCESS_RESIZE_Y_WD16
_mm_storeu_si128
(
(
__m128i
*
)
&
output
[
(
i
/
2
)
*
out_stride
+
j
]
CAST_LOW
(
res_8bit0
)
)
;
i
+
=
2
;
const
int
is_store_valid
=
(
i
<
height
-
1
)
;
if
(
is_store_valid
)
_mm_storeu_si128
(
(
__m128i
*
)
&
output
[
(
i
/
2
)
*
out_stride
+
j
]
_mm256_extracti128_si256
(
res_8bit0
1
)
)
;
i
+
=
2
;
if
(
i
<
height
-
1
)
{
l10
=
l11
=
l9
;
const
__m256i
s810
=
_mm256_permute2x128_si256
(
CAST_HI
(
l8
)
CAST_HI
(
l10
)
0x20
)
;
const
__m256i
s911
=
_mm256_permute2x128_si256
(
CAST_HI
(
l9
)
CAST_HI
(
l11
)
0x20
)
;
s
[
4
]
=
_mm256_unpacklo_epi8
(
s810
s911
)
;
s
[
9
]
=
_mm256_unpackhi_epi8
(
s810
s911
)
;
s
[
0
]
=
s
[
2
]
;
s
[
1
]
=
s
[
3
]
;
s
[
2
]
=
s
[
4
]
;
s
[
5
]
=
s
[
7
]
;
s
[
6
]
=
s
[
8
]
;
s
[
7
]
=
s
[
9
]
;
}
}
}
if
(
remain_col
>
7
)
{
const
int
processed_wd
=
num_col16
*
16
;
remain_col
=
stride
%
8
;
const
uint8_t
*
data
=
&
intbuf
[
processed_wd
]
;
const
__m128i
l3
=
_mm_loadl_epi64
(
(
__m128i
*
)
(
data
+
0
*
stride
)
)
;
const
__m128i
l0
=
l3
;
const
__m128i
l1
=
l3
;
const
__m128i
l2
=
l3
;
const
__m128i
l4
=
_mm_loadl_epi64
(
(
__m128i
*
)
(
data
+
1
*
stride
)
)
;
__m128i
l6
l7
l8
l9
;
__m128i
l5
=
_mm_loadl_epi64
(
(
__m128i
*
)
(
data
+
2
*
stride
)
)
;
__m128i
l10
=
_mm_loadl_epi64
(
(
__m128i
*
)
(
data
+
3
*
stride
)
)
;
__m128i
l11
=
_mm_loadl_epi64
(
(
__m128i
*
)
(
data
+
4
*
stride
)
)
;
const
__m128i
s01
=
_mm_unpacklo_epi8
(
l0
l1
)
;
const
__m128i
s23
=
_mm_unpacklo_epi8
(
l2
l3
)
;
const
__m128i
s45
=
_mm_unpacklo_epi8
(
l4
l5
)
;
__m128i
s67
=
_mm_unpacklo_epi8
(
l10
l11
)
;
s
[
0
]
=
_mm256_permute2x128_si256
(
CAST_HI
(
s01
)
CAST_HI
(
s23
)
0x20
)
;
s
[
1
]
=
_mm256_permute2x128_si256
(
CAST_HI
(
s23
)
CAST_HI
(
s45
)
0x20
)
;
s
[
2
]
=
_mm256_permute2x128_si256
(
CAST_HI
(
s45
)
CAST_HI
(
s67
)
0x20
)
;
const
int
process_ht
=
height
-
remain_row
;
for
(
int
i
=
0
;
i
<
process_ht
;
i
+
=
4
)
{
PROCESS_RESIZE_Y_WD8
_mm_storel_epi64
(
(
__m128i
*
)
&
output
[
(
i
/
2
)
*
out_stride
+
processed_wd
]
CAST_LOW
(
res_a_round_1
)
)
;
_mm_storel_epi64
(
(
__m128i
*
)
&
output
[
(
i
/
2
)
*
out_stride
+
processed_wd
+
out_stride
]
_mm256_extracti128_si256
(
res_a_round_1
1
)
)
;
const
int
idx7
=
AOMMIN
(
height
-
1
i
+
7
)
;
const
int
idx8
=
AOMMIN
(
height
-
1
i
+
8
)
;
l10
=
_mm_loadl_epi64
(
(
__m128i
*
)
(
data
+
idx7
*
stride
)
)
;
l11
=
_mm_loadl_epi64
(
(
__m128i
*
)
(
data
+
idx8
*
stride
)
)
;
const
__m128i
s10s11
=
_mm_unpacklo_epi8
(
l10
l11
)
;
s
[
4
]
=
_mm256_permute2x128_si256
(
CAST_HI
(
s89
)
CAST_HI
(
s10s11
)
0x20
)
;
s
[
0
]
=
s
[
2
]
;
s
[
1
]
=
s
[
3
]
;
s
[
2
]
=
s
[
4
]
;
}
int
i
=
process_ht
;
while
(
i
<
height
-
1
)
{
PROCESS_RESIZE_Y_WD8
_mm_storel_epi64
(
(
__m128i
*
)
&
output
[
(
i
/
2
)
*
out_stride
+
processed_wd
]
CAST_LOW
(
res_a_round_1
)
)
;
i
+
=
2
;
const
int
is_store_valid
=
(
i
<
height
-
1
)
;
if
(
is_store_valid
)
_mm_storel_epi64
(
(
__m128i
*
)
&
output
[
(
i
/
2
)
*
out_stride
+
processed_wd
]
_mm256_extracti128_si256
(
res_a_round_1
1
)
)
;
i
+
=
2
;
if
(
i
<
height
-
1
)
{
l10
=
l11
=
l9
;
const
__m128i
s10s11
=
_mm_unpacklo_epi8
(
l10
l11
)
;
s
[
4
]
=
_mm256_permute2x128_si256
(
CAST_HI
(
s89
)
CAST_HI
(
s10s11
)
0x20
)
;
s
[
0
]
=
s
[
2
]
;
s
[
1
]
=
s
[
3
]
;
s
[
2
]
=
s
[
4
]
;
}
}
}
if
(
remain_col
)
return
resize_vert_dir_c
(
intbuf
output
out_stride
height
height2
stride
stride
-
remain_col
)
;
return
true
;
}
