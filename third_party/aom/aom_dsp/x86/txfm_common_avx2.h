#
ifndef
AOM_DSP_X86_TXFM_COMMON_AVX2_H
#
define
AOM_DSP_X86_TXFM_COMMON_AVX2_H
#
include
<
immintrin
.
h
>
#
include
"
aom_dsp
/
txfm_common
.
h
"
#
define
pair256_set_epi16
(
a
b
)
\
_mm256_set_epi16
(
(
int16_t
)
(
b
)
(
int16_t
)
(
a
)
(
int16_t
)
(
b
)
(
int16_t
)
(
a
)
\
(
int16_t
)
(
b
)
(
int16_t
)
(
a
)
(
int16_t
)
(
b
)
(
int16_t
)
(
a
)
\
(
int16_t
)
(
b
)
(
int16_t
)
(
a
)
(
int16_t
)
(
b
)
(
int16_t
)
(
a
)
\
(
int16_t
)
(
b
)
(
int16_t
)
(
a
)
(
int16_t
)
(
b
)
(
int16_t
)
(
a
)
)
#
define
pair256_set_epi32
(
a
b
)
\
_mm256_set_epi32
(
(
int
)
(
b
)
(
int
)
(
a
)
(
int
)
(
b
)
(
int
)
(
a
)
(
int
)
(
b
)
(
int
)
(
a
)
\
(
int
)
(
b
)
(
int
)
(
a
)
)
static
INLINE
void
mm256_reverse_epi16
(
__m256i
*
u
)
{
const
__m256i
control
=
_mm256_set_epi16
(
0x0100
0x0302
0x0504
0x0706
0x0908
0x0B0A
0x0D0C
0x0F0E
0x0100
0x0302
0x0504
0x0706
0x0908
0x0B0A
0x0D0C
0x0F0E
)
;
__m256i
v
=
_mm256_shuffle_epi8
(
*
u
control
)
;
*
u
=
_mm256_permute2x128_si256
(
v
v
1
)
;
}
static
INLINE
void
mm256_transpose_16x16
(
__m256i
*
in
)
{
__m256i
tr0_0
=
_mm256_unpacklo_epi16
(
in
[
0
]
in
[
1
]
)
;
__m256i
tr0_1
=
_mm256_unpackhi_epi16
(
in
[
0
]
in
[
1
]
)
;
__m256i
tr0_2
=
_mm256_unpacklo_epi16
(
in
[
2
]
in
[
3
]
)
;
__m256i
tr0_3
=
_mm256_unpackhi_epi16
(
in
[
2
]
in
[
3
]
)
;
__m256i
tr0_4
=
_mm256_unpacklo_epi16
(
in
[
4
]
in
[
5
]
)
;
__m256i
tr0_5
=
_mm256_unpackhi_epi16
(
in
[
4
]
in
[
5
]
)
;
__m256i
tr0_6
=
_mm256_unpacklo_epi16
(
in
[
6
]
in
[
7
]
)
;
__m256i
tr0_7
=
_mm256_unpackhi_epi16
(
in
[
6
]
in
[
7
]
)
;
__m256i
tr0_8
=
_mm256_unpacklo_epi16
(
in
[
8
]
in
[
9
]
)
;
__m256i
tr0_9
=
_mm256_unpackhi_epi16
(
in
[
8
]
in
[
9
]
)
;
__m256i
tr0_a
=
_mm256_unpacklo_epi16
(
in
[
10
]
in
[
11
]
)
;
__m256i
tr0_b
=
_mm256_unpackhi_epi16
(
in
[
10
]
in
[
11
]
)
;
__m256i
tr0_c
=
_mm256_unpacklo_epi16
(
in
[
12
]
in
[
13
]
)
;
__m256i
tr0_d
=
_mm256_unpackhi_epi16
(
in
[
12
]
in
[
13
]
)
;
__m256i
tr0_e
=
_mm256_unpacklo_epi16
(
in
[
14
]
in
[
15
]
)
;
__m256i
tr0_f
=
_mm256_unpackhi_epi16
(
in
[
14
]
in
[
15
]
)
;
__m256i
tr1_0
=
_mm256_unpacklo_epi32
(
tr0_0
tr0_2
)
;
__m256i
tr1_1
=
_mm256_unpackhi_epi32
(
tr0_0
tr0_2
)
;
__m256i
tr1_2
=
_mm256_unpacklo_epi32
(
tr0_1
tr0_3
)
;
__m256i
tr1_3
=
_mm256_unpackhi_epi32
(
tr0_1
tr0_3
)
;
__m256i
tr1_4
=
_mm256_unpacklo_epi32
(
tr0_4
tr0_6
)
;
__m256i
tr1_5
=
_mm256_unpackhi_epi32
(
tr0_4
tr0_6
)
;
__m256i
tr1_6
=
_mm256_unpacklo_epi32
(
tr0_5
tr0_7
)
;
__m256i
tr1_7
=
_mm256_unpackhi_epi32
(
tr0_5
tr0_7
)
;
__m256i
tr1_8
=
_mm256_unpacklo_epi32
(
tr0_8
tr0_a
)
;
__m256i
tr1_9
=
_mm256_unpackhi_epi32
(
tr0_8
tr0_a
)
;
__m256i
tr1_a
=
_mm256_unpacklo_epi32
(
tr0_9
tr0_b
)
;
__m256i
tr1_b
=
_mm256_unpackhi_epi32
(
tr0_9
tr0_b
)
;
__m256i
tr1_c
=
_mm256_unpacklo_epi32
(
tr0_c
tr0_e
)
;
__m256i
tr1_d
=
_mm256_unpackhi_epi32
(
tr0_c
tr0_e
)
;
__m256i
tr1_e
=
_mm256_unpacklo_epi32
(
tr0_d
tr0_f
)
;
__m256i
tr1_f
=
_mm256_unpackhi_epi32
(
tr0_d
tr0_f
)
;
tr0_0
=
_mm256_unpacklo_epi64
(
tr1_0
tr1_4
)
;
tr0_1
=
_mm256_unpackhi_epi64
(
tr1_0
tr1_4
)
;
tr0_2
=
_mm256_unpacklo_epi64
(
tr1_1
tr1_5
)
;
tr0_3
=
_mm256_unpackhi_epi64
(
tr1_1
tr1_5
)
;
tr0_4
=
_mm256_unpacklo_epi64
(
tr1_2
tr1_6
)
;
tr0_5
=
_mm256_unpackhi_epi64
(
tr1_2
tr1_6
)
;
tr0_6
=
_mm256_unpacklo_epi64
(
tr1_3
tr1_7
)
;
tr0_7
=
_mm256_unpackhi_epi64
(
tr1_3
tr1_7
)
;
tr0_8
=
_mm256_unpacklo_epi64
(
tr1_8
tr1_c
)
;
tr0_9
=
_mm256_unpackhi_epi64
(
tr1_8
tr1_c
)
;
tr0_a
=
_mm256_unpacklo_epi64
(
tr1_9
tr1_d
)
;
tr0_b
=
_mm256_unpackhi_epi64
(
tr1_9
tr1_d
)
;
tr0_c
=
_mm256_unpacklo_epi64
(
tr1_a
tr1_e
)
;
tr0_d
=
_mm256_unpackhi_epi64
(
tr1_a
tr1_e
)
;
tr0_e
=
_mm256_unpacklo_epi64
(
tr1_b
tr1_f
)
;
tr0_f
=
_mm256_unpackhi_epi64
(
tr1_b
tr1_f
)
;
in
[
0
]
=
_mm256_permute2x128_si256
(
tr0_0
tr0_8
0x20
)
;
in
[
8
]
=
_mm256_permute2x128_si256
(
tr0_0
tr0_8
0x31
)
;
in
[
1
]
=
_mm256_permute2x128_si256
(
tr0_1
tr0_9
0x20
)
;
in
[
9
]
=
_mm256_permute2x128_si256
(
tr0_1
tr0_9
0x31
)
;
in
[
2
]
=
_mm256_permute2x128_si256
(
tr0_2
tr0_a
0x20
)
;
in
[
10
]
=
_mm256_permute2x128_si256
(
tr0_2
tr0_a
0x31
)
;
in
[
3
]
=
_mm256_permute2x128_si256
(
tr0_3
tr0_b
0x20
)
;
in
[
11
]
=
_mm256_permute2x128_si256
(
tr0_3
tr0_b
0x31
)
;
in
[
4
]
=
_mm256_permute2x128_si256
(
tr0_4
tr0_c
0x20
)
;
in
[
12
]
=
_mm256_permute2x128_si256
(
tr0_4
tr0_c
0x31
)
;
in
[
5
]
=
_mm256_permute2x128_si256
(
tr0_5
tr0_d
0x20
)
;
in
[
13
]
=
_mm256_permute2x128_si256
(
tr0_5
tr0_d
0x31
)
;
in
[
6
]
=
_mm256_permute2x128_si256
(
tr0_6
tr0_e
0x20
)
;
in
[
14
]
=
_mm256_permute2x128_si256
(
tr0_6
tr0_e
0x31
)
;
in
[
7
]
=
_mm256_permute2x128_si256
(
tr0_7
tr0_f
0x20
)
;
in
[
15
]
=
_mm256_permute2x128_si256
(
tr0_7
tr0_f
0x31
)
;
}
static
INLINE
__m256i
butter_fly
(
__m256i
a0
__m256i
a1
const
__m256i
cospi
)
{
const
__m256i
dct_rounding
=
_mm256_set1_epi32
(
DCT_CONST_ROUNDING
)
;
__m256i
y0
=
_mm256_madd_epi16
(
a0
cospi
)
;
__m256i
y1
=
_mm256_madd_epi16
(
a1
cospi
)
;
y0
=
_mm256_add_epi32
(
y0
dct_rounding
)
;
y1
=
_mm256_add_epi32
(
y1
dct_rounding
)
;
y0
=
_mm256_srai_epi32
(
y0
DCT_CONST_BITS
)
;
y1
=
_mm256_srai_epi32
(
y1
DCT_CONST_BITS
)
;
return
_mm256_packs_epi32
(
y0
y1
)
;
}
static
INLINE
void
txfm_scaling16_avx2
(
const
int16_t
c
__m256i
*
in
)
{
const
__m256i
zero
=
_mm256_setzero_si256
(
)
;
const
__m256i
sqrt2_epi16
=
_mm256_set1_epi16
(
c
)
;
const
__m256i
dct_const_rounding
=
_mm256_set1_epi32
(
DCT_CONST_ROUNDING
)
;
__m256i
u0
u1
;
int
i
=
0
;
while
(
i
<
16
)
{
in
[
i
]
=
_mm256_slli_epi16
(
in
[
i
]
1
)
;
u0
=
_mm256_unpacklo_epi16
(
zero
in
[
i
]
)
;
u1
=
_mm256_unpackhi_epi16
(
zero
in
[
i
]
)
;
u0
=
_mm256_madd_epi16
(
u0
sqrt2_epi16
)
;
u1
=
_mm256_madd_epi16
(
u1
sqrt2_epi16
)
;
u0
=
_mm256_add_epi32
(
u0
dct_const_rounding
)
;
u1
=
_mm256_add_epi32
(
u1
dct_const_rounding
)
;
u0
=
_mm256_srai_epi32
(
u0
DCT_CONST_BITS
)
;
u1
=
_mm256_srai_epi32
(
u1
DCT_CONST_BITS
)
;
in
[
i
]
=
_mm256_packs_epi32
(
u0
u1
)
;
i
+
+
;
}
}
#
endif
