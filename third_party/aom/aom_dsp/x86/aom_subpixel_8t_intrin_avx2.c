#
include
<
immintrin
.
h
>
#
include
"
config
/
aom_dsp_rtcd
.
h
"
#
include
"
aom_dsp
/
x86
/
convolve
.
h
"
#
include
"
aom_dsp
/
x86
/
convolve_avx2
.
h
"
#
include
"
aom_ports
/
mem
.
h
"
#
if
defined
(
__clang__
)
#
if
(
__clang_major__
>
0
&
&
__clang_major__
<
3
)
|
|
\
(
__clang_major__
=
=
3
&
&
__clang_minor__
<
=
3
)
|
|
\
(
defined
(
__APPLE__
)
&
&
defined
(
__apple_build_version__
)
&
&
\
(
(
__clang_major__
=
=
4
&
&
__clang_minor__
<
=
2
)
|
|
\
(
__clang_major__
=
=
5
&
&
__clang_minor__
=
=
0
)
)
)
#
define
MM256_BROADCASTSI128_SI256
(
x
)
\
_mm_broadcastsi128_si256
(
(
__m128i
const
*
)
&
(
x
)
)
#
else
#
define
MM256_BROADCASTSI128_SI256
(
x
)
_mm256_broadcastsi128_si256
(
x
)
#
endif
#
elif
defined
(
__GNUC__
)
#
if
__GNUC__
<
4
|
|
(
__GNUC__
=
=
4
&
&
__GNUC_MINOR__
<
=
6
)
#
define
MM256_BROADCASTSI128_SI256
(
x
)
\
_mm_broadcastsi128_si256
(
(
__m128i
const
*
)
&
(
x
)
)
#
elif
__GNUC__
=
=
4
&
&
__GNUC_MINOR__
=
=
7
#
define
MM256_BROADCASTSI128_SI256
(
x
)
_mm_broadcastsi128_si256
(
x
)
#
else
#
define
MM256_BROADCASTSI128_SI256
(
x
)
_mm256_broadcastsi128_si256
(
x
)
#
endif
#
else
#
define
MM256_BROADCASTSI128_SI256
(
x
)
_mm256_broadcastsi128_si256
(
x
)
#
endif
static
INLINE
void
xx_storeu2_epi32
(
const
uint8_t
*
output_ptr
const
ptrdiff_t
stride
const
__m256i
*
a
)
{
*
(
(
uint32_t
*
)
(
output_ptr
)
)
=
_mm_cvtsi128_si32
(
_mm256_castsi256_si128
(
*
a
)
)
;
*
(
(
uint32_t
*
)
(
output_ptr
+
stride
)
)
=
_mm_cvtsi128_si32
(
_mm256_extracti128_si256
(
*
a
1
)
)
;
}
static
INLINE
__m256i
xx_loadu2_epi64
(
const
void
*
hi
const
void
*
lo
)
{
__m256i
a
=
_mm256_castsi128_si256
(
_mm_loadl_epi64
(
(
const
__m128i
*
)
(
lo
)
)
)
;
a
=
_mm256_inserti128_si256
(
a
_mm_loadl_epi64
(
(
const
__m128i
*
)
(
hi
)
)
1
)
;
return
a
;
}
static
INLINE
void
xx_storeu2_epi64
(
const
uint8_t
*
output_ptr
const
ptrdiff_t
stride
const
__m256i
*
a
)
{
_mm_storel_epi64
(
(
__m128i
*
)
output_ptr
_mm256_castsi256_si128
(
*
a
)
)
;
_mm_storel_epi64
(
(
__m128i
*
)
(
output_ptr
+
stride
)
_mm256_extractf128_si256
(
*
a
1
)
)
;
}
static
INLINE
__m256i
xx_loadu2_mi128
(
const
void
*
hi
const
void
*
lo
)
{
__m256i
a
=
_mm256_castsi128_si256
(
_mm_loadu_si128
(
(
const
__m128i
*
)
(
lo
)
)
)
;
a
=
_mm256_inserti128_si256
(
a
_mm_loadu_si128
(
(
const
__m128i
*
)
(
hi
)
)
1
)
;
return
a
;
}
static
INLINE
void
xx_store2_mi128
(
const
uint8_t
*
output_ptr
const
ptrdiff_t
stride
const
__m256i
*
a
)
{
_mm_store_si128
(
(
__m128i
*
)
output_ptr
_mm256_castsi256_si128
(
*
a
)
)
;
_mm_store_si128
(
(
__m128i
*
)
(
output_ptr
+
stride
)
_mm256_extractf128_si256
(
*
a
1
)
)
;
}
static
void
aom_filter_block1d4_h8_avx2
(
const
uint8_t
*
src_ptr
ptrdiff_t
src_pixels_per_line
uint8_t
*
output_ptr
ptrdiff_t
output_pitch
uint32_t
output_height
const
int16_t
*
filter
)
{
__m128i
filtersReg
;
__m256i
addFilterReg32
filt1Reg
filt2Reg
;
__m256i
firstFilters
secondFilters
;
__m256i
srcRegFilt32b1_1
srcRegFilt32b2
;
__m256i
srcReg32b1
;
unsigned
int
i
;
ptrdiff_t
src_stride
dst_stride
;
src_ptr
-
=
3
;
addFilterReg32
=
_mm256_set1_epi16
(
32
)
;
filtersReg
=
_mm_loadu_si128
(
(
const
__m128i
*
)
filter
)
;
filtersReg
=
_mm_srai_epi16
(
filtersReg
1
)
;
filtersReg
=
_mm_packs_epi16
(
filtersReg
filtersReg
)
;
const
__m256i
filtersReg32
=
MM256_BROADCASTSI128_SI256
(
filtersReg
)
;
firstFilters
=
_mm256_shuffle_epi32
(
filtersReg32
0
)
;
secondFilters
=
_mm256_shuffle_epi32
(
filtersReg32
0x55
)
;
filt1Reg
=
_mm256_load_si256
(
(
__m256i
const
*
)
filt_d4_global_avx2
)
;
filt2Reg
=
_mm256_load_si256
(
(
__m256i
const
*
)
(
filt_d4_global_avx2
+
32
)
)
;
src_stride
=
src_pixels_per_line
<
<
1
;
dst_stride
=
output_pitch
<
<
1
;
for
(
i
=
output_height
;
i
>
1
;
i
-
=
2
)
{
srcReg32b1
=
xx_loadu2_mi128
(
src_ptr
+
src_pixels_per_line
src_ptr
)
;
srcRegFilt32b1_1
=
_mm256_shuffle_epi8
(
srcReg32b1
filt1Reg
)
;
srcRegFilt32b1_1
=
_mm256_maddubs_epi16
(
srcRegFilt32b1_1
firstFilters
)
;
srcRegFilt32b2
=
_mm256_shuffle_epi8
(
srcReg32b1
filt2Reg
)
;
srcRegFilt32b2
=
_mm256_maddubs_epi16
(
srcRegFilt32b2
secondFilters
)
;
srcRegFilt32b1_1
=
_mm256_adds_epi16
(
srcRegFilt32b1_1
srcRegFilt32b2
)
;
srcRegFilt32b1_1
=
_mm256_hadds_epi16
(
srcRegFilt32b1_1
_mm256_setzero_si256
(
)
)
;
srcRegFilt32b1_1
=
_mm256_adds_epi16
(
srcRegFilt32b1_1
addFilterReg32
)
;
srcRegFilt32b1_1
=
_mm256_srai_epi16
(
srcRegFilt32b1_1
6
)
;
srcRegFilt32b1_1
=
_mm256_packus_epi16
(
srcRegFilt32b1_1
_mm256_setzero_si256
(
)
)
;
src_ptr
+
=
src_stride
;
xx_storeu2_epi32
(
output_ptr
output_pitch
&
srcRegFilt32b1_1
)
;
output_ptr
+
=
dst_stride
;
}
if
(
i
>
0
)
{
__m128i
srcReg1
srcRegFilt1_1
;
__m128i
srcRegFilt2
;
srcReg1
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
)
)
;
srcRegFilt1_1
=
_mm_shuffle_epi8
(
srcReg1
_mm256_castsi256_si128
(
filt1Reg
)
)
;
srcRegFilt1_1
=
_mm_maddubs_epi16
(
srcRegFilt1_1
_mm256_castsi256_si128
(
firstFilters
)
)
;
srcRegFilt2
=
_mm_shuffle_epi8
(
srcReg1
_mm256_castsi256_si128
(
filt2Reg
)
)
;
srcRegFilt2
=
_mm_maddubs_epi16
(
srcRegFilt2
_mm256_castsi256_si128
(
secondFilters
)
)
;
srcRegFilt1_1
=
_mm_adds_epi16
(
srcRegFilt1_1
srcRegFilt2
)
;
srcRegFilt1_1
=
_mm_hadds_epi16
(
srcRegFilt1_1
_mm_setzero_si128
(
)
)
;
srcRegFilt1_1
=
_mm_adds_epi16
(
srcRegFilt1_1
_mm256_castsi256_si128
(
addFilterReg32
)
)
;
srcRegFilt1_1
=
_mm_srai_epi16
(
srcRegFilt1_1
6
)
;
srcRegFilt1_1
=
_mm_packus_epi16
(
srcRegFilt1_1
_mm_setzero_si128
(
)
)
;
*
(
(
uint32_t
*
)
(
output_ptr
)
)
=
_mm_cvtsi128_si32
(
srcRegFilt1_1
)
;
}
}
static
void
aom_filter_block1d8_h8_avx2
(
const
uint8_t
*
src_ptr
ptrdiff_t
src_pixels_per_line
uint8_t
*
output_ptr
ptrdiff_t
output_pitch
uint32_t
output_height
const
int16_t
*
filter
)
{
__m128i
filtersReg
;
__m256i
addFilterReg32
filt1Reg
filt2Reg
filt3Reg
filt4Reg
;
__m256i
firstFilters
secondFilters
thirdFilters
forthFilters
;
__m256i
srcRegFilt32b1_1
srcRegFilt32b2
srcRegFilt32b3
;
__m256i
srcReg32b1
;
unsigned
int
i
;
ptrdiff_t
src_stride
dst_stride
;
src_ptr
-
=
3
;
addFilterReg32
=
_mm256_set1_epi16
(
32
)
;
filtersReg
=
_mm_loadu_si128
(
(
const
__m128i
*
)
filter
)
;
filtersReg
=
_mm_srai_epi16
(
filtersReg
1
)
;
filtersReg
=
_mm_packs_epi16
(
filtersReg
filtersReg
)
;
const
__m256i
filtersReg32
=
MM256_BROADCASTSI128_SI256
(
filtersReg
)
;
firstFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x100u
)
)
;
secondFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x302u
)
)
;
thirdFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x504u
)
)
;
forthFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x706u
)
)
;
filt1Reg
=
_mm256_load_si256
(
(
__m256i
const
*
)
filt_global_avx2
)
;
filt2Reg
=
_mm256_load_si256
(
(
__m256i
const
*
)
(
filt_global_avx2
+
32
)
)
;
filt3Reg
=
_mm256_load_si256
(
(
__m256i
const
*
)
(
filt_global_avx2
+
32
*
2
)
)
;
filt4Reg
=
_mm256_load_si256
(
(
__m256i
const
*
)
(
filt_global_avx2
+
32
*
3
)
)
;
src_stride
=
src_pixels_per_line
<
<
1
;
dst_stride
=
output_pitch
<
<
1
;
for
(
i
=
output_height
;
i
>
1
;
i
-
=
2
)
{
srcReg32b1
=
xx_loadu2_mi128
(
src_ptr
+
src_pixels_per_line
src_ptr
)
;
srcRegFilt32b1_1
=
_mm256_shuffle_epi8
(
srcReg32b1
filt1Reg
)
;
srcRegFilt32b2
=
_mm256_shuffle_epi8
(
srcReg32b1
filt4Reg
)
;
srcRegFilt32b1_1
=
_mm256_maddubs_epi16
(
srcRegFilt32b1_1
firstFilters
)
;
srcRegFilt32b2
=
_mm256_maddubs_epi16
(
srcRegFilt32b2
forthFilters
)
;
srcRegFilt32b1_1
=
_mm256_adds_epi16
(
srcRegFilt32b1_1
srcRegFilt32b2
)
;
srcRegFilt32b3
=
_mm256_shuffle_epi8
(
srcReg32b1
filt2Reg
)
;
srcRegFilt32b2
=
_mm256_shuffle_epi8
(
srcReg32b1
filt3Reg
)
;
srcRegFilt32b3
=
_mm256_maddubs_epi16
(
srcRegFilt32b3
secondFilters
)
;
srcRegFilt32b2
=
_mm256_maddubs_epi16
(
srcRegFilt32b2
thirdFilters
)
;
__m256i
sum23
=
_mm256_adds_epi16
(
srcRegFilt32b3
srcRegFilt32b2
)
;
srcRegFilt32b1_1
=
_mm256_adds_epi16
(
srcRegFilt32b1_1
sum23
)
;
srcRegFilt32b1_1
=
_mm256_adds_epi16
(
srcRegFilt32b1_1
addFilterReg32
)
;
srcRegFilt32b1_1
=
_mm256_srai_epi16
(
srcRegFilt32b1_1
6
)
;
srcRegFilt32b1_1
=
_mm256_packus_epi16
(
srcRegFilt32b1_1
_mm256_setzero_si256
(
)
)
;
src_ptr
+
=
src_stride
;
xx_storeu2_epi64
(
output_ptr
output_pitch
&
srcRegFilt32b1_1
)
;
output_ptr
+
=
dst_stride
;
}
if
(
i
>
0
)
{
__m128i
srcReg1
srcRegFilt1_1
;
__m128i
srcRegFilt2
srcRegFilt3
;
srcReg1
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
)
)
;
srcRegFilt1_1
=
_mm_shuffle_epi8
(
srcReg1
_mm256_castsi256_si128
(
filt1Reg
)
)
;
srcRegFilt2
=
_mm_shuffle_epi8
(
srcReg1
_mm256_castsi256_si128
(
filt4Reg
)
)
;
srcRegFilt1_1
=
_mm_maddubs_epi16
(
srcRegFilt1_1
_mm256_castsi256_si128
(
firstFilters
)
)
;
srcRegFilt2
=
_mm_maddubs_epi16
(
srcRegFilt2
_mm256_castsi256_si128
(
forthFilters
)
)
;
srcRegFilt1_1
=
_mm_adds_epi16
(
srcRegFilt1_1
srcRegFilt2
)
;
srcRegFilt3
=
_mm_shuffle_epi8
(
srcReg1
_mm256_castsi256_si128
(
filt2Reg
)
)
;
srcRegFilt2
=
_mm_shuffle_epi8
(
srcReg1
_mm256_castsi256_si128
(
filt3Reg
)
)
;
srcRegFilt3
=
_mm_maddubs_epi16
(
srcRegFilt3
_mm256_castsi256_si128
(
secondFilters
)
)
;
srcRegFilt2
=
_mm_maddubs_epi16
(
srcRegFilt2
_mm256_castsi256_si128
(
thirdFilters
)
)
;
srcRegFilt1_1
=
_mm_adds_epi16
(
srcRegFilt1_1
_mm_adds_epi16
(
srcRegFilt3
srcRegFilt2
)
)
;
srcRegFilt1_1
=
_mm_adds_epi16
(
srcRegFilt1_1
_mm256_castsi256_si128
(
addFilterReg32
)
)
;
srcRegFilt1_1
=
_mm_srai_epi16
(
srcRegFilt1_1
6
)
;
srcRegFilt1_1
=
_mm_packus_epi16
(
srcRegFilt1_1
_mm_setzero_si128
(
)
)
;
_mm_storel_epi64
(
(
__m128i
*
)
output_ptr
srcRegFilt1_1
)
;
}
}
static
void
aom_filter_block1d16_h8_avx2
(
const
uint8_t
*
src_ptr
ptrdiff_t
src_pixels_per_line
uint8_t
*
output_ptr
ptrdiff_t
output_pitch
uint32_t
output_height
const
int16_t
*
filter
)
{
__m128i
filtersReg
;
__m256i
addFilterReg32
filt1Reg
filt2Reg
filt3Reg
filt4Reg
;
__m256i
firstFilters
secondFilters
thirdFilters
forthFilters
;
__m256i
srcRegFilt32b1_1
srcRegFilt32b2_1
srcRegFilt32b2
srcRegFilt32b3
;
__m256i
srcReg32b1
srcReg32b2
filtersReg32
;
unsigned
int
i
;
ptrdiff_t
src_stride
dst_stride
;
src_ptr
-
=
3
;
addFilterReg32
=
_mm256_set1_epi16
(
32
)
;
filtersReg
=
_mm_loadu_si128
(
(
const
__m128i
*
)
filter
)
;
filtersReg
=
_mm_srai_epi16
(
filtersReg
1
)
;
filtersReg
=
_mm_packs_epi16
(
filtersReg
filtersReg
)
;
filtersReg32
=
MM256_BROADCASTSI128_SI256
(
filtersReg
)
;
firstFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x100u
)
)
;
secondFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x302u
)
)
;
thirdFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x504u
)
)
;
forthFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x706u
)
)
;
filt1Reg
=
_mm256_load_si256
(
(
__m256i
const
*
)
filt_global_avx2
)
;
filt2Reg
=
_mm256_load_si256
(
(
__m256i
const
*
)
(
filt_global_avx2
+
32
)
)
;
filt3Reg
=
_mm256_load_si256
(
(
__m256i
const
*
)
(
filt_global_avx2
+
32
*
2
)
)
;
filt4Reg
=
_mm256_load_si256
(
(
__m256i
const
*
)
(
filt_global_avx2
+
32
*
3
)
)
;
src_stride
=
src_pixels_per_line
<
<
1
;
dst_stride
=
output_pitch
<
<
1
;
for
(
i
=
output_height
;
i
>
1
;
i
-
=
2
)
{
srcReg32b1
=
xx_loadu2_mi128
(
src_ptr
+
src_pixels_per_line
src_ptr
)
;
srcRegFilt32b1_1
=
_mm256_shuffle_epi8
(
srcReg32b1
filt1Reg
)
;
srcRegFilt32b2
=
_mm256_shuffle_epi8
(
srcReg32b1
filt4Reg
)
;
srcRegFilt32b1_1
=
_mm256_maddubs_epi16
(
srcRegFilt32b1_1
firstFilters
)
;
srcRegFilt32b2
=
_mm256_maddubs_epi16
(
srcRegFilt32b2
forthFilters
)
;
srcRegFilt32b1_1
=
_mm256_adds_epi16
(
srcRegFilt32b1_1
srcRegFilt32b2
)
;
srcRegFilt32b3
=
_mm256_shuffle_epi8
(
srcReg32b1
filt2Reg
)
;
srcRegFilt32b2
=
_mm256_shuffle_epi8
(
srcReg32b1
filt3Reg
)
;
srcRegFilt32b3
=
_mm256_maddubs_epi16
(
srcRegFilt32b3
secondFilters
)
;
srcRegFilt32b2
=
_mm256_maddubs_epi16
(
srcRegFilt32b2
thirdFilters
)
;
__m256i
sum23
=
_mm256_adds_epi16
(
srcRegFilt32b3
srcRegFilt32b2
)
;
srcRegFilt32b1_1
=
_mm256_adds_epi16
(
srcRegFilt32b1_1
sum23
)
;
srcReg32b2
=
xx_loadu2_mi128
(
src_ptr
+
src_pixels_per_line
+
8
src_ptr
+
8
)
;
srcRegFilt32b2_1
=
_mm256_shuffle_epi8
(
srcReg32b2
filt1Reg
)
;
srcRegFilt32b2
=
_mm256_shuffle_epi8
(
srcReg32b2
filt4Reg
)
;
srcRegFilt32b2_1
=
_mm256_maddubs_epi16
(
srcRegFilt32b2_1
firstFilters
)
;
srcRegFilt32b2
=
_mm256_maddubs_epi16
(
srcRegFilt32b2
forthFilters
)
;
srcRegFilt32b2_1
=
_mm256_adds_epi16
(
srcRegFilt32b2_1
srcRegFilt32b2
)
;
srcRegFilt32b3
=
_mm256_shuffle_epi8
(
srcReg32b2
filt2Reg
)
;
srcRegFilt32b2
=
_mm256_shuffle_epi8
(
srcReg32b2
filt3Reg
)
;
srcRegFilt32b3
=
_mm256_maddubs_epi16
(
srcRegFilt32b3
secondFilters
)
;
srcRegFilt32b2
=
_mm256_maddubs_epi16
(
srcRegFilt32b2
thirdFilters
)
;
srcRegFilt32b2_1
=
_mm256_adds_epi16
(
srcRegFilt32b2_1
_mm256_adds_epi16
(
srcRegFilt32b3
srcRegFilt32b2
)
)
;
srcRegFilt32b1_1
=
_mm256_adds_epi16
(
srcRegFilt32b1_1
addFilterReg32
)
;
srcRegFilt32b2_1
=
_mm256_adds_epi16
(
srcRegFilt32b2_1
addFilterReg32
)
;
srcRegFilt32b1_1
=
_mm256_srai_epi16
(
srcRegFilt32b1_1
6
)
;
srcRegFilt32b2_1
=
_mm256_srai_epi16
(
srcRegFilt32b2_1
6
)
;
srcRegFilt32b1_1
=
_mm256_packus_epi16
(
srcRegFilt32b1_1
srcRegFilt32b2_1
)
;
src_ptr
+
=
src_stride
;
xx_store2_mi128
(
output_ptr
output_pitch
&
srcRegFilt32b1_1
)
;
output_ptr
+
=
dst_stride
;
}
if
(
i
>
0
)
{
__m128i
srcReg1
srcReg2
srcRegFilt1_1
srcRegFilt2_1
;
__m128i
srcRegFilt2
srcRegFilt3
;
srcReg1
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
)
)
;
srcRegFilt1_1
=
_mm_shuffle_epi8
(
srcReg1
_mm256_castsi256_si128
(
filt1Reg
)
)
;
srcRegFilt2
=
_mm_shuffle_epi8
(
srcReg1
_mm256_castsi256_si128
(
filt4Reg
)
)
;
srcRegFilt1_1
=
_mm_maddubs_epi16
(
srcRegFilt1_1
_mm256_castsi256_si128
(
firstFilters
)
)
;
srcRegFilt2
=
_mm_maddubs_epi16
(
srcRegFilt2
_mm256_castsi256_si128
(
forthFilters
)
)
;
srcRegFilt1_1
=
_mm_adds_epi16
(
srcRegFilt1_1
srcRegFilt2
)
;
srcRegFilt3
=
_mm_shuffle_epi8
(
srcReg1
_mm256_castsi256_si128
(
filt2Reg
)
)
;
srcRegFilt2
=
_mm_shuffle_epi8
(
srcReg1
_mm256_castsi256_si128
(
filt3Reg
)
)
;
srcRegFilt3
=
_mm_maddubs_epi16
(
srcRegFilt3
_mm256_castsi256_si128
(
secondFilters
)
)
;
srcRegFilt2
=
_mm_maddubs_epi16
(
srcRegFilt2
_mm256_castsi256_si128
(
thirdFilters
)
)
;
srcRegFilt1_1
=
_mm_adds_epi16
(
srcRegFilt1_1
_mm_adds_epi16
(
srcRegFilt3
srcRegFilt2
)
)
;
srcReg2
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
8
)
)
;
srcRegFilt2_1
=
_mm_shuffle_epi8
(
srcReg2
_mm256_castsi256_si128
(
filt1Reg
)
)
;
srcRegFilt2
=
_mm_shuffle_epi8
(
srcReg2
_mm256_castsi256_si128
(
filt4Reg
)
)
;
srcRegFilt2_1
=
_mm_maddubs_epi16
(
srcRegFilt2_1
_mm256_castsi256_si128
(
firstFilters
)
)
;
srcRegFilt2
=
_mm_maddubs_epi16
(
srcRegFilt2
_mm256_castsi256_si128
(
forthFilters
)
)
;
srcRegFilt2_1
=
_mm_adds_epi16
(
srcRegFilt2_1
srcRegFilt2
)
;
srcRegFilt3
=
_mm_shuffle_epi8
(
srcReg2
_mm256_castsi256_si128
(
filt2Reg
)
)
;
srcRegFilt2
=
_mm_shuffle_epi8
(
srcReg2
_mm256_castsi256_si128
(
filt3Reg
)
)
;
srcRegFilt3
=
_mm_maddubs_epi16
(
srcRegFilt3
_mm256_castsi256_si128
(
secondFilters
)
)
;
srcRegFilt2
=
_mm_maddubs_epi16
(
srcRegFilt2
_mm256_castsi256_si128
(
thirdFilters
)
)
;
srcRegFilt2_1
=
_mm_adds_epi16
(
srcRegFilt2_1
_mm_adds_epi16
(
srcRegFilt3
srcRegFilt2
)
)
;
srcRegFilt1_1
=
_mm_adds_epi16
(
srcRegFilt1_1
_mm256_castsi256_si128
(
addFilterReg32
)
)
;
srcRegFilt1_1
=
_mm_srai_epi16
(
srcRegFilt1_1
6
)
;
srcRegFilt2_1
=
_mm_adds_epi16
(
srcRegFilt2_1
_mm256_castsi256_si128
(
addFilterReg32
)
)
;
srcRegFilt2_1
=
_mm_srai_epi16
(
srcRegFilt2_1
6
)
;
srcRegFilt1_1
=
_mm_packus_epi16
(
srcRegFilt1_1
srcRegFilt2_1
)
;
_mm_store_si128
(
(
__m128i
*
)
output_ptr
srcRegFilt1_1
)
;
}
}
static
void
aom_filter_block1d8_v8_avx2
(
const
uint8_t
*
src_ptr
ptrdiff_t
src_pitch
uint8_t
*
output_ptr
ptrdiff_t
out_pitch
uint32_t
output_height
const
int16_t
*
filter
)
{
__m128i
filtersReg
;
__m256i
addFilterReg32
;
__m256i
srcReg32b1
srcReg32b2
srcReg32b3
srcReg32b4
srcReg32b5
;
__m256i
srcReg32b6
srcReg32b7
srcReg32b8
srcReg32b9
srcReg32b10
;
__m256i
srcReg32b11
srcReg32b12
filtersReg32
;
__m256i
firstFilters
secondFilters
thirdFilters
forthFilters
;
unsigned
int
i
;
ptrdiff_t
src_stride
dst_stride
;
addFilterReg32
=
_mm256_set1_epi16
(
32
)
;
filtersReg
=
_mm_loadu_si128
(
(
const
__m128i
*
)
filter
)
;
filtersReg
=
_mm_srai_epi16
(
filtersReg
1
)
;
filtersReg
=
_mm_packs_epi16
(
filtersReg
filtersReg
)
;
filtersReg32
=
MM256_BROADCASTSI128_SI256
(
filtersReg
)
;
firstFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x100u
)
)
;
secondFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x302u
)
)
;
thirdFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x504u
)
)
;
forthFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x706u
)
)
;
src_stride
=
src_pitch
<
<
1
;
dst_stride
=
out_pitch
<
<
1
;
srcReg32b1
=
xx_loadu2_epi64
(
src_ptr
+
src_pitch
src_ptr
)
;
srcReg32b3
=
xx_loadu2_epi64
(
src_ptr
+
src_pitch
*
3
src_ptr
+
src_pitch
*
2
)
;
srcReg32b5
=
xx_loadu2_epi64
(
src_ptr
+
src_pitch
*
5
src_ptr
+
src_pitch
*
4
)
;
srcReg32b7
=
_mm256_castsi128_si256
(
_mm_loadl_epi64
(
(
const
__m128i
*
)
(
src_ptr
+
src_pitch
*
6
)
)
)
;
srcReg32b2
=
_mm256_permute2x128_si256
(
srcReg32b1
srcReg32b3
0x21
)
;
srcReg32b4
=
_mm256_permute2x128_si256
(
srcReg32b3
srcReg32b5
0x21
)
;
srcReg32b6
=
_mm256_permute2x128_si256
(
srcReg32b5
srcReg32b7
0x21
)
;
srcReg32b10
=
_mm256_unpacklo_epi8
(
srcReg32b1
srcReg32b2
)
;
srcReg32b11
=
_mm256_unpacklo_epi8
(
srcReg32b3
srcReg32b4
)
;
srcReg32b2
=
_mm256_unpacklo_epi8
(
srcReg32b5
srcReg32b6
)
;
for
(
i
=
output_height
;
i
>
1
;
i
-
=
2
)
{
srcReg32b8
=
_mm256_castsi128_si256
(
_mm_loadl_epi64
(
(
const
__m128i
*
)
(
src_ptr
+
src_pitch
*
7
)
)
)
;
srcReg32b7
=
_mm256_inserti128_si256
(
srcReg32b7
_mm256_castsi256_si128
(
srcReg32b8
)
1
)
;
srcReg32b9
=
_mm256_castsi128_si256
(
_mm_loadl_epi64
(
(
const
__m128i
*
)
(
src_ptr
+
src_pitch
*
8
)
)
)
;
srcReg32b8
=
_mm256_inserti128_si256
(
srcReg32b8
_mm256_castsi256_si128
(
srcReg32b9
)
1
)
;
srcReg32b4
=
_mm256_unpacklo_epi8
(
srcReg32b7
srcReg32b8
)
;
srcReg32b10
=
_mm256_maddubs_epi16
(
srcReg32b10
firstFilters
)
;
srcReg32b6
=
_mm256_maddubs_epi16
(
srcReg32b4
forthFilters
)
;
srcReg32b10
=
_mm256_adds_epi16
(
srcReg32b10
srcReg32b6
)
;
srcReg32b8
=
_mm256_maddubs_epi16
(
srcReg32b11
secondFilters
)
;
srcReg32b12
=
_mm256_maddubs_epi16
(
srcReg32b2
thirdFilters
)
;
srcReg32b10
=
_mm256_adds_epi16
(
srcReg32b10
_mm256_adds_epi16
(
srcReg32b8
srcReg32b12
)
)
;
srcReg32b10
=
_mm256_adds_epi16
(
srcReg32b10
addFilterReg32
)
;
srcReg32b10
=
_mm256_srai_epi16
(
srcReg32b10
6
)
;
srcReg32b1
=
_mm256_packus_epi16
(
srcReg32b10
_mm256_setzero_si256
(
)
)
;
src_ptr
+
=
src_stride
;
xx_storeu2_epi64
(
output_ptr
out_pitch
&
srcReg32b1
)
;
output_ptr
+
=
dst_stride
;
srcReg32b10
=
srcReg32b11
;
srcReg32b11
=
srcReg32b2
;
srcReg32b2
=
srcReg32b4
;
srcReg32b7
=
srcReg32b9
;
}
if
(
i
>
0
)
{
__m128i
srcRegFilt1
srcRegFilt4
srcRegFilt6
srcRegFilt8
;
srcRegFilt8
=
_mm_loadl_epi64
(
(
const
__m128i
*
)
(
src_ptr
+
src_pitch
*
7
)
)
;
srcRegFilt4
=
_mm_unpacklo_epi8
(
_mm256_castsi256_si128
(
srcReg32b7
)
srcRegFilt8
)
;
srcRegFilt1
=
_mm_maddubs_epi16
(
_mm256_castsi256_si128
(
srcReg32b10
)
_mm256_castsi256_si128
(
firstFilters
)
)
;
srcRegFilt4
=
_mm_maddubs_epi16
(
srcRegFilt4
_mm256_castsi256_si128
(
forthFilters
)
)
;
srcRegFilt1
=
_mm_adds_epi16
(
srcRegFilt1
srcRegFilt4
)
;
srcRegFilt4
=
_mm_maddubs_epi16
(
_mm256_castsi256_si128
(
srcReg32b11
)
_mm256_castsi256_si128
(
secondFilters
)
)
;
srcRegFilt6
=
_mm_maddubs_epi16
(
_mm256_castsi256_si128
(
srcReg32b2
)
_mm256_castsi256_si128
(
thirdFilters
)
)
;
srcRegFilt1
=
_mm_adds_epi16
(
srcRegFilt1
_mm_adds_epi16
(
srcRegFilt4
srcRegFilt6
)
)
;
srcRegFilt1
=
_mm_adds_epi16
(
srcRegFilt1
_mm256_castsi256_si128
(
addFilterReg32
)
)
;
srcRegFilt1
=
_mm_srai_epi16
(
srcRegFilt1
6
)
;
srcRegFilt1
=
_mm_packus_epi16
(
srcRegFilt1
_mm_setzero_si128
(
)
)
;
_mm_storel_epi64
(
(
__m128i
*
)
output_ptr
srcRegFilt1
)
;
}
}
static
void
aom_filter_block1d16_v8_avx2
(
const
uint8_t
*
src_ptr
ptrdiff_t
src_pitch
uint8_t
*
output_ptr
ptrdiff_t
out_pitch
uint32_t
output_height
const
int16_t
*
filter
)
{
__m128i
filtersReg
;
__m256i
addFilterReg32
;
__m256i
srcReg32b1
srcReg32b2
srcReg32b3
srcReg32b4
srcReg32b5
;
__m256i
srcReg32b6
srcReg32b7
srcReg32b8
srcReg32b9
srcReg32b10
;
__m256i
srcReg32b11
srcReg32b12
filtersReg32
;
__m256i
firstFilters
secondFilters
thirdFilters
forthFilters
;
unsigned
int
i
;
ptrdiff_t
src_stride
dst_stride
;
addFilterReg32
=
_mm256_set1_epi16
(
32
)
;
filtersReg
=
_mm_loadu_si128
(
(
const
__m128i
*
)
filter
)
;
filtersReg
=
_mm_srai_epi16
(
filtersReg
1
)
;
filtersReg
=
_mm_packs_epi16
(
filtersReg
filtersReg
)
;
filtersReg32
=
MM256_BROADCASTSI128_SI256
(
filtersReg
)
;
firstFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x100u
)
)
;
secondFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x302u
)
)
;
thirdFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x504u
)
)
;
forthFilters
=
_mm256_shuffle_epi8
(
filtersReg32
_mm256_set1_epi16
(
0x706u
)
)
;
src_stride
=
src_pitch
<
<
1
;
dst_stride
=
out_pitch
<
<
1
;
srcReg32b1
=
xx_loadu2_mi128
(
src_ptr
+
src_pitch
src_ptr
)
;
srcReg32b3
=
xx_loadu2_mi128
(
src_ptr
+
src_pitch
*
3
src_ptr
+
src_pitch
*
2
)
;
srcReg32b5
=
xx_loadu2_mi128
(
src_ptr
+
src_pitch
*
5
src_ptr
+
src_pitch
*
4
)
;
srcReg32b7
=
_mm256_castsi128_si256
(
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
src_pitch
*
6
)
)
)
;
srcReg32b2
=
_mm256_permute2x128_si256
(
srcReg32b1
srcReg32b3
0x21
)
;
srcReg32b4
=
_mm256_permute2x128_si256
(
srcReg32b3
srcReg32b5
0x21
)
;
srcReg32b6
=
_mm256_permute2x128_si256
(
srcReg32b5
srcReg32b7
0x21
)
;
srcReg32b10
=
_mm256_unpacklo_epi8
(
srcReg32b1
srcReg32b2
)
;
srcReg32b1
=
_mm256_unpackhi_epi8
(
srcReg32b1
srcReg32b2
)
;
srcReg32b11
=
_mm256_unpacklo_epi8
(
srcReg32b3
srcReg32b4
)
;
srcReg32b3
=
_mm256_unpackhi_epi8
(
srcReg32b3
srcReg32b4
)
;
srcReg32b2
=
_mm256_unpacklo_epi8
(
srcReg32b5
srcReg32b6
)
;
srcReg32b5
=
_mm256_unpackhi_epi8
(
srcReg32b5
srcReg32b6
)
;
for
(
i
=
output_height
;
i
>
1
;
i
-
=
2
)
{
srcReg32b8
=
_mm256_castsi128_si256
(
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
src_pitch
*
7
)
)
)
;
srcReg32b7
=
_mm256_inserti128_si256
(
srcReg32b7
_mm256_castsi256_si128
(
srcReg32b8
)
1
)
;
srcReg32b9
=
_mm256_castsi128_si256
(
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
src_pitch
*
8
)
)
)
;
srcReg32b8
=
_mm256_inserti128_si256
(
srcReg32b8
_mm256_castsi256_si128
(
srcReg32b9
)
1
)
;
srcReg32b4
=
_mm256_unpacklo_epi8
(
srcReg32b7
srcReg32b8
)
;
srcReg32b7
=
_mm256_unpackhi_epi8
(
srcReg32b7
srcReg32b8
)
;
srcReg32b10
=
_mm256_maddubs_epi16
(
srcReg32b10
firstFilters
)
;
srcReg32b6
=
_mm256_maddubs_epi16
(
srcReg32b4
forthFilters
)
;
srcReg32b10
=
_mm256_adds_epi16
(
srcReg32b10
srcReg32b6
)
;
srcReg32b8
=
_mm256_maddubs_epi16
(
srcReg32b11
secondFilters
)
;
srcReg32b12
=
_mm256_maddubs_epi16
(
srcReg32b2
thirdFilters
)
;
srcReg32b10
=
_mm256_adds_epi16
(
srcReg32b10
_mm256_adds_epi16
(
srcReg32b8
srcReg32b12
)
)
;
srcReg32b1
=
_mm256_maddubs_epi16
(
srcReg32b1
firstFilters
)
;
srcReg32b6
=
_mm256_maddubs_epi16
(
srcReg32b7
forthFilters
)
;
srcReg32b1
=
_mm256_adds_epi16
(
srcReg32b1
srcReg32b6
)
;
srcReg32b8
=
_mm256_maddubs_epi16
(
srcReg32b3
secondFilters
)
;
srcReg32b12
=
_mm256_maddubs_epi16
(
srcReg32b5
thirdFilters
)
;
srcReg32b1
=
_mm256_adds_epi16
(
srcReg32b1
_mm256_adds_epi16
(
srcReg32b8
srcReg32b12
)
)
;
srcReg32b10
=
_mm256_adds_epi16
(
srcReg32b10
addFilterReg32
)
;
srcReg32b1
=
_mm256_adds_epi16
(
srcReg32b1
addFilterReg32
)
;
srcReg32b10
=
_mm256_srai_epi16
(
srcReg32b10
6
)
;
srcReg32b1
=
_mm256_srai_epi16
(
srcReg32b1
6
)
;
srcReg32b1
=
_mm256_packus_epi16
(
srcReg32b10
srcReg32b1
)
;
src_ptr
+
=
src_stride
;
xx_store2_mi128
(
output_ptr
out_pitch
&
srcReg32b1
)
;
output_ptr
+
=
dst_stride
;
srcReg32b10
=
srcReg32b11
;
srcReg32b1
=
srcReg32b3
;
srcReg32b11
=
srcReg32b2
;
srcReg32b3
=
srcReg32b5
;
srcReg32b2
=
srcReg32b4
;
srcReg32b5
=
srcReg32b7
;
srcReg32b7
=
srcReg32b9
;
}
if
(
i
>
0
)
{
__m128i
srcRegFilt1
srcRegFilt3
srcRegFilt4
srcRegFilt5
;
__m128i
srcRegFilt6
srcRegFilt7
srcRegFilt8
;
srcRegFilt8
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
src_pitch
*
7
)
)
;
srcRegFilt4
=
_mm_unpacklo_epi8
(
_mm256_castsi256_si128
(
srcReg32b7
)
srcRegFilt8
)
;
srcRegFilt7
=
_mm_unpackhi_epi8
(
_mm256_castsi256_si128
(
srcReg32b7
)
srcRegFilt8
)
;
srcRegFilt1
=
_mm_maddubs_epi16
(
_mm256_castsi256_si128
(
srcReg32b10
)
_mm256_castsi256_si128
(
firstFilters
)
)
;
srcRegFilt4
=
_mm_maddubs_epi16
(
srcRegFilt4
_mm256_castsi256_si128
(
forthFilters
)
)
;
srcRegFilt3
=
_mm_maddubs_epi16
(
_mm256_castsi256_si128
(
srcReg32b1
)
_mm256_castsi256_si128
(
firstFilters
)
)
;
srcRegFilt7
=
_mm_maddubs_epi16
(
srcRegFilt7
_mm256_castsi256_si128
(
forthFilters
)
)
;
srcRegFilt1
=
_mm_adds_epi16
(
srcRegFilt1
srcRegFilt4
)
;
srcRegFilt3
=
_mm_adds_epi16
(
srcRegFilt3
srcRegFilt7
)
;
srcRegFilt4
=
_mm_maddubs_epi16
(
_mm256_castsi256_si128
(
srcReg32b11
)
_mm256_castsi256_si128
(
secondFilters
)
)
;
srcRegFilt5
=
_mm_maddubs_epi16
(
_mm256_castsi256_si128
(
srcReg32b3
)
_mm256_castsi256_si128
(
secondFilters
)
)
;
srcRegFilt6
=
_mm_maddubs_epi16
(
_mm256_castsi256_si128
(
srcReg32b2
)
_mm256_castsi256_si128
(
thirdFilters
)
)
;
srcRegFilt7
=
_mm_maddubs_epi16
(
_mm256_castsi256_si128
(
srcReg32b5
)
_mm256_castsi256_si128
(
thirdFilters
)
)
;
srcRegFilt1
=
_mm_adds_epi16
(
srcRegFilt1
_mm_adds_epi16
(
srcRegFilt4
srcRegFilt6
)
)
;
srcRegFilt3
=
_mm_adds_epi16
(
srcRegFilt3
_mm_adds_epi16
(
srcRegFilt5
srcRegFilt7
)
)
;
srcRegFilt1
=
_mm_adds_epi16
(
srcRegFilt1
_mm256_castsi256_si128
(
addFilterReg32
)
)
;
srcRegFilt3
=
_mm_adds_epi16
(
srcRegFilt3
_mm256_castsi256_si128
(
addFilterReg32
)
)
;
srcRegFilt1
=
_mm_srai_epi16
(
srcRegFilt1
6
)
;
srcRegFilt3
=
_mm_srai_epi16
(
srcRegFilt3
6
)
;
srcRegFilt1
=
_mm_packus_epi16
(
srcRegFilt1
srcRegFilt3
)
;
_mm_store_si128
(
(
__m128i
*
)
output_ptr
srcRegFilt1
)
;
}
}
#
if
HAVE_AVX2
&
&
HAVE_SSSE3
filter8_1dfunction
aom_filter_block1d4_v8_ssse3
;
filter8_1dfunction
aom_filter_block1d16_v2_ssse3
;
filter8_1dfunction
aom_filter_block1d16_h2_ssse3
;
filter8_1dfunction
aom_filter_block1d8_v2_ssse3
;
filter8_1dfunction
aom_filter_block1d8_h2_ssse3
;
filter8_1dfunction
aom_filter_block1d4_v2_ssse3
;
filter8_1dfunction
aom_filter_block1d4_h2_ssse3
;
#
define
aom_filter_block1d4_v8_avx2
aom_filter_block1d4_v8_ssse3
#
define
aom_filter_block1d16_v2_avx2
aom_filter_block1d16_v2_ssse3
#
define
aom_filter_block1d16_h2_avx2
aom_filter_block1d16_h2_ssse3
#
define
aom_filter_block1d8_v2_avx2
aom_filter_block1d8_v2_ssse3
#
define
aom_filter_block1d8_h2_avx2
aom_filter_block1d8_h2_ssse3
#
define
aom_filter_block1d4_v2_avx2
aom_filter_block1d4_v2_ssse3
#
define
aom_filter_block1d4_h2_avx2
aom_filter_block1d4_h2_ssse3
FUN_CONV_1D
(
horiz
x_step_q4
filter_x
h
src
avx2
)
;
FUN_CONV_1D
(
vert
y_step_q4
filter_y
v
src
-
src_stride
*
3
avx2
)
;
#
endif
