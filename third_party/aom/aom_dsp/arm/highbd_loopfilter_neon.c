#
include
<
arm_neon
.
h
>
#
include
"
config
/
aom_dsp_rtcd
.
h
"
#
include
"
config
/
aom_config
.
h
"
#
include
"
aom
/
aom_integer
.
h
"
#
include
"
aom_dsp
/
arm
/
transpose_neon
.
h
"
static
INLINE
int16x4_t
clip3_s16
(
const
int16x4_t
val
const
int16x4_t
low
const
int16x4_t
high
)
{
return
vmin_s16
(
vmax_s16
(
val
low
)
high
)
;
}
static
INLINE
uint16x8_t
convert_to_unsigned_pixel_u16
(
int16x8_t
val
int
bitdepth
)
{
const
int16x8_t
low
=
vdupq_n_s16
(
0
)
;
const
uint16x8_t
high
=
vdupq_n_u16
(
(
1
<
<
bitdepth
)
-
1
)
;
return
vminq_u16
(
vreinterpretq_u16_s16
(
vmaxq_s16
(
val
low
)
)
high
)
;
}
static
INLINE
uint16x4_t
hev
(
const
uint16x8_t
abd_p0p1_q0q1
const
uint16_t
thresh
)
{
const
uint16x8_t
a
=
vcgtq_u16
(
abd_p0p1_q0q1
vdupq_n_u16
(
thresh
)
)
;
return
vorr_u16
(
vget_low_u16
(
a
)
vget_high_u16
(
a
)
)
;
}
static
INLINE
uint16x4_t
outer_threshold
(
const
uint16x4_t
p1
const
uint16x4_t
p0
const
uint16x4_t
q0
const
uint16x4_t
q1
const
uint16_t
outer_thresh
)
{
const
uint16x4_t
abd_p0q0
=
vabd_u16
(
p0
q0
)
;
const
uint16x4_t
abd_p1q1
=
vabd_u16
(
p1
q1
)
;
const
uint16x4_t
p0q0_double
=
vshl_n_u16
(
abd_p0q0
1
)
;
const
uint16x4_t
p1q1_half
=
vshr_n_u16
(
abd_p1q1
1
)
;
const
uint16x4_t
sum
=
vadd_u16
(
p0q0_double
p1q1_half
)
;
return
vcle_u16
(
sum
vdup_n_u16
(
outer_thresh
)
)
;
}
static
INLINE
uint16x4_t
needs_filter4
(
const
uint16x8_t
abd_p0p1_q0q1
const
uint16_t
inner_thresh
const
uint16x4_t
outer_mask
)
{
const
uint16x8_t
a
=
vcleq_u16
(
abd_p0p1_q0q1
vdupq_n_u16
(
inner_thresh
)
)
;
const
uint16x4_t
inner_mask
=
vand_u16
(
vget_low_u16
(
a
)
vget_high_u16
(
a
)
)
;
return
vand_u16
(
inner_mask
outer_mask
)
;
}
static
INLINE
uint16x4_t
needs_filter6
(
const
uint16x8_t
abd_p0p1_q0q1
const
uint16x8_t
abd_p1p2_q1q2
const
uint16_t
inner_thresh
const
uint16x4_t
outer_mask
)
{
const
uint16x8_t
a
=
vmaxq_u16
(
abd_p0p1_q0q1
abd_p1p2_q1q2
)
;
const
uint16x8_t
b
=
vcleq_u16
(
a
vdupq_n_u16
(
inner_thresh
)
)
;
const
uint16x4_t
inner_mask
=
vand_u16
(
vget_low_u16
(
b
)
vget_high_u16
(
b
)
)
;
return
vand_u16
(
inner_mask
outer_mask
)
;
}
static
INLINE
uint16x4_t
needs_filter8
(
const
uint16x8_t
abd_p0p1_q0q1
const
uint16x8_t
abd_p1p2_q1q2
const
uint16x8_t
abd_p2p3_q2q3
const
uint16_t
inner_thresh
const
uint16x4_t
outer_mask
)
{
const
uint16x8_t
a
=
vmaxq_u16
(
abd_p0p1_q0q1
abd_p1p2_q1q2
)
;
const
uint16x8_t
b
=
vmaxq_u16
(
a
abd_p2p3_q2q3
)
;
const
uint16x8_t
c
=
vcleq_u16
(
b
vdupq_n_u16
(
inner_thresh
)
)
;
const
uint16x4_t
inner_mask
=
vand_u16
(
vget_low_u16
(
c
)
vget_high_u16
(
c
)
)
;
return
vand_u16
(
inner_mask
outer_mask
)
;
}
static
INLINE
void
filter4_masks
(
const
uint16x8_t
p0q0
const
uint16x8_t
p1q1
const
uint16_t
hev_thresh
const
uint16x4_t
outer_mask
const
uint16_t
inner_thresh
uint16x4_t
*
const
hev_mask
uint16x4_t
*
const
needs_filter4_mask
)
{
const
uint16x8_t
p0p1_q0q1
=
vabdq_u16
(
p0q0
p1q1
)
;
const
uint16x4_t
hev_tmp_mask
=
hev
(
p0p1_q0q1
hev_thresh
)
;
*
needs_filter4_mask
=
needs_filter4
(
p0p1_q0q1
inner_thresh
outer_mask
)
;
*
hev_mask
=
vand_u16
(
hev_tmp_mask
*
needs_filter4_mask
)
;
}
static
INLINE
uint16x4_t
is_flat3
(
const
uint16x8_t
abd_p0p1_q0q1
const
uint16x8_t
abd_p0p2_q0q2
const
int
bitdepth
)
{
const
int
flat_thresh
=
1
<
<
(
bitdepth
-
8
)
;
const
uint16x8_t
a
=
vmaxq_u16
(
abd_p0p1_q0q1
abd_p0p2_q0q2
)
;
const
uint16x8_t
b
=
vcleq_u16
(
a
vdupq_n_u16
(
flat_thresh
)
)
;
return
vand_u16
(
vget_low_u16
(
b
)
vget_high_u16
(
b
)
)
;
}
static
INLINE
void
filter6_masks
(
const
uint16x8_t
p2q2
const
uint16x8_t
p1q1
const
uint16x8_t
p0q0
const
uint16_t
hev_thresh
const
uint16x4_t
outer_mask
const
uint16_t
inner_thresh
const
int
bitdepth
uint16x4_t
*
const
needs_filter6_mask
uint16x4_t
*
const
is_flat3_mask
uint16x4_t
*
const
hev_mask
)
{
const
uint16x8_t
abd_p0p1_q0q1
=
vabdq_u16
(
p0q0
p1q1
)
;
*
hev_mask
=
hev
(
abd_p0p1_q0q1
hev_thresh
)
;
*
is_flat3_mask
=
is_flat3
(
abd_p0p1_q0q1
vabdq_u16
(
p0q0
p2q2
)
bitdepth
)
;
*
needs_filter6_mask
=
needs_filter6
(
abd_p0p1_q0q1
vabdq_u16
(
p1q1
p2q2
)
inner_thresh
outer_mask
)
;
}
static
INLINE
uint16x4_t
is_flat4
(
const
uint16x8_t
abd_pnp0_qnq0
const
uint16x8_t
abd_pn1p0_qn1q0
const
uint16x8_t
abd_pn2p0_qn2q0
const
int
bitdepth
)
{
const
int
flat_thresh
=
1
<
<
(
bitdepth
-
8
)
;
const
uint16x8_t
a
=
vmaxq_u16
(
abd_pnp0_qnq0
abd_pn1p0_qn1q0
)
;
const
uint16x8_t
b
=
vmaxq_u16
(
a
abd_pn2p0_qn2q0
)
;
const
uint16x8_t
c
=
vcleq_u16
(
b
vdupq_n_u16
(
flat_thresh
)
)
;
return
vand_u16
(
vget_low_u16
(
c
)
vget_high_u16
(
c
)
)
;
}
static
INLINE
void
filter8_masks
(
const
uint16x8_t
p3q3
const
uint16x8_t
p2q2
const
uint16x8_t
p1q1
const
uint16x8_t
p0q0
const
uint16_t
hev_thresh
const
uint16x4_t
outer_mask
const
uint16_t
inner_thresh
const
int
bitdepth
uint16x4_t
*
const
needs_filter8_mask
uint16x4_t
*
const
is_flat4_mask
uint16x4_t
*
const
hev_mask
)
{
const
uint16x8_t
abd_p0p1_q0q1
=
vabdq_u16
(
p0q0
p1q1
)
;
*
hev_mask
=
hev
(
abd_p0p1_q0q1
hev_thresh
)
;
const
uint16x4_t
v_is_flat4
=
is_flat4
(
abd_p0p1_q0q1
vabdq_u16
(
p0q0
p2q2
)
vabdq_u16
(
p0q0
p3q3
)
bitdepth
)
;
*
needs_filter8_mask
=
needs_filter8
(
abd_p0p1_q0q1
vabdq_u16
(
p1q1
p2q2
)
vabdq_u16
(
p2q2
p3q3
)
inner_thresh
outer_mask
)
;
*
is_flat4_mask
=
vand_u16
(
v_is_flat4
*
needs_filter8_mask
)
;
}
static
INLINE
void
filter4
(
const
uint16x8_t
p0q0
const
uint16x8_t
p0q1
const
uint16x8_t
p1q1
const
uint16x4_t
hev_mask
int
bitdepth
uint16x8_t
*
const
p1q1_result
uint16x8_t
*
const
p0q0_result
)
{
const
uint16x8_t
q0p1
=
vextq_u16
(
p0q0
p1q1
4
)
;
const
int16x8_t
q0mp0_p1mq1
=
vreinterpretq_s16_u16
(
vsubq_u16
(
q0p1
p0q1
)
)
;
const
int16x4_t
q0mp0_3
=
vmul_n_s16
(
vget_low_s16
(
q0mp0_p1mq1
)
3
)
;
const
int16x4_t
min_signed_pixel
=
vdup_n_s16
(
-
(
1
<
<
(
bitdepth
-
1
)
)
)
;
const
int16x4_t
max_signed_pixel
=
vdup_n_s16
(
(
1
<
<
(
bitdepth
-
1
)
)
-
1
)
;
const
int16x4_t
p1mq1
=
vget_high_s16
(
q0mp0_p1mq1
)
;
const
int16x4_t
p1mq1_saturated
=
clip3_s16
(
p1mq1
min_signed_pixel
max_signed_pixel
)
;
const
int16x4_t
hev_option
=
vand_s16
(
vreinterpret_s16_u16
(
hev_mask
)
p1mq1_saturated
)
;
const
int16x4_t
a
=
vadd_s16
(
q0mp0_3
hev_option
)
;
const
int16x4_t
plus_four
=
clip3_s16
(
vadd_s16
(
a
vdup_n_s16
(
4
)
)
min_signed_pixel
max_signed_pixel
)
;
const
int16x4_t
plus_three
=
clip3_s16
(
vadd_s16
(
a
vdup_n_s16
(
3
)
)
min_signed_pixel
max_signed_pixel
)
;
const
int16x4_t
a1
=
vshr_n_s16
(
plus_four
3
)
;
const
int16x4_t
a2
=
vshr_n_s16
(
plus_three
3
)
;
const
int16x4_t
a3
=
vrshr_n_s16
(
a1
1
)
;
const
int16x8_t
a3_ma3
=
vcombine_s16
(
a3
vneg_s16
(
a3
)
)
;
const
int16x8_t
p1q1_a3
=
vaddq_s16
(
vreinterpretq_s16_u16
(
p1q1
)
a3_ma3
)
;
const
int16x8_t
a2_ma1
=
vcombine_s16
(
a2
vneg_s16
(
a1
)
)
;
const
int16x8_t
p0q0_a
=
vaddq_s16
(
vreinterpretq_s16_u16
(
p0q0
)
a2_ma1
)
;
*
p1q1_result
=
convert_to_unsigned_pixel_u16
(
p1q1_a3
bitdepth
)
;
*
p0q0_result
=
convert_to_unsigned_pixel_u16
(
p0q0_a
bitdepth
)
;
}
void
aom_highbd_lpf_horizontal_4_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit
const
uint8_t
*
limit
const
uint8_t
*
thresh
int
bd
)
{
uint16_t
*
const
dst_p1
=
(
uint16_t
*
)
(
s
-
2
*
pitch
)
;
uint16_t
*
const
dst_p0
=
(
uint16_t
*
)
(
s
-
pitch
)
;
uint16_t
*
const
dst_q0
=
(
uint16_t
*
)
(
s
)
;
uint16_t
*
const
dst_q1
=
(
uint16_t
*
)
(
s
+
pitch
)
;
const
uint16x4_t
src
[
4
]
=
{
vld1_u16
(
dst_p1
)
vld1_u16
(
dst_p0
)
vld1_u16
(
dst_q0
)
vld1_u16
(
dst_q1
)
}
;
const
int
outer_thresh
=
*
blimit
<
<
(
bd
-
8
)
;
const
int
inner_thresh
=
*
limit
<
<
(
bd
-
8
)
;
const
int
hev_thresh
=
*
thresh
<
<
(
bd
-
8
)
;
const
uint16x4_t
outer_mask
=
outer_threshold
(
src
[
0
]
src
[
1
]
src
[
2
]
src
[
3
]
outer_thresh
)
;
uint16x4_t
hev_mask
;
uint16x4_t
needs_filter4_mask
;
const
uint16x8_t
p0q0
=
vcombine_u16
(
src
[
1
]
src
[
2
]
)
;
const
uint16x8_t
p1q1
=
vcombine_u16
(
src
[
0
]
src
[
3
]
)
;
filter4_masks
(
p0q0
p1q1
hev_thresh
outer_mask
inner_thresh
&
hev_mask
&
needs_filter4_mask
)
;
#
if
AOM_ARCH_AARCH64
if
(
vaddv_u16
(
needs_filter4_mask
)
=
=
0
)
{
return
;
}
#
endif
const
uint16x8_t
hev_mask_8
=
vcombine_u16
(
hev_mask
hev_mask
)
;
const
uint16x8_t
needs_filter4_mask_8
=
vcombine_u16
(
needs_filter4_mask
needs_filter4_mask
)
;
uint16x8_t
f_p1q1
;
uint16x8_t
f_p0q0
;
const
uint16x8_t
p0q1
=
vcombine_u16
(
src
[
1
]
src
[
3
]
)
;
filter4
(
p0q0
p0q1
p1q1
hev_mask
bd
&
f_p1q1
&
f_p0q0
)
;
const
uint16x8_t
p0q0_output
=
vbslq_u16
(
needs_filter4_mask_8
f_p0q0
p0q0
)
;
const
uint16x8_t
p1q1_mask
=
veorq_u16
(
hev_mask_8
needs_filter4_mask_8
)
;
const
uint16x8_t
p1q1_output
=
vbslq_u16
(
p1q1_mask
f_p1q1
p1q1
)
;
vst1_u16
(
dst_p1
vget_low_u16
(
p1q1_output
)
)
;
vst1_u16
(
dst_p0
vget_low_u16
(
p0q0_output
)
)
;
vst1_u16
(
dst_q0
vget_high_u16
(
p0q0_output
)
)
;
vst1_u16
(
dst_q1
vget_high_u16
(
p1q1_output
)
)
;
}
void
aom_highbd_lpf_horizontal_4_dual_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit0
const
uint8_t
*
limit0
const
uint8_t
*
thresh0
const
uint8_t
*
blimit1
const
uint8_t
*
limit1
const
uint8_t
*
thresh1
int
bd
)
{
aom_highbd_lpf_horizontal_4_neon
(
s
pitch
blimit0
limit0
thresh0
bd
)
;
aom_highbd_lpf_horizontal_4_neon
(
s
+
4
pitch
blimit1
limit1
thresh1
bd
)
;
}
void
aom_highbd_lpf_vertical_4_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit
const
uint8_t
*
limit
const
uint8_t
*
thresh
int
bd
)
{
uint16_t
*
dst
=
s
-
2
;
uint16_t
*
dst_p1
=
dst
;
uint16_t
*
dst_p0
=
dst
+
pitch
;
uint16_t
*
dst_q0
=
dst
+
pitch
*
2
;
uint16_t
*
dst_q1
=
dst
+
pitch
*
3
;
uint16x4_t
src
[
4
]
=
{
vld1_u16
(
dst_p1
)
vld1_u16
(
dst_p0
)
vld1_u16
(
dst_q0
)
vld1_u16
(
dst_q1
)
}
;
transpose_array_inplace_u16_4x4
(
src
)
;
const
int
outer_thresh
=
*
blimit
<
<
(
bd
-
8
)
;
const
int
inner_thresh
=
*
limit
<
<
(
bd
-
8
)
;
const
int
hev_thresh
=
*
thresh
<
<
(
bd
-
8
)
;
const
uint16x4_t
outer_mask
=
outer_threshold
(
src
[
0
]
src
[
1
]
src
[
2
]
src
[
3
]
outer_thresh
)
;
uint16x4_t
hev_mask
;
uint16x4_t
needs_filter4_mask
;
const
uint16x8_t
p0q0
=
vcombine_u16
(
src
[
1
]
src
[
2
]
)
;
const
uint16x8_t
p1q1
=
vcombine_u16
(
src
[
0
]
src
[
3
]
)
;
filter4_masks
(
p0q0
p1q1
hev_thresh
outer_mask
inner_thresh
&
hev_mask
&
needs_filter4_mask
)
;
#
if
AOM_ARCH_AARCH64
if
(
vaddv_u16
(
needs_filter4_mask
)
=
=
0
)
{
return
;
}
#
endif
const
uint16x8_t
hev_mask_8
=
vcombine_u16
(
hev_mask
hev_mask
)
;
const
uint16x8_t
needs_filter4_mask_8
=
vcombine_u16
(
needs_filter4_mask
needs_filter4_mask
)
;
uint16x8_t
f_p1q1
;
uint16x8_t
f_p0q0
;
const
uint16x8_t
p0q1
=
vcombine_u16
(
src
[
1
]
src
[
3
]
)
;
filter4
(
p0q0
p0q1
p1q1
hev_mask
bd
&
f_p1q1
&
f_p0q0
)
;
const
uint16x8_t
p0q0_output
=
vbslq_u16
(
needs_filter4_mask_8
f_p0q0
p0q0
)
;
const
uint16x8_t
p1q1_mask
=
veorq_u16
(
hev_mask_8
needs_filter4_mask_8
)
;
const
uint16x8_t
p1q1_output
=
vbslq_u16
(
p1q1_mask
f_p1q1
p1q1
)
;
uint16x4_t
output
[
4
]
=
{
vget_low_u16
(
p1q1_output
)
vget_low_u16
(
p0q0_output
)
vget_high_u16
(
p0q0_output
)
vget_high_u16
(
p1q1_output
)
}
;
transpose_array_inplace_u16_4x4
(
output
)
;
vst1_u16
(
dst_p1
output
[
0
]
)
;
vst1_u16
(
dst_p0
output
[
1
]
)
;
vst1_u16
(
dst_q0
output
[
2
]
)
;
vst1_u16
(
dst_q1
output
[
3
]
)
;
}
void
aom_highbd_lpf_vertical_4_dual_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit0
const
uint8_t
*
limit0
const
uint8_t
*
thresh0
const
uint8_t
*
blimit1
const
uint8_t
*
limit1
const
uint8_t
*
thresh1
int
bd
)
{
aom_highbd_lpf_vertical_4_neon
(
s
pitch
blimit0
limit0
thresh0
bd
)
;
aom_highbd_lpf_vertical_4_neon
(
s
+
4
*
pitch
pitch
blimit1
limit1
thresh1
bd
)
;
}
static
INLINE
void
filter6
(
const
uint16x8_t
p2q2
const
uint16x8_t
p1q1
const
uint16x8_t
p0q0
uint16x8_t
*
const
p1q1_output
uint16x8_t
*
const
p0q0_output
)
{
uint16x8_t
sum
=
vaddq_u16
(
p2q2
p1q1
)
;
sum
=
vaddq_u16
(
sum
p0q0
)
;
sum
=
vshlq_n_u16
(
sum
1
)
;
const
uint16x8_t
q0p0
=
vextq_u16
(
p0q0
p0q0
4
)
;
const
uint16x8_t
outer_sum
=
vaddq_u16
(
p2q2
q0p0
)
;
sum
=
vaddq_u16
(
sum
outer_sum
)
;
*
p1q1_output
=
vrshrq_n_u16
(
sum
3
)
;
const
uint16x8_t
p2q2_double
=
vshlq_n_u16
(
p2q2
1
)
;
sum
=
vsubq_u16
(
sum
p2q2_double
)
;
const
uint16x8_t
q1p1
=
vextq_u16
(
p1q1
p1q1
4
)
;
sum
=
vaddq_u16
(
sum
vaddq_u16
(
q0p0
q1p1
)
)
;
*
p0q0_output
=
vrshrq_n_u16
(
sum
3
)
;
}
void
aom_highbd_lpf_horizontal_6_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit
const
uint8_t
*
limit
const
uint8_t
*
thresh
int
bd
)
{
uint16_t
*
const
dst_p2
=
s
-
3
*
pitch
;
uint16_t
*
const
dst_p1
=
s
-
2
*
pitch
;
uint16_t
*
const
dst_p0
=
s
-
pitch
;
uint16_t
*
const
dst_q0
=
s
;
uint16_t
*
const
dst_q1
=
s
+
pitch
;
uint16_t
*
const
dst_q2
=
s
+
2
*
pitch
;
const
uint16x4_t
src
[
6
]
=
{
vld1_u16
(
dst_p2
)
vld1_u16
(
dst_p1
)
vld1_u16
(
dst_p0
)
vld1_u16
(
dst_q0
)
vld1_u16
(
dst_q1
)
vld1_u16
(
dst_q2
)
}
;
const
int
outer_thresh
=
*
blimit
<
<
(
bd
-
8
)
;
const
int
inner_thresh
=
*
limit
<
<
(
bd
-
8
)
;
const
int
hev_thresh
=
*
thresh
<
<
(
bd
-
8
)
;
const
uint16x4_t
outer_mask
=
outer_threshold
(
src
[
1
]
src
[
2
]
src
[
3
]
src
[
4
]
outer_thresh
)
;
uint16x4_t
hev_mask
;
uint16x4_t
needs_filter_mask
;
uint16x4_t
is_flat3_mask
;
const
uint16x8_t
p0q0
=
vcombine_u16
(
src
[
2
]
src
[
3
]
)
;
const
uint16x8_t
p1q1
=
vcombine_u16
(
src
[
1
]
src
[
4
]
)
;
const
uint16x8_t
p2q2
=
vcombine_u16
(
src
[
0
]
src
[
5
]
)
;
filter6_masks
(
p2q2
p1q1
p0q0
hev_thresh
outer_mask
inner_thresh
bd
&
needs_filter_mask
&
is_flat3_mask
&
hev_mask
)
;
#
if
AOM_ARCH_AARCH64
if
(
vaddv_u16
(
needs_filter_mask
)
=
=
0
)
{
return
;
}
#
endif
const
uint16x8_t
hev_mask_8
=
vcombine_u16
(
hev_mask
hev_mask
)
;
const
uint16x8_t
is_flat3_mask_8
=
vcombine_u16
(
is_flat3_mask
is_flat3_mask
)
;
const
uint16x8_t
needs_filter_mask_8
=
vcombine_u16
(
needs_filter_mask
needs_filter_mask
)
;
uint16x8_t
f4_p1q1
;
uint16x8_t
f4_p0q0
;
const
uint16x8_t
p0q1
=
vcombine_u16
(
src
[
2
]
src
[
4
]
)
;
filter4
(
p0q0
p0q1
p1q1
hev_mask
bd
&
f4_p1q1
&
f4_p0q0
)
;
f4_p1q1
=
vbslq_u16
(
hev_mask_8
p1q1
f4_p1q1
)
;
uint16x8_t
p0q0_output
p1q1_output
;
uint16x8_t
f6_p1q1
f6_p0q0
;
const
uint64x1_t
need_filter6
=
vreinterpret_u64_u16
(
is_flat3_mask
)
;
if
(
vget_lane_u64
(
need_filter6
0
)
=
=
0
)
{
p0q0_output
=
p0q0
;
p1q1_output
=
vbslq_u16
(
needs_filter_mask_8
f4_p1q1
p1q1
)
;
p0q0_output
=
vbslq_u16
(
needs_filter_mask_8
f4_p0q0
p0q0
)
;
}
else
{
filter6
(
p2q2
p1q1
p0q0
&
f6_p1q1
&
f6_p0q0
)
;
p1q1_output
=
vbslq_u16
(
is_flat3_mask_8
f6_p1q1
f4_p1q1
)
;
p1q1_output
=
vbslq_u16
(
needs_filter_mask_8
p1q1_output
p1q1
)
;
p0q0_output
=
vbslq_u16
(
is_flat3_mask_8
f6_p0q0
f4_p0q0
)
;
p0q0_output
=
vbslq_u16
(
needs_filter_mask_8
p0q0_output
p0q0
)
;
}
vst1_u16
(
dst_p1
vget_low_u16
(
p1q1_output
)
)
;
vst1_u16
(
dst_p0
vget_low_u16
(
p0q0_output
)
)
;
vst1_u16
(
dst_q0
vget_high_u16
(
p0q0_output
)
)
;
vst1_u16
(
dst_q1
vget_high_u16
(
p1q1_output
)
)
;
}
void
aom_highbd_lpf_horizontal_6_dual_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit0
const
uint8_t
*
limit0
const
uint8_t
*
thresh0
const
uint8_t
*
blimit1
const
uint8_t
*
limit1
const
uint8_t
*
thresh1
int
bd
)
{
aom_highbd_lpf_horizontal_6_neon
(
s
pitch
blimit0
limit0
thresh0
bd
)
;
aom_highbd_lpf_horizontal_6_neon
(
s
+
4
pitch
blimit1
limit1
thresh1
bd
)
;
}
void
aom_highbd_lpf_vertical_6_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit
const
uint8_t
*
limit
const
uint8_t
*
thresh
int
bd
)
{
uint16_t
*
const
dst
=
s
-
3
;
uint16_t
*
const
dst_0
=
dst
;
uint16_t
*
const
dst_1
=
dst
+
pitch
;
uint16_t
*
const
dst_2
=
dst
+
2
*
pitch
;
uint16_t
*
const
dst_3
=
dst
+
3
*
pitch
;
uint16x8_t
src_raw
[
4
]
=
{
vld1q_u16
(
dst_0
)
vld1q_u16
(
dst_1
)
vld1q_u16
(
dst_2
)
vld1q_u16
(
dst_3
)
}
;
transpose_array_inplace_u16_4x8
(
src_raw
)
;
const
uint16x4_t
src
[
6
]
=
{
vget_low_u16
(
src_raw
[
0
]
)
vget_low_u16
(
src_raw
[
1
]
)
vget_low_u16
(
src_raw
[
2
]
)
vget_low_u16
(
src_raw
[
3
]
)
vget_high_u16
(
src_raw
[
0
]
)
vget_high_u16
(
src_raw
[
1
]
)
}
;
const
int
outer_thresh
=
*
blimit
<
<
(
bd
-
8
)
;
const
int
inner_thresh
=
*
limit
<
<
(
bd
-
8
)
;
const
int
hev_thresh
=
*
thresh
<
<
(
bd
-
8
)
;
const
uint16x4_t
outer_mask
=
outer_threshold
(
src
[
1
]
src
[
2
]
src
[
3
]
src
[
4
]
outer_thresh
)
;
uint16x4_t
hev_mask
;
uint16x4_t
needs_filter_mask
;
uint16x4_t
is_flat3_mask
;
const
uint16x8_t
p0q0
=
vcombine_u16
(
src
[
2
]
src
[
3
]
)
;
const
uint16x8_t
p1q1
=
vcombine_u16
(
src
[
1
]
src
[
4
]
)
;
const
uint16x8_t
p2q2
=
vcombine_u16
(
src
[
0
]
src
[
5
]
)
;
filter6_masks
(
p2q2
p1q1
p0q0
hev_thresh
outer_mask
inner_thresh
bd
&
needs_filter_mask
&
is_flat3_mask
&
hev_mask
)
;
#
if
AOM_ARCH_AARCH64
if
(
vaddv_u16
(
needs_filter_mask
)
=
=
0
)
{
return
;
}
#
endif
const
uint16x8_t
hev_mask_8
=
vcombine_u16
(
hev_mask
hev_mask
)
;
const
uint16x8_t
is_flat3_mask_8
=
vcombine_u16
(
is_flat3_mask
is_flat3_mask
)
;
const
uint16x8_t
needs_filter_mask_8
=
vcombine_u16
(
needs_filter_mask
needs_filter_mask
)
;
uint16x8_t
f4_p1q1
;
uint16x8_t
f4_p0q0
;
const
uint16x8_t
p0q1
=
vcombine_u16
(
src
[
2
]
src
[
4
]
)
;
filter4
(
p0q0
p0q1
p1q1
hev_mask
bd
&
f4_p1q1
&
f4_p0q0
)
;
f4_p1q1
=
vbslq_u16
(
hev_mask_8
p1q1
f4_p1q1
)
;
uint16x8_t
p0q0_output
p1q1_output
;
uint16x8_t
f6_p1q1
f6_p0q0
;
const
uint64x1_t
need_filter6
=
vreinterpret_u64_u16
(
is_flat3_mask
)
;
if
(
vget_lane_u64
(
need_filter6
0
)
=
=
0
)
{
p0q0_output
=
p0q0
;
p1q1_output
=
vbslq_u16
(
needs_filter_mask_8
f4_p1q1
p1q1
)
;
p0q0_output
=
vbslq_u16
(
needs_filter_mask_8
f4_p0q0
p0q0
)
;
}
else
{
filter6
(
p2q2
p1q1
p0q0
&
f6_p1q1
&
f6_p0q0
)
;
p1q1_output
=
vbslq_u16
(
is_flat3_mask_8
f6_p1q1
f4_p1q1
)
;
p1q1_output
=
vbslq_u16
(
needs_filter_mask_8
p1q1_output
p1q1
)
;
p0q0_output
=
vbslq_u16
(
is_flat3_mask_8
f6_p0q0
f4_p0q0
)
;
p0q0_output
=
vbslq_u16
(
needs_filter_mask_8
p0q0_output
p0q0
)
;
}
uint16x4_t
output
[
4
]
=
{
vget_low_u16
(
p1q1_output
)
vget_low_u16
(
p0q0_output
)
vget_high_u16
(
p0q0_output
)
vget_high_u16
(
p1q1_output
)
}
;
transpose_array_inplace_u16_4x4
(
output
)
;
vst1_u16
(
dst_0
+
1
output
[
0
]
)
;
vst1_u16
(
dst_1
+
1
output
[
1
]
)
;
vst1_u16
(
dst_2
+
1
output
[
2
]
)
;
vst1_u16
(
dst_3
+
1
output
[
3
]
)
;
}
void
aom_highbd_lpf_vertical_6_dual_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit0
const
uint8_t
*
limit0
const
uint8_t
*
thresh0
const
uint8_t
*
blimit1
const
uint8_t
*
limit1
const
uint8_t
*
thresh1
int
bd
)
{
aom_highbd_lpf_vertical_6_neon
(
s
pitch
blimit0
limit0
thresh0
bd
)
;
aom_highbd_lpf_vertical_6_neon
(
s
+
4
*
pitch
pitch
blimit1
limit1
thresh1
bd
)
;
}
static
INLINE
void
filter8
(
const
uint16x8_t
p3q3
const
uint16x8_t
p2q2
const
uint16x8_t
p1q1
const
uint16x8_t
p0q0
uint16x8_t
*
const
p2q2_output
uint16x8_t
*
const
p1q1_output
uint16x8_t
*
const
p0q0_output
)
{
const
uint16x8_t
p23q23
=
vaddq_u16
(
p3q3
p2q2
)
;
uint16x8_t
sum
=
vshlq_n_u16
(
p23q23
1
)
;
const
uint16x8_t
p01q01
=
vaddq_u16
(
p0q0
p1q1
)
;
sum
=
vaddq_u16
(
sum
p01q01
)
;
sum
=
vaddq_u16
(
sum
p3q3
)
;
const
uint16x8_t
q0p0
=
vextq_u16
(
p0q0
p0q0
4
)
;
sum
=
vaddq_u16
(
sum
q0p0
)
;
*
p2q2_output
=
vrshrq_n_u16
(
sum
3
)
;
sum
=
vsubq_u16
(
sum
p23q23
)
;
const
uint16x8_t
q1p1
=
vextq_u16
(
p1q1
p1q1
4
)
;
sum
=
vaddq_u16
(
sum
vaddq_u16
(
p1q1
q1p1
)
)
;
*
p1q1_output
=
vrshrq_n_u16
(
sum
3
)
;
sum
=
vsubq_u16
(
sum
vaddq_u16
(
p3q3
p1q1
)
)
;
const
uint16x8_t
q2p2
=
vextq_u16
(
p2q2
p2q2
4
)
;
sum
=
vaddq_u16
(
sum
vaddq_u16
(
p0q0
q2p2
)
)
;
*
p0q0_output
=
vrshrq_n_u16
(
sum
3
)
;
}
void
aom_highbd_lpf_horizontal_8_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit
const
uint8_t
*
limit
const
uint8_t
*
thresh
int
bd
)
{
uint16_t
*
const
dst_p3
=
s
-
4
*
pitch
;
uint16_t
*
const
dst_p2
=
s
-
3
*
pitch
;
uint16_t
*
const
dst_p1
=
s
-
2
*
pitch
;
uint16_t
*
const
dst_p0
=
s
-
pitch
;
uint16_t
*
const
dst_q0
=
s
;
uint16_t
*
const
dst_q1
=
s
+
pitch
;
uint16_t
*
const
dst_q2
=
s
+
2
*
pitch
;
uint16_t
*
const
dst_q3
=
s
+
3
*
pitch
;
const
uint16x4_t
src
[
8
]
=
{
vld1_u16
(
dst_p3
)
vld1_u16
(
dst_p2
)
vld1_u16
(
dst_p1
)
vld1_u16
(
dst_p0
)
vld1_u16
(
dst_q0
)
vld1_u16
(
dst_q1
)
vld1_u16
(
dst_q2
)
vld1_u16
(
dst_q3
)
}
;
const
int
outer_thresh
=
*
blimit
<
<
(
bd
-
8
)
;
const
int
inner_thresh
=
*
limit
<
<
(
bd
-
8
)
;
const
int
hev_thresh
=
*
thresh
<
<
(
bd
-
8
)
;
const
uint16x4_t
outer_mask
=
outer_threshold
(
src
[
2
]
src
[
3
]
src
[
4
]
src
[
5
]
outer_thresh
)
;
uint16x4_t
hev_mask
;
uint16x4_t
needs_filter_mask
;
uint16x4_t
is_flat4_mask
;
const
uint16x8_t
p0q0
=
vcombine_u16
(
src
[
3
]
src
[
4
]
)
;
const
uint16x8_t
p1q1
=
vcombine_u16
(
src
[
2
]
src
[
5
]
)
;
const
uint16x8_t
p2q2
=
vcombine_u16
(
src
[
1
]
src
[
6
]
)
;
const
uint16x8_t
p3q3
=
vcombine_u16
(
src
[
0
]
src
[
7
]
)
;
filter8_masks
(
p3q3
p2q2
p1q1
p0q0
hev_thresh
outer_mask
inner_thresh
bd
&
needs_filter_mask
&
is_flat4_mask
&
hev_mask
)
;
#
if
AOM_ARCH_AARCH64
if
(
vaddv_u16
(
needs_filter_mask
)
=
=
0
)
{
return
;
}
#
endif
const
uint16x8_t
hev_mask_8
=
vcombine_u16
(
hev_mask
hev_mask
)
;
const
uint16x8_t
needs_filter_mask_8
=
vcombine_u16
(
needs_filter_mask
needs_filter_mask
)
;
uint16x8_t
f4_p1q1
;
uint16x8_t
f4_p0q0
;
const
uint16x8_t
p0q1
=
vcombine_u16
(
src
[
3
]
src
[
5
]
)
;
filter4
(
p0q0
p0q1
p1q1
hev_mask
bd
&
f4_p1q1
&
f4_p0q0
)
;
f4_p1q1
=
vbslq_u16
(
hev_mask_8
p1q1
f4_p1q1
)
;
uint16x8_t
p0q0_output
p1q1_output
p2q2_output
;
uint16x8_t
f8_p2q2
f8_p1q1
f8_p0q0
;
const
uint64x1_t
need_filter8
=
vreinterpret_u64_u16
(
is_flat4_mask
)
;
if
(
vget_lane_u64
(
need_filter8
0
)
=
=
0
)
{
p2q2_output
=
p2q2
;
p1q1_output
=
vbslq_u16
(
needs_filter_mask_8
f4_p1q1
p1q1
)
;
p0q0_output
=
vbslq_u16
(
needs_filter_mask_8
f4_p0q0
p0q0
)
;
}
else
{
const
uint16x8_t
is_flat4_mask_8
=
vcombine_u16
(
is_flat4_mask
is_flat4_mask
)
;
filter8
(
p3q3
p2q2
p1q1
p0q0
&
f8_p2q2
&
f8_p1q1
&
f8_p0q0
)
;
p2q2_output
=
vbslq_u16
(
is_flat4_mask_8
f8_p2q2
p2q2
)
;
p1q1_output
=
vbslq_u16
(
is_flat4_mask_8
f8_p1q1
f4_p1q1
)
;
p1q1_output
=
vbslq_u16
(
needs_filter_mask_8
p1q1_output
p1q1
)
;
p0q0_output
=
vbslq_u16
(
is_flat4_mask_8
f8_p0q0
f4_p0q0
)
;
p0q0_output
=
vbslq_u16
(
needs_filter_mask_8
p0q0_output
p0q0
)
;
}
vst1_u16
(
dst_p2
vget_low_u16
(
p2q2_output
)
)
;
vst1_u16
(
dst_p1
vget_low_u16
(
p1q1_output
)
)
;
vst1_u16
(
dst_p0
vget_low_u16
(
p0q0_output
)
)
;
vst1_u16
(
dst_q0
vget_high_u16
(
p0q0_output
)
)
;
vst1_u16
(
dst_q1
vget_high_u16
(
p1q1_output
)
)
;
vst1_u16
(
dst_q2
vget_high_u16
(
p2q2_output
)
)
;
}
void
aom_highbd_lpf_horizontal_8_dual_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit0
const
uint8_t
*
limit0
const
uint8_t
*
thresh0
const
uint8_t
*
blimit1
const
uint8_t
*
limit1
const
uint8_t
*
thresh1
int
bd
)
{
aom_highbd_lpf_horizontal_8_neon
(
s
pitch
blimit0
limit0
thresh0
bd
)
;
aom_highbd_lpf_horizontal_8_neon
(
s
+
4
pitch
blimit1
limit1
thresh1
bd
)
;
}
static
INLINE
uint16x8_t
reverse_low_half
(
const
uint16x8_t
a
)
{
return
vcombine_u16
(
vrev64_u16
(
vget_low_u16
(
a
)
)
vget_high_u16
(
a
)
)
;
}
void
aom_highbd_lpf_vertical_8_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit
const
uint8_t
*
limit
const
uint8_t
*
thresh
int
bd
)
{
uint16_t
*
const
dst
=
s
-
4
;
uint16_t
*
const
dst_0
=
dst
;
uint16_t
*
const
dst_1
=
dst
+
pitch
;
uint16_t
*
const
dst_2
=
dst
+
2
*
pitch
;
uint16_t
*
const
dst_3
=
dst
+
3
*
pitch
;
uint16x8_t
src
[
4
]
=
{
vld1q_u16
(
dst_0
)
vld1q_u16
(
dst_1
)
vld1q_u16
(
dst_2
)
vld1q_u16
(
dst_3
)
}
;
loop_filter_transpose_u16_4x8q
(
src
)
;
const
int
outer_thresh
=
*
blimit
<
<
(
bd
-
8
)
;
const
int
inner_thresh
=
*
limit
<
<
(
bd
-
8
)
;
const
int
hev_thresh
=
*
thresh
<
<
(
bd
-
8
)
;
const
uint16x4_t
outer_mask
=
outer_threshold
(
vget_low_u16
(
src
[
1
]
)
vget_low_u16
(
src
[
0
]
)
vget_high_u16
(
src
[
0
]
)
vget_high_u16
(
src
[
1
]
)
outer_thresh
)
;
uint16x4_t
hev_mask
;
uint16x4_t
needs_filter_mask
;
uint16x4_t
is_flat4_mask
;
const
uint16x8_t
p0q0
=
src
[
0
]
;
const
uint16x8_t
p1q1
=
src
[
1
]
;
const
uint16x8_t
p2q2
=
src
[
2
]
;
const
uint16x8_t
p3q3
=
src
[
3
]
;
filter8_masks
(
p3q3
p2q2
p1q1
p0q0
hev_thresh
outer_mask
inner_thresh
bd
&
needs_filter_mask
&
is_flat4_mask
&
hev_mask
)
;
#
if
AOM_ARCH_AARCH64
if
(
vaddv_u16
(
needs_filter_mask
)
=
=
0
)
{
return
;
}
#
endif
const
uint16x8_t
hev_mask_8
=
vcombine_u16
(
hev_mask
hev_mask
)
;
const
uint16x8_t
needs_filter_mask_8
=
vcombine_u16
(
needs_filter_mask
needs_filter_mask
)
;
uint16x8_t
f4_p1q1
;
uint16x8_t
f4_p0q0
;
const
uint16x8_t
p0q1
=
vcombine_u16
(
vget_low_u16
(
p0q0
)
vget_high_u16
(
p1q1
)
)
;
filter4
(
p0q0
p0q1
p1q1
hev_mask
bd
&
f4_p1q1
&
f4_p0q0
)
;
f4_p1q1
=
vbslq_u16
(
hev_mask_8
p1q1
f4_p1q1
)
;
uint16x8_t
p0q0_output
p1q1_output
p2q2_output
;
const
uint64x1_t
need_filter8
=
vreinterpret_u64_u16
(
is_flat4_mask
)
;
if
(
vget_lane_u64
(
need_filter8
0
)
=
=
0
)
{
p2q2_output
=
p2q2
;
p1q1_output
=
vbslq_u16
(
needs_filter_mask_8
f4_p1q1
p1q1
)
;
p0q0_output
=
vbslq_u16
(
needs_filter_mask_8
f4_p0q0
p0q0
)
;
}
else
{
const
uint16x8_t
is_flat4_mask_8
=
vcombine_u16
(
is_flat4_mask
is_flat4_mask
)
;
uint16x8_t
f8_p2q2
f8_p1q1
f8_p0q0
;
filter8
(
p3q3
p2q2
p1q1
p0q0
&
f8_p2q2
&
f8_p1q1
&
f8_p0q0
)
;
p2q2_output
=
vbslq_u16
(
is_flat4_mask_8
f8_p2q2
p2q2
)
;
p1q1_output
=
vbslq_u16
(
is_flat4_mask_8
f8_p1q1
f4_p1q1
)
;
p1q1_output
=
vbslq_u16
(
needs_filter_mask_8
p1q1_output
p1q1
)
;
p0q0_output
=
vbslq_u16
(
is_flat4_mask_8
f8_p0q0
f4_p0q0
)
;
p0q0_output
=
vbslq_u16
(
needs_filter_mask_8
p0q0_output
p0q0
)
;
}
uint16x8_t
output
[
4
]
=
{
p0q0_output
p1q1_output
p2q2_output
p3q3
}
;
transpose_array_inplace_u16_4x8
(
output
)
;
vst1q_u16
(
dst_0
reverse_low_half
(
output
[
0
]
)
)
;
vst1q_u16
(
dst_1
reverse_low_half
(
output
[
1
]
)
)
;
vst1q_u16
(
dst_2
reverse_low_half
(
output
[
2
]
)
)
;
vst1q_u16
(
dst_3
reverse_low_half
(
output
[
3
]
)
)
;
}
void
aom_highbd_lpf_vertical_8_dual_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit0
const
uint8_t
*
limit0
const
uint8_t
*
thresh0
const
uint8_t
*
blimit1
const
uint8_t
*
limit1
const
uint8_t
*
thresh1
int
bd
)
{
aom_highbd_lpf_vertical_8_neon
(
s
pitch
blimit0
limit0
thresh0
bd
)
;
aom_highbd_lpf_vertical_8_neon
(
s
+
4
*
pitch
pitch
blimit1
limit1
thresh1
bd
)
;
}
static
INLINE
void
filter14
(
const
uint16x8_t
p6q6
const
uint16x8_t
p5q5
const
uint16x8_t
p4q4
const
uint16x8_t
p3q3
const
uint16x8_t
p2q2
const
uint16x8_t
p1q1
const
uint16x8_t
p0q0
uint16x8_t
*
const
p5q5_output
uint16x8_t
*
const
p4q4_output
uint16x8_t
*
const
p3q3_output
uint16x8_t
*
const
p2q2_output
uint16x8_t
*
const
p1q1_output
uint16x8_t
*
const
p0q0_output
)
{
const
uint16x8_t
p6q6_x7
=
vsubq_u16
(
vshlq_n_u16
(
p6q6
3
)
p6q6
)
;
uint16x8_t
sum
=
vshlq_n_u16
(
vaddq_u16
(
p5q5
p4q4
)
1
)
;
sum
=
vaddq_u16
(
sum
p6q6_x7
)
;
sum
=
vaddq_u16
(
vaddq_u16
(
p3q3
p2q2
)
sum
)
;
sum
=
vaddq_u16
(
vaddq_u16
(
p1q1
p0q0
)
sum
)
;
const
uint16x8_t
q0p0
=
vextq_u16
(
p0q0
p0q0
4
)
;
sum
=
vaddq_u16
(
sum
q0p0
)
;
*
p5q5_output
=
vrshrq_n_u16
(
sum
4
)
;
sum
=
vsubq_u16
(
sum
vshlq_n_u16
(
p6q6
1
)
)
;
const
uint16x8_t
q1p1
=
vextq_u16
(
p1q1
p1q1
4
)
;
sum
=
vaddq_u16
(
vaddq_u16
(
p3q3
q1p1
)
sum
)
;
*
p4q4_output
=
vrshrq_n_u16
(
sum
4
)
;
sum
=
vsubq_u16
(
sum
vaddq_u16
(
p6q6
p5q5
)
)
;
const
uint16x8_t
q2p2
=
vextq_u16
(
p2q2
p2q2
4
)
;
sum
=
vaddq_u16
(
vaddq_u16
(
p2q2
q2p2
)
sum
)
;
*
p3q3_output
=
vrshrq_n_u16
(
sum
4
)
;
sum
=
vsubq_u16
(
sum
vaddq_u16
(
p6q6
p4q4
)
)
;
const
uint16x8_t
q3p3
=
vextq_u16
(
p3q3
p3q3
4
)
;
sum
=
vaddq_u16
(
vaddq_u16
(
p1q1
q3p3
)
sum
)
;
*
p2q2_output
=
vrshrq_n_u16
(
sum
4
)
;
sum
=
vsubq_u16
(
sum
vaddq_u16
(
p6q6
p3q3
)
)
;
const
uint16x8_t
q4p4
=
vextq_u16
(
p4q4
p4q4
4
)
;
sum
=
vaddq_u16
(
vaddq_u16
(
p0q0
q4p4
)
sum
)
;
*
p1q1_output
=
vrshrq_n_u16
(
sum
4
)
;
sum
=
vsubq_u16
(
sum
vaddq_u16
(
p6q6
p2q2
)
)
;
const
uint16x8_t
q5p5
=
vextq_u16
(
p5q5
p5q5
4
)
;
sum
=
vaddq_u16
(
vaddq_u16
(
q0p0
q5p5
)
sum
)
;
*
p0q0_output
=
vrshrq_n_u16
(
sum
4
)
;
}
void
aom_highbd_lpf_horizontal_14_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit
const
uint8_t
*
limit
const
uint8_t
*
thresh
int
bd
)
{
uint16_t
*
const
dst_p6
=
s
-
7
*
pitch
;
uint16_t
*
const
dst_p5
=
s
-
6
*
pitch
;
uint16_t
*
const
dst_p4
=
s
-
5
*
pitch
;
uint16_t
*
const
dst_p3
=
s
-
4
*
pitch
;
uint16_t
*
const
dst_p2
=
s
-
3
*
pitch
;
uint16_t
*
const
dst_p1
=
s
-
2
*
pitch
;
uint16_t
*
const
dst_p0
=
s
-
pitch
;
uint16_t
*
const
dst_q0
=
s
;
uint16_t
*
const
dst_q1
=
s
+
pitch
;
uint16_t
*
const
dst_q2
=
s
+
2
*
pitch
;
uint16_t
*
const
dst_q3
=
s
+
3
*
pitch
;
uint16_t
*
const
dst_q4
=
s
+
4
*
pitch
;
uint16_t
*
const
dst_q5
=
s
+
5
*
pitch
;
uint16_t
*
const
dst_q6
=
s
+
6
*
pitch
;
const
uint16x4_t
src
[
14
]
=
{
vld1_u16
(
dst_p6
)
vld1_u16
(
dst_p5
)
vld1_u16
(
dst_p4
)
vld1_u16
(
dst_p3
)
vld1_u16
(
dst_p2
)
vld1_u16
(
dst_p1
)
vld1_u16
(
dst_p0
)
vld1_u16
(
dst_q0
)
vld1_u16
(
dst_q1
)
vld1_u16
(
dst_q2
)
vld1_u16
(
dst_q3
)
vld1_u16
(
dst_q4
)
vld1_u16
(
dst_q5
)
vld1_u16
(
dst_q6
)
}
;
const
int
outer_thresh
=
*
blimit
<
<
(
bd
-
8
)
;
const
int
inner_thresh
=
*
limit
<
<
(
bd
-
8
)
;
const
int
hev_thresh
=
*
thresh
<
<
(
bd
-
8
)
;
const
uint16x4_t
outer_mask
=
outer_threshold
(
src
[
5
]
src
[
6
]
src
[
7
]
src
[
8
]
outer_thresh
)
;
uint16x4_t
hev_mask
;
uint16x4_t
needs_filter_mask
;
uint16x4_t
is_flat4_mask
;
const
uint16x8_t
p0q0
=
vcombine_u16
(
src
[
6
]
src
[
7
]
)
;
const
uint16x8_t
p1q1
=
vcombine_u16
(
src
[
5
]
src
[
8
]
)
;
const
uint16x8_t
p2q2
=
vcombine_u16
(
src
[
4
]
src
[
9
]
)
;
const
uint16x8_t
p3q3
=
vcombine_u16
(
src
[
3
]
src
[
10
]
)
;
filter8_masks
(
p3q3
p2q2
p1q1
p0q0
hev_thresh
outer_mask
inner_thresh
bd
&
needs_filter_mask
&
is_flat4_mask
&
hev_mask
)
;
#
if
AOM_ARCH_AARCH64
if
(
vaddv_u16
(
needs_filter_mask
)
=
=
0
)
{
return
;
}
#
endif
const
uint16x8_t
p4q4
=
vcombine_u16
(
src
[
2
]
src
[
11
]
)
;
const
uint16x8_t
p5q5
=
vcombine_u16
(
src
[
1
]
src
[
12
]
)
;
const
uint16x8_t
p6q6
=
vcombine_u16
(
src
[
0
]
src
[
13
]
)
;
const
uint16x4_t
is_flat4_outer_mask
=
vand_u16
(
is_flat4_mask
is_flat4
(
vabdq_u16
(
p0q0
p4q4
)
vabdq_u16
(
p0q0
p5q5
)
vabdq_u16
(
p0q0
p6q6
)
bd
)
)
;
const
uint16x8_t
hev_mask_8
=
vcombine_u16
(
hev_mask
hev_mask
)
;
const
uint16x8_t
needs_filter_mask_8
=
vcombine_u16
(
needs_filter_mask
needs_filter_mask
)
;
uint16x8_t
f4_p1q1
;
uint16x8_t
f4_p0q0
;
const
uint16x8_t
p0q1
=
vcombine_u16
(
src
[
6
]
src
[
8
]
)
;
filter4
(
p0q0
p0q1
p1q1
hev_mask
bd
&
f4_p1q1
&
f4_p0q0
)
;
f4_p1q1
=
vbslq_u16
(
hev_mask_8
p1q1
f4_p1q1
)
;
uint16x8_t
p0q0_output
p1q1_output
p2q2_output
p3q3_output
p4q4_output
p5q5_output
;
uint16x8_t
f8_p2q2
f8_p1q1
f8_p0q0
;
const
uint64x1_t
need_filter8
=
vreinterpret_u64_u16
(
is_flat4_mask
)
;
if
(
vget_lane_u64
(
need_filter8
0
)
=
=
0
)
{
p5q5_output
=
p5q5
;
p4q4_output
=
p4q4
;
p3q3_output
=
p3q3
;
p2q2_output
=
p2q2
;
p1q1_output
=
vbslq_u16
(
needs_filter_mask_8
f4_p1q1
p1q1
)
;
p0q0_output
=
vbslq_u16
(
needs_filter_mask_8
f4_p0q0
p0q0
)
;
}
else
{
const
uint16x8_t
use_filter8_mask
=
vcombine_u16
(
is_flat4_mask
is_flat4_mask
)
;
filter8
(
p3q3
p2q2
p1q1
p0q0
&
f8_p2q2
&
f8_p1q1
&
f8_p0q0
)
;
const
uint64x1_t
need_filter14
=
vreinterpret_u64_u16
(
is_flat4_outer_mask
)
;
if
(
vget_lane_u64
(
need_filter14
0
)
=
=
0
)
{
p5q5_output
=
p5q5
;
p4q4_output
=
p4q4
;
p3q3_output
=
p3q3
;
p2q2_output
=
vbslq_u16
(
use_filter8_mask
f8_p2q2
p2q2
)
;
p1q1_output
=
vbslq_u16
(
use_filter8_mask
f8_p1q1
f4_p1q1
)
;
p1q1_output
=
vbslq_u16
(
needs_filter_mask_8
p1q1_output
p1q1
)
;
p0q0_output
=
vbslq_u16
(
use_filter8_mask
f8_p0q0
f4_p0q0
)
;
p0q0_output
=
vbslq_u16
(
needs_filter_mask_8
p0q0_output
p0q0
)
;
}
else
{
const
uint16x8_t
use_filter14_mask
=
vcombine_u16
(
is_flat4_outer_mask
is_flat4_outer_mask
)
;
uint16x8_t
f14_p5q5
f14_p4q4
f14_p3q3
f14_p2q2
f14_p1q1
f14_p0q0
;
filter14
(
p6q6
p5q5
p4q4
p3q3
p2q2
p1q1
p0q0
&
f14_p5q5
&
f14_p4q4
&
f14_p3q3
&
f14_p2q2
&
f14_p1q1
&
f14_p0q0
)
;
p5q5_output
=
vbslq_u16
(
use_filter14_mask
f14_p5q5
p5q5
)
;
p4q4_output
=
vbslq_u16
(
use_filter14_mask
f14_p4q4
p4q4
)
;
p3q3_output
=
vbslq_u16
(
use_filter14_mask
f14_p3q3
p3q3
)
;
p2q2_output
=
vbslq_u16
(
use_filter14_mask
f14_p2q2
f8_p2q2
)
;
p2q2_output
=
vbslq_u16
(
use_filter8_mask
p2q2_output
p2q2
)
;
p2q2_output
=
vbslq_u16
(
needs_filter_mask_8
p2q2_output
p2q2
)
;
p1q1_output
=
vbslq_u16
(
use_filter14_mask
f14_p1q1
f8_p1q1
)
;
p1q1_output
=
vbslq_u16
(
use_filter8_mask
p1q1_output
f4_p1q1
)
;
p1q1_output
=
vbslq_u16
(
needs_filter_mask_8
p1q1_output
p1q1
)
;
p0q0_output
=
vbslq_u16
(
use_filter14_mask
f14_p0q0
f8_p0q0
)
;
p0q0_output
=
vbslq_u16
(
use_filter8_mask
p0q0_output
f4_p0q0
)
;
p0q0_output
=
vbslq_u16
(
needs_filter_mask_8
p0q0_output
p0q0
)
;
}
}
vst1_u16
(
dst_p5
vget_low_u16
(
p5q5_output
)
)
;
vst1_u16
(
dst_p4
vget_low_u16
(
p4q4_output
)
)
;
vst1_u16
(
dst_p3
vget_low_u16
(
p3q3_output
)
)
;
vst1_u16
(
dst_p2
vget_low_u16
(
p2q2_output
)
)
;
vst1_u16
(
dst_p1
vget_low_u16
(
p1q1_output
)
)
;
vst1_u16
(
dst_p0
vget_low_u16
(
p0q0_output
)
)
;
vst1_u16
(
dst_q0
vget_high_u16
(
p0q0_output
)
)
;
vst1_u16
(
dst_q1
vget_high_u16
(
p1q1_output
)
)
;
vst1_u16
(
dst_q2
vget_high_u16
(
p2q2_output
)
)
;
vst1_u16
(
dst_q3
vget_high_u16
(
p3q3_output
)
)
;
vst1_u16
(
dst_q4
vget_high_u16
(
p4q4_output
)
)
;
vst1_u16
(
dst_q5
vget_high_u16
(
p5q5_output
)
)
;
}
void
aom_highbd_lpf_horizontal_14_dual_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit0
const
uint8_t
*
limit0
const
uint8_t
*
thresh0
const
uint8_t
*
blimit1
const
uint8_t
*
limit1
const
uint8_t
*
thresh1
int
bd
)
{
aom_highbd_lpf_horizontal_14_neon
(
s
pitch
blimit0
limit0
thresh0
bd
)
;
aom_highbd_lpf_horizontal_14_neon
(
s
+
4
pitch
blimit1
limit1
thresh1
bd
)
;
}
static
INLINE
uint16x8x2_t
permute_acdb64
(
const
uint16x8_t
ab
const
uint16x8_t
cd
)
{
uint16x8x2_t
acdb
;
#
if
AOM_ARCH_AARCH64
acdb
.
val
[
0
]
=
vreinterpretq_u16_u64
(
vtrn1q_u64
(
vreinterpretq_u64_u16
(
ab
)
vreinterpretq_u64_u16
(
cd
)
)
)
;
acdb
.
val
[
1
]
=
vreinterpretq_u16_u64
(
vtrn2q_u64
(
vreinterpretq_u64_u16
(
cd
)
vreinterpretq_u64_u16
(
ab
)
)
)
;
#
else
acdb
.
val
[
0
]
=
vreinterpretq_u16_u64
(
vsetq_lane_u64
(
vgetq_lane_u64
(
vreinterpretq_u64_u16
(
cd
)
0
)
vreinterpretq_u64_u16
(
ab
)
1
)
)
;
acdb
.
val
[
1
]
=
vreinterpretq_u16_u64
(
vsetq_lane_u64
(
vgetq_lane_u64
(
vreinterpretq_u64_u16
(
cd
)
1
)
vreinterpretq_u64_u16
(
ab
)
0
)
)
;
#
endif
return
acdb
;
}
void
aom_highbd_lpf_vertical_14_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit
const
uint8_t
*
limit
const
uint8_t
*
thresh
int
bd
)
{
uint16_t
*
const
dst
=
s
-
8
;
uint16_t
*
const
dst_0
=
dst
;
uint16_t
*
const
dst_1
=
dst
+
pitch
;
uint16_t
*
const
dst_2
=
dst
+
2
*
pitch
;
uint16_t
*
const
dst_3
=
dst
+
3
*
pitch
;
uint16x8_t
src_p
[
4
]
=
{
vld1q_u16
(
dst_0
)
vld1q_u16
(
dst_1
)
vld1q_u16
(
dst_2
)
vld1q_u16
(
dst_3
)
}
;
transpose_array_inplace_u16_4x8
(
src_p
)
;
uint16x8_t
src_q
[
4
]
=
{
vld1q_u16
(
dst_0
+
8
)
vld1q_u16
(
dst_1
+
8
)
vld1q_u16
(
dst_2
+
8
)
vld1q_u16
(
dst_3
+
8
)
}
;
transpose_array_inplace_u16_4x8
(
src_q
)
;
const
int
outer_thresh
=
*
blimit
<
<
(
bd
-
8
)
;
const
int
inner_thresh
=
*
limit
<
<
(
bd
-
8
)
;
const
int
hev_thresh
=
*
thresh
<
<
(
bd
-
8
)
;
const
uint16x4_t
outer_mask
=
outer_threshold
(
vget_high_u16
(
src_p
[
2
]
)
vget_high_u16
(
src_p
[
3
]
)
vget_low_u16
(
src_q
[
0
]
)
vget_low_u16
(
src_q
[
1
]
)
outer_thresh
)
;
const
uint16x8_t
p0q0
=
vextq_u16
(
src_p
[
3
]
src_q
[
0
]
4
)
;
const
uint16x8_t
p1q1
=
vextq_u16
(
src_p
[
2
]
src_q
[
1
]
4
)
;
const
uint16x8_t
p2q2
=
vextq_u16
(
src_p
[
1
]
src_q
[
2
]
4
)
;
const
uint16x8_t
p3q3
=
vextq_u16
(
src_p
[
0
]
src_q
[
3
]
4
)
;
uint16x4_t
hev_mask
;
uint16x4_t
needs_filter_mask
;
uint16x4_t
is_flat4_mask
;
filter8_masks
(
p3q3
p2q2
p1q1
p0q0
hev_thresh
outer_mask
inner_thresh
bd
&
needs_filter_mask
&
is_flat4_mask
&
hev_mask
)
;
#
if
AOM_ARCH_AARCH64
if
(
vaddv_u16
(
needs_filter_mask
)
=
=
0
)
{
return
;
}
#
endif
const
uint16x8_t
p4q4
=
vcombine_u16
(
vget_low_u16
(
src_p
[
3
]
)
vget_high_u16
(
src_q
[
0
]
)
)
;
const
uint16x8_t
p5q5
=
vcombine_u16
(
vget_low_u16
(
src_p
[
2
]
)
vget_high_u16
(
src_q
[
1
]
)
)
;
const
uint16x8_t
p6q6
=
vcombine_u16
(
vget_low_u16
(
src_p
[
1
]
)
vget_high_u16
(
src_q
[
2
]
)
)
;
const
uint16x8_t
p7q7
=
vcombine_u16
(
vget_low_u16
(
src_p
[
0
]
)
vget_high_u16
(
src_q
[
3
]
)
)
;
const
uint16x4_t
is_flat4_outer_mask
=
vand_u16
(
is_flat4_mask
is_flat4
(
vabdq_u16
(
p0q0
p4q4
)
vabdq_u16
(
p0q0
p5q5
)
vabdq_u16
(
p0q0
p6q6
)
bd
)
)
;
const
uint16x8_t
hev_mask_8
=
vcombine_u16
(
hev_mask
hev_mask
)
;
const
uint16x8_t
needs_filter_mask_8
=
vcombine_u16
(
needs_filter_mask
needs_filter_mask
)
;
uint16x8_t
f4_p1q1
;
uint16x8_t
f4_p0q0
;
const
uint16x8_t
p0q1
=
vcombine_u16
(
vget_low_u16
(
p0q0
)
vget_high_u16
(
p1q1
)
)
;
filter4
(
p0q0
p0q1
p1q1
hev_mask
bd
&
f4_p1q1
&
f4_p0q0
)
;
f4_p1q1
=
vbslq_u16
(
hev_mask_8
p1q1
f4_p1q1
)
;
uint16x8_t
p0q0_output
p1q1_output
p2q2_output
p3q3_output
p4q4_output
p5q5_output
;
uint16x8_t
f8_p2q2
f8_p1q1
f8_p0q0
;
const
uint64x1_t
need_filter8
=
vreinterpret_u64_u16
(
is_flat4_mask
)
;
if
(
vget_lane_u64
(
need_filter8
0
)
=
=
0
)
{
p5q5_output
=
p5q5
;
p4q4_output
=
p4q4
;
p3q3_output
=
p3q3
;
p2q2_output
=
p2q2
;
p1q1_output
=
vbslq_u16
(
needs_filter_mask_8
f4_p1q1
p1q1
)
;
p0q0_output
=
vbslq_u16
(
needs_filter_mask_8
f4_p0q0
p0q0
)
;
}
else
{
const
uint16x8_t
use_filter8_mask
=
vcombine_u16
(
is_flat4_mask
is_flat4_mask
)
;
filter8
(
p3q3
p2q2
p1q1
p0q0
&
f8_p2q2
&
f8_p1q1
&
f8_p0q0
)
;
const
uint64x1_t
need_filter14
=
vreinterpret_u64_u16
(
is_flat4_outer_mask
)
;
if
(
vget_lane_u64
(
need_filter14
0
)
=
=
0
)
{
p5q5_output
=
p5q5
;
p4q4_output
=
p4q4
;
p3q3_output
=
p3q3
;
p2q2_output
=
vbslq_u16
(
use_filter8_mask
f8_p2q2
p2q2
)
;
p1q1_output
=
vbslq_u16
(
use_filter8_mask
f8_p1q1
f4_p1q1
)
;
p1q1_output
=
vbslq_u16
(
needs_filter_mask_8
p1q1_output
p1q1
)
;
p0q0_output
=
vbslq_u16
(
use_filter8_mask
f8_p0q0
f4_p0q0
)
;
p0q0_output
=
vbslq_u16
(
needs_filter_mask_8
p0q0_output
p0q0
)
;
}
else
{
const
uint16x8_t
use_filter14_mask
=
vcombine_u16
(
is_flat4_outer_mask
is_flat4_outer_mask
)
;
uint16x8_t
f14_p5q5
f14_p4q4
f14_p3q3
f14_p2q2
f14_p1q1
f14_p0q0
;
filter14
(
p6q6
p5q5
p4q4
p3q3
p2q2
p1q1
p0q0
&
f14_p5q5
&
f14_p4q4
&
f14_p3q3
&
f14_p2q2
&
f14_p1q1
&
f14_p0q0
)
;
p5q5_output
=
vbslq_u16
(
use_filter14_mask
f14_p5q5
p5q5
)
;
p4q4_output
=
vbslq_u16
(
use_filter14_mask
f14_p4q4
p4q4
)
;
p3q3_output
=
vbslq_u16
(
use_filter14_mask
f14_p3q3
p3q3
)
;
p2q2_output
=
vbslq_u16
(
use_filter14_mask
f14_p2q2
f8_p2q2
)
;
p2q2_output
=
vbslq_u16
(
use_filter8_mask
p2q2_output
p2q2
)
;
p2q2_output
=
vbslq_u16
(
needs_filter_mask_8
p2q2_output
p2q2
)
;
p1q1_output
=
vbslq_u16
(
use_filter14_mask
f14_p1q1
f8_p1q1
)
;
p1q1_output
=
vbslq_u16
(
use_filter8_mask
p1q1_output
f4_p1q1
)
;
p1q1_output
=
vbslq_u16
(
needs_filter_mask_8
p1q1_output
p1q1
)
;
p0q0_output
=
vbslq_u16
(
use_filter14_mask
f14_p0q0
f8_p0q0
)
;
p0q0_output
=
vbslq_u16
(
use_filter8_mask
p0q0_output
f4_p0q0
)
;
p0q0_output
=
vbslq_u16
(
needs_filter_mask_8
p0q0_output
p0q0
)
;
}
}
const
uint16x8x2_t
p7p3_q3q7
=
permute_acdb64
(
p7q7
p3q3_output
)
;
const
uint16x8x2_t
p6p2_q2q6
=
permute_acdb64
(
p6q6
p2q2_output
)
;
const
uint16x8x2_t
p5p1_q1q5
=
permute_acdb64
(
p5q5_output
p1q1_output
)
;
const
uint16x8x2_t
p4p0_q0q4
=
permute_acdb64
(
p4q4_output
p0q0_output
)
;
uint16x8_t
output_p
[
4
]
=
{
p7p3_q3q7
.
val
[
0
]
p6p2_q2q6
.
val
[
0
]
p5p1_q1q5
.
val
[
0
]
p4p0_q0q4
.
val
[
0
]
}
;
transpose_array_inplace_u16_4x8
(
output_p
)
;
uint16x8_t
output_q
[
4
]
=
{
p4p0_q0q4
.
val
[
1
]
p5p1_q1q5
.
val
[
1
]
p6p2_q2q6
.
val
[
1
]
p7p3_q3q7
.
val
[
1
]
}
;
transpose_array_inplace_u16_4x8
(
output_q
)
;
vst1q_u16
(
dst_0
output_p
[
0
]
)
;
vst1q_u16
(
dst_0
+
8
output_q
[
0
]
)
;
vst1q_u16
(
dst_1
output_p
[
1
]
)
;
vst1q_u16
(
dst_1
+
8
output_q
[
1
]
)
;
vst1q_u16
(
dst_2
output_p
[
2
]
)
;
vst1q_u16
(
dst_2
+
8
output_q
[
2
]
)
;
vst1q_u16
(
dst_3
output_p
[
3
]
)
;
vst1q_u16
(
dst_3
+
8
output_q
[
3
]
)
;
}
void
aom_highbd_lpf_vertical_14_dual_neon
(
uint16_t
*
s
int
pitch
const
uint8_t
*
blimit0
const
uint8_t
*
limit0
const
uint8_t
*
thresh0
const
uint8_t
*
blimit1
const
uint8_t
*
limit1
const
uint8_t
*
thresh1
int
bd
)
{
aom_highbd_lpf_vertical_14_neon
(
s
pitch
blimit0
limit0
thresh0
bd
)
;
aom_highbd_lpf_vertical_14_neon
(
s
+
4
*
pitch
pitch
blimit1
limit1
thresh1
bd
)
;
}
