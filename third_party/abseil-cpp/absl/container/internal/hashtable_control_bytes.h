#
ifndef
ABSL_CONTAINER_INTERNAL_HASHTABLE_CONTROL_BYTES_H_
#
define
ABSL_CONTAINER_INTERNAL_HASHTABLE_CONTROL_BYTES_H_
#
include
<
cassert
>
#
include
<
cstddef
>
#
include
<
cstdint
>
#
include
<
type_traits
>
#
include
"
absl
/
base
/
config
.
h
"
#
ifdef
ABSL_INTERNAL_HAVE_SSE2
#
include
<
emmintrin
.
h
>
#
endif
#
ifdef
ABSL_INTERNAL_HAVE_SSSE3
#
include
<
tmmintrin
.
h
>
#
endif
#
ifdef
_MSC_VER
#
include
<
intrin
.
h
>
#
endif
#
ifdef
ABSL_INTERNAL_HAVE_ARM_NEON
#
include
<
arm_neon
.
h
>
#
endif
#
include
"
absl
/
base
/
optimization
.
h
"
#
include
"
absl
/
numeric
/
bits
.
h
"
#
include
"
absl
/
base
/
internal
/
endian
.
h
"
namespace
absl
{
ABSL_NAMESPACE_BEGIN
namespace
container_internal
{
#
ifdef
ABSL_SWISSTABLE_ASSERT
#
error
ABSL_SWISSTABLE_ASSERT
cannot
be
directly
set
#
else
#
define
ABSL_SWISSTABLE_ASSERT
(
CONDITION
)
\
assert
(
(
CONDITION
)
&
&
"
Try
enabling
sanitizers
.
"
)
#
endif
template
<
typename
T
>
uint32_t
TrailingZeros
(
T
x
)
{
ABSL_ASSUME
(
x
!
=
0
)
;
return
static_cast
<
uint32_t
>
(
countr_zero
(
x
)
)
;
}
constexpr
uint64_t
kMsbs8Bytes
=
0x8080808080808080ULL
;
constexpr
uint64_t
k8EmptyBytes
=
kMsbs8Bytes
;
template
<
class
T
int
SignificantBits
int
Shift
=
0
>
class
NonIterableBitMask
{
public
:
explicit
NonIterableBitMask
(
T
mask
)
:
mask_
(
mask
)
{
}
explicit
operator
bool
(
)
const
{
return
this
-
>
mask_
!
=
0
;
}
uint32_t
LowestBitSet
(
)
const
{
return
container_internal
:
:
TrailingZeros
(
mask_
)
>
>
Shift
;
}
uint32_t
HighestBitSet
(
)
const
{
return
static_cast
<
uint32_t
>
(
(
bit_width
(
mask_
)
-
1
)
>
>
Shift
)
;
}
uint32_t
TrailingZeros
(
)
const
{
return
container_internal
:
:
TrailingZeros
(
mask_
)
>
>
Shift
;
}
uint32_t
LeadingZeros
(
)
const
{
constexpr
int
total_significant_bits
=
SignificantBits
<
<
Shift
;
constexpr
int
extra_bits
=
sizeof
(
T
)
*
8
-
total_significant_bits
;
return
static_cast
<
uint32_t
>
(
countl_zero
(
static_cast
<
T
>
(
mask_
<
<
extra_bits
)
)
)
>
>
Shift
;
}
T
mask_
;
}
;
template
<
class
T
int
SignificantBits
int
Shift
=
0
bool
NullifyBitsOnIteration
=
false
>
class
BitMask
:
public
NonIterableBitMask
<
T
SignificantBits
Shift
>
{
using
Base
=
NonIterableBitMask
<
T
SignificantBits
Shift
>
;
static_assert
(
std
:
:
is_unsigned
<
T
>
:
:
value
"
"
)
;
static_assert
(
Shift
=
=
0
|
|
Shift
=
=
3
"
"
)
;
static_assert
(
!
NullifyBitsOnIteration
|
|
Shift
=
=
3
"
"
)
;
public
:
explicit
BitMask
(
T
mask
)
:
Base
(
mask
)
{
if
(
Shift
=
=
3
&
&
!
NullifyBitsOnIteration
)
{
ABSL_SWISSTABLE_ASSERT
(
this
-
>
mask_
=
=
(
this
-
>
mask_
&
kMsbs8Bytes
)
)
;
}
}
using
value_type
=
int
;
using
iterator
=
BitMask
;
using
const_iterator
=
BitMask
;
BitMask
&
operator
+
+
(
)
{
if
(
Shift
=
=
3
&
&
NullifyBitsOnIteration
)
{
this
-
>
mask_
&
=
kMsbs8Bytes
;
}
this
-
>
mask_
&
=
(
this
-
>
mask_
-
1
)
;
return
*
this
;
}
uint32_t
operator
*
(
)
const
{
return
Base
:
:
LowestBitSet
(
)
;
}
BitMask
begin
(
)
const
{
return
*
this
;
}
BitMask
end
(
)
const
{
return
BitMask
(
0
)
;
}
private
:
friend
bool
operator
=
=
(
const
BitMask
&
a
const
BitMask
&
b
)
{
return
a
.
mask_
=
=
b
.
mask_
;
}
friend
bool
operator
!
=
(
const
BitMask
&
a
const
BitMask
&
b
)
{
return
a
.
mask_
!
=
b
.
mask_
;
}
}
;
using
h2_t
=
uint8_t
;
enum
class
ctrl_t
:
int8_t
{
kEmpty
=
-
128
kDeleted
=
-
2
kSentinel
=
-
1
}
;
static_assert
(
(
static_cast
<
int8_t
>
(
ctrl_t
:
:
kEmpty
)
&
static_cast
<
int8_t
>
(
ctrl_t
:
:
kDeleted
)
&
static_cast
<
int8_t
>
(
ctrl_t
:
:
kSentinel
)
&
0x80
)
!
=
0
"
Special
markers
need
to
have
the
MSB
to
make
checking
for
them
efficient
"
)
;
static_assert
(
ctrl_t
:
:
kEmpty
<
ctrl_t
:
:
kSentinel
&
&
ctrl_t
:
:
kDeleted
<
ctrl_t
:
:
kSentinel
"
ctrl_t
:
:
kEmpty
and
ctrl_t
:
:
kDeleted
must
be
smaller
than
"
"
ctrl_t
:
:
kSentinel
to
make
the
SIMD
test
of
IsEmptyOrDeleted
(
)
efficient
"
)
;
static_assert
(
ctrl_t
:
:
kSentinel
=
=
static_cast
<
ctrl_t
>
(
-
1
)
"
ctrl_t
:
:
kSentinel
must
be
-
1
to
elide
loading
it
from
memory
into
SIMD
"
"
registers
(
pcmpeqd
xmm
xmm
)
"
)
;
static_assert
(
ctrl_t
:
:
kEmpty
=
=
static_cast
<
ctrl_t
>
(
-
128
)
"
ctrl_t
:
:
kEmpty
must
be
-
128
to
make
the
SIMD
check
for
its
"
"
existence
efficient
(
psignb
xmm
xmm
)
"
)
;
static_assert
(
(
~
static_cast
<
int8_t
>
(
ctrl_t
:
:
kEmpty
)
&
~
static_cast
<
int8_t
>
(
ctrl_t
:
:
kDeleted
)
&
static_cast
<
int8_t
>
(
ctrl_t
:
:
kSentinel
)
&
0x7F
)
!
=
0
"
ctrl_t
:
:
kEmpty
and
ctrl_t
:
:
kDeleted
must
share
an
unset
bit
that
is
not
"
"
shared
by
ctrl_t
:
:
kSentinel
to
make
the
scalar
test
for
"
"
MaskEmptyOrDeleted
(
)
efficient
"
)
;
static_assert
(
ctrl_t
:
:
kDeleted
=
=
static_cast
<
ctrl_t
>
(
-
2
)
"
ctrl_t
:
:
kDeleted
must
be
-
2
to
make
the
implementation
of
"
"
ConvertSpecialToEmptyAndFullToDeleted
efficient
"
)
;
inline
bool
IsEmpty
(
ctrl_t
c
)
{
return
c
=
=
ctrl_t
:
:
kEmpty
;
}
inline
bool
IsFull
(
ctrl_t
c
)
{
return
static_cast
<
std
:
:
underlying_type_t
<
ctrl_t
>
>
(
c
)
>
=
0
;
}
inline
bool
IsDeleted
(
ctrl_t
c
)
{
return
c
=
=
ctrl_t
:
:
kDeleted
;
}
inline
bool
IsEmptyOrDeleted
(
ctrl_t
c
)
{
return
c
<
ctrl_t
:
:
kSentinel
;
}
#
ifdef
ABSL_INTERNAL_HAVE_SSE2
inline
__m128i
_mm_cmpgt_epi8_fixed
(
__m128i
a
__m128i
b
)
{
#
if
defined
(
__GNUC__
)
&
&
!
defined
(
__clang__
)
if
(
std
:
:
is_unsigned
<
char
>
:
:
value
)
{
const
__m128i
mask
=
_mm_set1_epi8
(
0x80
)
;
const
__m128i
diff
=
_mm_subs_epi8
(
b
a
)
;
return
_mm_cmpeq_epi8
(
_mm_and_si128
(
diff
mask
)
mask
)
;
}
#
endif
return
_mm_cmpgt_epi8
(
a
b
)
;
}
struct
GroupSse2Impl
{
static
constexpr
size_t
kWidth
=
16
;
using
BitMaskType
=
BitMask
<
uint16_t
kWidth
>
;
using
NonIterableBitMaskType
=
NonIterableBitMask
<
uint16_t
kWidth
>
;
explicit
GroupSse2Impl
(
const
ctrl_t
*
pos
)
{
ctrl
=
_mm_loadu_si128
(
reinterpret_cast
<
const
__m128i
*
>
(
pos
)
)
;
}
BitMaskType
Match
(
h2_t
hash
)
const
{
auto
match
=
_mm_set1_epi8
(
static_cast
<
char
>
(
hash
)
)
;
return
BitMaskType
(
static_cast
<
uint16_t
>
(
_mm_movemask_epi8
(
_mm_cmpeq_epi8
(
match
ctrl
)
)
)
)
;
}
NonIterableBitMaskType
MaskEmpty
(
)
const
{
#
ifdef
ABSL_INTERNAL_HAVE_SSSE3
return
NonIterableBitMaskType
(
static_cast
<
uint16_t
>
(
_mm_movemask_epi8
(
_mm_sign_epi8
(
ctrl
ctrl
)
)
)
)
;
#
else
auto
match
=
_mm_set1_epi8
(
static_cast
<
char
>
(
ctrl_t
:
:
kEmpty
)
)
;
return
NonIterableBitMaskType
(
static_cast
<
uint16_t
>
(
_mm_movemask_epi8
(
_mm_cmpeq_epi8
(
match
ctrl
)
)
)
)
;
#
endif
}
BitMaskType
MaskFull
(
)
const
{
return
BitMaskType
(
static_cast
<
uint16_t
>
(
_mm_movemask_epi8
(
ctrl
)
^
0xffff
)
)
;
}
auto
MaskNonFull
(
)
const
{
return
BitMaskType
(
static_cast
<
uint16_t
>
(
_mm_movemask_epi8
(
ctrl
)
)
)
;
}
NonIterableBitMaskType
MaskEmptyOrDeleted
(
)
const
{
auto
special
=
_mm_set1_epi8
(
static_cast
<
char
>
(
ctrl_t
:
:
kSentinel
)
)
;
return
NonIterableBitMaskType
(
static_cast
<
uint16_t
>
(
_mm_movemask_epi8
(
_mm_cmpgt_epi8_fixed
(
special
ctrl
)
)
)
)
;
}
uint32_t
CountLeadingEmptyOrDeleted
(
)
const
{
auto
special
=
_mm_set1_epi8
(
static_cast
<
char
>
(
ctrl_t
:
:
kSentinel
)
)
;
return
TrailingZeros
(
static_cast
<
uint32_t
>
(
_mm_movemask_epi8
(
_mm_cmpgt_epi8_fixed
(
special
ctrl
)
)
+
1
)
)
;
}
void
ConvertSpecialToEmptyAndFullToDeleted
(
ctrl_t
*
dst
)
const
{
auto
msbs
=
_mm_set1_epi8
(
static_cast
<
char
>
(
-
128
)
)
;
auto
x126
=
_mm_set1_epi8
(
126
)
;
#
ifdef
ABSL_INTERNAL_HAVE_SSSE3
auto
res
=
_mm_or_si128
(
_mm_shuffle_epi8
(
x126
ctrl
)
msbs
)
;
#
else
auto
zero
=
_mm_setzero_si128
(
)
;
auto
special_mask
=
_mm_cmpgt_epi8_fixed
(
zero
ctrl
)
;
auto
res
=
_mm_or_si128
(
msbs
_mm_andnot_si128
(
special_mask
x126
)
)
;
#
endif
_mm_storeu_si128
(
reinterpret_cast
<
__m128i
*
>
(
dst
)
res
)
;
}
__m128i
ctrl
;
}
;
#
endif
#
if
defined
(
ABSL_INTERNAL_HAVE_ARM_NEON
)
&
&
defined
(
ABSL_IS_LITTLE_ENDIAN
)
struct
GroupAArch64Impl
{
static
constexpr
size_t
kWidth
=
8
;
using
BitMaskType
=
BitMask
<
uint64_t
kWidth
3
true
>
;
using
NonIterableBitMaskType
=
NonIterableBitMask
<
uint64_t
kWidth
3
>
;
explicit
GroupAArch64Impl
(
const
ctrl_t
*
pos
)
{
ctrl
=
vld1_u8
(
reinterpret_cast
<
const
uint8_t
*
>
(
pos
)
)
;
}
auto
Match
(
h2_t
hash
)
const
{
uint8x8_t
dup
=
vdup_n_u8
(
hash
)
;
auto
mask
=
vceq_u8
(
ctrl
dup
)
;
return
BitMaskType
(
vget_lane_u64
(
vreinterpret_u64_u8
(
mask
)
0
)
)
;
}
auto
MaskEmpty
(
)
const
{
uint64_t
mask
=
vget_lane_u64
(
vreinterpret_u64_u8
(
vceq_s8
(
vdup_n_s8
(
static_cast
<
int8_t
>
(
ctrl_t
:
:
kEmpty
)
)
vreinterpret_s8_u8
(
ctrl
)
)
)
0
)
;
return
NonIterableBitMaskType
(
mask
)
;
}
auto
MaskFull
(
)
const
{
uint64_t
mask
=
vget_lane_u64
(
vreinterpret_u64_u8
(
vcge_s8
(
vreinterpret_s8_u8
(
ctrl
)
vdup_n_s8
(
static_cast
<
int8_t
>
(
0
)
)
)
)
0
)
;
return
BitMaskType
(
mask
)
;
}
auto
MaskNonFull
(
)
const
{
uint64_t
mask
=
vget_lane_u64
(
vreinterpret_u64_u8
(
vclt_s8
(
vreinterpret_s8_u8
(
ctrl
)
vdup_n_s8
(
static_cast
<
int8_t
>
(
0
)
)
)
)
0
)
;
return
BitMaskType
(
mask
)
;
}
auto
MaskEmptyOrDeleted
(
)
const
{
uint64_t
mask
=
vget_lane_u64
(
vreinterpret_u64_u8
(
vcgt_s8
(
vdup_n_s8
(
static_cast
<
int8_t
>
(
ctrl_t
:
:
kSentinel
)
)
vreinterpret_s8_u8
(
ctrl
)
)
)
0
)
;
return
NonIterableBitMaskType
(
mask
)
;
}
uint32_t
CountLeadingEmptyOrDeleted
(
)
const
{
uint64_t
mask
=
vget_lane_u64
(
vreinterpret_u64_u8
(
vcle_s8
(
vdup_n_s8
(
static_cast
<
int8_t
>
(
ctrl_t
:
:
kSentinel
)
)
vreinterpret_s8_u8
(
ctrl
)
)
)
0
)
;
return
static_cast
<
uint32_t
>
(
countr_zero
(
mask
)
)
>
>
3
;
}
void
ConvertSpecialToEmptyAndFullToDeleted
(
ctrl_t
*
dst
)
const
{
uint64_t
mask
=
vget_lane_u64
(
vreinterpret_u64_u8
(
ctrl
)
0
)
;
constexpr
uint64_t
slsbs
=
0x0202020202020202ULL
;
constexpr
uint64_t
midbs
=
0x7e7e7e7e7e7e7e7eULL
;
auto
x
=
slsbs
&
(
mask
>
>
6
)
;
auto
res
=
(
x
+
midbs
)
|
kMsbs8Bytes
;
little_endian
:
:
Store64
(
dst
res
)
;
}
uint8x8_t
ctrl
;
}
;
#
endif
struct
GroupPortableImpl
{
static
constexpr
size_t
kWidth
=
8
;
using
BitMaskType
=
BitMask
<
uint64_t
kWidth
3
false
>
;
using
NonIterableBitMaskType
=
NonIterableBitMask
<
uint64_t
kWidth
3
>
;
explicit
GroupPortableImpl
(
const
ctrl_t
*
pos
)
:
ctrl
(
little_endian
:
:
Load64
(
pos
)
)
{
}
BitMaskType
Match
(
h2_t
hash
)
const
{
constexpr
uint64_t
lsbs
=
0x0101010101010101ULL
;
auto
x
=
ctrl
^
(
lsbs
*
hash
)
;
return
BitMaskType
(
(
x
-
lsbs
)
&
~
x
&
kMsbs8Bytes
)
;
}
auto
MaskEmpty
(
)
const
{
return
NonIterableBitMaskType
(
(
ctrl
&
~
(
ctrl
<
<
6
)
)
&
kMsbs8Bytes
)
;
}
auto
MaskFull
(
)
const
{
return
BitMaskType
(
(
ctrl
^
kMsbs8Bytes
)
&
kMsbs8Bytes
)
;
}
auto
MaskNonFull
(
)
const
{
return
BitMaskType
(
ctrl
&
kMsbs8Bytes
)
;
}
auto
MaskEmptyOrDeleted
(
)
const
{
return
NonIterableBitMaskType
(
(
ctrl
&
~
(
ctrl
<
<
7
)
)
&
kMsbs8Bytes
)
;
}
uint32_t
CountLeadingEmptyOrDeleted
(
)
const
{
constexpr
uint64_t
bits
=
0x0101010101010101ULL
;
return
static_cast
<
uint32_t
>
(
countr_zero
(
(
ctrl
|
~
(
ctrl
>
>
7
)
)
&
bits
)
>
>
3
)
;
}
void
ConvertSpecialToEmptyAndFullToDeleted
(
ctrl_t
*
dst
)
const
{
constexpr
uint64_t
lsbs
=
0x0101010101010101ULL
;
auto
x
=
ctrl
&
kMsbs8Bytes
;
auto
res
=
(
~
x
+
(
x
>
>
7
)
)
&
~
lsbs
;
little_endian
:
:
Store64
(
dst
res
)
;
}
uint64_t
ctrl
;
}
;
#
ifdef
ABSL_INTERNAL_HAVE_SSE2
using
Group
=
GroupSse2Impl
;
using
GroupFullEmptyOrDeleted
=
GroupSse2Impl
;
#
elif
defined
(
ABSL_INTERNAL_HAVE_ARM_NEON
)
&
&
defined
(
ABSL_IS_LITTLE_ENDIAN
)
using
Group
=
GroupAArch64Impl
;
using
GroupFullEmptyOrDeleted
=
GroupPortableImpl
;
#
else
using
Group
=
GroupPortableImpl
;
using
GroupFullEmptyOrDeleted
=
GroupPortableImpl
;
#
endif
}
ABSL_NAMESPACE_END
}
#
undef
ABSL_SWISSTABLE_ASSERT
#
endif
