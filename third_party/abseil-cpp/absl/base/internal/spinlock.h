#
ifndef
ABSL_BASE_INTERNAL_SPINLOCK_H_
#
define
ABSL_BASE_INTERNAL_SPINLOCK_H_
#
include
<
atomic
>
#
include
<
cstdint
>
#
include
"
absl
/
base
/
attributes
.
h
"
#
include
"
absl
/
base
/
const_init
.
h
"
#
include
"
absl
/
base
/
dynamic_annotations
.
h
"
#
include
"
absl
/
base
/
internal
/
low_level_scheduling
.
h
"
#
include
"
absl
/
base
/
internal
/
raw_logging
.
h
"
#
include
"
absl
/
base
/
internal
/
scheduling_mode
.
h
"
#
include
"
absl
/
base
/
internal
/
tsan_mutex_interface
.
h
"
#
include
"
absl
/
base
/
thread_annotations
.
h
"
namespace
tcmalloc
{
namespace
tcmalloc_internal
{
class
AllocationGuardSpinLockHolder
;
}
}
namespace
absl
{
ABSL_NAMESPACE_BEGIN
namespace
base_internal
{
class
ABSL_LOCKABLE
ABSL_ATTRIBUTE_WARN_UNUSED
SpinLock
{
public
:
SpinLock
(
)
:
lockword_
(
kSpinLockCooperative
)
{
ABSL_TSAN_MUTEX_CREATE
(
this
__tsan_mutex_not_static
)
;
}
explicit
SpinLock
(
base_internal
:
:
SchedulingMode
mode
)
;
constexpr
SpinLock
(
absl
:
:
ConstInitType
base_internal
:
:
SchedulingMode
mode
)
:
lockword_
(
IsCooperative
(
mode
)
?
kSpinLockCooperative
:
0
)
{
}
#
ifdef
ABSL_INTERNAL_HAVE_TSAN_INTERFACE
~
SpinLock
(
)
{
ABSL_TSAN_MUTEX_DESTROY
(
this
__tsan_mutex_not_static
)
;
}
#
else
~
SpinLock
(
)
=
default
;
#
endif
inline
void
Lock
(
)
ABSL_EXCLUSIVE_LOCK_FUNCTION
(
)
{
ABSL_TSAN_MUTEX_PRE_LOCK
(
this
0
)
;
if
(
!
TryLockImpl
(
)
)
{
SlowLock
(
)
;
}
ABSL_TSAN_MUTEX_POST_LOCK
(
this
0
0
)
;
}
[
[
nodiscard
]
]
inline
bool
TryLock
(
)
ABSL_EXCLUSIVE_TRYLOCK_FUNCTION
(
true
)
{
ABSL_TSAN_MUTEX_PRE_LOCK
(
this
__tsan_mutex_try_lock
)
;
bool
res
=
TryLockImpl
(
)
;
ABSL_TSAN_MUTEX_POST_LOCK
(
this
__tsan_mutex_try_lock
|
(
res
?
0
:
__tsan_mutex_try_lock_failed
)
0
)
;
return
res
;
}
inline
void
Unlock
(
)
ABSL_UNLOCK_FUNCTION
(
)
{
ABSL_TSAN_MUTEX_PRE_UNLOCK
(
this
0
)
;
uint32_t
lock_value
=
lockword_
.
load
(
std
:
:
memory_order_relaxed
)
;
lock_value
=
lockword_
.
exchange
(
lock_value
&
kSpinLockCooperative
std
:
:
memory_order_release
)
;
if
(
(
lock_value
&
kSpinLockDisabledScheduling
)
!
=
0
)
{
base_internal
:
:
SchedulingGuard
:
:
EnableRescheduling
(
true
)
;
}
if
(
(
lock_value
&
kWaitTimeMask
)
!
=
0
)
{
SlowUnlock
(
lock_value
)
;
}
ABSL_TSAN_MUTEX_POST_UNLOCK
(
this
0
)
;
}
[
[
nodiscard
]
]
inline
bool
IsHeld
(
)
const
{
return
(
lockword_
.
load
(
std
:
:
memory_order_relaxed
)
&
kSpinLockHeld
)
!
=
0
;
}
inline
void
AssertHeld
(
)
const
ABSL_ASSERT_EXCLUSIVE_LOCK
(
)
{
if
(
!
IsHeld
(
)
)
{
ABSL_RAW_LOG
(
FATAL
"
thread
should
hold
the
lock
on
SpinLock
"
)
;
}
}
protected
:
static
uint32_t
EncodeWaitCycles
(
int64_t
wait_start_time
int64_t
wait_end_time
)
;
static
int64_t
DecodeWaitCycles
(
uint32_t
lock_value
)
;
friend
struct
SpinLockTest
;
friend
class
tcmalloc
:
:
tcmalloc_internal
:
:
AllocationGuardSpinLockHolder
;
private
:
static
constexpr
uint32_t
kSpinLockHeld
=
1
;
static
constexpr
uint32_t
kSpinLockCooperative
=
2
;
static
constexpr
uint32_t
kSpinLockDisabledScheduling
=
4
;
static
constexpr
uint32_t
kSpinLockSleeper
=
8
;
static
constexpr
uint32_t
kWaitTimeMask
=
~
(
kSpinLockHeld
|
kSpinLockCooperative
|
kSpinLockDisabledScheduling
)
;
static
constexpr
bool
IsCooperative
(
base_internal
:
:
SchedulingMode
scheduling_mode
)
{
return
scheduling_mode
=
=
base_internal
:
:
SCHEDULE_COOPERATIVE_AND_KERNEL
;
}
bool
IsCooperative
(
)
const
{
return
lockword_
.
load
(
std
:
:
memory_order_relaxed
)
&
kSpinLockCooperative
;
}
uint32_t
TryLockInternal
(
uint32_t
lock_value
uint32_t
wait_cycles
)
;
void
SlowLock
(
)
ABSL_ATTRIBUTE_COLD
;
void
SlowUnlock
(
uint32_t
lock_value
)
ABSL_ATTRIBUTE_COLD
;
uint32_t
SpinLoop
(
)
;
inline
bool
TryLockImpl
(
)
{
uint32_t
lock_value
=
lockword_
.
load
(
std
:
:
memory_order_relaxed
)
;
return
(
TryLockInternal
(
lock_value
0
)
&
kSpinLockHeld
)
=
=
0
;
}
std
:
:
atomic
<
uint32_t
>
lockword_
;
SpinLock
(
const
SpinLock
&
)
=
delete
;
SpinLock
&
operator
=
(
const
SpinLock
&
)
=
delete
;
}
;
class
ABSL_SCOPED_LOCKABLE
[
[
nodiscard
]
]
SpinLockHolder
{
public
:
inline
explicit
SpinLockHolder
(
SpinLock
*
l
)
ABSL_EXCLUSIVE_LOCK_FUNCTION
(
l
)
:
lock_
(
l
)
{
l
-
>
Lock
(
)
;
}
inline
~
SpinLockHolder
(
)
ABSL_UNLOCK_FUNCTION
(
)
{
lock_
-
>
Unlock
(
)
;
}
SpinLockHolder
(
const
SpinLockHolder
&
)
=
delete
;
SpinLockHolder
&
operator
=
(
const
SpinLockHolder
&
)
=
delete
;
private
:
SpinLock
*
lock_
;
}
;
void
RegisterSpinLockProfiler
(
void
(
*
fn
)
(
const
void
*
lock
int64_t
wait_cycles
)
)
;
inline
uint32_t
SpinLock
:
:
TryLockInternal
(
uint32_t
lock_value
uint32_t
wait_cycles
)
{
if
(
(
lock_value
&
kSpinLockHeld
)
!
=
0
)
{
return
lock_value
;
}
uint32_t
sched_disabled_bit
=
0
;
if
(
(
lock_value
&
kSpinLockCooperative
)
=
=
0
)
{
if
(
base_internal
:
:
SchedulingGuard
:
:
DisableRescheduling
(
)
)
{
sched_disabled_bit
=
kSpinLockDisabledScheduling
;
}
}
if
(
!
lockword_
.
compare_exchange_strong
(
lock_value
kSpinLockHeld
|
lock_value
|
wait_cycles
|
sched_disabled_bit
std
:
:
memory_order_acquire
std
:
:
memory_order_relaxed
)
)
{
base_internal
:
:
SchedulingGuard
:
:
EnableRescheduling
(
sched_disabled_bit
!
=
0
)
;
}
return
lock_value
;
}
}
ABSL_NAMESPACE_END
}
#
endif
