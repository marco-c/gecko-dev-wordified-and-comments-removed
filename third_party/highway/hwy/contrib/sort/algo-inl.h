#
ifndef
HIGHWAY_HWY_CONTRIB_SORT_ALGO_INL_H_
#
define
HIGHWAY_HWY_CONTRIB_SORT_ALGO_INL_H_
#
include
<
stddef
.
h
>
#
include
<
stdint
.
h
>
#
include
<
algorithm
>
#
include
<
functional
>
#
include
<
vector
>
#
include
"
hwy
/
base
.
h
"
#
include
"
hwy
/
contrib
/
sort
/
vqsort
.
h
"
#
include
"
hwy
/
highway
.
h
"
#
include
"
hwy
/
print
.
h
"
#
define
HAVE_AVX2SORT
0
#
define
HAVE_IPS4O
0
#
define
HAVE_PARALLEL_IPS4O
(
HAVE_IPS4O
&
&
1
)
#
define
HAVE_PDQSORT
0
#
define
HAVE_SORT512
0
#
define
HAVE_VXSORT
0
#
if
HWY_ARCH_X86
#
define
HAVE_INTEL
0
#
else
#
define
HAVE_INTEL
0
#
endif
#
if
HAVE_PARALLEL_IPS4O
#
include
<
thread
>
#
endif
#
if
HAVE_AVX2SORT
HWY_PUSH_ATTRIBUTES
(
"
avx2
avx
"
)
#
include
"
avx2sort
.
h
"
HWY_POP_ATTRIBUTES
#
endif
#
if
HAVE_IPS4O
|
|
HAVE_PARALLEL_IPS4O
#
include
"
third_party
/
ips4o
/
include
/
ips4o
.
hpp
"
#
include
"
third_party
/
ips4o
/
include
/
ips4o
/
thread_pool
.
hpp
"
#
endif
#
if
HAVE_PDQSORT
#
include
"
third_party
/
boost
/
allowed
/
sort
/
sort
.
hpp
"
#
endif
#
if
HAVE_SORT512
#
include
"
sort512
.
h
"
#
endif
#
define
VXSORT_AVX3
1
#
if
HAVE_VXSORT
#
ifdef
__GNUC__
#
ifdef
__clang__
#
if
VXSORT_AVX3
#
pragma
clang
attribute
push
(
__attribute__
(
(
target
(
"
avx512f
avx512dq
"
)
)
)
\
apply_to
=
any
(
function
)
)
#
else
#
pragma
clang
attribute
push
(
__attribute__
(
(
target
(
"
avx2
"
)
)
)
\
apply_to
=
any
(
function
)
)
#
endif
#
else
#
pragma
GCC
push_options
#
if
VXSORT_AVX3
#
pragma
GCC
target
(
"
avx512f
avx512dq
"
)
#
else
#
pragma
GCC
target
(
"
avx2
"
)
#
endif
#
endif
#
endif
#
if
VXSORT_AVX3
#
include
"
vxsort
/
machine_traits
.
avx512
.
h
"
#
else
#
include
"
vxsort
/
machine_traits
.
avx2
.
h
"
#
endif
#
include
"
vxsort
/
vxsort
.
h
"
#
ifdef
__GNUC__
#
ifdef
__clang__
#
pragma
clang
attribute
pop
#
else
#
pragma
GCC
pop_options
#
endif
#
endif
#
endif
namespace
hwy
{
enum
class
Dist
{
kUniform8
kUniform16
kUniform32
}
;
static
inline
std
:
:
vector
<
Dist
>
AllDist
(
)
{
return
{
Dist
:
:
kUniform8
Dist
:
:
kUniform32
}
;
}
static
inline
const
char
*
DistName
(
Dist
dist
)
{
switch
(
dist
)
{
case
Dist
:
:
kUniform8
:
return
"
uniform8
"
;
case
Dist
:
:
kUniform16
:
return
"
uniform16
"
;
case
Dist
:
:
kUniform32
:
return
"
uniform32
"
;
}
return
"
unreachable
"
;
}
template
<
typename
T
>
class
InputStats
{
public
:
void
Notify
(
T
value
)
{
min_
=
HWY_MIN
(
min_
value
)
;
max_
=
HWY_MAX
(
max_
value
)
;
uint64_t
bits
=
0
;
static_assert
(
sizeof
(
T
)
<
=
8
"
Expected
a
built
-
in
type
"
)
;
CopyBytes
<
sizeof
(
T
)
>
(
&
value
&
bits
)
;
sum_
+
=
bits
;
count_
+
=
1
;
}
bool
operator
=
=
(
const
InputStats
&
other
)
const
{
char
type_name
[
100
]
;
detail
:
:
TypeName
(
hwy
:
:
detail
:
:
MakeTypeInfo
<
T
>
(
)
1
type_name
)
;
if
(
count_
!
=
other
.
count_
)
{
HWY_ABORT
(
"
Sort
%
s
:
count
%
d
vs
%
d
\
n
"
type_name
static_cast
<
int
>
(
count_
)
static_cast
<
int
>
(
other
.
count_
)
)
;
}
if
(
min_
!
=
other
.
min_
|
|
max_
!
=
other
.
max_
)
{
HWY_ABORT
(
"
Sort
%
s
:
minmax
%
f
/
%
f
vs
%
f
/
%
f
\
n
"
type_name
static_cast
<
double
>
(
min_
)
static_cast
<
double
>
(
max_
)
static_cast
<
double
>
(
other
.
min_
)
static_cast
<
double
>
(
other
.
max_
)
)
;
}
if
(
sum_
!
=
other
.
sum_
)
{
HWY_ABORT
(
"
Sort
%
s
:
Sum
mismatch
%
g
%
g
;
min
%
g
max
%
g
\
n
"
type_name
static_cast
<
double
>
(
sum_
)
static_cast
<
double
>
(
other
.
sum_
)
static_cast
<
double
>
(
min_
)
static_cast
<
double
>
(
max_
)
)
;
}
return
true
;
}
private
:
T
min_
=
hwy
:
:
HighestValue
<
T
>
(
)
;
T
max_
=
hwy
:
:
LowestValue
<
T
>
(
)
;
uint64_t
sum_
=
0
;
size_t
count_
=
0
;
}
;
enum
class
Algo
{
#
if
HAVE_INTEL
kIntel
#
endif
#
if
HAVE_AVX2SORT
kSEA
#
endif
#
if
HAVE_IPS4O
kIPS4O
#
endif
#
if
HAVE_PARALLEL_IPS4O
kParallelIPS4O
#
endif
#
if
HAVE_PDQSORT
kPDQ
#
endif
#
if
HAVE_SORT512
kSort512
#
endif
#
if
HAVE_VXSORT
kVXSort
#
endif
kStdSort
kStdSelect
kStdPartialSort
kVQSort
kVQPartialSort
kVQSelect
kHeapSort
kHeapPartialSort
kHeapSelect
}
;
static
inline
bool
IsVQ
(
Algo
algo
)
{
return
algo
=
=
Algo
:
:
kVQSort
|
|
algo
=
=
Algo
:
:
kVQPartialSort
|
|
algo
=
=
Algo
:
:
kVQSelect
;
}
static
inline
bool
IsSelect
(
Algo
algo
)
{
return
algo
=
=
Algo
:
:
kStdSelect
|
|
algo
=
=
Algo
:
:
kVQSelect
|
|
algo
=
=
Algo
:
:
kHeapSelect
;
}
static
inline
bool
IsPartialSort
(
Algo
algo
)
{
return
algo
=
=
Algo
:
:
kStdPartialSort
|
|
algo
=
=
Algo
:
:
kVQPartialSort
|
|
algo
=
=
Algo
:
:
kHeapPartialSort
;
}
static
inline
Algo
ReferenceAlgoFor
(
Algo
algo
)
{
if
(
IsPartialSort
(
algo
)
)
return
Algo
:
:
kStdPartialSort
;
#
if
HAVE_PDQSORT
return
Algo
:
:
kPDQ
;
#
else
return
Algo
:
:
kStdSort
;
#
endif
}
static
inline
const
char
*
AlgoName
(
Algo
algo
)
{
switch
(
algo
)
{
#
if
HAVE_INTEL
case
Algo
:
:
kIntel
:
return
"
intel
"
;
#
endif
#
if
HAVE_AVX2SORT
case
Algo
:
:
kSEA
:
return
"
sea
"
;
#
endif
#
if
HAVE_IPS4O
case
Algo
:
:
kIPS4O
:
return
"
ips4o
"
;
#
endif
#
if
HAVE_PARALLEL_IPS4O
case
Algo
:
:
kParallelIPS4O
:
return
"
par_ips4o
"
;
#
endif
#
if
HAVE_PDQSORT
case
Algo
:
:
kPDQ
:
return
"
pdq
"
;
#
endif
#
if
HAVE_SORT512
case
Algo
:
:
kSort512
:
return
"
sort512
"
;
#
endif
#
if
HAVE_VXSORT
case
Algo
:
:
kVXSort
:
return
"
vxsort
"
;
#
endif
case
Algo
:
:
kStdSort
:
return
"
std
"
;
case
Algo
:
:
kStdPartialSort
:
return
"
std_partial
"
;
case
Algo
:
:
kStdSelect
:
return
"
std_select
"
;
case
Algo
:
:
kVQSort
:
return
"
vq
"
;
case
Algo
:
:
kVQPartialSort
:
return
"
vq_partial
"
;
case
Algo
:
:
kVQSelect
:
return
"
vq_select
"
;
case
Algo
:
:
kHeapSort
:
return
"
heap
"
;
case
Algo
:
:
kHeapPartialSort
:
return
"
heap_partial
"
;
case
Algo
:
:
kHeapSelect
:
return
"
heap_select
"
;
}
return
"
unreachable
"
;
}
}
#
endif
#
if
defined
(
HIGHWAY_HWY_CONTRIB_SORT_ALGO_TOGGLE
)
=
=
defined
(
HWY_TARGET_TOGGLE
)
#
ifdef
HIGHWAY_HWY_CONTRIB_SORT_ALGO_TOGGLE
#
undef
HIGHWAY_HWY_CONTRIB_SORT_ALGO_TOGGLE
#
else
#
define
HIGHWAY_HWY_CONTRIB_SORT_ALGO_TOGGLE
#
endif
#
include
"
hwy
/
aligned_allocator
.
h
"
#
include
"
hwy
/
contrib
/
sort
/
traits
-
inl
.
h
"
#
include
"
hwy
/
contrib
/
sort
/
traits128
-
inl
.
h
"
#
include
"
hwy
/
contrib
/
sort
/
vqsort
-
inl
.
h
"
HWY_BEFORE_NAMESPACE
(
)
;
#
if
HAVE_INTEL
&
&
HWY_TARGET
<
=
HWY_AVX3
#
include
"
avx512
-
32bit
-
qsort
.
hpp
"
#
include
"
avx512
-
64bit
-
qsort
.
hpp
"
#
endif
namespace
hwy
{
namespace
HWY_NAMESPACE
{
#
if
HAVE_INTEL
|
|
HAVE_VXSORT
template
<
typename
T
>
using
OtherOrder
=
detail
:
:
OrderAscending
<
T
>
;
#
else
template
<
typename
T
>
using
OtherOrder
=
detail
:
:
OrderDescending
<
T
>
;
#
endif
class
Xorshift128Plus
{
static
HWY_INLINE
uint64_t
SplitMix64
(
uint64_t
z
)
{
z
=
(
z
^
(
z
>
>
30
)
)
*
0xBF58476D1CE4E5B9ull
;
z
=
(
z
^
(
z
>
>
27
)
)
*
0x94D049BB133111EBull
;
return
z
^
(
z
>
>
31
)
;
}
public
:
template
<
class
DU64
>
static
void
GenerateSeeds
(
DU64
du64
TFromD
<
DU64
>
*
HWY_RESTRICT
seeds
)
{
seeds
[
0
]
=
SplitMix64
(
0x9E3779B97F4A7C15ull
)
;
for
(
size_t
i
=
1
;
i
<
2
*
Lanes
(
du64
)
;
+
+
i
)
{
seeds
[
i
]
=
SplitMix64
(
seeds
[
i
-
1
]
)
;
}
}
template
<
class
VU64
>
static
VU64
RandomBits
(
VU64
&
state0
VU64
&
state1
)
{
VU64
s1
=
state0
;
VU64
s0
=
state1
;
const
VU64
bits
=
Add
(
s1
s0
)
;
state0
=
s0
;
s1
=
Xor
(
s1
ShiftLeft
<
23
>
(
s1
)
)
;
state1
=
Xor
(
s1
Xor
(
s0
Xor
(
ShiftRight
<
18
>
(
s1
)
ShiftRight
<
5
>
(
s0
)
)
)
)
;
return
bits
;
}
}
;
template
<
class
D
class
VU64
HWY_IF_NOT_FLOAT_D
(
D
)
>
Vec
<
D
>
RandomValues
(
D
d
VU64
&
s0
VU64
&
s1
const
VU64
mask
)
{
const
VU64
bits
=
Xorshift128Plus
:
:
RandomBits
(
s0
s1
)
;
return
BitCast
(
d
And
(
bits
mask
)
)
;
}
template
<
class
DF
class
VU64
HWY_IF_FLOAT_D
(
DF
)
>
Vec
<
DF
>
RandomValues
(
DF
df
VU64
&
s0
VU64
&
s1
const
VU64
mask
)
{
using
TF
=
TFromD
<
DF
>
;
const
RebindToUnsigned
<
decltype
(
df
)
>
du
;
using
VU
=
Vec
<
decltype
(
du
)
>
;
const
VU64
bits64
=
And
(
Xorshift128Plus
:
:
RandomBits
(
s0
s1
)
mask
)
;
#
if
HWY_TARGET
=
=
HWY_SCALAR
using
TU
=
MakeUnsigned
<
TF
>
;
const
VU
bits
=
Set
(
du
static_cast
<
TU
>
(
GetLane
(
bits64
)
&
LimitsMax
<
TU
>
(
)
)
)
;
#
else
const
VU
bits
=
BitCast
(
du
bits64
)
;
#
endif
const
VU
k1
=
BitCast
(
du
Set
(
df
TF
{
1
.
0
}
)
)
;
const
VU
mantissa_mask
=
Set
(
du
MantissaMask
<
TF
>
(
)
)
;
const
VU
representation
=
OrAnd
(
k1
bits
mantissa_mask
)
;
return
BitCast
(
df
representation
)
;
}
template
<
class
DU64
>
Vec
<
DU64
>
MaskForDist
(
DU64
du64
const
Dist
dist
size_t
sizeof_t
)
{
switch
(
sizeof_t
)
{
case
2
:
return
Set
(
du64
(
dist
=
=
Dist
:
:
kUniform8
)
?
0x00FF00FF00FF00FFull
:
0xFFFFFFFFFFFFFFFFull
)
;
case
4
:
return
Set
(
du64
(
dist
=
=
Dist
:
:
kUniform8
)
?
0x000000FF000000FFull
:
(
dist
=
=
Dist
:
:
kUniform16
)
?
0x0000FFFF0000FFFFull
:
0xFFFFFFFFFFFFFFFFull
)
;
case
8
:
return
Set
(
du64
(
dist
=
=
Dist
:
:
kUniform8
)
?
0x00000000000000FFull
:
(
dist
=
=
Dist
:
:
kUniform16
)
?
0x000000000000FFFFull
:
0x00000000FFFFFFFFull
)
;
default
:
HWY_ABORT
(
"
Logic
error
"
)
;
return
Zero
(
du64
)
;
}
}
template
<
typename
T
>
InputStats
<
T
>
GenerateInput
(
const
Dist
dist
T
*
v
size_t
num_lanes
)
{
SortTag
<
uint64_t
>
du64
;
using
VU64
=
Vec
<
decltype
(
du64
)
>
;
const
size_t
N64
=
Lanes
(
du64
)
;
auto
seeds
=
hwy
:
:
AllocateAligned
<
uint64_t
>
(
2
*
N64
)
;
Xorshift128Plus
:
:
GenerateSeeds
(
du64
seeds
.
get
(
)
)
;
VU64
s0
=
Load
(
du64
seeds
.
get
(
)
)
;
VU64
s1
=
Load
(
du64
seeds
.
get
(
)
+
N64
)
;
#
if
HWY_TARGET
=
=
HWY_SCALAR
const
Sisd
<
T
>
d
;
#
else
const
Repartition
<
T
decltype
(
du64
)
>
d
;
#
endif
using
V
=
Vec
<
decltype
(
d
)
>
;
const
size_t
N
=
Lanes
(
d
)
;
const
VU64
mask
=
MaskForDist
(
du64
dist
sizeof
(
T
)
)
;
auto
buf
=
hwy
:
:
AllocateAligned
<
T
>
(
N
)
;
size_t
i
=
0
;
for
(
;
i
+
N
<
=
num_lanes
;
i
+
=
N
)
{
const
V
values
=
RandomValues
(
d
s0
s1
mask
)
;
StoreU
(
values
d
v
+
i
)
;
}
if
(
i
<
num_lanes
)
{
const
V
values
=
RandomValues
(
d
s0
s1
mask
)
;
StoreU
(
values
d
buf
.
get
(
)
)
;
CopyBytes
(
buf
.
get
(
)
v
+
i
(
num_lanes
-
i
)
*
sizeof
(
T
)
)
;
}
InputStats
<
T
>
input_stats
;
for
(
size_t
j
=
0
;
j
<
num_lanes
;
+
+
j
)
{
input_stats
.
Notify
(
v
[
j
]
)
;
}
return
input_stats
;
}
struct
SharedState
{
#
if
HAVE_PARALLEL_IPS4O
const
unsigned
max_threads
=
hwy
:
:
LimitsMax
<
unsigned
>
(
)
;
ips4o
:
:
StdThreadPool
pool
{
static_cast
<
int
>
(
HWY_MIN
(
max_threads
std
:
:
thread
:
:
hardware_concurrency
(
)
/
2
)
)
}
;
#
endif
}
;
template
<
typename
KeyType
class
Order
>
void
CallHeapSort
(
KeyType
*
keys
const
size_t
num_keys
Order
)
{
const
detail
:
:
MakeTraits
<
KeyType
Order
>
st
;
using
LaneType
=
typename
decltype
(
st
)
:
:
LaneType
;
return
detail
:
:
HeapSort
(
st
reinterpret_cast
<
LaneType
*
>
(
keys
)
num_keys
*
st
.
LanesPerKey
(
)
)
;
}
template
<
typename
KeyType
class
Order
>
void
CallHeapPartialSort
(
KeyType
*
keys
const
size_t
num_keys
const
size_t
k_keys
Order
)
{
const
detail
:
:
MakeTraits
<
KeyType
Order
>
st
;
using
LaneType
=
typename
decltype
(
st
)
:
:
LaneType
;
detail
:
:
HeapPartialSort
(
st
reinterpret_cast
<
LaneType
*
>
(
keys
)
num_keys
*
st
.
LanesPerKey
(
)
k_keys
*
st
.
LanesPerKey
(
)
)
;
}
template
<
typename
KeyType
class
Order
>
void
CallHeapSelect
(
KeyType
*
keys
const
size_t
num_keys
const
size_t
k_keys
Order
)
{
const
detail
:
:
MakeTraits
<
KeyType
Order
>
st
;
using
LaneType
=
typename
decltype
(
st
)
:
:
LaneType
;
detail
:
:
HeapSelect
(
st
reinterpret_cast
<
LaneType
*
>
(
keys
)
num_keys
*
st
.
LanesPerKey
(
)
k_keys
*
st
.
LanesPerKey
(
)
)
;
}
template
<
typename
KeyType
class
Order
>
void
Run
(
Algo
algo
KeyType
*
inout
size_t
num_keys
SharedState
&
shared
size_t
size_t
k_keys
Order
)
{
const
std
:
:
less
<
KeyType
>
less
;
const
std
:
:
greater
<
KeyType
>
greater
;
constexpr
bool
kAscending
=
Order
:
:
IsAscending
(
)
;
#
if
!
HAVE_PARALLEL_IPS4O
(
void
)
shared
;
#
endif
switch
(
algo
)
{
#
if
HAVE_INTEL
&
&
HWY_TARGET
<
=
HWY_AVX3
case
Algo
:
:
kIntel
:
return
avx512_qsort
<
KeyType
>
(
inout
static_cast
<
int64_t
>
(
num_keys
)
)
;
#
endif
#
if
HAVE_AVX2SORT
case
Algo
:
:
kSEA
:
return
avx2
:
:
quicksort
(
inout
static_cast
<
int
>
(
num_keys
)
)
;
#
endif
#
if
HAVE_IPS4O
case
Algo
:
:
kIPS4O
:
if
(
kAscending
)
{
return
ips4o
:
:
sort
(
inout
inout
+
num_keys
less
)
;
}
else
{
return
ips4o
:
:
sort
(
inout
inout
+
num_keys
greater
)
;
}
#
endif
#
if
HAVE_PARALLEL_IPS4O
case
Algo
:
:
kParallelIPS4O
:
if
(
kAscending
)
{
return
ips4o
:
:
parallel
:
:
sort
(
inout
inout
+
num_keys
less
shared
.
pool
)
;
}
else
{
return
ips4o
:
:
parallel
:
:
sort
(
inout
inout
+
num_keys
greater
shared
.
pool
)
;
}
#
endif
#
if
HAVE_SORT512
case
Algo
:
:
kSort512
:
HWY_ABORT
(
"
not
supported
"
)
;
#
endif
#
if
HAVE_PDQSORT
case
Algo
:
:
kPDQ
:
if
(
kAscending
)
{
return
boost
:
:
sort
:
:
pdqsort_branchless
(
inout
inout
+
num_keys
less
)
;
}
else
{
return
boost
:
:
sort
:
:
pdqsort_branchless
(
inout
inout
+
num_keys
greater
)
;
}
#
endif
#
if
HAVE_VXSORT
case
Algo
:
:
kVXSort
:
{
#
if
(
VXSORT_AVX3
&
&
HWY_TARGET
!
=
HWY_AVX3
)
|
|
\
(
!
VXSORT_AVX3
&
&
HWY_TARGET
!
=
HWY_AVX2
)
HWY_WARN
(
"
Do
not
call
for
target
%
s
\
n
"
hwy
:
:
TargetName
(
HWY_TARGET
)
)
;
return
;
#
else
#
if
VXSORT_AVX3
vxsort
:
:
vxsort
<
KeyType
vxsort
:
:
AVX512
>
vx
;
#
else
vxsort
:
:
vxsort
<
KeyType
vxsort
:
:
AVX2
>
vx
;
#
endif
if
(
kAscending
)
{
return
vx
.
sort
(
inout
inout
+
num_keys
-
1
)
;
}
else
{
HWY_WARN
(
"
Skipping
VX
-
does
not
support
descending
order
\
n
"
)
;
return
;
}
#
endif
}
#
endif
case
Algo
:
:
kStdSort
:
if
(
kAscending
)
{
return
std
:
:
sort
(
inout
inout
+
num_keys
less
)
;
}
else
{
return
std
:
:
sort
(
inout
inout
+
num_keys
greater
)
;
}
case
Algo
:
:
kStdPartialSort
:
if
(
kAscending
)
{
return
std
:
:
partial_sort
(
inout
inout
+
k_keys
inout
+
num_keys
less
)
;
}
else
{
return
std
:
:
partial_sort
(
inout
inout
+
k_keys
inout
+
num_keys
greater
)
;
}
case
Algo
:
:
kStdSelect
:
if
(
kAscending
)
{
return
std
:
:
nth_element
(
inout
inout
+
k_keys
inout
+
num_keys
less
)
;
}
else
{
return
std
:
:
nth_element
(
inout
inout
+
k_keys
inout
+
num_keys
greater
)
;
}
case
Algo
:
:
kVQSort
:
return
VQSort
(
inout
num_keys
Order
(
)
)
;
case
Algo
:
:
kVQPartialSort
:
return
VQPartialSort
(
inout
num_keys
k_keys
Order
(
)
)
;
case
Algo
:
:
kVQSelect
:
return
VQSelect
(
inout
num_keys
k_keys
Order
(
)
)
;
case
Algo
:
:
kHeapSort
:
return
CallHeapSort
(
inout
num_keys
Order
(
)
)
;
case
Algo
:
:
kHeapPartialSort
:
return
CallHeapPartialSort
(
inout
num_keys
k_keys
Order
(
)
)
;
case
Algo
:
:
kHeapSelect
:
return
CallHeapSelect
(
inout
num_keys
k_keys
Order
(
)
)
;
}
}
}
}
HWY_AFTER_NAMESPACE
(
)
;
#
endif
