#
include
<
stdint
.
h
>
#
include
<
stdio
.
h
>
#
include
<
condition_variable
>
#
include
<
functional
>
#
include
<
memory
>
#
include
<
mutex
>
#
include
<
thread
>
#
include
<
utility
>
#
include
<
vector
>
#
undef
HWY_TARGET_INCLUDE
#
define
HWY_TARGET_INCLUDE
"
hwy
/
contrib
/
sort
/
bench_parallel
.
cc
"
/
/
NOLINT
#
include
"
hwy
/
foreach_target
.
h
"
#
include
"
hwy
/
contrib
/
sort
/
algo
-
inl
.
h
"
#
include
"
hwy
/
contrib
/
sort
/
result
-
inl
.
h
"
#
include
"
hwy
/
aligned_allocator
.
h
"
#
include
"
hwy
/
tests
/
test_util
-
inl
.
h
"
HWY_BEFORE_NAMESPACE
(
)
;
namespace
hwy
{
namespace
HWY_NAMESPACE
{
namespace
{
class
ThreadPool
{
public
:
explicit
ThreadPool
(
const
size_t
num_threads
=
std
:
:
thread
:
:
hardware_concurrency
(
)
)
:
num_threads_
(
num_threads
)
{
HWY_ASSERT
(
num_threads_
>
0
)
;
threads_
.
reserve
(
num_threads_
)
;
for
(
size_t
i
=
0
;
i
<
num_threads_
;
+
+
i
)
{
threads_
.
emplace_back
(
ThreadFunc
this
i
)
;
}
WorkersReadyBarrier
(
)
;
}
ThreadPool
(
const
ThreadPool
&
)
=
delete
;
ThreadPool
&
operator
&
(
const
ThreadPool
&
)
=
delete
;
~
ThreadPool
(
)
{
StartWorkers
(
kWorkerExit
)
;
for
(
std
:
:
thread
&
thread
:
threads_
)
{
thread
.
join
(
)
;
}
}
size_t
NumThreads
(
)
const
{
return
threads_
.
size
(
)
;
}
template
<
class
Func
>
void
RunOnThreads
(
size_t
max_threads
const
Func
&
func
)
{
task_
=
&
CallClosure
<
Func
>
;
data_
=
&
func
;
StartWorkers
(
max_threads
)
;
WorkersReadyBarrier
(
)
;
}
private
:
using
WorkerCommand
=
uint64_t
;
static
constexpr
WorkerCommand
kWorkerWait
=
~
1ULL
;
static
constexpr
WorkerCommand
kWorkerExit
=
~
2ULL
;
template
<
class
Closure
>
static
void
CallClosure
(
const
void
*
f
size_t
thread
)
{
(
*
reinterpret_cast
<
const
Closure
*
>
(
f
)
)
(
thread
)
;
}
void
WorkersReadyBarrier
(
)
{
std
:
:
unique_lock
<
std
:
:
mutex
>
lock
(
mutex_
)
;
while
(
workers_ready_
!
=
threads_
.
size
(
)
)
{
workers_ready_cv_
.
wait
(
lock
)
;
}
workers_ready_
=
0
;
worker_start_command_
=
kWorkerWait
;
}
void
StartWorkers
(
const
WorkerCommand
worker_command
)
{
std
:
:
unique_lock
<
std
:
:
mutex
>
lock
(
mutex_
)
;
worker_start_command_
=
worker_command
;
lock
.
unlock
(
)
;
worker_start_cv_
.
notify_all
(
)
;
}
static
void
ThreadFunc
(
ThreadPool
*
self
size_t
thread
)
{
for
(
;
;
)
{
std
:
:
unique_lock
<
std
:
:
mutex
>
lock
(
self
-
>
mutex_
)
;
if
(
+
+
self
-
>
workers_ready_
=
=
self
-
>
num_threads_
)
{
self
-
>
workers_ready_cv_
.
notify_one
(
)
;
}
RESUME_WAIT
:
self
-
>
worker_start_cv_
.
wait
(
lock
)
;
const
WorkerCommand
command
=
self
-
>
worker_start_command_
;
switch
(
command
)
{
case
kWorkerWait
:
goto
RESUME_WAIT
;
case
kWorkerExit
:
return
;
default
:
break
;
}
lock
.
unlock
(
)
;
HWY_ASSERT
(
command
<
self
-
>
NumThreads
(
)
)
;
if
(
thread
<
command
)
{
self
-
>
task_
(
self
-
>
data_
thread
)
;
}
}
}
const
size_t
num_threads_
;
std
:
:
vector
<
std
:
:
thread
>
threads_
;
std
:
:
mutex
mutex_
;
std
:
:
condition_variable
workers_ready_cv_
;
size_t
workers_ready_
=
0
;
std
:
:
condition_variable
worker_start_cv_
;
WorkerCommand
worker_start_command_
;
std
:
:
function
<
void
(
const
void
*
size_t
)
>
task_
;
const
void
*
data_
;
}
;
template
<
class
Traits
>
void
RunWithoutVerify
(
Traits
st
const
Dist
dist
const
size_t
num_keys
const
Algo
algo
SharedState
&
shared
size_t
thread
)
{
using
LaneType
=
typename
Traits
:
:
LaneType
;
using
KeyType
=
typename
Traits
:
:
KeyType
;
using
Order
=
typename
Traits
:
:
Order
;
const
size_t
num_lanes
=
num_keys
*
st
.
LanesPerKey
(
)
;
auto
aligned
=
hwy
:
:
AllocateAligned
<
LaneType
>
(
num_lanes
)
;
(
void
)
GenerateInput
(
dist
aligned
.
get
(
)
num_lanes
)
;
const
Timestamp
t0
;
Run
<
Order
>
(
algo
reinterpret_cast
<
KeyType
*
>
(
aligned
.
get
(
)
)
num_keys
shared
thread
)
;
HWY_ASSERT
(
aligned
[
0
]
<
aligned
[
num_lanes
-
1
]
)
;
}
void
BenchParallel
(
)
{
if
(
HWY_ARCH_X86
&
&
(
HWY_TARGET
!
=
HWY_AVX2
&
&
HWY_TARGET
!
=
HWY_AVX3
)
)
{
return
;
}
ThreadPool
pool
;
const
size_t
NT
=
pool
.
NumThreads
(
)
;
detail
:
:
SharedTraits
<
detail
:
:
TraitsLane
<
detail
:
:
OrderAscending
<
int64_t
>
>
>
st
;
using
KeyType
=
typename
decltype
(
st
)
:
:
KeyType
;
const
size_t
num_keys
=
size_t
{
100
}
*
1000
*
1000
;
#
if
HAVE_IPS4O
const
Algo
algo
=
Algo
:
:
kIPS4O
;
#
else
const
Algo
algo
=
Algo
:
:
kVQSort
;
#
endif
const
Dist
dist
=
Dist
:
:
kUniform32
;
SharedState
shared
;
shared
.
tls
.
resize
(
NT
)
;
std
:
:
vector
<
Result
>
results
;
for
(
size_t
nt
=
1
;
nt
<
NT
;
nt
+
=
HWY_MAX
(
1
NT
/
16
)
)
{
Timestamp
t0
;
pool
.
RunOnThreads
(
nt
[
=
&
shared
]
(
size_t
thread
)
{
RunWithoutVerify
(
st
dist
num_keys
algo
shared
thread
)
;
}
)
;
const
double
sec
=
SecondsSince
(
t0
)
;
results
.
emplace_back
(
algo
dist
num_keys
nt
sec
sizeof
(
KeyType
)
st
.
KeyString
(
)
)
;
results
.
back
(
)
.
Print
(
)
;
}
}
}
}
}
HWY_AFTER_NAMESPACE
(
)
;
#
if
HWY_ONCE
namespace
hwy
{
namespace
{
HWY_BEFORE_TEST
(
BenchParallel
)
;
HWY_EXPORT_AND_TEST_P
(
BenchParallel
BenchParallel
)
;
}
}
#
endif
