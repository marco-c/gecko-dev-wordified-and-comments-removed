#
ifndef
HIGHWAY_HWY_CONTRIB_THREAD_POOL_THREAD_POOL_H_
#
define
HIGHWAY_HWY_CONTRIB_THREAD_POOL_THREAD_POOL_H_
#
include
<
stddef
.
h
>
#
include
<
stdint
.
h
>
#
include
<
stdio
.
h
>
#
include
<
array
>
#
include
<
thread
>
#
include
<
atomic
>
#
include
<
vector
>
#
include
"
hwy
/
aligned_allocator
.
h
"
#
include
"
hwy
/
base
.
h
"
#
include
"
hwy
/
cache_control
.
h
"
#
include
"
hwy
/
contrib
/
thread_pool
/
futex
.
h
"
#
define
HWY_POOL_INLINE
HWY_NOINLINE
#
ifndef
HWY_POOL_SETRANGE_INLINE
#
if
HWY_ARCH_ARM
#
define
HWY_POOL_SETRANGE_INLINE
HWY_NOINLINE
#
else
#
define
HWY_POOL_SETRANGE_INLINE
#
endif
#
endif
namespace
hwy
{
class
Divisor
{
public
:
Divisor
(
)
=
default
;
explicit
Divisor
(
uint32_t
divisor
)
:
divisor_
(
divisor
)
{
if
(
divisor
<
=
1
)
return
;
const
uint32_t
len
=
static_cast
<
uint32_t
>
(
31
-
Num0BitsAboveMS1Bit_Nonzero32
(
divisor
-
1
)
)
;
const
uint64_t
u_hi
=
(
2ULL
<
<
len
)
-
divisor
;
const
uint32_t
q
=
Truncate
(
(
u_hi
<
<
32
)
/
divisor
)
;
mul_
=
q
+
1
;
shift1_
=
1
;
shift2_
=
len
;
}
uint32_t
GetDivisor
(
)
const
{
return
divisor_
;
}
uint32_t
Divide
(
uint32_t
n
)
const
{
const
uint64_t
mul
=
mul_
;
const
uint32_t
t
=
Truncate
(
(
mul
*
n
)
>
>
32
)
;
return
(
t
+
(
(
n
-
t
)
>
>
shift1_
)
)
>
>
shift2_
;
}
uint32_t
Remainder
(
uint32_t
n
)
const
{
return
n
-
(
Divide
(
n
)
*
divisor_
)
;
}
private
:
static
uint32_t
Truncate
(
uint64_t
x
)
{
return
static_cast
<
uint32_t
>
(
x
&
0xFFFFFFFFu
)
;
}
uint32_t
divisor_
;
uint32_t
mul_
=
1
;
uint32_t
shift1_
=
0
;
uint32_t
shift2_
=
0
;
}
;
class
ShuffledIota
{
public
:
ShuffledIota
(
)
:
coprime_
(
1
)
{
}
explicit
ShuffledIota
(
uint32_t
coprime
)
:
coprime_
(
coprime
)
{
}
uint32_t
Next
(
uint32_t
current
const
Divisor
&
divisor
)
const
{
HWY_DASSERT
(
current
<
divisor
.
GetDivisor
(
)
)
;
return
divisor
.
Remainder
(
current
+
coprime_
)
;
}
static
bool
CoprimeNonzero
(
uint32_t
a
uint32_t
b
)
{
const
size_t
trailing_a
=
Num0BitsBelowLS1Bit_Nonzero32
(
a
)
;
const
size_t
trailing_b
=
Num0BitsBelowLS1Bit_Nonzero32
(
b
)
;
if
(
HWY_MIN
(
trailing_a
trailing_b
)
!
=
0
)
return
false
;
a
>
>
=
trailing_a
;
b
>
>
=
trailing_b
;
for
(
;
;
)
{
const
uint32_t
tmp_a
=
a
;
a
=
HWY_MAX
(
tmp_a
b
)
;
b
=
HWY_MIN
(
tmp_a
b
)
;
if
(
b
=
=
1
)
return
true
;
a
-
=
b
;
if
(
a
=
=
0
)
return
false
;
a
>
>
=
Num0BitsBelowLS1Bit_Nonzero32
(
a
)
;
}
}
static
uint32_t
FindAnotherCoprime
(
uint32_t
size
uint32_t
start
)
{
if
(
size
<
=
2
)
{
return
1
;
}
const
uint32_t
inc
=
(
size
&
1
)
?
1
:
2
;
for
(
uint32_t
x
=
start
|
1
;
x
<
start
+
size
*
16
;
x
+
=
inc
)
{
if
(
CoprimeNonzero
(
x
static_cast
<
uint32_t
>
(
size
)
)
)
{
return
x
;
}
}
HWY_ABORT
(
"
unreachable
"
)
;
}
uint32_t
coprime_
;
}
;
#
pragma
pack
(
push
1
)
enum
class
PoolWaitMode
:
uint32_t
{
kBlock
kSpin
}
;
class
PoolWorker
{
static
constexpr
size_t
kMaxVictims
=
4
;
public
:
PoolWorker
(
size_t
thread
size_t
num_workers
)
{
wait_mode_
=
PoolWaitMode
:
:
kBlock
;
num_victims_
=
static_cast
<
uint32_t
>
(
HWY_MIN
(
kMaxVictims
num_workers
)
)
;
const
Divisor
div_workers
(
static_cast
<
uint32_t
>
(
num_workers
)
)
;
const
uint32_t
coprime
=
ShuffledIota
:
:
FindAnotherCoprime
(
static_cast
<
uint32_t
>
(
num_workers
)
static_cast
<
uint32_t
>
(
(
thread
+
1
)
*
257
+
thread
*
13
)
)
;
const
ShuffledIota
shuffled_iota
(
coprime
)
;
victims_
[
0
]
=
static_cast
<
uint32_t
>
(
thread
)
;
for
(
uint32_t
i
=
1
;
i
<
num_victims_
;
+
+
i
)
{
victims_
[
i
]
=
shuffled_iota
.
Next
(
victims_
[
i
-
1
]
div_workers
)
;
HWY_DASSERT
(
victims_
[
i
]
!
=
thread
)
;
}
(
void
)
padding_
;
}
~
PoolWorker
(
)
=
default
;
void
SetWaitMode
(
PoolWaitMode
wait_mode
)
{
wait_mode_
.
store
(
wait_mode
std
:
:
memory_order_release
)
;
}
PoolWaitMode
WorkerGetWaitMode
(
)
const
{
return
wait_mode_
.
load
(
std
:
:
memory_order_acquire
)
;
}
hwy
:
:
Span
<
const
uint32_t
>
Victims
(
)
const
{
return
hwy
:
:
Span
<
const
uint32_t
>
(
victims_
.
data
(
)
static_cast
<
size_t
>
(
num_victims_
)
)
;
}
HWY_POOL_SETRANGE_INLINE
void
SetRange
(
uint64_t
begin
uint64_t
end
)
{
const
auto
rel
=
std
:
:
memory_order_release
;
begin_
.
store
(
begin
rel
)
;
end_
.
store
(
end
rel
)
;
}
uint64_t
WorkerGetEnd
(
)
const
{
return
end_
.
load
(
std
:
:
memory_order_acquire
)
;
}
uint64_t
WorkerReserveTask
(
)
{
return
begin_
.
fetch_add
(
1
std
:
:
memory_order_relaxed
)
;
}
private
:
std
:
:
atomic
<
uint64_t
>
begin_
;
std
:
:
atomic
<
uint64_t
>
end_
;
std
:
:
atomic
<
PoolWaitMode
>
wait_mode_
;
uint32_t
num_victims_
;
std
:
:
array
<
uint32_t
kMaxVictims
>
victims_
;
uint8_t
padding_
[
HWY_ALIGNMENT
-
16
-
8
-
sizeof
(
victims_
)
]
;
}
;
static_assert
(
sizeof
(
PoolWorker
)
=
=
HWY_ALIGNMENT
"
"
)
;
class
PoolTasks
{
typedef
void
(
*
RunFunc
)
(
const
void
*
opaque
uint64_t
task
size_t
thread_id
)
;
template
<
class
Closure
>
static
void
CallClosure
(
const
void
*
opaque
uint64_t
task
size_t
thread
)
{
(
*
reinterpret_cast
<
const
Closure
*
>
(
opaque
)
)
(
task
thread
)
;
}
public
:
template
<
class
Closure
>
void
Store
(
const
Closure
&
closure
uint64_t
begin
uint64_t
end
)
{
const
auto
rel
=
std
:
:
memory_order_release
;
func_
.
store
(
static_cast
<
RunFunc
>
(
&
CallClosure
<
Closure
>
)
rel
)
;
opaque_
.
store
(
reinterpret_cast
<
const
void
*
>
(
&
closure
)
rel
)
;
begin_
.
store
(
begin
rel
)
;
end_
.
store
(
end
rel
)
;
}
RunFunc
WorkerGet
(
uint64_t
&
begin
uint64_t
&
end
const
void
*
&
opaque
)
const
{
const
auto
acq
=
std
:
:
memory_order_acquire
;
begin
=
begin_
.
load
(
acq
)
;
end
=
end_
.
load
(
acq
)
;
opaque
=
opaque_
.
load
(
acq
)
;
return
func_
.
load
(
acq
)
;
}
private
:
std
:
:
atomic
<
RunFunc
>
func_
;
std
:
:
atomic
<
const
void
*
>
opaque_
;
std
:
:
atomic
<
uint64_t
>
begin_
;
std
:
:
atomic
<
uint64_t
>
end_
;
}
;
class
PoolCommands
{
static
constexpr
uint32_t
kInitial
=
0
;
static
constexpr
uint32_t
kMask
=
0xF
;
static
constexpr
size_t
kShift
=
hwy
:
:
CeilLog2
(
kMask
)
;
public
:
static
constexpr
uint32_t
kTerminate
=
1
;
static
constexpr
uint32_t
kWork
=
2
;
static
constexpr
uint32_t
kNop
=
3
;
static
uint32_t
WorkerInitialSeqCmd
(
)
{
return
kInitial
;
}
void
Broadcast
(
uint32_t
cmd
)
{
HWY_DASSERT
(
cmd
<
=
kMask
)
;
const
uint32_t
epoch
=
+
+
epoch_
;
const
uint32_t
seq_cmd
=
(
epoch
<
<
kShift
)
|
cmd
;
seq_cmd_
.
store
(
seq_cmd
std
:
:
memory_order_release
)
;
WakeAll
(
seq_cmd_
)
;
}
uint32_t
WorkerWaitForNewCommand
(
PoolWaitMode
wait_mode
uint32_t
&
prev_seq_cmd
)
{
uint32_t
seq_cmd
;
if
(
HWY_LIKELY
(
wait_mode
=
=
PoolWaitMode
:
:
kSpin
)
)
{
seq_cmd
=
SpinUntilDifferent
(
prev_seq_cmd
seq_cmd_
)
;
}
else
{
seq_cmd
=
BlockUntilDifferent
(
prev_seq_cmd
seq_cmd_
)
;
}
prev_seq_cmd
=
seq_cmd
;
return
seq_cmd
&
kMask
;
}
private
:
static
HWY_INLINE
uint32_t
SpinUntilDifferent
(
const
uint32_t
prev_seq_cmd
std
:
:
atomic
<
uint32_t
>
&
current
)
{
for
(
;
;
)
{
hwy
:
:
Pause
(
)
;
const
uint32_t
seq_cmd
=
current
.
load
(
std
:
:
memory_order_acquire
)
;
if
(
seq_cmd
!
=
prev_seq_cmd
)
return
seq_cmd
;
}
}
uint32_t
epoch_
{
0
}
;
std
:
:
atomic
<
uint32_t
>
seq_cmd_
{
kInitial
}
;
}
;
class
alignas
(
HWY_ALIGNMENT
)
PoolBarrier
{
static
constexpr
size_t
kU64PerCacheLine
=
HWY_ALIGNMENT
/
sizeof
(
uint64_t
)
;
public
:
void
Reset
(
)
{
for
(
size_t
i
=
0
;
i
<
4
;
+
+
i
)
{
num_finished_
[
i
*
kU64PerCacheLine
]
.
store
(
0
std
:
:
memory_order_release
)
;
}
}
void
WorkerArrive
(
size_t
thread
)
{
const
size_t
i
=
(
thread
&
3
)
;
num_finished_
[
i
*
kU64PerCacheLine
]
.
fetch_add
(
1
std
:
:
memory_order_release
)
;
}
HWY_POOL_INLINE
void
WaitAll
(
size_t
num_workers
)
{
const
auto
acq
=
std
:
:
memory_order_acquire
;
for
(
;
;
)
{
hwy
:
:
Pause
(
)
;
const
uint64_t
sum
=
num_finished_
[
0
*
kU64PerCacheLine
]
.
load
(
acq
)
+
num_finished_
[
1
*
kU64PerCacheLine
]
.
load
(
acq
)
+
num_finished_
[
2
*
kU64PerCacheLine
]
.
load
(
acq
)
+
num_finished_
[
3
*
kU64PerCacheLine
]
.
load
(
acq
)
;
if
(
sum
=
=
num_workers
)
break
;
}
}
private
:
std
:
:
atomic
<
uint64_t
>
num_finished_
[
4
*
kU64PerCacheLine
]
;
}
;
struct
alignas
(
HWY_ALIGNMENT
)
PoolMem
{
PoolWorker
&
Worker
(
size_t
thread
)
{
return
*
reinterpret_cast
<
PoolWorker
*
>
(
reinterpret_cast
<
uint8_t
*
>
(
&
barrier
)
+
sizeof
(
barrier
)
+
thread
*
sizeof
(
PoolWorker
)
)
;
}
PoolTasks
tasks
;
PoolCommands
commands
;
uint8_t
padding
[
HWY_ALIGNMENT
-
sizeof
(
tasks
)
-
sizeof
(
commands
)
]
;
PoolBarrier
barrier
;
static_assert
(
sizeof
(
barrier
)
%
HWY_ALIGNMENT
=
=
0
"
"
)
;
}
;
class
PoolMemOwner
{
public
:
explicit
PoolMemOwner
(
size_t
num_threads
)
:
num_workers_
(
HWY_MAX
(
num_threads
size_t
{
1
}
)
)
{
const
size_t
size
=
sizeof
(
PoolMem
)
+
num_workers_
*
sizeof
(
PoolWorker
)
;
bytes_
=
hwy
:
:
AllocateAligned
<
uint8_t
>
(
size
)
;
HWY_ASSERT
(
bytes_
)
;
mem_
=
new
(
bytes_
.
get
(
)
)
PoolMem
(
)
;
for
(
size_t
thread
=
0
;
thread
<
num_workers_
;
+
+
thread
)
{
new
(
&
mem_
-
>
Worker
(
thread
)
)
PoolWorker
(
thread
num_workers_
)
;
}
std
:
:
atomic_thread_fence
(
std
:
:
memory_order_release
)
;
}
~
PoolMemOwner
(
)
{
for
(
size_t
thread
=
0
;
thread
<
num_workers_
;
+
+
thread
)
{
mem_
-
>
Worker
(
thread
)
.
~
PoolWorker
(
)
;
}
mem_
-
>
~
PoolMem
(
)
;
}
size_t
NumWorkers
(
)
const
{
return
num_workers_
;
}
PoolMem
*
Mem
(
)
const
{
return
mem_
;
}
private
:
const
size_t
num_workers_
;
hwy
:
:
AlignedFreeUniquePtr
<
uint8_t
[
]
>
bytes_
;
PoolMem
*
mem_
;
}
;
class
ParallelFor
{
public
:
template
<
class
Closure
>
static
bool
Plan
(
uint64_t
begin
uint64_t
end
size_t
num_workers
const
Closure
&
closure
PoolMem
&
mem
)
{
HWY_DASSERT
(
begin
<
=
end
)
;
const
size_t
num_tasks
=
static_cast
<
size_t
>
(
end
-
begin
)
;
if
(
HWY_UNLIKELY
(
num_tasks
=
=
0
)
)
return
false
;
if
(
HWY_UNLIKELY
(
num_workers
<
=
1
)
)
{
for
(
uint64_t
task
=
begin
;
task
<
end
;
+
+
task
)
{
closure
(
task
0
)
;
}
return
false
;
}
mem
.
tasks
.
Store
(
closure
begin
end
)
;
const
size_t
remainder
=
num_tasks
%
num_workers
;
const
size_t
min_tasks
=
num_tasks
/
num_workers
;
uint64_t
task
=
begin
;
for
(
size_t
thread
=
0
;
thread
<
num_workers
;
+
+
thread
)
{
const
uint64_t
my_end
=
task
+
min_tasks
+
(
thread
<
remainder
)
;
mem
.
Worker
(
thread
)
.
SetRange
(
task
my_end
)
;
task
=
my_end
;
}
HWY_DASSERT
(
task
=
=
end
)
;
return
true
;
}
static
HWY_POOL_INLINE
void
WorkerRun
(
const
size_t
thread
size_t
num_workers
PoolMem
&
mem
)
{
HWY_DASSERT
(
num_workers
!
=
0
)
;
HWY_DASSERT
(
thread
<
num_workers
)
;
const
PoolTasks
&
tasks
=
mem
.
tasks
;
uint64_t
begin
end
;
const
void
*
opaque
;
const
auto
func
=
tasks
.
WorkerGet
(
begin
end
opaque
)
;
if
(
HWY_UNLIKELY
(
end
<
=
begin
+
num_workers
)
)
{
const
uint64_t
task
=
begin
+
thread
;
if
(
HWY_LIKELY
(
task
<
end
)
)
{
func
(
opaque
task
thread
)
;
}
return
;
}
for
(
uint32_t
victim
:
mem
.
Worker
(
thread
)
.
Victims
(
)
)
{
PoolWorker
*
other_worker
=
&
mem
.
Worker
(
victim
)
;
const
uint64_t
end
=
other_worker
-
>
WorkerGetEnd
(
)
;
for
(
;
;
)
{
uint64_t
task
=
other_worker
-
>
WorkerReserveTask
(
)
;
HWY_DASSERT
(
task
<
end
+
num_workers
)
;
if
(
HWY_UNLIKELY
(
task
>
=
end
)
)
{
hwy
:
:
Pause
(
)
;
break
;
}
func
(
opaque
task
thread
)
;
}
}
}
}
;
#
pragma
pack
(
pop
)
static
inline
void
SetThreadName
(
const
char
*
format
int
thread
)
{
#
if
HWY_OS_LINUX
char
buf
[
16
]
=
{
}
;
const
int
chars_written
=
snprintf
(
buf
sizeof
(
buf
)
format
thread
)
;
HWY_ASSERT
(
0
<
chars_written
&
&
chars_written
<
=
static_cast
<
int
>
(
sizeof
(
buf
)
-
1
)
)
;
HWY_ASSERT
(
0
=
=
pthread_setname_np
(
pthread_self
(
)
buf
)
)
;
#
else
(
void
)
format
;
(
void
)
thread
;
#
endif
}
class
ThreadPool
{
static
void
ThreadFunc
(
size_t
thread
size_t
num_workers
PoolMem
*
mem
)
{
HWY_DASSERT
(
thread
<
num_workers
)
;
SetThreadName
(
"
worker
%
03zu
"
static_cast
<
int
>
(
thread
)
)
;
std
:
:
atomic_thread_fence
(
std
:
:
memory_order_acquire
)
;
PoolWorker
&
worker
=
mem
-
>
Worker
(
thread
)
;
PoolCommands
&
commands
=
mem
-
>
commands
;
uint32_t
prev_seq_cmd
=
PoolCommands
:
:
WorkerInitialSeqCmd
(
)
;
for
(
;
;
)
{
const
PoolWaitMode
wait_mode
=
worker
.
WorkerGetWaitMode
(
)
;
const
uint32_t
command
=
commands
.
WorkerWaitForNewCommand
(
wait_mode
prev_seq_cmd
)
;
if
(
HWY_UNLIKELY
(
command
=
=
PoolCommands
:
:
kTerminate
)
)
{
return
;
}
else
if
(
HWY_LIKELY
(
command
=
=
PoolCommands
:
:
kWork
)
)
{
ParallelFor
:
:
WorkerRun
(
thread
num_workers
*
mem
)
;
mem
-
>
barrier
.
WorkerArrive
(
thread
)
;
}
else
if
(
command
=
=
PoolCommands
:
:
kNop
)
{
}
else
{
HWY_DASSERT
(
false
)
;
}
}
}
public
:
static
size_t
MaxThreads
(
)
{
return
static_cast
<
size_t
>
(
std
:
:
thread
:
:
hardware_concurrency
(
)
-
1
)
;
}
explicit
ThreadPool
(
size_t
num_threads
)
:
owner_
(
num_threads
)
{
(
void
)
busy_
;
const
size_t
num_workers
=
owner_
.
NumWorkers
(
)
;
threads_
.
reserve
(
num_workers
-
1
)
;
for
(
size_t
thread
=
0
;
thread
<
num_workers
-
1
;
+
+
thread
)
{
threads_
.
emplace_back
(
ThreadFunc
thread
num_workers
owner_
.
Mem
(
)
)
;
}
}
~
ThreadPool
(
)
{
PoolMem
&
mem
=
*
owner_
.
Mem
(
)
;
mem
.
commands
.
Broadcast
(
PoolCommands
:
:
kTerminate
)
;
for
(
std
:
:
thread
&
thread
:
threads_
)
{
HWY_ASSERT
(
thread
.
joinable
(
)
)
;
thread
.
join
(
)
;
}
}
ThreadPool
(
const
ThreadPool
&
)
=
delete
;
ThreadPool
&
operator
&
(
const
ThreadPool
&
)
=
delete
;
size_t
NumWorkers
(
)
const
{
return
owner_
.
NumWorkers
(
)
;
}
void
SetWaitMode
(
PoolWaitMode
mode
)
{
HWY_DASSERT
(
busy_
.
fetch_add
(
1
)
=
=
0
)
;
PoolMem
&
mem
=
*
owner_
.
Mem
(
)
;
for
(
size_t
thread
=
0
;
thread
<
owner_
.
NumWorkers
(
)
;
+
+
thread
)
{
mem
.
Worker
(
thread
)
.
SetWaitMode
(
mode
)
;
}
mem
.
commands
.
Broadcast
(
PoolCommands
:
:
kNop
)
;
HWY_DASSERT
(
busy_
.
fetch_add
(
-
1
)
=
=
1
)
;
}
template
<
class
Closure
>
void
Run
(
uint64_t
begin
uint64_t
end
const
Closure
&
closure
)
{
const
size_t
num_workers
=
NumWorkers
(
)
;
PoolMem
&
mem
=
*
owner_
.
Mem
(
)
;
if
(
HWY_LIKELY
(
ParallelFor
:
:
Plan
(
begin
end
num_workers
closure
mem
)
)
)
{
HWY_DASSERT
(
busy_
.
fetch_add
(
1
)
=
=
0
)
;
mem
.
barrier
.
Reset
(
)
;
mem
.
commands
.
Broadcast
(
PoolCommands
:
:
kWork
)
;
const
size_t
thread
=
num_workers
-
1
;
ParallelFor
:
:
WorkerRun
(
thread
num_workers
mem
)
;
mem
.
barrier
.
WorkerArrive
(
thread
)
;
mem
.
barrier
.
WaitAll
(
num_workers
)
;
HWY_DASSERT
(
busy_
.
fetch_add
(
-
1
)
=
=
1
)
;
}
}
static
bool
NoInit
(
size_t
)
{
return
true
;
}
size_t
NumThreads
(
)
const
{
return
NumWorkers
(
)
;
}
template
<
class
InitClosure
class
RunClosure
>
bool
Run
(
uint64_t
begin
uint64_t
end
const
InitClosure
&
init_closure
const
RunClosure
&
run_closure
)
{
if
(
!
init_closure
(
NumThreads
(
)
)
)
return
false
;
Run
(
begin
end
run_closure
)
;
return
true
;
}
PoolMem
&
InternalMem
(
)
const
{
return
*
owner_
.
Mem
(
)
;
}
private
:
std
:
:
vector
<
std
:
:
thread
>
threads_
;
PoolMemOwner
owner_
;
std
:
:
atomic
<
int
>
busy_
{
0
}
;
}
;
}
#
endif
