#
ifndef
HIGHWAY_HWY_CONTRIB_THREAD_POOL_THREAD_POOL_H_
#
define
HIGHWAY_HWY_CONTRIB_THREAD_POOL_THREAD_POOL_H_
#
include
<
stddef
.
h
>
#
include
<
stdint
.
h
>
#
include
<
stdio
.
h
>
#
include
<
string
.
h
>
#
include
<
array
>
#
include
<
atomic
>
#
include
<
string
>
#
include
<
thread
>
#
include
<
vector
>
#
include
"
hwy
/
aligned_allocator
.
h
"
#
include
"
hwy
/
auto_tune
.
h
"
#
include
"
hwy
/
base
.
h
"
#
include
"
hwy
/
cache_control
.
h
"
#
include
"
hwy
/
contrib
/
thread_pool
/
futex
.
h
"
#
include
"
hwy
/
contrib
/
thread_pool
/
spin
.
h
"
#
include
"
hwy
/
contrib
/
thread_pool
/
topology
.
h
"
#
include
"
hwy
/
profiler
.
h
"
#
include
"
hwy
/
stats
.
h
"
#
include
"
hwy
/
timer
.
h
"
#
if
HWY_OS_APPLE
#
include
<
AvailabilityMacros
.
h
>
#
endif
#
if
PROFILER_ENABLED
#
include
<
algorithm
>
#
include
"
hwy
/
bit_set
.
h
"
#
endif
namespace
hwy
{
static
inline
void
SetThreadName
(
const
char
*
format
int
thread
)
{
char
buf
[
16
]
=
{
}
;
const
int
chars_written
=
snprintf
(
buf
sizeof
(
buf
)
format
thread
)
;
HWY_ASSERT
(
0
<
chars_written
&
&
chars_written
<
=
static_cast
<
int
>
(
sizeof
(
buf
)
-
1
)
)
;
#
if
(
HWY_OS_LINUX
&
&
(
!
defined
(
__ANDROID__
)
|
|
__ANDROID_API__
>
=
19
)
)
|
|
\
HWY_OS_FREEBSD
HWY_ASSERT
(
0
=
=
pthread_setname_np
(
pthread_self
(
)
buf
)
)
;
#
elif
HWY_OS_APPLE
&
&
(
MAC_OS_X_VERSION_MIN_REQUIRED
>
=
1060
)
HWY_ASSERT
(
0
=
=
pthread_setname_np
(
buf
)
)
;
#
elif
defined
(
__EMSCRIPTEN__
)
emscripten_set_thread_name
(
pthread_self
(
)
buf
)
;
#
else
(
void
)
format
;
(
void
)
thread
;
#
endif
}
enum
class
PoolWaitMode
:
uint8_t
{
kBlock
=
1
kSpin
}
;
enum
class
Exit
:
uint32_t
{
kNone
kLoop
kThread
}
;
HWY_INLINE_VAR
constexpr
size_t
kMaxClusters
=
32
+
1
;
HWY_INLINE_VAR
constexpr
size_t
kAllClusters
=
kMaxClusters
-
1
;
class
PoolWorkerMapping
{
public
:
PoolWorkerMapping
(
)
:
cluster_idx_
(
0
)
max_cluster_workers_
(
0
)
{
}
PoolWorkerMapping
(
size_t
cluster_idx
size_t
max_cluster_workers
)
:
cluster_idx_
(
cluster_idx
)
max_cluster_workers_
(
max_cluster_workers
)
{
HWY_DASSERT
(
cluster_idx
<
=
kAllClusters
)
;
HWY_DASSERT
(
max_cluster_workers
!
=
0
)
;
}
size_t
ClusterIdx
(
)
const
{
return
cluster_idx_
;
}
size_t
MaxClusterWorkers
(
)
const
{
return
max_cluster_workers_
;
}
size_t
operator
(
)
(
size_t
worker_idx
)
const
{
if
(
cluster_idx_
=
=
kAllClusters
)
{
const
size_t
cluster_idx
=
worker_idx
;
HWY_DASSERT
(
cluster_idx
<
kAllClusters
)
;
return
cluster_idx
*
max_cluster_workers_
;
}
HWY_DASSERT
(
max_cluster_workers_
=
=
0
|
|
worker_idx
<
max_cluster_workers_
)
;
return
cluster_idx_
*
max_cluster_workers_
+
worker_idx
;
}
private
:
size_t
cluster_idx_
;
size_t
max_cluster_workers_
;
}
;
namespace
pool
{
#
ifndef
HWY_POOL_VERBOSITY
#
define
HWY_POOL_VERBOSITY
0
#
endif
static
constexpr
int
kVerbosity
=
HWY_POOL_VERBOSITY
;
static
constexpr
size_t
kMaxThreads
=
127
;
class
ShuffledIota
{
public
:
ShuffledIota
(
)
:
coprime_
(
1
)
{
}
explicit
ShuffledIota
(
uint32_t
coprime
)
:
coprime_
(
coprime
)
{
}
uint32_t
Next
(
uint32_t
current
const
Divisor64
&
divisor
)
const
{
HWY_DASSERT
(
current
<
divisor
.
GetDivisor
(
)
)
;
return
static_cast
<
uint32_t
>
(
divisor
.
Remainder
(
current
+
coprime_
)
)
;
}
static
bool
CoprimeNonzero
(
uint32_t
a
uint32_t
b
)
{
const
size_t
trailing_a
=
Num0BitsBelowLS1Bit_Nonzero32
(
a
)
;
const
size_t
trailing_b
=
Num0BitsBelowLS1Bit_Nonzero32
(
b
)
;
if
(
HWY_MIN
(
trailing_a
trailing_b
)
!
=
0
)
return
false
;
a
>
>
=
trailing_a
;
b
>
>
=
trailing_b
;
for
(
;
;
)
{
const
uint32_t
tmp_a
=
a
;
a
=
HWY_MAX
(
tmp_a
b
)
;
b
=
HWY_MIN
(
tmp_a
b
)
;
if
(
b
=
=
1
)
return
true
;
a
-
=
b
;
if
(
a
=
=
0
)
return
false
;
a
>
>
=
Num0BitsBelowLS1Bit_Nonzero32
(
a
)
;
}
}
static
uint32_t
FindAnotherCoprime
(
uint32_t
size
uint32_t
start
)
{
if
(
size
<
=
2
)
{
return
1
;
}
const
uint32_t
inc
=
(
size
&
1
)
?
1
:
2
;
for
(
uint32_t
x
=
start
|
1
;
x
<
start
+
size
*
16
;
x
+
=
inc
)
{
if
(
CoprimeNonzero
(
x
static_cast
<
uint32_t
>
(
size
)
)
)
{
return
x
;
}
}
HWY_UNREACHABLE
;
}
uint32_t
coprime_
;
}
;
enum
class
WaitType
:
uint8_t
{
kBlock
kSpin1
kSpinSeparate
kSentinel
}
;
static
inline
const
char
*
ToString
(
WaitType
type
)
{
switch
(
type
)
{
case
WaitType
:
:
kBlock
:
return
"
Block
"
;
case
WaitType
:
:
kSpin1
:
return
"
Single
"
;
case
WaitType
:
:
kSpinSeparate
:
return
"
Separate
"
;
case
WaitType
:
:
kSentinel
:
return
nullptr
;
}
}
struct
Config
{
static
std
:
:
vector
<
Config
>
AllCandidates
(
PoolWaitMode
wait_mode
)
{
std
:
:
vector
<
Config
>
candidates
;
if
(
wait_mode
=
=
PoolWaitMode
:
:
kSpin
)
{
std
:
:
vector
<
SpinType
>
spin_types
;
spin_types
.
reserve
(
2
)
;
spin_types
.
push_back
(
DetectSpin
(
)
)
;
if
(
spin_types
[
0
]
!
=
SpinType
:
:
kPause
)
{
spin_types
.
push_back
(
SpinType
:
:
kPause
)
;
}
std
:
:
vector
<
WaitType
>
wait_types
;
for
(
size_t
wait
=
0
;
;
+
+
wait
)
{
const
WaitType
wait_type
=
static_cast
<
WaitType
>
(
wait
)
;
if
(
wait_type
=
=
WaitType
:
:
kSentinel
)
break
;
if
(
wait_type
!
=
WaitType
:
:
kBlock
)
wait_types
.
push_back
(
wait_type
)
;
}
candidates
.
reserve
(
spin_types
.
size
(
)
*
wait_types
.
size
(
)
)
;
for
(
const
SpinType
spin_type
:
spin_types
)
{
for
(
const
WaitType
wait_type
:
wait_types
)
{
candidates
.
emplace_back
(
spin_type
wait_type
)
;
}
}
}
else
{
candidates
.
emplace_back
(
SpinType
:
:
kPause
WaitType
:
:
kBlock
)
;
}
return
candidates
;
}
std
:
:
string
ToString
(
)
const
{
char
buf
[
128
]
;
snprintf
(
buf
sizeof
(
buf
)
"
%
-
14s
%
-
9s
"
hwy
:
:
ToString
(
spin_type
)
pool
:
:
ToString
(
wait_type
)
)
;
return
buf
;
}
Config
(
SpinType
spin_type_in
WaitType
wait_type_in
)
:
spin_type
(
spin_type_in
)
wait_type
(
wait_type_in
)
{
}
Config
(
)
:
Config
(
SpinType
:
:
kPause
WaitType
:
:
kSpinSeparate
)
{
}
SpinType
spin_type
;
WaitType
wait_type
;
HWY_MEMBER_VAR_MAYBE_UNUSED
uint8_t
reserved
[
2
]
;
}
;
static_assert
(
sizeof
(
Config
)
=
=
4
"
"
)
;
#
if
PROFILER_ENABLED
class
Stats
{
static
constexpr
size_t
kDWait
=
0
;
static
constexpr
size_t
kWaitReps
=
1
;
static
constexpr
size_t
kTBeforeRun
=
2
;
static
constexpr
size_t
kDRun
=
3
;
static
constexpr
size_t
kTasksStatic
=
4
;
static
constexpr
size_t
kTasksDynamic
=
5
;
static
constexpr
size_t
kTasksStolen
=
6
;
static
constexpr
size_t
kDFuncStatic
=
7
;
static
constexpr
size_t
kDFuncDynamic
=
8
;
static
constexpr
size_t
kSentinel
=
9
;
public
:
Stats
(
)
{
for
(
size_t
thread_idx
=
0
;
thread_idx
<
kMaxThreads
;
+
+
thread_idx
)
{
for
(
size_t
offset
=
0
;
offset
<
kSentinel
;
+
+
offset
)
{
PerThread
(
thread_idx
offset
)
=
0
;
}
}
Reset
(
)
;
}
void
NotifyRunStatic
(
size_t
worker_idx
timer
:
:
Ticks
d_func
)
{
if
(
worker_idx
=
=
0
)
{
num_run_static_
+
+
;
sum_tasks_static_
+
+
;
sum_d_func_static_
+
=
d_func
;
}
else
{
const
size_t
thread_idx
=
worker_idx
-
1
;
PerThread
(
thread_idx
kTasksStatic
)
+
+
;
PerThread
(
thread_idx
kDFuncStatic
)
+
=
d_func
;
}
}
void
NotifyRunDynamic
(
size_t
worker_idx
size_t
tasks
size_t
stolen
timer
:
:
Ticks
d_func
)
{
if
(
worker_idx
=
=
0
)
{
num_run_dynamic_
+
+
;
sum_tasks_dynamic_
+
=
tasks
;
sum_tasks_stolen_
+
=
stolen
;
sum_d_func_dynamic_
+
=
d_func
;
}
else
{
const
size_t
thread_idx
=
worker_idx
-
1
;
PerThread
(
thread_idx
kTasksDynamic
)
+
=
tasks
;
PerThread
(
thread_idx
kTasksStolen
)
+
=
stolen
;
PerThread
(
thread_idx
kDFuncDynamic
)
+
=
d_func
;
}
}
void
NotifyThreadRun
(
size_t
worker_idx
timer
:
:
Ticks
d_wait
size_t
wait_reps
timer
:
:
Ticks
t_before_run
timer
:
:
Ticks
d_run
)
{
HWY_DASSERT
(
worker_idx
!
=
0
)
;
const
size_t
thread_idx
=
worker_idx
-
1
;
HWY_DASSERT
(
PerThread
(
thread_idx
kDWait
)
=
=
0
)
;
HWY_DASSERT
(
PerThread
(
thread_idx
kWaitReps
)
=
=
0
)
;
HWY_DASSERT
(
PerThread
(
thread_idx
kTBeforeRun
)
=
=
0
)
;
HWY_DASSERT
(
PerThread
(
thread_idx
kDRun
)
=
=
0
)
;
PerThread
(
thread_idx
kDWait
)
=
d_wait
;
PerThread
(
thread_idx
kWaitReps
)
=
wait_reps
;
PerThread
(
thread_idx
kTBeforeRun
)
=
t_before_run
;
PerThread
(
thread_idx
kDRun
)
=
d_run
;
}
void
NotifyMainRun
(
size_t
num_threads
timer
:
:
Ticks
t_before_wake
timer
:
:
Ticks
d_wake
timer
:
:
Ticks
d_main_run
timer
:
:
Ticks
d_barrier
)
{
HWY_DASSERT
(
num_threads
<
=
kMaxThreads
)
;
timer
:
:
Ticks
min_d_run
=
~
timer
:
:
Ticks
{
0
}
;
timer
:
:
Ticks
max_d_run
=
0
;
timer
:
:
Ticks
sum_d_run
=
0
;
for
(
size_t
thread_idx
=
0
;
thread_idx
<
num_threads
;
+
+
thread_idx
)
{
sum_tasks_static_
+
=
PerThread
(
thread_idx
kTasksStatic
)
;
sum_tasks_dynamic_
+
=
PerThread
(
thread_idx
kTasksDynamic
)
;
sum_tasks_stolen_
+
=
PerThread
(
thread_idx
kTasksStolen
)
;
sum_d_func_static_
+
=
PerThread
(
thread_idx
kDFuncStatic
)
;
sum_d_func_dynamic_
+
=
PerThread
(
thread_idx
kDFuncDynamic
)
;
sum_d_wait_
+
=
PerThread
(
thread_idx
kDWait
)
;
sum_wait_reps_
+
=
PerThread
(
thread_idx
kWaitReps
)
;
const
timer
:
:
Ticks
d_thread_run
=
PerThread
(
thread_idx
kDRun
)
;
min_d_run
=
HWY_MIN
(
min_d_run
d_thread_run
)
;
max_d_run
=
HWY_MAX
(
max_d_run
d_thread_run
)
;
sum_d_run
+
=
d_thread_run
;
const
timer
:
:
Ticks
t_before_run
=
PerThread
(
thread_idx
kTBeforeRun
)
;
for
(
size_t
offset
=
0
;
offset
<
kSentinel
;
+
+
offset
)
{
PerThread
(
thread_idx
offset
)
=
0
;
}
HWY_DASSERT
(
t_before_run
!
=
0
)
;
const
timer
:
:
Ticks
d_latency
=
t_before_run
-
t_before_wake
;
sum_wake_latency_
+
=
d_latency
;
max_wake_latency_
=
HWY_MAX
(
max_wake_latency_
d_latency
)
;
}
const
double
inv_avg_d_run
=
static_cast
<
double
>
(
num_threads
)
/
static_cast
<
double
>
(
sum_d_run
)
;
const
double
r_min
=
static_cast
<
double
>
(
min_d_run
)
*
inv_avg_d_run
;
const
double
r_max
=
static_cast
<
double
>
(
max_d_run
)
*
inv_avg_d_run
;
num_run_
+
+
;
sum_d_run_
+
=
sum_d_run
;
sum_r_min_
+
=
r_min
;
sum_r_max_
+
=
r_max
;
sum_d_wake_
+
=
d_wake
;
sum_d_barrier_
+
=
d_barrier
;
sum_d_run_
+
=
d_main_run
;
sum_d_run_main_
+
=
d_main_run
;
}
void
PrintAndReset
(
size_t
num_threads
timer
:
:
Ticks
d_thread_lifetime_ticks
)
{
if
(
num_run_
=
=
0
)
return
;
HWY_ASSERT
(
num_run_
=
=
num_run_static_
+
num_run_dynamic_
)
;
const
double
d_func_static
=
Seconds
(
sum_d_func_static_
)
;
const
double
d_func_dynamic
=
Seconds
(
sum_d_func_dynamic_
)
;
const
double
sum_d_run
=
Seconds
(
sum_d_run_
)
;
const
double
func_div_run
=
(
d_func_static
+
d_func_dynamic
)
/
sum_d_run
;
if
(
!
(
0
.
95
<
=
func_div_run
&
&
func_div_run
<
=
1
.
0
)
)
{
HWY_WARN
(
"
Func
time
%
f
should
be
similar
to
total
run
%
f
.
"
d_func_static
+
d_func_dynamic
sum_d_run
)
;
}
const
double
sum_d_run_main
=
Seconds
(
sum_d_run_main_
)
;
const
double
max_wake_latency
=
Seconds
(
max_wake_latency_
)
;
const
double
sum_d_wait
=
Seconds
(
sum_d_wait_
)
;
const
double
d_thread_lifetime
=
Seconds
(
d_thread_lifetime_ticks
)
;
const
double
inv_run
=
1
.
0
/
static_cast
<
double
>
(
num_run_
)
;
const
auto
per_run
=
[
inv_run
]
(
double
sum
)
{
return
sum
*
inv_run
;
}
;
const
double
avg_d_wake
=
per_run
(
Seconds
(
sum_d_wake_
)
)
;
const
double
avg_wake_latency
=
per_run
(
Seconds
(
sum_wake_latency_
)
)
;
const
double
avg_d_wait
=
per_run
(
sum_d_wait
)
;
const
double
avg_wait_reps
=
per_run
(
static_cast
<
double
>
(
sum_wait_reps_
)
)
;
const
double
avg_d_barrier
=
per_run
(
Seconds
(
sum_d_barrier_
)
)
;
const
double
avg_r_min
=
per_run
(
sum_r_min_
)
;
const
double
avg_r_max
=
per_run
(
sum_r_max_
)
;
const
size_t
num_workers
=
1
+
num_threads
;
const
double
avg_tasks_static
=
Avg
(
sum_tasks_static_
num_run_static_
*
num_workers
)
;
const
double
avg_tasks_dynamic
=
Avg
(
sum_tasks_dynamic_
num_run_dynamic_
*
num_workers
)
;
const
double
avg_steals
=
Avg
(
sum_tasks_stolen_
num_run_dynamic_
*
num_workers
)
;
const
double
avg_d_run
=
sum_d_run
/
num_workers
;
const
double
pc_wait
=
sum_d_wait
/
d_thread_lifetime
*
100
.
0
;
const
double
pc_run
=
sum_d_run
/
d_thread_lifetime
*
100
.
0
;
const
double
pc_main
=
sum_d_run_main
/
avg_d_run
*
100
.
0
;
const
auto
us
=
[
]
(
double
sec
)
{
return
sec
*
1E6
;
}
;
const
auto
ns
=
[
]
(
double
sec
)
{
return
sec
*
1E9
;
}
;
printf
(
"
%
3zu
:
%
5d
x
%
.
2f
/
%
5d
x
%
4
.
1f
tasks
%
.
2f
steals
;
"
"
wake
%
7
.
3f
ns
latency
%
6
.
3f
<
%
7
.
3f
us
barrier
%
7
.
3f
us
;
"
"
wait
%
.
1f
us
(
%
6
.
0f
reps
%
4
.
1f
%
%
)
balance
%
4
.
1f
%
%
-
%
5
.
1f
%
%
"
"
func
:
%
6
.
3f
+
%
7
.
3f
"
"
%
.
1f
%
%
of
thread
time
%
7
.
3f
s
;
main
:
worker
%
5
.
1f
%
%
\
n
"
num_threads
num_run_static_
avg_tasks_static
num_run_dynamic_
avg_tasks_dynamic
avg_steals
ns
(
avg_d_wake
)
us
(
avg_wake_latency
)
us
(
max_wake_latency
)
us
(
avg_d_barrier
)
us
(
avg_d_wait
)
avg_wait_reps
pc_wait
avg_r_min
*
100
.
0
avg_r_max
*
100
.
0
d_func_static
d_func_dynamic
pc_run
d_thread_lifetime
pc_main
)
;
Reset
(
num_threads
)
;
}
void
Reset
(
size_t
num_threads
=
kMaxThreads
)
{
num_run_
=
0
;
num_run_static_
=
0
;
num_run_dynamic_
=
0
;
sum_tasks_stolen_
=
0
;
sum_tasks_static_
=
0
;
sum_tasks_dynamic_
=
0
;
sum_d_wake_
=
0
;
sum_wake_latency_
=
0
;
max_wake_latency_
=
0
;
sum_d_wait_
=
0
;
sum_wait_reps_
=
0
;
sum_d_barrier_
=
0
;
sum_d_func_static_
=
0
;
sum_d_func_dynamic_
=
0
;
sum_r_min_
=
0
.
0
;
sum_r_max_
=
0
.
0
;
sum_d_run_
=
0
;
sum_d_run_main_
=
0
;
}
private
:
template
<
typename
T
>
static
double
Avg
(
T
sum
size_t
div
)
{
return
div
=
=
0
?
0
.
0
:
static_cast
<
double
>
(
sum
)
/
static_cast
<
double
>
(
div
)
;
}
static
constexpr
size_t
kU64PerLine
=
HWY_ALIGNMENT
/
sizeof
(
uint64_t
)
;
uint64_t
&
PerThread
(
size_t
thread_idx
size_t
offset
)
{
HWY_DASSERT
(
thread_idx
<
kMaxThreads
)
;
HWY_DASSERT
(
offset
<
kSentinel
)
;
return
per_thread_
[
thread_idx
*
kU64PerLine
+
offset
]
;
}
int32_t
num_run_
;
int32_t
num_run_static_
;
int32_t
num_run_dynamic_
;
int32_t
sum_tasks_stolen_
;
int64_t
sum_tasks_static_
;
int64_t
sum_tasks_dynamic_
;
timer
:
:
Ticks
sum_d_wake_
;
timer
:
:
Ticks
sum_wake_latency_
;
timer
:
:
Ticks
max_wake_latency_
;
timer
:
:
Ticks
sum_d_wait_
;
uint64_t
sum_wait_reps_
;
timer
:
:
Ticks
sum_d_barrier_
;
timer
:
:
Ticks
sum_d_func_static_
;
timer
:
:
Ticks
sum_d_func_dynamic_
;
double
sum_r_min_
;
double
sum_r_max_
;
timer
:
:
Ticks
sum_d_run_
;
timer
:
:
Ticks
sum_d_run_main_
;
uint64_t
per_thread_
[
kMaxThreads
*
kU64PerLine
]
;
}
;
static_assert
(
sizeof
(
Stats
)
=
=
(
kMaxThreads
+
1
)
*
HWY_ALIGNMENT
"
Wrong
size
"
)
;
HWY_INLINE_VAR
constexpr
size_t
kMaxCallers
=
60
;
class
CallerAccumulator
{
public
:
bool
Any
(
)
const
{
return
calls_
!
=
0
;
}
void
Add
(
size_t
tasks
size_t
workers
bool
is_root
timer
:
:
Ticks
wait_before
timer
:
:
Ticks
elapsed
)
{
calls_
+
+
;
root_
+
=
is_root
;
workers_
+
=
workers
;
min_tasks_
=
HWY_MIN
(
min_tasks_
tasks
)
;
max_tasks_
=
HWY_MAX
(
max_tasks_
tasks
)
;
tasks_
+
=
tasks
;
wait_before_
+
=
wait_before
;
elapsed_
+
=
elapsed
;
}
void
AddFrom
(
const
CallerAccumulator
&
other
)
{
calls_
+
=
other
.
calls_
;
root_
+
=
other
.
root_
;
workers_
+
=
other
.
workers_
;
min_tasks_
=
HWY_MIN
(
min_tasks_
other
.
min_tasks_
)
;
max_tasks_
=
HWY_MAX
(
max_tasks_
other
.
max_tasks_
)
;
tasks_
+
=
other
.
tasks_
;
wait_before_
+
=
other
.
wait_before_
;
elapsed_
+
=
other
.
elapsed_
;
}
bool
operator
>
(
const
CallerAccumulator
&
other
)
const
{
return
elapsed_
>
other
.
elapsed_
;
}
void
PrintAndReset
(
const
char
*
caller
size_t
active_clusters
)
{
if
(
!
Any
(
)
)
return
;
HWY_ASSERT
(
root_
<
=
calls_
)
;
const
double
inv_calls
=
1
.
0
/
static_cast
<
double
>
(
calls_
)
;
const
double
pc_root
=
static_cast
<
double
>
(
root_
)
*
inv_calls
*
100
.
0
;
const
double
avg_workers
=
static_cast
<
double
>
(
workers_
)
*
inv_calls
;
const
double
avg_tasks
=
static_cast
<
double
>
(
tasks_
)
*
inv_calls
;
const
double
avg_tasks_per_worker
=
avg_tasks
/
avg_workers
;
const
double
inv_freq
=
1
.
0
/
platform
:
:
InvariantTicksPerSecond
(
)
;
const
double
sum_wait_before
=
static_cast
<
double
>
(
wait_before_
)
*
inv_freq
;
const
double
avg_wait_before
=
root_
?
sum_wait_before
/
static_cast
<
double
>
(
root_
)
:
0
.
0
;
const
double
elapsed
=
static_cast
<
double
>
(
elapsed_
)
*
inv_freq
;
const
double
avg_elapsed
=
elapsed
*
inv_calls
;
const
double
task_len
=
avg_elapsed
/
avg_tasks_per_worker
;
printf
(
"
%
40s
:
%
7
.
0f
x
(
%
3
.
0f
%
%
)
%
2zu
clusters
%
4
.
1f
workers
"
"
%
5
.
1f
tasks
(
%
5u
-
%
5u
)
"
"
%
5
.
0f
us
wait
%
6
.
1E
us
run
(
task
len
%
6
.
1E
us
)
total
%
6
.
2f
s
\
n
"
caller
static_cast
<
double
>
(
calls_
)
pc_root
active_clusters
avg_workers
avg_tasks_per_worker
static_cast
<
uint32_t
>
(
min_tasks_
)
static_cast
<
uint32_t
>
(
max_tasks_
)
avg_wait_before
*
1E6
avg_elapsed
*
1E6
task_len
*
1E6
elapsed
)
;
*
this
=
CallerAccumulator
(
)
;
}
void
PrintTotal
(
)
{
if
(
!
Any
(
)
)
return
;
HWY_ASSERT
(
root_
<
=
calls_
)
;
const
double
elapsed
=
static_cast
<
double
>
(
elapsed_
)
/
platform
:
:
InvariantTicksPerSecond
(
)
;
printf
(
"
TOTAL
:
%
7
.
0f
x
run
%
6
.
2f
s
\
n
"
static_cast
<
double
>
(
calls_
)
elapsed
)
;
}
private
:
int64_t
calls_
=
0
;
int64_t
root_
=
0
;
uint64_t
workers_
=
0
;
uint64_t
min_tasks_
=
~
uint64_t
{
0
}
;
uint64_t
max_tasks_
=
0
;
uint64_t
tasks_
=
0
;
timer
:
:
Ticks
wait_before_
=
0
;
timer
:
:
Ticks
elapsed_
=
0
;
}
;
static_assert
(
sizeof
(
CallerAccumulator
)
=
=
64
"
"
)
;
class
PerCluster
{
public
:
CallerAccumulator
&
Get
(
size_t
caller_idx
)
{
HWY_DASSERT
(
caller_idx
<
kMaxCallers
)
;
callers_
.
Set
(
caller_idx
)
;
return
accumulators_
[
caller_idx
]
;
}
template
<
class
Func
>
void
ForeachCaller
(
Func
&
&
func
)
{
callers_
.
Foreach
(
[
&
]
(
size_t
caller_idx
)
{
func
(
caller_idx
accumulators_
[
caller_idx
]
)
;
}
)
;
}
std
:
:
vector
<
size_t
>
Sorted
(
)
{
std
:
:
vector
<
size_t
>
vec
;
vec
.
reserve
(
kMaxCallers
)
;
ForeachCaller
(
[
&
]
(
size_t
caller_idx
CallerAccumulator
&
)
{
vec
.
push_back
(
caller_idx
)
;
}
)
;
std
:
:
sort
(
vec
.
begin
(
)
vec
.
end
(
)
[
&
]
(
size_t
a
size_t
b
)
{
return
accumulators_
[
a
]
>
accumulators_
[
b
]
;
}
)
;
return
vec
;
}
void
ResetBits
(
)
{
callers_
=
hwy
:
:
BitSet
<
kMaxCallers
>
(
)
;
}
private
:
CallerAccumulator
accumulators_
[
kMaxCallers
]
;
hwy
:
:
BitSet
<
kMaxCallers
>
callers_
;
}
;
class
Caller
{
public
:
Caller
(
)
:
idx_
(
0
)
{
}
explicit
Caller
(
size_t
idx
)
:
idx_
(
idx
)
{
HWY_DASSERT
(
idx
<
kMaxCallers
)
;
}
size_t
Idx
(
)
const
{
return
idx_
;
}
private
:
size_t
idx_
;
}
;
class
Shared
{
public
:
static
HWY_DLLEXPORT
Shared
&
Get
(
)
;
Stopwatch
MakeStopwatch
(
)
const
{
return
Stopwatch
(
timer_
)
;
}
Stopwatch
&
LastRootEnd
(
)
{
return
last_root_end_
;
}
Caller
AddCaller
(
const
char
*
name
)
{
return
Caller
(
callers_
.
Add
(
name
)
)
;
}
PerCluster
&
Cluster
(
size_t
cluster_idx
)
{
HWY_DASSERT
(
cluster_idx
<
kMaxClusters
)
;
return
per_cluster_
[
cluster_idx
]
;
}
void
PrintAndReset
(
)
{
size_t
active_clusters
[
kMaxCallers
]
=
{
}
;
per_cluster_
[
0
]
.
ForeachCaller
(
[
&
]
(
size_t
caller_idx
CallerAccumulator
&
acc
)
{
active_clusters
[
caller_idx
]
=
acc
.
Any
(
)
;
}
)
;
for
(
size_t
cluster_idx
=
1
;
cluster_idx
<
kMaxClusters
;
+
+
cluster_idx
)
{
per_cluster_
[
cluster_idx
]
.
ForeachCaller
(
[
&
]
(
size_t
caller_idx
CallerAccumulator
&
acc
)
{
active_clusters
[
caller_idx
]
+
=
acc
.
Any
(
)
;
per_cluster_
[
0
]
.
Get
(
caller_idx
)
.
AddFrom
(
acc
)
;
acc
=
CallerAccumulator
(
)
;
}
)
;
per_cluster_
[
cluster_idx
]
.
ResetBits
(
)
;
}
CallerAccumulator
total
;
for
(
size_t
caller_idx
:
per_cluster_
[
0
]
.
Sorted
(
)
)
{
CallerAccumulator
&
acc
=
per_cluster_
[
0
]
.
Get
(
caller_idx
)
;
total
.
AddFrom
(
acc
)
;
acc
.
PrintAndReset
(
callers_
.
Name
(
caller_idx
)
active_clusters
[
caller_idx
]
)
;
}
total
.
PrintTotal
(
)
;
per_cluster_
[
0
]
.
ResetBits
(
)
;
}
private
:
Shared
(
)
:
last_root_end_
(
timer_
)
send_config
(
callers_
.
Add
(
"
SendConfig
"
)
)
dtor
(
callers_
.
Add
(
"
PoolDtor
"
)
)
print_stats
(
callers_
.
Add
(
"
PrintStats
"
)
)
{
Profiler
:
:
Get
(
)
.
AddFunc
(
this
[
this
]
(
)
{
PrintAndReset
(
)
;
}
)
;
}
const
Timer
timer_
;
Stopwatch
last_root_end_
;
PerCluster
per_cluster_
[
kMaxClusters
]
;
StringTable
<
kMaxCallers
>
callers_
;
public
:
Caller
send_config
;
Caller
dtor
;
Caller
print_stats
;
}
;
#
else
struct
Stats
{
void
NotifyRunStatic
(
size_t
timer
:
:
Ticks
)
{
}
void
NotifyRunDynamic
(
size_t
size_t
size_t
timer
:
:
Ticks
)
{
}
void
NotifyThreadRun
(
size_t
timer
:
:
Ticks
size_t
timer
:
:
Ticks
timer
:
:
Ticks
)
{
}
void
NotifyMainRun
(
size_t
timer
:
:
Ticks
timer
:
:
Ticks
timer
:
:
Ticks
timer
:
:
Ticks
)
{
}
void
PrintAndReset
(
size_t
timer
:
:
Ticks
)
{
}
void
Reset
(
size_t
=
kMaxThreads
)
{
}
}
;
struct
Caller
{
}
;
class
Shared
{
public
:
static
HWY_DLLEXPORT
Shared
&
Get
(
)
;
Stopwatch
MakeStopwatch
(
)
const
{
return
Stopwatch
(
timer_
)
;
}
Caller
AddCaller
(
const
char
*
)
{
return
Caller
(
)
;
}
private
:
Shared
(
)
{
}
const
Timer
timer_
;
public
:
Caller
send_config
;
Caller
dtor
;
Caller
print_stats
;
}
;
#
endif
class
alignas
(
HWY_ALIGNMENT
)
Worker
{
static
constexpr
size_t
kMaxVictims
=
4
;
static
constexpr
auto
kAcq
=
std
:
:
memory_order_acquire
;
static
constexpr
auto
kRel
=
std
:
:
memory_order_release
;
bool
OwnsGlobalIdx
(
)
const
{
#
if
PROFILER_ENABLED
if
(
global_idx_
>
=
profiler
:
:
kMaxWorkers
)
{
HWY_WARN
(
"
Windows
-
only
bug
?
global_idx
%
zu
>
=
%
zu
.
"
global_idx_
profiler
:
:
kMaxWorkers
)
;
}
#
endif
if
(
cluster_idx_
=
=
kAllClusters
)
return
global_idx_
!
=
0
;
return
worker_
!
=
0
;
}
public
:
Worker
(
const
size_t
worker
const
size_t
num_threads
const
PoolWorkerMapping
mapping
const
Divisor64
&
div_workers
const
Stopwatch
&
stopwatch
)
:
workers_
(
this
-
worker
)
worker_
(
worker
)
num_threads_
(
num_threads
)
stopwatch_
(
stopwatch
)
global_idx_
(
num_threads
=
=
0
?
Profiler
:
:
GlobalIdx
(
)
:
mapping
(
worker
)
)
cluster_idx_
(
mapping
.
ClusterIdx
(
)
)
{
HWY_DASSERT
(
IsAligned
(
this
HWY_ALIGNMENT
)
)
;
HWY_DASSERT
(
worker
<
=
num_threads
)
;
const
size_t
num_workers
=
static_cast
<
size_t
>
(
div_workers
.
GetDivisor
(
)
)
;
num_victims_
=
static_cast
<
uint32_t
>
(
HWY_MIN
(
kMaxVictims
num_workers
)
)
;
const
uint32_t
coprime
=
ShuffledIota
:
:
FindAnotherCoprime
(
static_cast
<
uint32_t
>
(
num_workers
)
static_cast
<
uint32_t
>
(
(
worker
+
1
)
*
257
+
worker
*
13
)
)
;
const
ShuffledIota
shuffled_iota
(
coprime
)
;
victims_
[
0
]
=
static_cast
<
uint32_t
>
(
worker
)
;
for
(
uint32_t
i
=
1
;
i
<
num_victims_
;
+
+
i
)
{
victims_
[
i
]
=
shuffled_iota
.
Next
(
victims_
[
i
-
1
]
div_workers
)
;
HWY_DASSERT
(
victims_
[
i
]
!
=
worker
)
;
}
HWY_IF_CONSTEXPR
(
PROFILER_ENABLED
)
{
if
(
HWY_LIKELY
(
OwnsGlobalIdx
(
)
)
)
{
Profiler
:
:
Get
(
)
.
ReserveWorker
(
global_idx_
)
;
}
}
}
~
Worker
(
)
{
HWY_IF_CONSTEXPR
(
PROFILER_ENABLED
)
{
if
(
HWY_LIKELY
(
OwnsGlobalIdx
(
)
)
)
{
Profiler
:
:
Get
(
)
.
FreeWorker
(
global_idx_
)
;
}
}
}
Worker
(
const
Worker
&
)
=
delete
;
Worker
&
operator
=
(
const
Worker
&
)
=
delete
;
size_t
Index
(
)
const
{
return
worker_
;
}
Worker
*
AllWorkers
(
)
{
return
workers_
;
}
const
Worker
*
AllWorkers
(
)
const
{
return
workers_
;
}
size_t
NumThreads
(
)
const
{
return
num_threads_
;
}
size_t
GlobalIdx
(
)
const
{
return
global_idx_
;
}
size_t
ClusterIdx
(
)
const
{
return
cluster_idx_
;
}
void
SetStartTime
(
)
{
stopwatch_
.
Reset
(
)
;
}
timer
:
:
Ticks
ElapsedTime
(
)
{
return
stopwatch_
.
Elapsed
(
)
;
}
Config
NextConfig
(
)
const
{
return
next_config_
;
}
void
SetNextConfig
(
Config
copy
)
{
next_config_
=
copy
;
}
Exit
GetExit
(
)
const
{
return
exit_
;
}
void
SetExit
(
Exit
exit
)
{
exit_
=
exit
;
}
uint32_t
WorkerEpoch
(
)
const
{
return
worker_epoch_
;
}
uint32_t
AdvanceWorkerEpoch
(
)
{
return
+
+
worker_epoch_
;
}
void
SetRange
(
const
uint64_t
begin
const
uint64_t
end
)
{
my_begin_
.
store
(
begin
kRel
)
;
my_end_
.
store
(
end
kRel
)
;
}
uint64_t
MyEnd
(
)
const
{
return
my_end_
.
load
(
kAcq
)
;
}
Span
<
const
uint32_t
>
Victims
(
)
const
{
return
hwy
:
:
Span
<
const
uint32_t
>
(
victims_
.
data
(
)
static_cast
<
size_t
>
(
num_victims_
)
)
;
}
uint64_t
WorkerReserveTask
(
)
{
return
my_begin_
.
fetch_add
(
1
std
:
:
memory_order_relaxed
)
;
}
const
std
:
:
atomic
<
uint32_t
>
&
Waiter
(
)
const
{
return
wait_epoch_
;
}
std
:
:
atomic
<
uint32_t
>
&
MutableWaiter
(
)
{
return
wait_epoch_
;
}
void
StoreWaiter
(
uint32_t
epoch
)
{
wait_epoch_
.
store
(
epoch
kRel
)
;
}
const
std
:
:
atomic
<
uint32_t
>
&
Barrier
(
)
const
{
return
barrier_epoch_
;
}
void
StoreBarrier
(
uint32_t
epoch
)
{
barrier_epoch_
.
store
(
epoch
kRel
)
;
}
private
:
std
:
:
atomic
<
uint64_t
>
my_begin_
;
std
:
:
atomic
<
uint64_t
>
my_end_
;
Worker
*
const
workers_
;
const
size_t
worker_
;
const
size_t
num_threads_
;
Stopwatch
stopwatch_
;
const
size_t
global_idx_
;
const
size_t
cluster_idx_
;
std
:
:
atomic
<
uint32_t
>
wait_epoch_
{
1
}
;
std
:
:
atomic
<
uint32_t
>
barrier_epoch_
{
1
}
;
uint32_t
num_victims_
;
std
:
:
array
<
uint32_t
kMaxVictims
>
victims_
;
Config
next_config_
;
Exit
exit_
=
Exit
:
:
kNone
;
uint32_t
worker_epoch_
=
1
;
HWY_MEMBER_VAR_MAYBE_UNUSED
uint8_t
padding_
[
HWY_ALIGNMENT
-
56
-
6
*
sizeof
(
void
*
)
-
sizeof
(
victims_
)
]
;
}
;
static_assert
(
sizeof
(
Worker
)
=
=
HWY_ALIGNMENT
"
"
)
;
class
WorkerLifecycle
{
public
:
static
Worker
*
Init
(
uint8_t
*
storage
size_t
num_threads
PoolWorkerMapping
mapping
const
Divisor64
&
div_workers
Shared
&
shared
)
{
Worker
*
workers
=
new
(
storage
)
Worker
(
0
num_threads
mapping
div_workers
shared
.
MakeStopwatch
(
)
)
;
for
(
size_t
worker
=
1
;
worker
<
=
num_threads
;
+
+
worker
)
{
new
(
Addr
(
storage
worker
)
)
Worker
(
worker
num_threads
mapping
div_workers
shared
.
MakeStopwatch
(
)
)
;
HWY_DASSERT
(
reinterpret_cast
<
uintptr_t
>
(
workers
+
worker
)
=
=
reinterpret_cast
<
uintptr_t
>
(
Addr
(
storage
worker
)
)
)
;
}
std
:
:
atomic_thread_fence
(
std
:
:
memory_order_release
)
;
return
workers
;
}
static
void
Destroy
(
Worker
*
workers
size_t
num_threads
)
{
for
(
size_t
worker
=
0
;
worker
<
=
num_threads
;
+
+
worker
)
{
workers
[
worker
]
.
~
Worker
(
)
;
}
}
private
:
static
uint8_t
*
Addr
(
uint8_t
*
storage
size_t
worker
)
{
return
storage
+
worker
*
sizeof
(
Worker
)
;
}
}
;
class
Tasks
{
static
constexpr
auto
kAcq
=
std
:
:
memory_order_acquire
;
typedef
void
(
*
RunFunc
)
(
const
void
*
opaque
uint64_t
task
size_t
worker
)
;
public
:
Tasks
(
)
{
HWY_DASSERT
(
IsAligned
(
this
8
)
)
;
}
template
<
class
Closure
>
void
Set
(
uint64_t
begin
uint64_t
end
const
Closure
&
closure
)
{
constexpr
auto
kRel
=
std
:
:
memory_order_release
;
HWY_DASSERT
(
begin
<
=
end
)
;
begin_
.
store
(
begin
kRel
)
;
end_
.
store
(
end
kRel
)
;
func_
.
store
(
static_cast
<
RunFunc
>
(
&
CallClosure
<
Closure
>
)
kRel
)
;
opaque_
.
store
(
reinterpret_cast
<
const
void
*
>
(
&
closure
)
kRel
)
;
}
static
void
DivideRangeAmongWorkers
(
const
uint64_t
begin
const
uint64_t
end
const
Divisor64
&
div_workers
Worker
*
workers
)
{
const
size_t
num_workers
=
static_cast
<
size_t
>
(
div_workers
.
GetDivisor
(
)
)
;
HWY_DASSERT
(
num_workers
>
1
)
;
HWY_DASSERT
(
begin
<
=
end
)
;
const
size_t
num_tasks
=
static_cast
<
size_t
>
(
end
-
begin
)
;
const
size_t
min_tasks
=
static_cast
<
size_t
>
(
div_workers
.
Divide
(
num_tasks
)
)
;
const
size_t
remainder
=
static_cast
<
size_t
>
(
div_workers
.
Remainder
(
num_tasks
)
)
;
uint64_t
my_begin
=
begin
;
for
(
size_t
worker
=
0
;
worker
<
num_workers
;
+
+
worker
)
{
const
uint64_t
my_end
=
my_begin
+
min_tasks
+
(
worker
<
remainder
)
;
workers
[
worker
]
.
SetRange
(
my_begin
my_end
)
;
my_begin
=
my_end
;
}
HWY_DASSERT
(
my_begin
=
=
end
)
;
}
void
WorkerRun
(
Worker
*
worker
const
Shared
&
shared
Stats
&
stats
)
const
{
if
(
NumTasks
(
)
>
worker
-
>
NumThreads
(
)
+
1
)
{
WorkerRunDynamic
(
worker
shared
stats
)
;
}
else
{
WorkerRunStatic
(
worker
shared
stats
)
;
}
}
private
:
void
WorkerRunStatic
(
Worker
*
worker
const
Shared
&
shared
Stats
&
stats
)
const
{
const
uint64_t
begin
=
begin_
.
load
(
kAcq
)
;
const
uint64_t
end
=
end_
.
load
(
kAcq
)
;
HWY_DASSERT
(
begin
<
=
end
)
;
const
size_t
index
=
worker
-
>
Index
(
)
;
const
uint64_t
task
=
begin
+
index
;
if
(
HWY_LIKELY
(
task
<
end
)
)
{
const
void
*
opaque
=
Opaque
(
)
;
const
RunFunc
func
=
Func
(
)
;
Stopwatch
stopwatch
=
shared
.
MakeStopwatch
(
)
;
func
(
opaque
task
index
)
;
stats
.
NotifyRunStatic
(
index
stopwatch
.
Elapsed
(
)
)
;
}
}
void
WorkerRunDynamic
(
Worker
*
worker
const
Shared
&
shared
Stats
&
stats
)
const
{
Worker
*
workers
=
worker
-
>
AllWorkers
(
)
;
const
size_t
index
=
worker
-
>
Index
(
)
;
const
RunFunc
func
=
Func
(
)
;
const
void
*
opaque
=
Opaque
(
)
;
size_t
sum_tasks
=
0
;
size_t
sum_stolen
=
0
;
timer
:
:
Ticks
sum_d_func
=
0
;
for
(
uint32_t
victim
:
worker
-
>
Victims
(
)
)
{
Worker
*
other_worker
=
workers
+
victim
;
const
uint64_t
other_end
=
other_worker
-
>
MyEnd
(
)
;
for
(
;
;
)
{
const
uint64_t
task
=
other_worker
-
>
WorkerReserveTask
(
)
;
if
(
HWY_UNLIKELY
(
task
>
=
other_end
)
)
{
hwy
:
:
Pause
(
)
;
break
;
}
Stopwatch
stopwatch
=
shared
.
MakeStopwatch
(
)
;
func
(
opaque
task
index
)
;
sum_tasks
+
+
;
sum_stolen
+
=
worker
!
=
other_worker
;
sum_d_func
+
=
stopwatch
.
Elapsed
(
)
;
}
}
stats
.
NotifyRunDynamic
(
index
sum_tasks
sum_stolen
sum_d_func
)
;
}
size_t
NumTasks
(
)
const
{
return
static_cast
<
size_t
>
(
end_
.
load
(
kAcq
)
-
begin_
.
load
(
kAcq
)
)
;
}
const
void
*
Opaque
(
)
const
{
return
opaque_
.
load
(
kAcq
)
;
}
RunFunc
Func
(
)
const
{
return
func_
.
load
(
kAcq
)
;
}
template
<
class
Closure
>
static
void
CallClosure
(
const
void
*
opaque
uint64_t
task
size_t
worker
)
{
(
*
reinterpret_cast
<
const
Closure
*
>
(
opaque
)
)
(
task
worker
)
;
}
std
:
:
atomic
<
uint64_t
>
begin_
;
std
:
:
atomic
<
uint64_t
>
end_
;
std
:
:
atomic
<
RunFunc
>
func_
;
std
:
:
atomic
<
const
void
*
>
opaque_
;
}
;
static_assert
(
sizeof
(
Tasks
)
=
=
16
+
2
*
sizeof
(
void
*
)
"
"
)
;
struct
WaitBlock
{
void
WakeWorkers
(
Worker
*
workers
const
uint32_t
epoch
)
const
{
HWY_DASSERT
(
epoch
!
=
0
)
;
workers
[
1
]
.
StoreWaiter
(
epoch
)
;
WakeAll
(
workers
[
1
]
.
MutableWaiter
(
)
)
;
}
template
<
class
Spin
>
size_t
UntilWoken
(
const
Worker
&
worker
const
Spin
&
)
const
{
HWY_DASSERT
(
worker
.
Index
(
)
!
=
0
)
;
const
uint32_t
epoch
=
worker
.
WorkerEpoch
(
)
;
const
Worker
*
workers
=
worker
.
AllWorkers
(
)
;
BlockUntilDifferent
(
epoch
-
1
workers
[
1
]
.
Waiter
(
)
)
;
return
1
;
}
}
;
struct
WaitSpin1
{
void
WakeWorkers
(
Worker
*
workers
const
uint32_t
epoch
)
const
{
workers
[
1
]
.
StoreWaiter
(
epoch
)
;
}
template
<
class
Spin
>
size_t
UntilWoken
(
const
Worker
&
worker
const
Spin
&
spin
)
const
{
HWY_DASSERT
(
worker
.
Index
(
)
!
=
0
)
;
const
Worker
*
workers
=
worker
.
AllWorkers
(
)
;
const
uint32_t
epoch
=
worker
.
WorkerEpoch
(
)
;
return
spin
.
UntilEqual
(
epoch
workers
[
1
]
.
Waiter
(
)
)
;
}
}
;
struct
WaitSpinSeparate
{
void
WakeWorkers
(
Worker
*
workers
const
uint32_t
epoch
)
const
{
for
(
size_t
thread
=
0
;
thread
<
workers
-
>
NumThreads
(
)
;
+
+
thread
)
{
workers
[
1
+
thread
]
.
StoreWaiter
(
epoch
)
;
}
}
template
<
class
Spin
>
size_t
UntilWoken
(
const
Worker
&
worker
const
Spin
&
spin
)
const
{
HWY_DASSERT
(
worker
.
Index
(
)
!
=
0
)
;
const
uint32_t
epoch
=
worker
.
WorkerEpoch
(
)
;
return
spin
.
UntilEqual
(
epoch
worker
.
Waiter
(
)
)
;
}
}
;
template
<
class
Func
typename
.
.
.
Args
>
HWY_INLINE
void
CallWithConfig
(
const
Config
&
config
Func
&
&
func
Args
&
&
.
.
.
args
)
{
switch
(
config
.
wait_type
)
{
case
WaitType
:
:
kBlock
:
return
func
(
SpinPause
(
)
WaitBlock
(
)
std
:
:
forward
<
Args
>
(
args
)
.
.
.
)
;
case
WaitType
:
:
kSpin1
:
return
CallWithSpin
(
config
.
spin_type
func
WaitSpin1
(
)
std
:
:
forward
<
Args
>
(
args
)
.
.
.
)
;
case
WaitType
:
:
kSpinSeparate
:
return
CallWithSpin
(
config
.
spin_type
func
WaitSpinSeparate
(
)
std
:
:
forward
<
Args
>
(
args
)
.
.
.
)
;
case
WaitType
:
:
kSentinel
:
HWY_UNREACHABLE
;
}
}
class
Barrier
{
public
:
void
WorkerReached
(
Worker
&
worker
uint32_t
epoch
)
const
{
HWY_DASSERT
(
worker
.
Index
(
)
!
=
0
)
;
worker
.
StoreBarrier
(
epoch
)
;
}
bool
HasReached
(
const
Worker
*
worker
uint32_t
epoch
)
const
{
const
uint32_t
barrier
=
worker
-
>
Barrier
(
)
.
load
(
std
:
:
memory_order_acquire
)
;
HWY_DASSERT
(
barrier
<
=
epoch
)
;
return
barrier
=
=
epoch
;
}
template
<
class
Spin
>
void
UntilReached
(
size_t
num_threads
Worker
*
workers
const
Spin
&
spin
uint32_t
epoch
)
const
{
workers
[
0
]
.
StoreBarrier
(
epoch
)
;
for
(
size_t
i
=
0
;
i
<
num_threads
;
+
+
i
)
{
(
void
)
spin
.
UntilEqual
(
epoch
workers
[
1
+
i
]
.
Barrier
(
)
)
;
}
}
}
;
class
BusyFlag
{
public
:
void
Set
(
)
{
HWY_DASSERT
(
!
busy_
.
test_and_set
(
)
)
;
}
void
Clear
(
)
{
HWY_IF_CONSTEXPR
(
HWY_IS_DEBUG_BUILD
)
busy_
.
clear
(
)
;
}
private
:
std
:
:
atomic_flag
busy_
=
ATOMIC_FLAG_INIT
;
}
;
}
class
alignas
(
HWY_ALIGNMENT
)
ThreadPool
{
static
size_t
ClampedNumThreads
(
size_t
num_threads
)
{
if
(
HWY_UNLIKELY
(
num_threads
>
pool
:
:
kMaxThreads
)
)
{
HWY_WARN
(
"
ThreadPool
:
clamping
num_threads
%
zu
to
%
zu
.
"
num_threads
pool
:
:
kMaxThreads
)
;
num_threads
=
pool
:
:
kMaxThreads
;
}
return
num_threads
;
}
public
:
static
size_t
MaxThreads
(
)
{
LogicalProcessorSet
lps
;
if
(
GetThreadAffinity
(
lps
)
)
{
return
lps
.
Count
(
)
-
1
;
}
return
static_cast
<
size_t
>
(
std
:
:
thread
:
:
hardware_concurrency
(
)
-
1
)
;
}
ThreadPool
(
size_t
num_threads
PoolWorkerMapping
mapping
=
PoolWorkerMapping
(
)
)
:
num_threads_
(
ClampedNumThreads
(
num_threads
)
)
div_workers_
(
1
+
num_threads_
)
shared_
(
pool
:
:
Shared
:
:
Get
(
)
)
workers_
(
pool
:
:
WorkerLifecycle
:
:
Init
(
worker_bytes_
num_threads_
mapping
div_workers_
shared_
)
)
{
for
(
PoolWaitMode
mode
:
{
PoolWaitMode
:
:
kSpin
PoolWaitMode
:
:
kBlock
}
)
{
wait_mode_
=
mode
;
AutoTuner
(
)
.
SetCandidates
(
pool
:
:
Config
:
:
AllCandidates
(
mode
)
)
;
}
if
(
num_threads_
>
0
)
{
Profiler
:
:
Get
(
)
.
AddFunc
(
this
[
this
]
(
)
{
PrintStats
(
)
;
}
)
;
}
threads_
.
reserve
(
num_threads_
)
;
for
(
size_t
thread
=
0
;
thread
<
num_threads_
;
+
+
thread
)
{
threads_
.
emplace_back
(
ThreadFunc
(
workers_
[
1
+
thread
]
tasks_
shared_
stats_
)
)
;
}
SendConfig
(
AutoTuner
(
)
.
Candidates
(
)
[
0
]
)
;
}
~
ThreadPool
(
)
{
(
void
)
RunWithoutAutotune
(
0
NumWorkers
(
)
shared_
.
dtor
[
this
]
(
HWY_MAYBE_UNUSED
uint64_t
task
size_t
worker
)
{
HWY_DASSERT
(
task
=
=
worker
)
;
workers_
[
worker
]
.
SetExit
(
Exit
:
:
kThread
)
;
}
)
;
for
(
std
:
:
thread
&
thread
:
threads_
)
{
HWY_DASSERT
(
thread
.
joinable
(
)
)
;
thread
.
join
(
)
;
}
if
(
num_threads_
>
0
)
{
Profiler
:
:
Get
(
)
.
RemoveFunc
(
this
)
;
}
pool
:
:
WorkerLifecycle
:
:
Destroy
(
workers_
num_threads_
)
;
}
ThreadPool
(
const
ThreadPool
&
)
=
delete
;
ThreadPool
&
operator
&
(
const
ThreadPool
&
)
=
delete
;
size_t
NumWorkers
(
)
const
{
return
static_cast
<
size_t
>
(
div_workers_
.
GetDivisor
(
)
)
;
}
void
SetWaitMode
(
PoolWaitMode
mode
)
{
wait_mode_
=
mode
;
SendConfig
(
AutoTuneComplete
(
)
?
*
AutoTuner
(
)
.
Best
(
)
:
AutoTuner
(
)
.
NextConfig
(
)
)
;
}
pool
:
:
Config
config
(
)
const
{
return
workers_
[
0
]
.
NextConfig
(
)
;
}
bool
AutoTuneComplete
(
)
const
{
return
AutoTuner
(
)
.
Best
(
)
;
}
Span
<
CostDistribution
>
AutoTuneCosts
(
)
{
return
AutoTuner
(
)
.
Costs
(
)
;
}
static
pool
:
:
Caller
AddCaller
(
const
char
*
name
)
{
return
pool
:
:
Shared
:
:
Get
(
)
.
AddCaller
(
name
)
;
}
template
<
class
Closure
>
void
Run
(
uint64_t
begin
uint64_t
end
pool
:
:
Caller
caller
const
Closure
&
closure
)
{
AutoTuneT
&
auto_tuner
=
AutoTuner
(
)
;
if
(
HWY_LIKELY
(
auto_tuner
.
Best
(
)
)
)
{
(
void
)
RunWithoutAutotune
(
begin
end
caller
closure
)
;
return
;
}
Stopwatch
stopwatch
(
shared_
.
MakeStopwatch
(
)
)
;
if
(
!
RunWithoutAutotune
(
begin
end
caller
closure
)
)
return
;
auto_tuner
.
NotifyCost
(
stopwatch
.
Elapsed
(
)
)
;
pool
:
:
Config
next
=
auto_tuner
.
NextConfig
(
)
;
if
(
auto_tuner
.
Best
(
)
)
{
next
=
*
auto_tuner
.
Best
(
)
;
HWY_IF_CONSTEXPR
(
pool
:
:
kVerbosity
>
=
1
)
{
const
size_t
idx_best
=
static_cast
<
size_t
>
(
auto_tuner
.
Best
(
)
-
auto_tuner
.
Candidates
(
)
.
data
(
)
)
;
HWY_DASSERT
(
idx_best
<
auto_tuner
.
Costs
(
)
.
size
(
)
)
;
auto
&
AT
=
auto_tuner
.
Costs
(
)
[
idx_best
]
;
const
double
best_cost
=
AT
.
EstimateCost
(
)
;
HWY_DASSERT
(
best_cost
>
0
.
0
)
;
Stats
s_ratio
;
for
(
size_t
i
=
0
;
i
<
auto_tuner
.
Costs
(
)
.
size
(
)
;
+
+
i
)
{
if
(
i
=
=
idx_best
)
continue
;
const
double
cost
=
auto_tuner
.
Costs
(
)
[
i
]
.
EstimateCost
(
)
;
s_ratio
.
Notify
(
static_cast
<
float
>
(
cost
/
best_cost
)
)
;
}
fprintf
(
stderr
"
Pool
%
3zu
:
%
s
%
8
.
0f
+
/
-
%
6
.
0f
.
Gain
%
.
2fx
[
%
.
2fx
%
.
2fx
]
\
n
"
NumWorkers
(
)
auto_tuner
.
Best
(
)
-
>
ToString
(
)
.
c_str
(
)
best_cost
AT
.
Stddev
(
)
s_ratio
.
GeometricMean
(
)
static_cast
<
double
>
(
s_ratio
.
Min
(
)
)
static_cast
<
double
>
(
s_ratio
.
Max
(
)
)
)
;
}
}
SendConfig
(
next
)
;
}
template
<
class
Closure
>
void
Run
(
uint64_t
begin
uint64_t
end
const
Closure
&
closure
)
{
Run
(
begin
end
pool
:
:
Caller
(
)
closure
)
;
}
private
:
struct
MainWakeAndBarrier
{
template
<
class
Spin
class
Wait
>
void
operator
(
)
(
const
Spin
&
spin
const
Wait
&
wait
pool
:
:
Worker
&
main
const
pool
:
:
Tasks
&
tasks
const
pool
:
:
Shared
&
shared
pool
:
:
Stats
&
stats
)
const
{
const
pool
:
:
Barrier
barrier
;
pool
:
:
Worker
*
workers
=
main
.
AllWorkers
(
)
;
HWY_DASSERT
(
&
main
=
=
main
.
AllWorkers
(
)
)
;
const
size_t
num_threads
=
main
.
NumThreads
(
)
;
const
uint32_t
epoch
=
main
.
AdvanceWorkerEpoch
(
)
;
HWY_IF_CONSTEXPR
(
HWY_IS_DEBUG_BUILD
)
{
for
(
size_t
i
=
0
;
i
<
1
+
num_threads
;
+
+
i
)
{
HWY_DASSERT
(
!
barrier
.
HasReached
(
workers
+
i
epoch
)
)
;
}
}
Stopwatch
stopwatch
(
shared
.
MakeStopwatch
(
)
)
;
const
timer
:
:
Ticks
t_before_wake
=
stopwatch
.
Origin
(
)
;
wait
.
WakeWorkers
(
workers
epoch
)
;
const
timer
:
:
Ticks
d_wake
=
stopwatch
.
Elapsed
(
)
;
tasks
.
WorkerRun
(
&
main
shared
stats
)
;
const
timer
:
:
Ticks
d_run
=
stopwatch
.
Elapsed
(
)
;
barrier
.
UntilReached
(
num_threads
workers
spin
epoch
)
;
const
timer
:
:
Ticks
d_barrier
=
stopwatch
.
Elapsed
(
)
;
stats
.
NotifyMainRun
(
main
.
NumThreads
(
)
t_before_wake
d_wake
d_run
d_barrier
)
;
HWY_IF_CONSTEXPR
(
HWY_IS_DEBUG_BUILD
)
{
for
(
size_t
i
=
0
;
i
<
1
+
num_threads
;
+
+
i
)
{
HWY_DASSERT
(
barrier
.
HasReached
(
workers
+
i
epoch
)
)
;
}
}
}
}
;
class
ThreadFunc
{
struct
WorkerLoop
{
template
<
class
Spin
class
Wait
>
void
operator
(
)
(
const
Spin
&
spin
const
Wait
&
wait
pool
:
:
Worker
&
worker
pool
:
:
Tasks
&
tasks
const
pool
:
:
Shared
&
shared
pool
:
:
Stats
&
stats
)
const
{
do
{
const
uint32_t
epoch
=
worker
.
AdvanceWorkerEpoch
(
)
;
Stopwatch
stopwatch
(
shared
.
MakeStopwatch
(
)
)
;
const
size_t
wait_reps
=
wait
.
UntilWoken
(
worker
spin
)
;
const
timer
:
:
Ticks
d_wait
=
stopwatch
.
Elapsed
(
)
;
const
timer
:
:
Ticks
t_before_run
=
stopwatch
.
Origin
(
)
;
tasks
.
WorkerRun
(
&
worker
shared
stats
)
;
const
timer
:
:
Ticks
d_run
=
stopwatch
.
Elapsed
(
)
;
stats
.
NotifyThreadRun
(
worker
.
Index
(
)
d_wait
wait_reps
t_before_run
d_run
)
;
pool
:
:
Barrier
(
)
.
WorkerReached
(
worker
epoch
)
;
}
while
(
worker
.
GetExit
(
)
=
=
Exit
:
:
kNone
)
;
}
}
;
public
:
ThreadFunc
(
pool
:
:
Worker
&
worker
pool
:
:
Tasks
&
tasks
const
pool
:
:
Shared
&
shared
pool
:
:
Stats
&
stats
)
:
worker_
(
worker
)
tasks_
(
tasks
)
shared_
(
shared
)
stats_
(
stats
)
{
}
void
operator
(
)
(
)
{
std
:
:
atomic_thread_fence
(
std
:
:
memory_order_acquire
)
;
HWY_DASSERT
(
worker_
.
Index
(
)
!
=
0
)
;
SetThreadName
(
"
worker
%
03zu
"
static_cast
<
int
>
(
worker_
.
Index
(
)
-
1
)
)
;
worker_
.
SetStartTime
(
)
;
Profiler
&
profiler
=
Profiler
:
:
Get
(
)
;
profiler
.
SetGlobalIdx
(
worker_
.
GlobalIdx
(
)
)
;
for
(
;
;
)
{
CallWithConfig
(
worker_
.
NextConfig
(
)
WorkerLoop
(
)
worker_
tasks_
shared_
stats_
)
;
if
(
worker_
.
GetExit
(
)
=
=
Exit
:
:
kThread
)
break
;
worker_
.
SetExit
(
Exit
:
:
kNone
)
;
}
profiler
.
SetGlobalIdx
(
~
size_t
{
0
}
)
;
}
private
:
pool
:
:
Worker
&
worker_
;
pool
:
:
Tasks
&
tasks_
;
const
pool
:
:
Shared
&
shared_
;
pool
:
:
Stats
&
stats_
;
}
;
void
PrintStats
(
)
{
std
:
:
atomic
<
timer
:
:
Ticks
>
sum_thread_elapsed
{
0
}
;
(
void
)
RunWithoutAutotune
(
0
NumWorkers
(
)
shared_
.
print_stats
[
this
&
sum_thread_elapsed
]
(
HWY_MAYBE_UNUSED
uint64_t
task
size_t
worker
)
{
HWY_DASSERT
(
task
=
=
worker
)
;
if
(
worker
!
=
0
)
{
sum_thread_elapsed
.
fetch_add
(
workers_
[
worker
]
.
ElapsedTime
(
)
)
;
}
}
)
;
const
timer
:
:
Ticks
thread_total
=
sum_thread_elapsed
.
load
(
std
:
:
memory_order_acquire
)
;
stats_
.
PrintAndReset
(
num_threads_
thread_total
)
;
}
template
<
class
Closure
>
bool
RunWithoutAutotune
(
uint64_t
begin
uint64_t
end
pool
:
:
Caller
caller
const
Closure
&
closure
)
{
pool
:
:
Worker
&
main
=
workers_
[
0
]
;
const
size_t
num_tasks
=
static_cast
<
size_t
>
(
end
-
begin
)
;
const
size_t
num_workers
=
NumWorkers
(
)
;
if
(
HWY_UNLIKELY
(
num_tasks
<
=
1
|
|
num_workers
=
=
1
)
)
{
for
(
uint64_t
task
=
begin
;
task
<
end
;
+
+
task
)
{
closure
(
task
0
)
;
}
return
false
;
}
busy_
.
Set
(
)
;
#
if
PROFILER_ENABLED
const
bool
is_root
=
PROFILER_IS_ROOT_RUN
(
)
;
Stopwatch
stopwatch
(
shared_
.
MakeStopwatch
(
)
)
;
const
timer
:
:
Ticks
wait_before
=
is_root
?
shared_
.
LastRootEnd
(
)
.
Elapsed
(
)
:
0
;
#
endif
tasks_
.
Set
(
begin
end
closure
)
;
if
(
HWY_LIKELY
(
num_tasks
>
num_workers
)
)
{
pool
:
:
Tasks
:
:
DivideRangeAmongWorkers
(
begin
end
div_workers_
workers_
)
;
}
CallWithConfig
(
config
(
)
MainWakeAndBarrier
(
)
main
tasks_
shared_
stats_
)
;
#
if
PROFILER_ENABLED
pool
:
:
CallerAccumulator
&
acc
=
shared_
.
Cluster
(
main
.
ClusterIdx
(
)
)
.
Get
(
caller
.
Idx
(
)
)
;
acc
.
Add
(
num_tasks
num_workers
is_root
wait_before
stopwatch
.
Elapsed
(
)
)
;
if
(
is_root
)
{
PROFILER_END_ROOT_RUN
(
)
;
shared_
.
LastRootEnd
(
)
.
Reset
(
)
;
}
#
else
(
void
)
caller
;
#
endif
busy_
.
Clear
(
)
;
return
true
;
}
HWY_NOINLINE
void
SendConfig
(
pool
:
:
Config
next_config
)
{
(
void
)
RunWithoutAutotune
(
0
NumWorkers
(
)
shared_
.
send_config
[
this
next_config
]
(
HWY_MAYBE_UNUSED
uint64_t
task
size_t
worker
)
{
HWY_DASSERT
(
task
=
=
worker
)
;
workers_
[
worker
]
.
SetNextConfig
(
next_config
)
;
workers_
[
worker
]
.
SetExit
(
Exit
:
:
kLoop
)
;
}
)
;
workers_
[
0
]
.
SetNextConfig
(
next_config
)
;
}
using
AutoTuneT
=
AutoTune
<
pool
:
:
Config
30
>
;
AutoTuneT
&
AutoTuner
(
)
{
static_assert
(
static_cast
<
size_t
>
(
PoolWaitMode
:
:
kBlock
)
=
=
1
"
"
)
;
return
auto_tune_
[
static_cast
<
size_t
>
(
wait_mode_
)
-
1
]
;
}
const
AutoTuneT
&
AutoTuner
(
)
const
{
return
auto_tune_
[
static_cast
<
size_t
>
(
wait_mode_
)
-
1
]
;
}
const
size_t
num_threads_
;
const
Divisor64
div_workers_
;
pool
:
:
Shared
&
shared_
;
pool
:
:
Worker
*
const
workers_
;
alignas
(
HWY_ALIGNMENT
)
pool
:
:
Stats
stats_
;
alignas
(
HWY_ALIGNMENT
)
pool
:
:
Tasks
tasks_
;
HWY_MEMBER_VAR_MAYBE_UNUSED
char
padding_
[
HWY_ALIGNMENT
-
sizeof
(
pool
:
:
Tasks
)
]
;
pool
:
:
BusyFlag
busy_
;
std
:
:
vector
<
std
:
:
thread
>
threads_
;
PoolWaitMode
wait_mode_
;
AutoTuneT
auto_tune_
[
2
]
;
alignas
(
HWY_ALIGNMENT
)
uint8_t
worker_bytes_
[
sizeof
(
pool
:
:
Worker
)
*
(
pool
:
:
kMaxThreads
+
1
)
]
;
}
;
}
#
endif
