#
include
<
riscv_vector
.
h
>
#
include
"
hwy
/
ops
/
shared
-
inl
.
h
"
HWY_BEFORE_NAMESPACE
(
)
;
namespace
hwy
{
namespace
HWY_NAMESPACE
{
#
ifdef
__riscv_zvfhmin
#
define
HWY_RVV_HAVE_F16C
1
#
else
#
define
HWY_RVV_HAVE_F16C
0
#
endif
template
<
class
V
>
struct
DFromV_t
{
}
;
template
<
class
V
>
using
DFromV
=
typename
DFromV_t
<
RemoveConst
<
V
>
>
:
:
type
;
template
<
class
V
>
using
TFromV
=
TFromD
<
DFromV
<
V
>
>
;
template
<
typename
T
size_t
N
int
kPow2
>
constexpr
size_t
MLenFromD
(
Simd
<
T
N
kPow2
>
)
{
return
HWY_MIN
(
64
sizeof
(
T
)
*
8
*
8
/
detail
:
:
ScaleByPower
(
8
kPow2
)
)
;
}
namespace
detail
{
template
<
class
D
>
class
AdjustSimdTagToMinVecPow2_t
{
}
;
template
<
typename
T
size_t
N
int
kPow2
>
class
AdjustSimdTagToMinVecPow2_t
<
Simd
<
T
N
kPow2
>
>
{
private
:
using
D
=
Simd
<
T
N
kPow2
>
;
static
constexpr
int
kMinVecPow2
=
-
3
+
static_cast
<
int
>
(
FloorLog2
(
sizeof
(
T
)
)
)
;
static
constexpr
size_t
kNumMaxLanes
=
HWY_MAX_LANES_D
(
D
)
;
static
constexpr
int
kNewPow2
=
HWY_MAX
(
kPow2
kMinVecPow2
)
;
static
constexpr
size_t
kNewN
=
D
:
:
template
NewN
<
kNewPow2
kNumMaxLanes
>
(
)
;
public
:
using
type
=
Simd
<
T
kNewN
kNewPow2
>
;
}
;
template
<
class
D
>
using
AdjustSimdTagToMinVecPow2
=
typename
AdjustSimdTagToMinVecPow2_t
<
RemoveConst
<
D
>
>
:
:
type
;
}
namespace
detail
{
#
define
HWY_RVV_FOREACH_B
(
X_MACRO
NAME
OP
)
\
X_MACRO
(
64
0
64
NAME
OP
)
\
X_MACRO
(
32
0
32
NAME
OP
)
\
X_MACRO
(
16
0
16
NAME
OP
)
\
X_MACRO
(
8
0
8
NAME
OP
)
\
X_MACRO
(
8
1
4
NAME
OP
)
\
X_MACRO
(
8
2
2
NAME
OP
)
\
X_MACRO
(
8
3
1
NAME
OP
)
#
define
HWY_RVV_FOREACH_08_TRUNC
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
mf4
mf2
mf8
-
2
/
*
MLEN
=
*
/
32
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
mf2
m1
mf4
-
1
/
*
MLEN
=
*
/
16
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
m1
m2
mf2
0
/
*
MLEN
=
*
/
8
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
m2
m4
m1
1
/
*
MLEN
=
*
/
4
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
m4
m8
m2
2
/
*
MLEN
=
*
/
2
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
m8
__
m4
3
/
*
MLEN
=
*
/
1
NAME
OP
)
#
define
HWY_RVV_FOREACH_16_TRUNC
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
mf2
m1
mf4
-
1
/
*
MLEN
=
*
/
32
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
m1
m2
mf2
0
/
*
MLEN
=
*
/
16
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
m2
m4
m1
1
/
*
MLEN
=
*
/
8
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
m4
m8
m2
2
/
*
MLEN
=
*
/
4
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
m8
__
m4
3
/
*
MLEN
=
*
/
2
NAME
OP
)
#
define
HWY_RVV_FOREACH_32_TRUNC
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
m1
m2
mf2
0
/
*
MLEN
=
*
/
32
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
m2
m4
m1
1
/
*
MLEN
=
*
/
16
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
m4
m8
m2
2
/
*
MLEN
=
*
/
8
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
m8
__
m4
3
/
*
MLEN
=
*
/
4
NAME
OP
)
#
define
HWY_RVV_FOREACH_64_TRUNC
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
64
__
32
m2
m4
m1
1
/
*
MLEN
=
*
/
32
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
64
__
32
m4
m8
m2
2
/
*
MLEN
=
*
/
16
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
64
__
32
m8
__
m4
3
/
*
MLEN
=
*
/
8
NAME
OP
)
#
define
HWY_RVV_FOREACH_08_DEMOTE
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
mf4
mf2
mf8
-
2
/
*
MLEN
=
*
/
32
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
mf2
m1
mf4
-
1
/
*
MLEN
=
*
/
16
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
m1
m2
mf2
0
/
*
MLEN
=
*
/
8
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
m2
m4
m1
1
/
*
MLEN
=
*
/
4
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
m4
m8
m2
2
/
*
MLEN
=
*
/
2
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
m8
__
m4
3
/
*
MLEN
=
*
/
1
NAME
OP
)
#
define
HWY_RVV_FOREACH_16_DEMOTE
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
mf4
mf2
mf8
-
2
/
*
MLEN
=
*
/
64
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
mf2
m1
mf4
-
1
/
*
MLEN
=
*
/
32
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
m1
m2
mf2
0
/
*
MLEN
=
*
/
16
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
m2
m4
m1
1
/
*
MLEN
=
*
/
8
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
m4
m8
m2
2
/
*
MLEN
=
*
/
4
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
m8
__
m4
3
/
*
MLEN
=
*
/
2
NAME
OP
)
#
define
HWY_RVV_FOREACH_32_DEMOTE
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
mf2
m1
mf4
-
1
/
*
MLEN
=
*
/
64
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
m1
m2
mf2
0
/
*
MLEN
=
*
/
32
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
m2
m4
m1
1
/
*
MLEN
=
*
/
16
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
m4
m8
m2
2
/
*
MLEN
=
*
/
8
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
m8
__
m4
3
/
*
MLEN
=
*
/
4
NAME
OP
)
#
define
HWY_RVV_FOREACH_64_DEMOTE
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
64
__
32
m1
m2
mf2
0
/
*
MLEN
=
*
/
64
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
64
__
32
m2
m4
m1
1
/
*
MLEN
=
*
/
32
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
64
__
32
m4
m8
m2
2
/
*
MLEN
=
*
/
16
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
64
__
32
m8
__
m4
3
/
*
MLEN
=
*
/
8
NAME
OP
)
#
define
HWY_RVV_FOREACH_08_LE2
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
mf8
mf4
__
-
3
/
*
MLEN
=
*
/
64
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
mf4
mf2
mf8
-
2
/
*
MLEN
=
*
/
32
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
mf2
m1
mf4
-
1
/
*
MLEN
=
*
/
16
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
m1
m2
mf2
0
/
*
MLEN
=
*
/
8
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
m2
m4
m1
1
/
*
MLEN
=
*
/
4
NAME
OP
)
#
define
HWY_RVV_FOREACH_16_LE2
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
mf4
mf2
mf8
-
2
/
*
MLEN
=
*
/
64
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
mf2
m1
mf4
-
1
/
*
MLEN
=
*
/
32
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
m1
m2
mf2
0
/
*
MLEN
=
*
/
16
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
m2
m4
m1
1
/
*
MLEN
=
*
/
8
NAME
OP
)
#
define
HWY_RVV_FOREACH_32_LE2
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
mf2
m1
mf4
-
1
/
*
MLEN
=
*
/
64
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
m1
m2
mf2
0
/
*
MLEN
=
*
/
32
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
m2
m4
m1
1
/
*
MLEN
=
*
/
16
NAME
OP
)
#
define
HWY_RVV_FOREACH_64_LE2
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
64
__
32
m1
m2
mf2
0
/
*
MLEN
=
*
/
64
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
64
__
32
m2
m4
m1
1
/
*
MLEN
=
*
/
32
NAME
OP
)
#
define
HWY_RVV_FOREACH_08_EXT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_08_LE2
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
m4
m8
m2
2
/
*
MLEN
=
*
/
2
NAME
OP
)
#
define
HWY_RVV_FOREACH_16_EXT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_16_LE2
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
m4
m8
m2
2
/
*
MLEN
=
*
/
4
NAME
OP
)
#
define
HWY_RVV_FOREACH_32_EXT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_32_LE2
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
m4
m8
m2
2
/
*
MLEN
=
*
/
8
NAME
OP
)
#
define
HWY_RVV_FOREACH_64_EXT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_64_LE2
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
64
__
32
m4
m8
m2
2
/
*
MLEN
=
*
/
16
NAME
OP
)
#
define
HWY_RVV_FOREACH_08_ALL
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_08_EXT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
8
16
__
m8
__
m4
3
/
*
MLEN
=
*
/
1
NAME
OP
)
#
define
HWY_RVV_FOREACH_16_ALL
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_16_EXT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
m8
__
m4
3
/
*
MLEN
=
*
/
2
NAME
OP
)
#
define
HWY_RVV_FOREACH_32_ALL
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_32_EXT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
m8
__
m4
3
/
*
MLEN
=
*
/
4
NAME
OP
)
#
define
HWY_RVV_FOREACH_64_ALL
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_64_EXT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
64
__
32
m8
__
m4
3
/
*
MLEN
=
*
/
8
NAME
OP
)
#
define
HWY_RVV_FOREACH_08_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_16_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
16
32
8
mf4
mf2
mf8
-
3
/
*
MLEN
=
*
/
64
NAME
OP
)
#
define
HWY_RVV_FOREACH_32_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
32
64
16
mf2
m1
mf4
-
2
/
*
MLEN
=
*
/
64
NAME
OP
)
#
define
HWY_RVV_FOREACH_64_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
X_MACRO
(
BASE
CHAR
64
__
32
m1
m2
mf2
-
1
/
*
MLEN
=
*
/
64
NAME
OP
)
#
define
HWY_RVV_FOREACH_08_ALL_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_08_ALL
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_08_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_16_ALL_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_16_ALL
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_16_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_32_ALL_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_32_ALL
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_32_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_64_ALL_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_64_ALL
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_64_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_08_LE2_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_08_LE2
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_08_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_16_LE2_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_16_LE2
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_16_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_32_LE2_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_32_LE2
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_32_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_64_LE2_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_64_LE2
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_64_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_08_EXT_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_08_EXT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_08_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_16_EXT_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_16_EXT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_16_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_32_EXT_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_32_EXT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_32_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_64_EXT_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_64_EXT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_64_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_08_DEMOTE_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_08_DEMOTE
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_08_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_16_DEMOTE_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_16_DEMOTE
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_16_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_32_DEMOTE_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_32_DEMOTE
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_32_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_64_DEMOTE_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_64_DEMOTE
(
X_MACRO
BASE
CHAR
NAME
OP
)
\
HWY_RVV_FOREACH_64_VIRT
(
X_MACRO
BASE
CHAR
NAME
OP
)
#
define
HWY_RVV_FOREACH_U08
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_CONCAT
(
HWY_RVV_FOREACH_08
LMULS
)
(
X_MACRO
uint
u
NAME
OP
)
#
define
HWY_RVV_FOREACH_U16
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_CONCAT
(
HWY_RVV_FOREACH_16
LMULS
)
(
X_MACRO
uint
u
NAME
OP
)
#
define
HWY_RVV_FOREACH_U32
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_CONCAT
(
HWY_RVV_FOREACH_32
LMULS
)
(
X_MACRO
uint
u
NAME
OP
)
#
define
HWY_RVV_FOREACH_U64
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_CONCAT
(
HWY_RVV_FOREACH_64
LMULS
)
(
X_MACRO
uint
u
NAME
OP
)
#
define
HWY_RVV_FOREACH_I08
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_CONCAT
(
HWY_RVV_FOREACH_08
LMULS
)
(
X_MACRO
int
i
NAME
OP
)
#
define
HWY_RVV_FOREACH_I16
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_CONCAT
(
HWY_RVV_FOREACH_16
LMULS
)
(
X_MACRO
int
i
NAME
OP
)
#
define
HWY_RVV_FOREACH_I32
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_CONCAT
(
HWY_RVV_FOREACH_32
LMULS
)
(
X_MACRO
int
i
NAME
OP
)
#
define
HWY_RVV_FOREACH_I64
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_CONCAT
(
HWY_RVV_FOREACH_64
LMULS
)
(
X_MACRO
int
i
NAME
OP
)
#
define
HWY_RVV_FOREACH_F16_UNCONDITIONAL
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_CONCAT
(
HWY_RVV_FOREACH_16
LMULS
)
(
X_MACRO
float
f
NAME
OP
)
#
if
HWY_HAVE_FLOAT16
#
define
HWY_RVV_FOREACH_F16
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_F16_UNCONDITIONAL
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_IF_EMULATED_D
(
D
)
HWY_IF_BF16_D
(
D
)
#
else
#
define
HWY_RVV_FOREACH_F16
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_IF_EMULATED_D
(
D
)
HWY_IF_SPECIAL_FLOAT_D
(
D
)
#
endif
#
define
HWY_RVV_FOREACH_F32
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_CONCAT
(
HWY_RVV_FOREACH_32
LMULS
)
(
X_MACRO
float
f
NAME
OP
)
#
define
HWY_RVV_FOREACH_F64
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_CONCAT
(
HWY_RVV_FOREACH_64
LMULS
)
(
X_MACRO
float
f
NAME
OP
)
#
define
HWY_RVV_FOREACH_UI08
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_U08
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_I08
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_FOREACH_UI16
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_U16
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_I16
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_FOREACH_UI32
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_U32
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_I32
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_FOREACH_UI64
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_U64
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_I64
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_FOREACH_UI3264
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_UI32
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_UI64
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_FOREACH_U163264
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_U16
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_U32
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_U64
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_FOREACH_I163264
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_I16
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_I32
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_I64
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_FOREACH_UI163264
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_U163264
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_I163264
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_FOREACH_F3264
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_F32
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_F64
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_FOREACH_U
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_U08
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_U163264
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_FOREACH_I
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_I08
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_I163264
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_FOREACH_F
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_F16
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_F3264
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_FOREACH_UI
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_U
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_I
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_FOREACH
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_UI
(
X_MACRO
NAME
OP
LMULS
)
\
HWY_RVV_FOREACH_F
(
X_MACRO
NAME
OP
LMULS
)
#
define
HWY_RVV_T
(
BASE
SEW
)
BASE
#
#
SEW
#
#
_t
#
define
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
Simd
<
HWY_RVV_T
(
BASE
SEW
)
N
SHIFT
>
#
define
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
#
#
BASE
#
#
SEW
#
#
LMUL
#
#
_t
#
define
HWY_RVV_TUP
(
BASE
SEW
LMUL
TUP
)
v
#
#
BASE
#
#
SEW
#
#
LMUL
#
#
x
#
#
TUP
#
#
_t
#
define
HWY_RVV_M
(
MLEN
)
vbool
#
#
MLEN
#
#
_t
}
#
define
HWY_SPECIALIZE
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
>
\
struct
DFromV_t
<
HWY_RVV_V
(
BASE
SEW
LMUL
)
>
{
\
using
Lane
=
HWY_RVV_T
(
BASE
SEW
)
;
\
using
type
=
ScalableTag
<
Lane
SHIFT
>
;
\
}
;
HWY_RVV_FOREACH
(
HWY_SPECIALIZE
_
_
_ALL
)
#
undef
HWY_SPECIALIZE
#
define
HWY_RVV_LANES
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
size_t
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
)
{
\
constexpr
size_t
kFull
=
HWY_LANES
(
HWY_RVV_T
(
BASE
SEW
)
)
;
\
constexpr
size_t
kCap
=
MaxLanes
(
d
)
;
\
/
*
If
no
cap
avoid
generating
a
constant
by
using
VLMAX
.
*
/
\
return
N
=
=
kFull
?
__riscv_vsetvlmax_e
#
#
SEW
#
#
LMUL
(
)
\
:
__riscv_vsetvl_e
#
#
SEW
#
#
LMUL
(
kCap
)
;
\
}
\
template
<
size_t
N
>
\
HWY_API
size_t
Capped
#
#
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
size_t
cap
)
{
\
/
*
If
no
cap
avoid
the
HWY_MIN
.
*
/
\
return
detail
:
:
IsFull
(
d
)
\
?
__riscv_vsetvl_e
#
#
SEW
#
#
LMUL
(
cap
)
\
:
__riscv_vsetvl_e
#
#
SEW
#
#
LMUL
(
HWY_MIN
(
cap
MaxLanes
(
d
)
)
)
;
\
}
#
define
HWY_RVV_LANES_VIRT
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
size_t
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
)
{
\
constexpr
size_t
kCap
=
MaxLanes
(
d
)
;
\
/
*
In
case
of
virtual
LMUL
(
intrinsics
do
not
provide
"
uint16mf8_t
"
)
*
/
\
/
*
vsetvl
may
or
may
not
be
correct
so
do
it
ourselves
.
*
/
\
const
size_t
actual
=
\
detail
:
:
ScaleByPower
(
__riscv_vlenb
(
)
/
(
SEW
/
8
)
SHIFT
)
;
\
return
HWY_MIN
(
actual
kCap
)
;
\
}
\
template
<
size_t
N
>
\
HWY_API
size_t
Capped
#
#
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
size_t
cap
)
{
\
/
*
In
case
of
virtual
LMUL
(
intrinsics
do
not
provide
"
uint16mf8_t
"
)
*
/
\
/
*
vsetvl
may
or
may
not
be
correct
so
do
it
ourselves
.
*
/
\
const
size_t
actual
=
\
detail
:
:
ScaleByPower
(
__riscv_vlenb
(
)
/
(
SEW
/
8
)
SHIFT
)
;
\
/
*
If
no
cap
avoid
an
extra
HWY_MIN
.
*
/
\
return
detail
:
:
IsFull
(
d
)
?
HWY_MIN
(
actual
cap
)
\
:
HWY_MIN
(
HWY_MIN
(
actual
cap
)
MaxLanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_LANES
Lanes
setvlmax_e
_ALL
)
HWY_RVV_FOREACH
(
HWY_RVV_LANES_VIRT
Lanes
lenb
_VIRT
)
#
undef
HWY_RVV_LANES
#
undef
HWY_RVV_LANES_VIRT
template
<
class
D
HWY_RVV_IF_EMULATED_D
(
D
)
>
HWY_API
size_t
Lanes
(
D
)
{
return
Lanes
(
RebindToUnsigned
<
D
>
(
)
)
;
}
#
define
HWY_RVV_AVL
(
SEW
SHIFT
)
\
Lanes
(
ScalableTag
<
HWY_RVV_T
(
uint
SEW
)
SHIFT
>
(
)
)
#
define
HWY_RVV_RETV_ARGV
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
v
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
#
define
HWY_RVV_RETV_ARGVS
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
a
HWY_RVV_T
(
BASE
SEW
)
b
)
{
\
return
__riscv_v
#
#
OP
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
a
b
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
#
define
HWY_RVV_RETV_ARGVV
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
a
HWY_RVV_V
(
BASE
SEW
LMUL
)
b
)
{
\
return
__riscv_v
#
#
OP
#
#
_vv_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
a
b
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
#
define
HWY_RVV_RETV_ARGMVV
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
no
HWY_RVV_M
(
MLEN
)
m
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
a
HWY_RVV_V
(
BASE
SEW
LMUL
)
b
)
{
\
return
__riscv_v
#
#
OP
#
#
_vv_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_mu
(
m
no
a
b
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
#
define
HWY_RVV_RETM_ARGM
(
SEW
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_M
(
MLEN
)
NAME
(
HWY_RVV_M
(
MLEN
)
m
)
{
\
return
__riscv_vm
#
#
OP
#
#
_m_b
#
#
MLEN
(
m
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
#
define
HWY_RVV_SET
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
HWY_RVV_T
(
BASE
SEW
)
arg
)
{
\
return
__riscv_v
#
#
OP
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
arg
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_SET
Set
mv_v_x
_ALL_VIRT
)
HWY_RVV_FOREACH_F
(
HWY_RVV_SET
Set
fmv_v_f
_ALL_VIRT
)
#
undef
HWY_RVV_SET
template
<
size_t
N
int
kPow2
>
decltype
(
Set
(
Simd
<
int16_t
N
kPow2
>
(
)
0
)
)
Set
(
Simd
<
hwy
:
:
bfloat16_t
N
kPow2
>
d
hwy
:
:
bfloat16_t
arg
)
{
return
Set
(
RebindToSigned
<
decltype
(
d
)
>
(
)
BitCastScalar
<
int16_t
>
(
arg
)
)
;
}
#
if
!
HWY_HAVE_FLOAT16
template
<
size_t
N
int
kPow2
>
decltype
(
Set
(
Simd
<
uint16_t
N
kPow2
>
(
)
0
)
)
Set
(
Simd
<
hwy
:
:
float16_t
N
kPow2
>
d
hwy
:
:
float16_t
arg
)
{
return
Set
(
RebindToUnsigned
<
decltype
(
d
)
>
(
)
BitCastScalar
<
uint16_t
>
(
arg
)
)
;
}
#
endif
template
<
class
D
>
using
VFromD
=
decltype
(
Set
(
D
(
)
TFromD
<
D
>
(
)
)
)
;
template
<
class
D
>
HWY_API
VFromD
<
D
>
Zero
(
D
d
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
return
BitCast
(
d
Set
(
du
0
)
)
;
}
namespace
detail
{
#
define
HWY_RVV_UNDEFINED
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
/
*
tag
*
/
)
{
\
return
__riscv_v
#
#
OP
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
)
;
/
*
no
AVL
*
/
\
}
HWY_RVV_FOREACH
(
HWY_RVV_UNDEFINED
Undefined
undefined
_ALL
)
#
undef
HWY_RVV_UNDEFINED
}
template
<
class
D
>
HWY_API
VFromD
<
D
>
Undefined
(
D
d
)
{
return
Zero
(
d
)
;
}
namespace
detail
{
#
define
HWY_RVV_TRUNC
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMULH
)
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMULH
(
\
v
)
;
/
*
no
AVL
*
/
\
}
HWY_RVV_FOREACH
(
HWY_RVV_TRUNC
Trunc
lmul_trunc
_TRUNC
)
#
undef
HWY_RVV_TRUNC
#
define
HWY_RVV_EXT
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMULD
)
\
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
+
1
)
/
*
d2
*
/
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMULD
(
\
v
)
;
/
*
no
AVL
*
/
\
}
HWY_RVV_FOREACH
(
HWY_RVV_EXT
Ext
lmul_ext
_EXT
)
#
undef
HWY_RVV_EXT
#
define
HWY_RVV_EXT_VIRT
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
+
1
)
/
*
d2
*
/
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
v
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_EXT_VIRT
Ext
lmul_ext
_VIRT
)
#
undef
HWY_RVV_EXT_VIRT
template
<
class
D
HWY_RVV_IF_EMULATED_D
(
D
)
>
VFromD
<
D
>
Ext
(
D
d
VFromD
<
Half
<
D
>
>
v
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
const
Half
<
decltype
(
du
)
>
duh
;
return
BitCast
(
d
Ext
(
du
BitCast
(
duh
v
)
)
)
;
}
#
define
HWY_RVV_CAST_U8
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
typename
T
size_t
N
>
\
HWY_API
vuint8
#
#
LMUL
#
#
_t
BitCastToByte
(
Simd
<
T
N
SHIFT
>
/
*
d
*
/
\
vuint8
#
#
LMUL
#
#
_t
v
)
{
\
return
v
;
\
}
\
template
<
size_t
N
>
\
HWY_API
vuint8
#
#
LMUL
#
#
_t
BitCastFromByte
(
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
/
*
d
*
/
vuint8
#
#
LMUL
#
#
_t
v
)
{
\
return
v
;
\
}
#
define
HWY_RVV_CAST_I8
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
typename
T
size_t
N
>
\
HWY_API
vuint8
#
#
LMUL
#
#
_t
BitCastToByte
(
Simd
<
T
N
SHIFT
>
/
*
d
*
/
\
vint8
#
#
LMUL
#
#
_t
v
)
{
\
return
__riscv_vreinterpret_v_i8
#
#
LMUL
#
#
_u8
#
#
LMUL
(
v
)
;
\
}
\
template
<
size_t
N
>
\
HWY_API
vint8
#
#
LMUL
#
#
_t
BitCastFromByte
(
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
/
*
d
*
/
vuint8
#
#
LMUL
#
#
_t
v
)
{
\
return
__riscv_vreinterpret_v_u8
#
#
LMUL
#
#
_i8
#
#
LMUL
(
v
)
;
\
}
#
define
HWY_RVV_CAST_U
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
typename
T
size_t
N
>
\
HWY_API
vuint8
#
#
LMUL
#
#
_t
BitCastToByte
(
Simd
<
T
N
SHIFT
>
/
*
d
*
/
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_u8
#
#
LMUL
(
v
)
;
\
}
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
BitCastFromByte
(
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
/
*
d
*
/
vuint8
#
#
LMUL
#
#
_t
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_v_u8
#
#
LMUL
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
v
)
;
\
}
#
define
HWY_RVV_CAST_IF
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
typename
T
size_t
N
>
\
HWY_API
vuint8
#
#
LMUL
#
#
_t
BitCastToByte
(
Simd
<
T
N
SHIFT
>
/
*
d
*
/
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_v_u
#
#
SEW
#
#
LMUL
#
#
_u8
#
#
LMUL
(
\
__riscv_v
#
#
OP
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_u
#
#
SEW
#
#
LMUL
(
v
)
)
;
\
}
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
BitCastFromByte
(
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
/
*
d
*
/
vuint8
#
#
LMUL
#
#
_t
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_v_u
#
#
SEW
#
#
LMUL
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
\
__riscv_v
#
#
OP
#
#
_v_u8
#
#
LMUL
#
#
_u
#
#
SEW
#
#
LMUL
(
v
)
)
;
\
}
#
define
HWY_RVV_CAST_VIRT_U
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
typename
T
size_t
N
>
\
HWY_API
vuint8
#
#
LMULH
#
#
_t
BitCastToByte
(
Simd
<
T
N
SHIFT
>
/
*
d
*
/
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
detail
:
:
Trunc
(
__riscv_v
#
#
OP
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_u8
#
#
LMUL
(
v
)
)
;
\
}
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
BitCastFromByte
(
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
/
*
d
*
/
vuint8
#
#
LMULH
#
#
_t
v
)
{
\
HWY_RVV_D
(
uint
8
N
SHIFT
+
1
)
d2
;
\
const
vuint8
#
#
LMUL
#
#
_t
v2
=
detail
:
:
Ext
(
d2
v
)
;
\
return
__riscv_v
#
#
OP
#
#
_v_u8
#
#
LMUL
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
v2
)
;
\
}
#
define
HWY_RVV_CAST_VIRT_IF
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
typename
T
size_t
N
>
\
HWY_API
vuint8
#
#
LMULH
#
#
_t
BitCastToByte
(
Simd
<
T
N
SHIFT
>
/
*
d
*
/
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
detail
:
:
Trunc
(
__riscv_v
#
#
OP
#
#
_v_u
#
#
SEW
#
#
LMUL
#
#
_u8
#
#
LMUL
(
\
__riscv_v
#
#
OP
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_u
#
#
SEW
#
#
LMUL
(
v
)
)
)
;
\
}
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
BitCastFromByte
(
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
/
*
d
*
/
vuint8
#
#
LMULH
#
#
_t
v
)
{
\
HWY_RVV_D
(
uint
8
N
SHIFT
+
1
)
d2
;
\
const
vuint8
#
#
LMUL
#
#
_t
v2
=
detail
:
:
Ext
(
d2
v
)
;
\
return
__riscv_v
#
#
OP
#
#
_v_u
#
#
SEW
#
#
LMUL
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
\
__riscv_v
#
#
OP
#
#
_v_u8
#
#
LMUL
#
#
_u
#
#
SEW
#
#
LMUL
(
v2
)
)
;
\
}
HWY_RVV_FOREACH_U08
(
HWY_RVV_CAST_U8
_
reinterpret
_ALL
)
HWY_RVV_FOREACH_I08
(
HWY_RVV_CAST_I8
_
reinterpret
_ALL
)
HWY_RVV_FOREACH_U163264
(
HWY_RVV_CAST_U
_
reinterpret
_ALL
)
HWY_RVV_FOREACH_I163264
(
HWY_RVV_CAST_IF
_
reinterpret
_ALL
)
HWY_RVV_FOREACH_U163264
(
HWY_RVV_CAST_VIRT_U
_
reinterpret
_VIRT
)
HWY_RVV_FOREACH_I163264
(
HWY_RVV_CAST_VIRT_IF
_
reinterpret
_VIRT
)
HWY_RVV_FOREACH_F
(
HWY_RVV_CAST_IF
_
reinterpret
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_CAST_VIRT_IF
_
reinterpret
_VIRT
)
#
if
HWY_HAVE_FLOAT16
#
elif
HWY_RVV_HAVE_F16C
HWY_RVV_FOREACH_F16_UNCONDITIONAL
(
HWY_RVV_CAST_IF
_
reinterpret
_ALL
)
HWY_RVV_FOREACH_F16_UNCONDITIONAL
(
HWY_RVV_CAST_VIRT_IF
_
reinterpret
_VIRT
)
#
else
template
<
size_t
N
int
kPow2
>
HWY_INLINE
VFromD
<
Simd
<
uint16_t
N
kPow2
>
>
BitCastFromByte
(
Simd
<
hwy
:
:
float16_t
N
kPow2
>
VFromD
<
Simd
<
uint8_t
N
kPow2
>
>
v
)
{
return
BitCastFromByte
(
Simd
<
uint16_t
N
kPow2
>
(
)
v
)
;
}
#
endif
#
undef
HWY_RVV_CAST_U8
#
undef
HWY_RVV_CAST_I8
#
undef
HWY_RVV_CAST_U
#
undef
HWY_RVV_CAST_IF
#
undef
HWY_RVV_CAST_VIRT_U
#
undef
HWY_RVV_CAST_VIRT_IF
template
<
size_t
N
int
kPow2
>
HWY_INLINE
VFromD
<
Simd
<
int16_t
N
kPow2
>
>
BitCastFromByte
(
Simd
<
hwy
:
:
bfloat16_t
N
kPow2
>
VFromD
<
Simd
<
uint8_t
N
kPow2
>
>
v
)
{
return
BitCastFromByte
(
Simd
<
int16_t
N
kPow2
>
(
)
v
)
;
}
}
template
<
class
D
class
FromV
>
HWY_API
VFromD
<
D
>
BitCast
(
D
d
FromV
v
)
{
return
detail
:
:
BitCastFromByte
(
d
detail
:
:
BitCastToByte
(
d
v
)
)
;
}
namespace
detail
{
#
define
HWY_RVV_IOTA
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
)
{
\
return
__riscv_v
#
#
OP
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH_U
(
HWY_RVV_IOTA
Iota0
id_v
_ALL_VIRT
)
#
undef
HWY_RVV_IOTA
#
define
HWY_RVV_MASKED_IOTA
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
HWY_RVV_M
(
MLEN
)
mask
)
{
\
return
__riscv_v
#
#
OP
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
mask
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH_U
(
HWY_RVV_MASKED_IOTA
MaskedIota
iota_m
_ALL_VIRT
)
#
undef
HWY_RVV_MASKED_IOTA
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGV
Not
not
_ALL
)
template
<
class
V
HWY_IF_FLOAT_V
(
V
)
>
HWY_API
V
Not
(
const
V
v
)
{
using
DF
=
DFromV
<
V
>
;
using
DU
=
RebindToUnsigned
<
DF
>
;
return
BitCast
(
DF
(
)
Not
(
BitCast
(
DU
(
)
v
)
)
)
;
}
namespace
detail
{
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGVS
AndS
and_vx
_ALL
)
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGVV
And
and
_ALL
)
template
<
class
V
HWY_IF_FLOAT_V
(
V
)
>
HWY_API
V
And
(
const
V
a
const
V
b
)
{
using
DF
=
DFromV
<
V
>
;
using
DU
=
RebindToUnsigned
<
DF
>
;
return
BitCast
(
DF
(
)
And
(
BitCast
(
DU
(
)
a
)
BitCast
(
DU
(
)
b
)
)
)
;
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGVV
Or
or
_ALL
)
template
<
class
V
HWY_IF_FLOAT_V
(
V
)
>
HWY_API
V
Or
(
const
V
a
const
V
b
)
{
using
DF
=
DFromV
<
V
>
;
using
DU
=
RebindToUnsigned
<
DF
>
;
return
BitCast
(
DF
(
)
Or
(
BitCast
(
DU
(
)
a
)
BitCast
(
DU
(
)
b
)
)
)
;
}
namespace
detail
{
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGVS
XorS
xor_vx
_ALL
)
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGVV
Xor
xor
_ALL
)
template
<
class
V
HWY_IF_FLOAT_V
(
V
)
>
HWY_API
V
Xor
(
const
V
a
const
V
b
)
{
using
DF
=
DFromV
<
V
>
;
using
DU
=
RebindToUnsigned
<
DF
>
;
return
BitCast
(
DF
(
)
Xor
(
BitCast
(
DU
(
)
a
)
BitCast
(
DU
(
)
b
)
)
)
;
}
template
<
class
V
>
HWY_API
V
AndNot
(
const
V
not_a
const
V
b
)
{
return
And
(
Not
(
not_a
)
b
)
;
}
template
<
class
V
>
HWY_API
V
Xor3
(
V
x1
V
x2
V
x3
)
{
return
Xor
(
x1
Xor
(
x2
x3
)
)
;
}
template
<
class
V
>
HWY_API
V
Or3
(
V
o1
V
o2
V
o3
)
{
return
Or
(
o1
Or
(
o2
o3
)
)
;
}
template
<
class
V
>
HWY_API
V
OrAnd
(
const
V
o
const
V
a1
const
V
a2
)
{
return
Or
(
o
And
(
a1
a2
)
)
;
}
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGVV
CopySign
fsgnj
_ALL
)
template
<
class
V
>
HWY_API
V
CopySignToAbs
(
const
V
abs
const
V
sign
)
{
return
CopySign
(
abs
sign
)
;
}
#
ifdef
HWY_NATIVE_OPERATOR_REPLACEMENTS
#
undef
HWY_NATIVE_OPERATOR_REPLACEMENTS
#
else
#
define
HWY_NATIVE_OPERATOR_REPLACEMENTS
#
endif
namespace
detail
{
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGVS
AddS
add_vx
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGVS
AddS
fadd_vf
_ALL
)
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGVS
ReverseSubS
rsub_vx
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGVS
ReverseSubS
frsub_vf
_ALL
)
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGVV
Add
add
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGVV
Add
fadd
_ALL
)
namespace
detail
{
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGVS
SubS
sub_vx
_ALL
)
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGVV
Sub
sub
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGVV
Sub
fsub
_ALL
)
#
ifdef
HWY_NATIVE_I32_SATURATED_ADDSUB
#
undef
HWY_NATIVE_I32_SATURATED_ADDSUB
#
else
#
define
HWY_NATIVE_I32_SATURATED_ADDSUB
#
endif
#
ifdef
HWY_NATIVE_U32_SATURATED_ADDSUB
#
undef
HWY_NATIVE_U32_SATURATED_ADDSUB
#
else
#
define
HWY_NATIVE_U32_SATURATED_ADDSUB
#
endif
#
ifdef
HWY_NATIVE_I64_SATURATED_ADDSUB
#
undef
HWY_NATIVE_I64_SATURATED_ADDSUB
#
else
#
define
HWY_NATIVE_I64_SATURATED_ADDSUB
#
endif
#
ifdef
HWY_NATIVE_U64_SATURATED_ADDSUB
#
undef
HWY_NATIVE_U64_SATURATED_ADDSUB
#
else
#
define
HWY_NATIVE_U64_SATURATED_ADDSUB
#
endif
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGVV
SaturatedAdd
saddu
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGVV
SaturatedAdd
sadd
_ALL
)
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGVV
SaturatedSub
ssubu
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGVV
SaturatedSub
ssub
_ALL
)
#
ifndef
HWY_RVV_CHOOSE_VXRM
#
if
HWY_COMPILER_GCC_ACTUAL
&
&
HWY_COMPILER_GCC_ACTUAL
<
1400
#
define
HWY_RVV_AVOID_VXRM
#
elif
HWY_COMPILER_CLANG
&
&
\
(
HWY_COMPILER_CLANG
<
1600
|
|
__riscv_v_intrinsic
<
11000
)
#
define
HWY_RVV_AVOID_VXRM
#
endif
#
endif
#
ifdef
HWY_RVV_AVOID_VXRM
#
define
HWY_RVV_INSERT_VXRM
(
vxrm
avl
)
avl
#
define
__RISCV_VXRM_RNU
#
define
__RISCV_VXRM_RDN
#
else
#
define
HWY_RVV_INSERT_VXRM
(
vxrm
avl
)
vxrm
avl
#
endif
#
define
HWY_RVV_RETV_AVERAGE
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
a
HWY_RVV_V
(
BASE
SEW
LMUL
)
b
)
{
\
return
__riscv_v
#
#
OP
#
#
_vv_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
\
a
b
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RNU
HWY_RVV_AVL
(
SEW
SHIFT
)
)
)
;
\
}
HWY_RVV_FOREACH_U08
(
HWY_RVV_RETV_AVERAGE
AverageRound
aaddu
_ALL
)
HWY_RVV_FOREACH_U16
(
HWY_RVV_RETV_AVERAGE
AverageRound
aaddu
_ALL
)
#
undef
HWY_RVV_RETV_AVERAGE
#
define
HWY_RVV_SHIFT
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
int
kBits
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_vx_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
v
kBits
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
#
#
Same
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
int
bits
)
{
\
return
__riscv_v
#
#
OP
#
#
_vx_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
v
static_cast
<
uint8_t
>
(
bits
)
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_SHIFT
ShiftLeft
sll
_ALL
)
HWY_RVV_FOREACH_U
(
HWY_RVV_SHIFT
ShiftRight
srl
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_SHIFT
ShiftRight
sra
_ALL
)
#
undef
HWY_RVV_SHIFT
template
<
class
VU8
HWY_IF_U8_D
(
DFromV
<
VU8
>
)
>
HWY_API
VFromD
<
Repartition
<
uint64_t
DFromV
<
VU8
>
>
>
SumsOf8
(
const
VU8
v
)
{
const
DFromV
<
VU8
>
du8
;
const
RepartitionToWide
<
decltype
(
du8
)
>
du16
;
const
RepartitionToWide
<
decltype
(
du16
)
>
du32
;
const
RepartitionToWide
<
decltype
(
du32
)
>
du64
;
using
VU16
=
VFromD
<
decltype
(
du16
)
>
;
const
VU16
vFDB97531
=
ShiftRight
<
8
>
(
BitCast
(
du16
v
)
)
;
const
VU16
vECA86420
=
detail
:
:
AndS
(
BitCast
(
du16
v
)
0xFF
)
;
const
VU16
sFE_DC_BA_98_76_54_32_10
=
Add
(
vFDB97531
vECA86420
)
;
const
VU16
szz_FE_zz_BA_zz_76_zz_32
=
BitCast
(
du16
ShiftRight
<
16
>
(
BitCast
(
du32
sFE_DC_BA_98_76_54_32_10
)
)
)
;
const
VU16
sxx_FC_xx_B8_xx_74_xx_30
=
Add
(
sFE_DC_BA_98_76_54_32_10
szz_FE_zz_BA_zz_76_zz_32
)
;
const
VU16
szz_zz_xx_FC_zz_zz_xx_74
=
BitCast
(
du16
ShiftRight
<
32
>
(
BitCast
(
du64
sxx_FC_xx_B8_xx_74_xx_30
)
)
)
;
const
VU16
sxx_xx_xx_F8_xx_xx_xx_70
=
Add
(
sxx_FC_xx_B8_xx_74_xx_30
szz_zz_xx_FC_zz_zz_xx_74
)
;
return
detail
:
:
AndS
(
BitCast
(
du64
sxx_xx_xx_F8_xx_xx_xx_70
)
0xFFFFull
)
;
}
template
<
class
VI8
HWY_IF_I8_D
(
DFromV
<
VI8
>
)
>
HWY_API
VFromD
<
Repartition
<
int64_t
DFromV
<
VI8
>
>
>
SumsOf8
(
const
VI8
v
)
{
const
DFromV
<
VI8
>
di8
;
const
RepartitionToWide
<
decltype
(
di8
)
>
di16
;
const
RepartitionToWide
<
decltype
(
di16
)
>
di32
;
const
RepartitionToWide
<
decltype
(
di32
)
>
di64
;
const
RebindToUnsigned
<
decltype
(
di32
)
>
du32
;
const
RebindToUnsigned
<
decltype
(
di64
)
>
du64
;
using
VI16
=
VFromD
<
decltype
(
di16
)
>
;
const
VI16
vFDB97531
=
ShiftRight
<
8
>
(
BitCast
(
di16
v
)
)
;
const
VI16
vECA86420
=
ShiftRight
<
8
>
(
ShiftLeft
<
8
>
(
BitCast
(
di16
v
)
)
)
;
const
VI16
sFE_DC_BA_98_76_54_32_10
=
Add
(
vFDB97531
vECA86420
)
;
const
VI16
sDC_zz_98_zz_54_zz_10_zz
=
BitCast
(
di16
ShiftLeft
<
16
>
(
BitCast
(
du32
sFE_DC_BA_98_76_54_32_10
)
)
)
;
const
VI16
sFC_xx_B8_xx_74_xx_30_xx
=
Add
(
sFE_DC_BA_98_76_54_32_10
sDC_zz_98_zz_54_zz_10_zz
)
;
const
VI16
sB8_xx_zz_zz_30_xx_zz_zz
=
BitCast
(
di16
ShiftLeft
<
32
>
(
BitCast
(
du64
sFC_xx_B8_xx_74_xx_30_xx
)
)
)
;
const
VI16
sF8_xx_xx_xx_70_xx_xx_xx
=
Add
(
sFC_xx_B8_xx_74_xx_30_xx
sB8_xx_zz_zz_30_xx_zz_zz
)
;
return
ShiftRight
<
48
>
(
BitCast
(
di64
sF8_xx_xx_xx_70_xx_xx_xx
)
)
;
}
template
<
int
kBits
class
V
>
HWY_API
V
RotateRight
(
const
V
v
)
{
constexpr
size_t
kSizeInBits
=
sizeof
(
TFromV
<
V
>
)
*
8
;
static_assert
(
0
<
=
kBits
&
&
kBits
<
kSizeInBits
"
Invalid
shift
count
"
)
;
if
(
kBits
=
=
0
)
return
v
;
return
Or
(
ShiftRight
<
kBits
>
(
v
)
ShiftLeft
<
HWY_MIN
(
kSizeInBits
-
1
kSizeInBits
-
kBits
)
>
(
v
)
)
;
}
#
define
HWY_RVV_SHIFT_VV
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
HWY_RVV_V
(
BASE
SEW
LMUL
)
bits
)
{
\
return
__riscv_v
#
#
OP
#
#
_vv_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
v
bits
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH_U
(
HWY_RVV_SHIFT_VV
Shl
sll
_ALL
)
#
define
HWY_RVV_SHIFT_II
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
HWY_RVV_V
(
BASE
SEW
LMUL
)
bits
)
{
\
const
HWY_RVV_D
(
uint
SEW
HWY_LANES
(
HWY_RVV_T
(
BASE
SEW
)
)
SHIFT
)
du
;
\
return
__riscv_v
#
#
OP
#
#
_vv_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
v
BitCast
(
du
bits
)
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH_I
(
HWY_RVV_SHIFT_II
Shl
sll
_ALL
)
HWY_RVV_FOREACH_U
(
HWY_RVV_SHIFT_VV
Shr
srl
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_SHIFT_II
Shr
sra
_ALL
)
#
undef
HWY_RVV_SHIFT_II
#
undef
HWY_RVV_SHIFT_VV
namespace
detail
{
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGVS
MinS
minu_vx
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGVS
MinS
min_vx
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGVS
MinS
fmin_vf
_ALL
)
}
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGVV
Min
minu
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGVV
Min
min
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGVV
Min
fmin
_ALL
)
namespace
detail
{
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGVS
MaxS
maxu_vx
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGVS
MaxS
max_vx
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGVS
MaxS
fmax_vf
_ALL
)
}
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGVV
Max
maxu
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGVV
Max
max
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGVV
Max
fmax
_ALL
)
#
ifdef
HWY_NATIVE_MUL_8
#
undef
HWY_NATIVE_MUL_8
#
else
#
define
HWY_NATIVE_MUL_8
#
endif
#
ifdef
HWY_NATIVE_MUL_64
#
undef
HWY_NATIVE_MUL_64
#
else
#
define
HWY_NATIVE_MUL_64
#
endif
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGVV
Mul
mul
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGVV
Mul
fmul
_ALL
)
namespace
detail
{
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGVV
MulHigh
mulh
_ALL
)
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGVV
MulHigh
mulhu
_ALL
)
}
HWY_RVV_FOREACH_U16
(
HWY_RVV_RETV_ARGVV
MulHigh
mulhu
_ALL
)
HWY_RVV_FOREACH_I16
(
HWY_RVV_RETV_ARGVV
MulHigh
mulh
_ALL
)
#
define
HWY_RVV_MUL15
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
a
HWY_RVV_V
(
BASE
SEW
LMUL
)
b
)
{
\
return
__riscv_v
#
#
OP
#
#
_vv_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
\
a
b
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RNU
HWY_RVV_AVL
(
SEW
SHIFT
)
)
)
;
\
}
HWY_RVV_FOREACH_I16
(
HWY_RVV_MUL15
MulFixedPoint15
smul
_ALL
)
#
undef
HWY_RVV_MUL15
#
ifdef
HWY_NATIVE_INT_DIV
#
undef
HWY_NATIVE_INT_DIV
#
else
#
define
HWY_NATIVE_INT_DIV
#
endif
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGVV
Div
divu
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGVV
Div
div
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGVV
Div
fdiv
_ALL
)
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGVV
Mod
remu
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGVV
Mod
rem
_ALL
)
#
ifdef
HWY_NATIVE_MASKED_ARITH
#
undef
HWY_NATIVE_MASKED_ARITH
#
else
#
define
HWY_NATIVE_MASKED_ARITH
#
endif
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGMVV
MaskedMinOr
minu
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGMVV
MaskedMinOr
min
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGMVV
MaskedMinOr
fmin
_ALL
)
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGMVV
MaskedMaxOr
maxu
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGMVV
MaskedMaxOr
max
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGMVV
MaskedMaxOr
fmax
_ALL
)
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGMVV
MaskedAddOr
add
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGMVV
MaskedAddOr
fadd
_ALL
)
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGMVV
MaskedSubOr
sub
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGMVV
MaskedSubOr
fsub
_ALL
)
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETV_ARGMVV
MaskedMulOr
mul
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGMVV
MaskedMulOr
fmul
_ALL
)
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGMVV
MaskedDivOr
divu
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGMVV
MaskedDivOr
div
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGMVV
MaskedDivOr
fdiv
_ALL
)
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGMVV
MaskedModOr
remu
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGMVV
MaskedModOr
rem
_ALL
)
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGMVV
MaskedSatAddOr
saddu
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGMVV
MaskedSatAddOr
sadd
_ALL
)
HWY_RVV_FOREACH_U
(
HWY_RVV_RETV_ARGMVV
MaskedSatSubOr
ssubu
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETV_ARGMVV
MaskedSatSubOr
ssub
_ALL
)
#
ifdef
HWY_NATIVE_F64_APPROX_RECIP
#
undef
HWY_NATIVE_F64_APPROX_RECIP
#
else
#
define
HWY_NATIVE_F64_APPROX_RECIP
#
endif
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGV
ApproximateReciprocal
frec7
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGV
Sqrt
fsqrt
_ALL
)
#
ifdef
HWY_NATIVE_F64_APPROX_RSQRT
#
undef
HWY_NATIVE_F64_APPROX_RSQRT
#
else
#
define
HWY_NATIVE_F64_APPROX_RSQRT
#
endif
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGV
ApproximateReciprocalSqrt
frsqrt7
_ALL
)
#
ifdef
HWY_NATIVE_INT_FMA
#
undef
HWY_NATIVE_INT_FMA
#
else
#
define
HWY_NATIVE_INT_FMA
#
endif
#
define
HWY_RVV_FMA
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
mul
HWY_RVV_V
(
BASE
SEW
LMUL
)
x
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
add
)
{
\
return
__riscv_v
#
#
OP
#
#
_vv_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
add
mul
x
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_FMA
MulAdd
macc
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_FMA
MulAdd
fmacc
_ALL
)
HWY_RVV_FOREACH_UI
(
HWY_RVV_FMA
NegMulAdd
nmsac
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_FMA
NegMulAdd
fnmsac
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_FMA
MulSub
fmsac
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_FMA
NegMulSub
fnmacc
_ALL
)
#
undef
HWY_RVV_FMA
#
define
HWY_RVV_RETM_ARGVV
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_M
(
MLEN
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
a
HWY_RVV_V
(
BASE
SEW
LMUL
)
b
)
{
\
return
__riscv_v
#
#
OP
#
#
_vv_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_b
#
#
MLEN
(
\
a
b
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
#
define
HWY_RVV_RETM_ARGVS
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_M
(
MLEN
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
a
HWY_RVV_T
(
BASE
SEW
)
b
)
{
\
return
__riscv_v
#
#
OP
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_b
#
#
MLEN
(
\
a
b
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETM_ARGVV
Eq
mseq
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETM_ARGVV
Eq
mfeq
_ALL
)
namespace
detail
{
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETM_ARGVS
EqS
mseq_vx
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETM_ARGVS
EqS
mfeq_vf
_ALL
)
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETM_ARGVV
Ne
msne
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETM_ARGVV
Ne
mfne
_ALL
)
namespace
detail
{
HWY_RVV_FOREACH_UI
(
HWY_RVV_RETM_ARGVS
NeS
msne_vx
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETM_ARGVS
NeS
mfne_vf
_ALL
)
}
HWY_RVV_FOREACH_U
(
HWY_RVV_RETM_ARGVV
Lt
msltu
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETM_ARGVV
Lt
mslt
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETM_ARGVV
Lt
mflt
_ALL
)
namespace
detail
{
HWY_RVV_FOREACH_I
(
HWY_RVV_RETM_ARGVS
LtS
mslt_vx
_ALL
)
HWY_RVV_FOREACH_U
(
HWY_RVV_RETM_ARGVS
LtS
msltu_vx
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETM_ARGVS
LtS
mflt_vf
_ALL
)
}
HWY_RVV_FOREACH_U
(
HWY_RVV_RETM_ARGVV
Le
msleu
_ALL
)
HWY_RVV_FOREACH_I
(
HWY_RVV_RETM_ARGVV
Le
msle
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_RETM_ARGVV
Le
mfle
_ALL
)
#
undef
HWY_RVV_RETM_ARGVV
#
undef
HWY_RVV_RETM_ARGVS
template
<
class
V
>
HWY_API
auto
Ge
(
const
V
a
const
V
b
)
-
>
decltype
(
Le
(
a
b
)
)
{
return
Le
(
b
a
)
;
}
template
<
class
V
>
HWY_API
auto
Gt
(
const
V
a
const
V
b
)
-
>
decltype
(
Lt
(
a
b
)
)
{
return
Lt
(
b
a
)
;
}
template
<
class
V
>
HWY_API
auto
TestBit
(
const
V
a
const
V
bit
)
-
>
decltype
(
Eq
(
a
bit
)
)
{
return
detail
:
:
NeS
(
And
(
a
bit
)
0
)
;
}
HWY_RVV_FOREACH_B
(
HWY_RVV_RETM_ARGM
Not
not
)
#
define
HWY_RVV_RETM_ARGMM
(
SEW
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_M
(
MLEN
)
NAME
(
HWY_RVV_M
(
MLEN
)
a
HWY_RVV_M
(
MLEN
)
b
)
{
\
return
__riscv_vm
#
#
OP
#
#
_mm_b
#
#
MLEN
(
b
a
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH_B
(
HWY_RVV_RETM_ARGMM
And
and
)
HWY_RVV_FOREACH_B
(
HWY_RVV_RETM_ARGMM
AndNot
andn
)
HWY_RVV_FOREACH_B
(
HWY_RVV_RETM_ARGMM
Or
or
)
HWY_RVV_FOREACH_B
(
HWY_RVV_RETM_ARGMM
Xor
xor
)
HWY_RVV_FOREACH_B
(
HWY_RVV_RETM_ARGMM
ExclusiveNeither
xnor
)
#
undef
HWY_RVV_RETM_ARGMM
#
define
HWY_RVV_IF_THEN_ELSE
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_M
(
MLEN
)
m
HWY_RVV_V
(
BASE
SEW
LMUL
)
yes
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
no
)
{
\
return
__riscv_v
#
#
OP
#
#
_vvm_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
no
yes
m
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_IF_THEN_ELSE
IfThenElse
merge
_ALL
)
#
undef
HWY_RVV_IF_THEN_ELSE
template
<
class
M
class
V
>
HWY_API
V
IfThenElseZero
(
const
M
mask
const
V
yes
)
{
return
IfThenElse
(
mask
yes
Zero
(
DFromV
<
V
>
(
)
)
)
;
}
#
define
HWY_RVV_IF_THEN_ZERO_ELSE
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
\
LMULH
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_M
(
MLEN
)
m
HWY_RVV_V
(
BASE
SEW
LMUL
)
no
)
{
\
return
__riscv_v
#
#
OP
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
no
0
m
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_IF_THEN_ZERO_ELSE
IfThenZeroElse
merge_vxm
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_IF_THEN_ZERO_ELSE
IfThenZeroElse
fmerge_vfm
_ALL
)
#
undef
HWY_RVV_IF_THEN_ZERO_ELSE
template
<
class
D
>
using
MFromD
=
decltype
(
Eq
(
Zero
(
D
(
)
)
Zero
(
D
(
)
)
)
)
;
template
<
class
V
>
HWY_API
MFromD
<
DFromV
<
V
>
>
MaskFromVec
(
const
V
v
)
{
return
detail
:
:
NeS
(
v
0
)
;
}
#
ifdef
HWY_NATIVE_MASK_FALSE
#
undef
HWY_NATIVE_MASK_FALSE
#
else
#
define
HWY_NATIVE_MASK_FALSE
#
endif
template
<
class
D
>
HWY_API
MFromD
<
D
>
MaskFalse
(
D
d
)
{
const
DFromV
<
VFromD
<
decltype
(
d
)
>
>
d_full
;
return
MaskFromVec
(
Zero
(
d_full
)
)
;
}
template
<
class
D
typename
MFrom
>
HWY_API
MFromD
<
D
>
RebindMask
(
const
D
const
MFrom
mask
)
{
return
mask
;
}
#
define
HWY_RVV_VEC_FROM_MASK
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
HWY_RVV_M
(
MLEN
)
m
)
{
\
/
*
MaskFalse
requires
we
set
all
lanes
for
capped
d
and
virtual
LMUL
.
*
/
\
const
DFromV
<
VFromD
<
decltype
(
d
)
>
>
d_full
;
\
const
RebindToSigned
<
decltype
(
d_full
)
>
di
;
\
using
TI
=
TFromD
<
decltype
(
di
)
>
;
\
return
BitCast
(
d_full
__riscv_v
#
#
OP
#
#
_i
#
#
SEW
#
#
LMUL
(
Zero
(
di
)
TI
{
-
1
}
m
\
Lanes
(
d_full
)
)
)
;
\
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_VEC_FROM_MASK
VecFromMask
merge_vxm
_ALL_VIRT
)
#
undef
HWY_RVV_VEC_FROM_MASK
template
<
class
D
HWY_IF_FLOAT_D
(
D
)
>
HWY_API
VFromD
<
D
>
VecFromMask
(
const
D
d
MFromD
<
D
>
mask
)
{
return
BitCast
(
d
VecFromMask
(
RebindToUnsigned
<
D
>
(
)
mask
)
)
;
}
template
<
class
V
>
HWY_API
V
IfVecThenElse
(
const
V
mask
const
V
yes
const
V
no
)
{
return
IfThenElse
(
MaskFromVec
(
mask
)
yes
no
)
;
}
template
<
class
V
>
HWY_API
V
ZeroIfNegative
(
const
V
v
)
{
return
IfThenZeroElse
(
detail
:
:
LtS
(
v
0
)
v
)
;
}
template
<
class
V
>
HWY_API
V
BroadcastSignBit
(
const
V
v
)
{
return
ShiftRight
<
sizeof
(
TFromV
<
V
>
)
*
8
-
1
>
(
v
)
;
}
template
<
class
V
>
HWY_API
V
IfNegativeThenElse
(
V
v
V
yes
V
no
)
{
static_assert
(
IsSigned
<
TFromV
<
V
>
>
(
)
"
Only
works
for
signed
/
float
"
)
;
const
DFromV
<
V
>
d
;
const
RebindToSigned
<
decltype
(
d
)
>
di
;
MFromD
<
decltype
(
d
)
>
m
=
detail
:
:
LtS
(
BitCast
(
di
v
)
0
)
;
return
IfThenElse
(
m
yes
no
)
;
}
#
define
HWY_RVV_FIND_FIRST_TRUE
(
SEW
SHIFT
MLEN
NAME
OP
)
\
template
<
class
D
>
\
HWY_API
intptr_t
FindFirstTrue
(
D
d
HWY_RVV_M
(
MLEN
)
m
)
{
\
static_assert
(
MLenFromD
(
d
)
=
=
MLEN
"
Type
mismatch
"
)
;
\
return
__riscv_vfirst_m_b
#
#
MLEN
(
m
Lanes
(
d
)
)
;
\
}
\
template
<
class
D
>
\
HWY_API
size_t
FindKnownFirstTrue
(
D
d
HWY_RVV_M
(
MLEN
)
m
)
{
\
static_assert
(
MLenFromD
(
d
)
=
=
MLEN
"
Type
mismatch
"
)
;
\
return
static_cast
<
size_t
>
(
__riscv_vfirst_m_b
#
#
MLEN
(
m
Lanes
(
d
)
)
)
;
\
}
HWY_RVV_FOREACH_B
(
HWY_RVV_FIND_FIRST_TRUE
_
)
#
undef
HWY_RVV_FIND_FIRST_TRUE
template
<
class
D
>
HWY_API
bool
AllFalse
(
D
d
MFromD
<
D
>
m
)
{
return
FindFirstTrue
(
d
m
)
<
0
;
}
#
define
HWY_RVV_ALL_TRUE
(
SEW
SHIFT
MLEN
NAME
OP
)
\
template
<
class
D
>
\
HWY_API
bool
AllTrue
(
D
d
HWY_RVV_M
(
MLEN
)
m
)
{
\
static_assert
(
MLenFromD
(
d
)
=
=
MLEN
"
Type
mismatch
"
)
;
\
return
AllFalse
(
d
__riscv_vmnot_m_b
#
#
MLEN
(
m
Lanes
(
d
)
)
)
;
\
}
HWY_RVV_FOREACH_B
(
HWY_RVV_ALL_TRUE
_
_
)
#
undef
HWY_RVV_ALL_TRUE
#
define
HWY_RVV_COUNT_TRUE
(
SEW
SHIFT
MLEN
NAME
OP
)
\
template
<
class
D
>
\
HWY_API
size_t
CountTrue
(
D
d
HWY_RVV_M
(
MLEN
)
m
)
{
\
static_assert
(
MLenFromD
(
d
)
=
=
MLEN
"
Type
mismatch
"
)
;
\
return
__riscv_vcpop_m_b
#
#
MLEN
(
m
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH_B
(
HWY_RVV_COUNT_TRUE
_
_
)
#
undef
HWY_RVV_COUNT_TRUE
#
ifdef
HWY_NATIVE_PROMOTE_MASK_TO
#
undef
HWY_NATIVE_PROMOTE_MASK_TO
#
else
#
define
HWY_NATIVE_PROMOTE_MASK_TO
#
endif
template
<
class
DTo
class
DFrom
HWY_IF_T_SIZE_GT_D
(
DTo
sizeof
(
TFromD
<
DFrom
>
)
)
hwy
:
:
EnableIf
<
IsSame
<
MFromD
<
DTo
>
MFromD
<
DFrom
>
>
(
)
>
*
=
nullptr
>
HWY_API
MFromD
<
DTo
>
PromoteMaskTo
(
DTo
DFrom
MFromD
<
DFrom
>
m
)
{
return
m
;
}
#
ifdef
HWY_NATIVE_DEMOTE_MASK_TO
#
undef
HWY_NATIVE_DEMOTE_MASK_TO
#
else
#
define
HWY_NATIVE_DEMOTE_MASK_TO
#
endif
template
<
class
DTo
class
DFrom
HWY_IF_T_SIZE_LE_D
(
DTo
sizeof
(
TFromD
<
DFrom
>
)
-
1
)
hwy
:
:
EnableIf
<
IsSame
<
MFromD
<
DTo
>
MFromD
<
DFrom
>
>
(
)
>
*
=
nullptr
>
HWY_API
MFromD
<
DTo
>
DemoteMaskTo
(
DTo
DFrom
MFromD
<
DFrom
>
m
)
{
return
m
;
}
#
define
HWY_RVV_LOAD
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
const
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
p
)
{
\
return
__riscv_v
#
#
OP
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
\
detail
:
:
NativeLanePointer
(
p
)
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_LOAD
Load
le
_ALL_VIRT
)
#
undef
HWY_RVV_LOAD
template
<
class
D
HWY_RVV_IF_EMULATED_D
(
D
)
>
HWY_API
VFromD
<
D
>
Load
(
D
d
const
TFromD
<
D
>
*
HWY_RESTRICT
p
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
return
BitCast
(
d
Load
(
du
detail
:
:
U16LanePointer
(
p
)
)
)
;
}
template
<
class
D
>
HWY_API
VFromD
<
D
>
LoadU
(
D
d
const
TFromD
<
D
>
*
HWY_RESTRICT
p
)
{
return
Load
(
d
p
)
;
}
#
define
HWY_RVV_MASKED_LOAD
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_M
(
MLEN
)
m
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
const
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
p
)
{
\
return
__riscv_v
#
#
OP
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_mu
(
\
m
Zero
(
d
)
detail
:
:
NativeLanePointer
(
p
)
Lanes
(
d
)
)
;
\
}
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
#
#
Or
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
HWY_RVV_M
(
MLEN
)
m
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
const
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
p
)
{
\
return
__riscv_v
#
#
OP
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_mu
(
\
m
v
detail
:
:
NativeLanePointer
(
p
)
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_MASKED_LOAD
MaskedLoad
le
_ALL_VIRT
)
#
undef
HWY_RVV_MASKED_LOAD
template
<
class
D
HWY_RVV_IF_EMULATED_D
(
D
)
>
HWY_API
VFromD
<
D
>
MaskedLoad
(
MFromD
<
D
>
m
D
d
const
TFromD
<
D
>
*
HWY_RESTRICT
p
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
return
BitCast
(
d
MaskedLoad
(
RebindMask
(
du
m
)
du
detail
:
:
U16LanePointer
(
p
)
)
)
;
}
template
<
class
D
HWY_RVV_IF_EMULATED_D
(
D
)
>
HWY_API
VFromD
<
D
>
MaskedLoadOr
(
VFromD
<
D
>
no
MFromD
<
D
>
m
D
d
const
TFromD
<
D
>
*
HWY_RESTRICT
p
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
return
BitCast
(
d
MaskedLoadOr
(
BitCast
(
du
no
)
RebindMask
(
du
m
)
du
detail
:
:
U16LanePointer
(
p
)
)
)
;
}
#
ifdef
HWY_NATIVE_LOAD_N
#
undef
HWY_NATIVE_LOAD_N
#
else
#
define
HWY_NATIVE_LOAD_N
#
endif
#
define
HWY_RVV_LOADN
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
const
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
p
size_t
num_lanes
)
{
\
/
*
Use
a
tail
-
undisturbed
load
in
LoadN
as
the
tail
-
undisturbed
load
*
/
\
/
*
operation
below
will
leave
any
lanes
past
the
first
*
/
\
/
*
(
lowest
-
indexed
)
HWY_MIN
(
num_lanes
Lanes
(
d
)
)
lanes
unchanged
*
/
\
return
__riscv_v
#
#
OP
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_tu
(
\
Zero
(
d
)
detail
:
:
NativeLanePointer
(
p
)
CappedLanes
(
d
num_lanes
)
)
;
\
}
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
NAME
#
#
Or
(
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
no
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
const
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
p
size_t
num_lanes
)
{
\
/
*
Use
a
tail
-
undisturbed
load
in
LoadNOr
as
the
tail
-
undisturbed
load
*
/
\
/
*
operation
below
will
set
any
lanes
past
the
first
*
/
\
/
*
(
lowest
-
indexed
)
HWY_MIN
(
num_lanes
Lanes
(
d
)
)
lanes
to
the
*
/
\
/
*
corresponding
lanes
in
no
*
/
\
return
__riscv_v
#
#
OP
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_tu
(
\
no
detail
:
:
NativeLanePointer
(
p
)
CappedLanes
(
d
num_lanes
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_LOADN
LoadN
le
_ALL_VIRT
)
#
undef
HWY_RVV_LOADN
template
<
class
D
HWY_RVV_IF_EMULATED_D
(
D
)
>
HWY_API
VFromD
<
D
>
LoadN
(
D
d
const
TFromD
<
D
>
*
HWY_RESTRICT
p
size_t
num_lanes
)
{
const
RebindToUnsigned
<
D
>
du
;
return
BitCast
(
d
LoadN
(
du
detail
:
:
U16LanePointer
(
p
)
num_lanes
)
)
;
}
template
<
class
D
HWY_RVV_IF_EMULATED_D
(
D
)
>
HWY_API
VFromD
<
D
>
LoadNOr
(
VFromD
<
D
>
v
D
d
const
TFromD
<
D
>
*
HWY_RESTRICT
p
size_t
num_lanes
)
{
const
RebindToUnsigned
<
D
>
du
;
return
BitCast
(
d
LoadNOr
(
BitCast
(
du
v
)
du
detail
:
:
U16LanePointer
(
p
)
num_lanes
)
)
;
}
#
define
HWY_RVV_STORE
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
void
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
p
)
{
\
return
__riscv_v
#
#
OP
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
\
detail
:
:
NativeLanePointer
(
p
)
v
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_STORE
Store
se
_ALL_VIRT
)
#
undef
HWY_RVV_STORE
template
<
class
D
HWY_RVV_IF_EMULATED_D
(
D
)
>
HWY_API
void
Store
(
VFromD
<
D
>
v
D
d
TFromD
<
D
>
*
HWY_RESTRICT
p
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
Store
(
BitCast
(
du
v
)
du
detail
:
:
U16LanePointer
(
p
)
)
;
}
#
define
HWY_RVV_BLENDED_STORE
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
void
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
HWY_RVV_M
(
MLEN
)
m
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
p
)
{
\
return
__riscv_v
#
#
OP
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_m
(
\
m
detail
:
:
NativeLanePointer
(
p
)
v
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_BLENDED_STORE
BlendedStore
se
_ALL_VIRT
)
#
undef
HWY_RVV_BLENDED_STORE
template
<
class
D
HWY_RVV_IF_EMULATED_D
(
D
)
>
HWY_API
void
BlendedStore
(
VFromD
<
D
>
v
MFromD
<
D
>
m
D
d
TFromD
<
D
>
*
HWY_RESTRICT
p
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
BlendedStore
(
BitCast
(
du
v
)
RebindMask
(
du
m
)
du
detail
:
:
U16LanePointer
(
p
)
)
;
}
namespace
detail
{
#
define
HWY_RVV_STOREN
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
void
NAME
(
size_t
count
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
/
*
d
*
/
\
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
p
)
{
\
return
__riscv_v
#
#
OP
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
\
detail
:
:
NativeLanePointer
(
p
)
v
count
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_STOREN
StoreN
se
_ALL_VIRT
)
#
undef
HWY_RVV_STOREN
template
<
class
D
HWY_RVV_IF_EMULATED_D
(
D
)
>
HWY_API
void
StoreN
(
size_t
count
VFromD
<
D
>
v
D
d
TFromD
<
D
>
*
HWY_RESTRICT
p
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
StoreN
(
count
BitCast
(
du
v
)
du
detail
:
:
U16LanePointer
(
p
)
)
;
}
}
#
ifdef
HWY_NATIVE_STORE_N
#
undef
HWY_NATIVE_STORE_N
#
else
#
define
HWY_NATIVE_STORE_N
#
endif
template
<
class
D
>
HWY_API
void
StoreN
(
VFromD
<
D
>
v
D
d
TFromD
<
D
>
*
HWY_RESTRICT
p
size_t
max_lanes_to_store
)
{
const
size_t
N
=
Lanes
(
d
)
;
detail
:
:
StoreN
(
HWY_MIN
(
max_lanes_to_store
N
)
v
d
p
)
;
}
template
<
class
V
class
D
>
HWY_API
void
StoreU
(
const
V
v
D
d
TFromD
<
D
>
*
HWY_RESTRICT
p
)
{
Store
(
v
d
p
)
;
}
template
<
class
V
class
D
typename
T
>
HWY_API
void
Stream
(
const
V
v
D
d
T
*
HWY_RESTRICT
aligned
)
{
Store
(
v
d
aligned
)
;
}
#
ifdef
HWY_NATIVE_SCATTER
#
undef
HWY_NATIVE_SCATTER
#
else
#
define
HWY_NATIVE_SCATTER
#
endif
#
define
HWY_RVV_SCATTER
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
void
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
base
\
HWY_RVV_V
(
int
SEW
LMUL
)
offset
)
{
\
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
\
return
__riscv_v
#
#
OP
#
#
ei
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
\
detail
:
:
NativeLanePointer
(
base
)
BitCast
(
du
offset
)
v
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_SCATTER
ScatterOffset
sux
_ALL_VIRT
)
#
undef
HWY_RVV_SCATTER
template
<
class
D
>
HWY_API
void
ScatterIndex
(
VFromD
<
D
>
v
D
d
TFromD
<
D
>
*
HWY_RESTRICT
base
VFromD
<
RebindToSigned
<
D
>
>
indices
)
{
constexpr
size_t
kBits
=
CeilLog2
(
sizeof
(
TFromD
<
D
>
)
)
;
return
ScatterOffset
(
v
d
base
ShiftLeft
<
kBits
>
(
indices
)
)
;
}
#
define
HWY_RVV_MASKED_SCATTER
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
\
LMULH
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
void
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
HWY_RVV_M
(
MLEN
)
m
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
base
\
HWY_RVV_V
(
int
SEW
LMUL
)
indices
)
{
\
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
\
constexpr
size_t
kBits
=
CeilLog2
(
sizeof
(
TFromD
<
decltype
(
d
)
>
)
)
;
\
return
__riscv_v
#
#
OP
#
#
ei
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_m
(
\
m
detail
:
:
NativeLanePointer
(
base
)
\
ShiftLeft
<
kBits
>
(
BitCast
(
du
indices
)
)
v
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_MASKED_SCATTER
MaskedScatterIndex
sux
_ALL_VIRT
)
#
undef
HWY_RVV_MASKED_SCATTER
#
ifdef
HWY_NATIVE_GATHER
#
undef
HWY_NATIVE_GATHER
#
else
#
define
HWY_NATIVE_GATHER
#
endif
#
define
HWY_RVV_GATHER
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
const
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
base
\
HWY_RVV_V
(
int
SEW
LMUL
)
offset
)
{
\
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
\
return
__riscv_v
#
#
OP
#
#
ei
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
\
detail
:
:
NativeLanePointer
(
base
)
BitCast
(
du
offset
)
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_GATHER
GatherOffset
lux
_ALL_VIRT
)
#
undef
HWY_RVV_GATHER
template
<
class
D
>
HWY_API
VFromD
<
D
>
GatherIndex
(
D
d
const
TFromD
<
D
>
*
HWY_RESTRICT
base
const
VFromD
<
RebindToSigned
<
D
>
>
index
)
{
constexpr
size_t
kBits
=
CeilLog2
(
sizeof
(
TFromD
<
D
>
)
)
;
return
GatherOffset
(
d
base
ShiftLeft
<
kBits
>
(
index
)
)
;
}
#
define
HWY_RVV_MASKED_GATHER
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
no
HWY_RVV_M
(
MLEN
)
m
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
const
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
base
\
HWY_RVV_V
(
int
SEW
LMUL
)
indices
)
{
\
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
\
const
RebindToSigned
<
decltype
(
d
)
>
di
;
\
(
void
)
di
;
/
*
for
HWY_DASSERT
*
/
\
constexpr
size_t
kBits
=
CeilLog2
(
SEW
/
8
)
;
\
HWY_DASSERT
(
AllFalse
(
di
Lt
(
indices
Zero
(
di
)
)
)
)
;
\
return
__riscv_v
#
#
OP
#
#
ei
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_mu
(
\
m
no
detail
:
:
NativeLanePointer
(
base
)
\
ShiftLeft
<
kBits
>
(
BitCast
(
du
indices
)
)
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_MASKED_GATHER
MaskedGatherIndexOr
lux
_ALL_VIRT
)
#
undef
HWY_RVV_MASKED_GATHER
template
<
class
D
>
HWY_API
VFromD
<
D
>
MaskedGatherIndex
(
MFromD
<
D
>
m
D
d
const
TFromD
<
D
>
*
base
VFromD
<
RebindToSigned
<
D
>
>
indices
)
{
return
MaskedGatherIndexOr
(
Zero
(
d
)
m
d
base
indices
)
;
}
#
define
HWY_RVV_PROMOTE
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEWD
LMULD
)
NAME
(
\
HWY_RVV_D
(
BASE
SEWD
N
SHIFT
+
1
)
d
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
CHAR
#
#
SEWD
#
#
LMULD
(
v
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH_U08
(
HWY_RVV_PROMOTE
PromoteTo
zext_vf2_
_EXT_VIRT
)
HWY_RVV_FOREACH_U16
(
HWY_RVV_PROMOTE
PromoteTo
zext_vf2_
_EXT_VIRT
)
HWY_RVV_FOREACH_U32
(
HWY_RVV_PROMOTE
PromoteTo
zext_vf2_
_EXT_VIRT
)
HWY_RVV_FOREACH_I08
(
HWY_RVV_PROMOTE
PromoteTo
sext_vf2_
_EXT_VIRT
)
HWY_RVV_FOREACH_I16
(
HWY_RVV_PROMOTE
PromoteTo
sext_vf2_
_EXT_VIRT
)
HWY_RVV_FOREACH_I32
(
HWY_RVV_PROMOTE
PromoteTo
sext_vf2_
_EXT_VIRT
)
HWY_RVV_FOREACH_F32
(
HWY_RVV_PROMOTE
PromoteTo
fwcvt_f_f_v_
_EXT_VIRT
)
#
if
HWY_HAVE_FLOAT16
|
|
HWY_RVV_HAVE_F16C
HWY_RVV_FOREACH_F16_UNCONDITIONAL
(
HWY_RVV_PROMOTE
PromoteTo
fwcvt_f_f_v_
_EXT_VIRT
)
#
ifdef
HWY_NATIVE_F16C
#
undef
HWY_NATIVE_F16C
#
else
#
define
HWY_NATIVE_F16C
#
endif
#
endif
#
undef
HWY_RVV_PROMOTE
#
define
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
LMUL
LMUL_IN
\
SHIFT
ADD
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
BITS
LMUL
)
\
PromoteTo
(
HWY_RVV_D
(
BASE
BITS
N
SHIFT
+
ADD
)
d
\
HWY_RVV_V
(
BASE_IN
BITS_IN
LMUL_IN
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
CHAR
#
#
BITS
#
#
LMUL
(
v
Lanes
(
d
)
)
;
\
}
#
define
HWY_RVV_PROMOTE_X2
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
)
\
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
m1
mf2
-
2
1
)
\
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
m1
mf2
-
1
1
)
\
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
m2
m1
0
1
)
\
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
m4
m2
1
1
)
\
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
m8
m4
2
1
)
#
define
HWY_RVV_PROMOTE_X4
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
)
\
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
m1
mf4
-
2
2
)
\
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
m2
mf2
-
1
2
)
\
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
m4
m1
0
2
)
\
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
m8
m2
1
2
)
#
define
HWY_RVV_PROMOTE_X4_FROM_U8
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
)
\
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
mf2
mf8
-
3
2
)
\
HWY_RVV_PROMOTE_X4
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
)
#
define
HWY_RVV_PROMOTE_X8
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
)
\
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
m1
mf8
-
3
3
)
\
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
m2
mf4
-
2
3
)
\
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
m4
mf2
-
1
3
)
\
HWY_RVV_PROMOTE
(
OP
BASE
CHAR
BITS
BASE_IN
BITS_IN
m8
m1
0
3
)
HWY_RVV_PROMOTE_X8
(
zext_vf8_
uint
u
64
uint
8
)
HWY_RVV_PROMOTE_X8
(
sext_vf8_
int
i
64
int
8
)
HWY_RVV_PROMOTE_X4_FROM_U8
(
zext_vf4_
uint
u
32
uint
8
)
HWY_RVV_PROMOTE_X4_FROM_U8
(
sext_vf4_
int
i
32
int
8
)
HWY_RVV_PROMOTE_X4
(
zext_vf4_
uint
u
64
uint
16
)
HWY_RVV_PROMOTE_X4
(
sext_vf4_
int
i
64
int
16
)
HWY_RVV_PROMOTE_X2
(
fwcvt_f_x_v_
float
f
64
int
32
)
HWY_RVV_PROMOTE_X2
(
fwcvt_f_xu_v_
float
f
64
uint
32
)
HWY_RVV_PROMOTE_X2
(
fwcvt_rtz_x_f_v_
int
i
64
float
32
)
HWY_RVV_PROMOTE_X2
(
fwcvt_rtz_xu_f_v_
uint
u
64
float
32
)
#
undef
HWY_RVV_PROMOTE_X8
#
undef
HWY_RVV_PROMOTE_X4_FROM_U8
#
undef
HWY_RVV_PROMOTE_X4
#
undef
HWY_RVV_PROMOTE_X2
#
undef
HWY_RVV_PROMOTE
template
<
size_t
N
>
HWY_API
auto
PromoteTo
(
Simd
<
int64_t
N
-
1
>
d
VFromD
<
Rebind
<
int16_t
decltype
(
d
)
>
>
v
)
-
>
VFromD
<
decltype
(
d
)
>
{
return
PromoteTo
(
ScalableTag
<
int64_t
>
(
)
v
)
;
}
template
<
size_t
N
>
HWY_API
auto
PromoteTo
(
Simd
<
uint64_t
N
-
1
>
d
VFromD
<
Rebind
<
uint16_t
decltype
(
d
)
>
>
v
)
-
>
VFromD
<
decltype
(
d
)
>
{
return
PromoteTo
(
ScalableTag
<
uint64_t
>
(
)
v
)
;
}
template
<
size_t
N
int
kPow2
>
HWY_API
auto
PromoteTo
(
Simd
<
int16_t
N
kPow2
>
d
VFromD
<
Rebind
<
uint8_t
decltype
(
d
)
>
>
v
)
-
>
VFromD
<
decltype
(
d
)
>
{
return
BitCast
(
d
PromoteTo
(
RebindToUnsigned
<
decltype
(
d
)
>
(
)
v
)
)
;
}
template
<
size_t
N
int
kPow2
>
HWY_API
auto
PromoteTo
(
Simd
<
int32_t
N
kPow2
>
d
VFromD
<
Rebind
<
uint8_t
decltype
(
d
)
>
>
v
)
-
>
VFromD
<
decltype
(
d
)
>
{
return
BitCast
(
d
PromoteTo
(
RebindToUnsigned
<
decltype
(
d
)
>
(
)
v
)
)
;
}
template
<
size_t
N
int
kPow2
>
HWY_API
auto
PromoteTo
(
Simd
<
int32_t
N
kPow2
>
d
VFromD
<
Rebind
<
uint16_t
decltype
(
d
)
>
>
v
)
-
>
VFromD
<
decltype
(
d
)
>
{
return
BitCast
(
d
PromoteTo
(
RebindToUnsigned
<
decltype
(
d
)
>
(
)
v
)
)
;
}
template
<
size_t
N
int
kPow2
>
HWY_API
auto
PromoteTo
(
Simd
<
int64_t
N
kPow2
>
d
VFromD
<
Rebind
<
uint32_t
decltype
(
d
)
>
>
v
)
-
>
VFromD
<
decltype
(
d
)
>
{
return
BitCast
(
d
PromoteTo
(
RebindToUnsigned
<
decltype
(
d
)
>
(
)
v
)
)
;
}
template
<
size_t
N
int
kPow2
>
HWY_API
auto
PromoteTo
(
Simd
<
int64_t
N
kPow2
>
d
VFromD
<
Rebind
<
uint16_t
decltype
(
d
)
>
>
v
)
-
>
VFromD
<
decltype
(
d
)
>
{
return
BitCast
(
d
PromoteTo
(
RebindToUnsigned
<
decltype
(
d
)
>
(
)
v
)
)
;
}
template
<
size_t
N
int
kPow2
>
HWY_API
auto
PromoteTo
(
Simd
<
int64_t
N
kPow2
>
d
VFromD
<
Rebind
<
uint8_t
decltype
(
d
)
>
>
v
)
-
>
VFromD
<
decltype
(
d
)
>
{
return
BitCast
(
d
PromoteTo
(
RebindToUnsigned
<
decltype
(
d
)
>
(
)
v
)
)
;
}
template
<
size_t
N
int
kPow2
>
HWY_API
auto
PromoteTo
(
Simd
<
float32_t
N
kPow2
>
d
VFromD
<
Rebind
<
hwy
:
:
bfloat16_t
decltype
(
d
)
>
>
v
)
-
>
VFromD
<
decltype
(
d
)
>
{
const
RebindToSigned
<
decltype
(
d
)
>
di32
;
const
Rebind
<
uint16_t
decltype
(
d
)
>
du16
;
return
BitCast
(
d
ShiftLeft
<
16
>
(
PromoteTo
(
di32
BitCast
(
du16
v
)
)
)
)
;
}
#
define
HWY_RVV_DEMOTE
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEWH
LMULH
)
NAME
(
\
HWY_RVV_D
(
BASE
SEWH
N
SHIFT
-
1
)
d
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
CHAR
#
#
SEWH
#
#
LMULH
(
\
v
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
Lanes
(
d
)
)
)
;
\
}
HWY_RVV_FOREACH_U16
(
HWY_RVV_DEMOTE
DemoteTo
nclipu_wx_
_DEMOTE_VIRT
)
HWY_RVV_FOREACH_U32
(
HWY_RVV_DEMOTE
DemoteTo
nclipu_wx_
_DEMOTE_VIRT
)
HWY_RVV_FOREACH_U64
(
HWY_RVV_DEMOTE
DemoteTo
nclipu_wx_
_DEMOTE_VIRT
)
#
define
HWY_RVV_DEMOTE_I_TO_U
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
uint
SEWH
LMULH
)
NAME
(
\
HWY_RVV_D
(
uint
SEWH
N
SHIFT
-
1
)
dn
HWY_RVV_V
(
int
SEW
LMUL
)
v
)
{
\
const
HWY_RVV_D
(
uint
SEW
N
SHIFT
)
du
;
\
\
return
DemoteTo
(
dn
BitCast
(
du
detail
:
:
MaxS
(
v
0
)
)
)
;
\
}
HWY_RVV_FOREACH_I64
(
HWY_RVV_DEMOTE_I_TO_U
DemoteTo
_
_DEMOTE_VIRT
)
HWY_RVV_FOREACH_I32
(
HWY_RVV_DEMOTE_I_TO_U
DemoteTo
_
_DEMOTE_VIRT
)
HWY_RVV_FOREACH_I16
(
HWY_RVV_DEMOTE_I_TO_U
DemoteTo
_
_DEMOTE_VIRT
)
#
undef
HWY_RVV_DEMOTE_I_TO_U
template
<
size_t
N
>
HWY_API
vuint8mf8_t
DemoteTo
(
Simd
<
uint8_t
N
-
3
>
d
const
vint32mf2_t
v
)
{
return
__riscv_vnclipu_wx_u8mf8
(
DemoteTo
(
Simd
<
uint16_t
N
-
2
>
(
)
v
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
Lanes
(
d
)
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8mf4_t
DemoteTo
(
Simd
<
uint8_t
N
-
2
>
d
const
vint32m1_t
v
)
{
return
__riscv_vnclipu_wx_u8mf4
(
DemoteTo
(
Simd
<
uint16_t
N
-
1
>
(
)
v
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
Lanes
(
d
)
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8mf2_t
DemoteTo
(
Simd
<
uint8_t
N
-
1
>
d
const
vint32m2_t
v
)
{
return
__riscv_vnclipu_wx_u8mf2
(
DemoteTo
(
Simd
<
uint16_t
N
0
>
(
)
v
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
Lanes
(
d
)
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8m1_t
DemoteTo
(
Simd
<
uint8_t
N
0
>
d
const
vint32m4_t
v
)
{
return
__riscv_vnclipu_wx_u8m1
(
DemoteTo
(
Simd
<
uint16_t
N
1
>
(
)
v
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
Lanes
(
d
)
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8m2_t
DemoteTo
(
Simd
<
uint8_t
N
1
>
d
const
vint32m8_t
v
)
{
return
__riscv_vnclipu_wx_u8m2
(
DemoteTo
(
Simd
<
uint16_t
N
2
>
(
)
v
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
Lanes
(
d
)
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8mf8_t
DemoteTo
(
Simd
<
uint8_t
N
-
3
>
d
const
vuint32mf2_t
v
)
{
return
__riscv_vnclipu_wx_u8mf8
(
DemoteTo
(
Simd
<
uint16_t
N
-
2
>
(
)
v
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
Lanes
(
d
)
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8mf4_t
DemoteTo
(
Simd
<
uint8_t
N
-
2
>
d
const
vuint32m1_t
v
)
{
return
__riscv_vnclipu_wx_u8mf4
(
DemoteTo
(
Simd
<
uint16_t
N
-
1
>
(
)
v
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
Lanes
(
d
)
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8mf2_t
DemoteTo
(
Simd
<
uint8_t
N
-
1
>
d
const
vuint32m2_t
v
)
{
return
__riscv_vnclipu_wx_u8mf2
(
DemoteTo
(
Simd
<
uint16_t
N
0
>
(
)
v
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
Lanes
(
d
)
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8m1_t
DemoteTo
(
Simd
<
uint8_t
N
0
>
d
const
vuint32m4_t
v
)
{
return
__riscv_vnclipu_wx_u8m1
(
DemoteTo
(
Simd
<
uint16_t
N
1
>
(
)
v
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
Lanes
(
d
)
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8m2_t
DemoteTo
(
Simd
<
uint8_t
N
1
>
d
const
vuint32m8_t
v
)
{
return
__riscv_vnclipu_wx_u8m2
(
DemoteTo
(
Simd
<
uint16_t
N
2
>
(
)
v
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
Lanes
(
d
)
)
)
;
}
template
<
size_t
N
int
kPow2
>
HWY_API
VFromD
<
Simd
<
uint8_t
N
kPow2
>
>
DemoteTo
(
Simd
<
uint8_t
N
kPow2
>
d
VFromD
<
Simd
<
int64_t
N
kPow2
+
3
>
>
v
)
{
return
DemoteTo
(
d
DemoteTo
(
Simd
<
uint32_t
N
kPow2
+
2
>
(
)
v
)
)
;
}
template
<
size_t
N
int
kPow2
>
HWY_API
VFromD
<
Simd
<
uint8_t
N
kPow2
>
>
DemoteTo
(
Simd
<
uint8_t
N
kPow2
>
d
VFromD
<
Simd
<
uint64_t
N
kPow2
+
3
>
>
v
)
{
return
DemoteTo
(
d
DemoteTo
(
Simd
<
uint32_t
N
kPow2
+
2
>
(
)
v
)
)
;
}
template
<
size_t
N
int
kPow2
>
HWY_API
VFromD
<
Simd
<
uint16_t
N
kPow2
>
>
DemoteTo
(
Simd
<
uint16_t
N
kPow2
>
d
VFromD
<
Simd
<
int64_t
N
kPow2
+
2
>
>
v
)
{
return
DemoteTo
(
d
DemoteTo
(
Simd
<
uint32_t
N
kPow2
+
1
>
(
)
v
)
)
;
}
template
<
size_t
N
int
kPow2
>
HWY_API
VFromD
<
Simd
<
uint16_t
N
kPow2
>
>
DemoteTo
(
Simd
<
uint16_t
N
kPow2
>
d
VFromD
<
Simd
<
uint64_t
N
kPow2
+
2
>
>
v
)
{
return
DemoteTo
(
d
DemoteTo
(
Simd
<
uint32_t
N
kPow2
+
1
>
(
)
v
)
)
;
}
HWY_API
vuint8mf8_t
U8FromU32
(
const
vuint32mf2_t
v
)
{
const
size_t
avl
=
Lanes
(
ScalableTag
<
uint8_t
-
3
>
(
)
)
;
return
__riscv_vnclipu_wx_u8mf8
(
__riscv_vnclipu_wx_u16mf4
(
v
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
HWY_API
vuint8mf4_t
U8FromU32
(
const
vuint32m1_t
v
)
{
const
size_t
avl
=
Lanes
(
ScalableTag
<
uint8_t
-
2
>
(
)
)
;
return
__riscv_vnclipu_wx_u8mf4
(
__riscv_vnclipu_wx_u16mf2
(
v
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
HWY_API
vuint8mf2_t
U8FromU32
(
const
vuint32m2_t
v
)
{
const
size_t
avl
=
Lanes
(
ScalableTag
<
uint8_t
-
1
>
(
)
)
;
return
__riscv_vnclipu_wx_u8mf2
(
__riscv_vnclipu_wx_u16m1
(
v
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
HWY_API
vuint8m1_t
U8FromU32
(
const
vuint32m4_t
v
)
{
const
size_t
avl
=
Lanes
(
ScalableTag
<
uint8_t
0
>
(
)
)
;
return
__riscv_vnclipu_wx_u8m1
(
__riscv_vnclipu_wx_u16m2
(
v
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
HWY_API
vuint8m2_t
U8FromU32
(
const
vuint32m8_t
v
)
{
const
size_t
avl
=
Lanes
(
ScalableTag
<
uint8_t
1
>
(
)
)
;
return
__riscv_vnclipu_wx_u8m2
(
__riscv_vnclipu_wx_u16m4
(
v
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8mf8_t
TruncateTo
(
Simd
<
uint8_t
N
-
3
>
d
const
VFromD
<
Simd
<
uint64_t
N
0
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint64m1_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
const
vuint32mf2_t
v2
=
__riscv_vnclipu_wx_u32mf2
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
const
vuint16mf4_t
v3
=
__riscv_vnclipu_wx_u16mf4
(
v2
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
return
__riscv_vnclipu_wx_u8mf8
(
v3
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8mf4_t
TruncateTo
(
Simd
<
uint8_t
N
-
2
>
d
const
VFromD
<
Simd
<
uint64_t
N
1
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint64m2_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
const
vuint32m1_t
v2
=
__riscv_vnclipu_wx_u32m1
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
const
vuint16mf2_t
v3
=
__riscv_vnclipu_wx_u16mf2
(
v2
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
return
__riscv_vnclipu_wx_u8mf4
(
v3
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8mf2_t
TruncateTo
(
Simd
<
uint8_t
N
-
1
>
d
const
VFromD
<
Simd
<
uint64_t
N
2
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint64m4_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
const
vuint32m2_t
v2
=
__riscv_vnclipu_wx_u32m2
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
const
vuint16m1_t
v3
=
__riscv_vnclipu_wx_u16m1
(
v2
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
return
__riscv_vnclipu_wx_u8mf2
(
v3
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8m1_t
TruncateTo
(
Simd
<
uint8_t
N
0
>
d
const
VFromD
<
Simd
<
uint64_t
N
3
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint64m8_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
const
vuint32m4_t
v2
=
__riscv_vnclipu_wx_u32m4
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
const
vuint16m2_t
v3
=
__riscv_vnclipu_wx_u16m2
(
v2
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
return
__riscv_vnclipu_wx_u8m1
(
v3
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint16mf4_t
TruncateTo
(
Simd
<
uint16_t
N
-
3
>
d
const
VFromD
<
Simd
<
uint64_t
N
-
1
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint64m1_t
v1
=
__riscv_vand
(
v
0xFFFF
avl
)
;
const
vuint32mf2_t
v2
=
__riscv_vnclipu_wx_u32mf2
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
return
__riscv_vnclipu_wx_u16mf4
(
v2
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint16mf4_t
TruncateTo
(
Simd
<
uint16_t
N
-
2
>
d
const
VFromD
<
Simd
<
uint64_t
N
0
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint64m1_t
v1
=
__riscv_vand
(
v
0xFFFF
avl
)
;
const
vuint32mf2_t
v2
=
__riscv_vnclipu_wx_u32mf2
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
return
__riscv_vnclipu_wx_u16mf4
(
v2
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint16mf2_t
TruncateTo
(
Simd
<
uint16_t
N
-
1
>
d
const
VFromD
<
Simd
<
uint64_t
N
1
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint64m2_t
v1
=
__riscv_vand
(
v
0xFFFF
avl
)
;
const
vuint32m1_t
v2
=
__riscv_vnclipu_wx_u32m1
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
return
__riscv_vnclipu_wx_u16mf2
(
v2
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint16m1_t
TruncateTo
(
Simd
<
uint16_t
N
0
>
d
const
VFromD
<
Simd
<
uint64_t
N
2
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint64m4_t
v1
=
__riscv_vand
(
v
0xFFFF
avl
)
;
const
vuint32m2_t
v2
=
__riscv_vnclipu_wx_u32m2
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
return
__riscv_vnclipu_wx_u16m1
(
v2
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint16m2_t
TruncateTo
(
Simd
<
uint16_t
N
1
>
d
const
VFromD
<
Simd
<
uint64_t
N
3
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint64m8_t
v1
=
__riscv_vand
(
v
0xFFFF
avl
)
;
const
vuint32m4_t
v2
=
__riscv_vnclipu_wx_u32m4
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
return
__riscv_vnclipu_wx_u16m2
(
v2
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint32mf2_t
TruncateTo
(
Simd
<
uint32_t
N
-
2
>
d
const
VFromD
<
Simd
<
uint64_t
N
-
1
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint64m1_t
v1
=
__riscv_vand
(
v
0xFFFFFFFFu
avl
)
;
return
__riscv_vnclipu_wx_u32mf2
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint32mf2_t
TruncateTo
(
Simd
<
uint32_t
N
-
1
>
d
const
VFromD
<
Simd
<
uint64_t
N
0
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint64m1_t
v1
=
__riscv_vand
(
v
0xFFFFFFFFu
avl
)
;
return
__riscv_vnclipu_wx_u32mf2
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint32m1_t
TruncateTo
(
Simd
<
uint32_t
N
0
>
d
const
VFromD
<
Simd
<
uint64_t
N
1
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint64m2_t
v1
=
__riscv_vand
(
v
0xFFFFFFFFu
avl
)
;
return
__riscv_vnclipu_wx_u32m1
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint32m2_t
TruncateTo
(
Simd
<
uint32_t
N
1
>
d
const
VFromD
<
Simd
<
uint64_t
N
2
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint64m4_t
v1
=
__riscv_vand
(
v
0xFFFFFFFFu
avl
)
;
return
__riscv_vnclipu_wx_u32m2
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint32m4_t
TruncateTo
(
Simd
<
uint32_t
N
2
>
d
const
VFromD
<
Simd
<
uint64_t
N
3
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint64m8_t
v1
=
__riscv_vand
(
v
0xFFFFFFFFu
avl
)
;
return
__riscv_vnclipu_wx_u32m4
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8mf8_t
TruncateTo
(
Simd
<
uint8_t
N
-
3
>
d
const
VFromD
<
Simd
<
uint32_t
N
-
1
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint32mf2_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
const
vuint16mf4_t
v2
=
__riscv_vnclipu_wx_u16mf4
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
return
__riscv_vnclipu_wx_u8mf8
(
v2
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8mf4_t
TruncateTo
(
Simd
<
uint8_t
N
-
2
>
d
const
VFromD
<
Simd
<
uint32_t
N
0
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint32m1_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
const
vuint16mf2_t
v2
=
__riscv_vnclipu_wx_u16mf2
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
return
__riscv_vnclipu_wx_u8mf4
(
v2
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8mf2_t
TruncateTo
(
Simd
<
uint8_t
N
-
1
>
d
const
VFromD
<
Simd
<
uint32_t
N
1
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint32m2_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
const
vuint16m1_t
v2
=
__riscv_vnclipu_wx_u16m1
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
return
__riscv_vnclipu_wx_u8mf2
(
v2
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8m1_t
TruncateTo
(
Simd
<
uint8_t
N
0
>
d
const
VFromD
<
Simd
<
uint32_t
N
2
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint32m4_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
const
vuint16m2_t
v2
=
__riscv_vnclipu_wx_u16m2
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
return
__riscv_vnclipu_wx_u8m1
(
v2
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8m2_t
TruncateTo
(
Simd
<
uint8_t
N
1
>
d
const
VFromD
<
Simd
<
uint32_t
N
3
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint32m8_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
const
vuint16m4_t
v2
=
__riscv_vnclipu_wx_u16m4
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
return
__riscv_vnclipu_wx_u8m2
(
v2
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint16mf4_t
TruncateTo
(
Simd
<
uint16_t
N
-
3
>
d
const
VFromD
<
Simd
<
uint32_t
N
-
2
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint32mf2_t
v1
=
__riscv_vand
(
v
0xFFFF
avl
)
;
return
__riscv_vnclipu_wx_u16mf4
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint16mf4_t
TruncateTo
(
Simd
<
uint16_t
N
-
2
>
d
const
VFromD
<
Simd
<
uint32_t
N
-
1
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint32mf2_t
v1
=
__riscv_vand
(
v
0xFFFF
avl
)
;
return
__riscv_vnclipu_wx_u16mf4
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint16mf2_t
TruncateTo
(
Simd
<
uint16_t
N
-
1
>
d
const
VFromD
<
Simd
<
uint32_t
N
0
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint32m1_t
v1
=
__riscv_vand
(
v
0xFFFF
avl
)
;
return
__riscv_vnclipu_wx_u16mf2
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint16m1_t
TruncateTo
(
Simd
<
uint16_t
N
0
>
d
const
VFromD
<
Simd
<
uint32_t
N
1
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint32m2_t
v1
=
__riscv_vand
(
v
0xFFFF
avl
)
;
return
__riscv_vnclipu_wx_u16m1
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint16m2_t
TruncateTo
(
Simd
<
uint16_t
N
1
>
d
const
VFromD
<
Simd
<
uint32_t
N
2
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint32m4_t
v1
=
__riscv_vand
(
v
0xFFFF
avl
)
;
return
__riscv_vnclipu_wx_u16m2
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint16m4_t
TruncateTo
(
Simd
<
uint16_t
N
2
>
d
const
VFromD
<
Simd
<
uint32_t
N
3
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint32m8_t
v1
=
__riscv_vand
(
v
0xFFFF
avl
)
;
return
__riscv_vnclipu_wx_u16m4
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8mf8_t
TruncateTo
(
Simd
<
uint8_t
N
-
3
>
d
const
VFromD
<
Simd
<
uint16_t
N
-
2
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint16mf4_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
return
__riscv_vnclipu_wx_u8mf8
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8mf4_t
TruncateTo
(
Simd
<
uint8_t
N
-
2
>
d
const
VFromD
<
Simd
<
uint16_t
N
-
1
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint16mf2_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
return
__riscv_vnclipu_wx_u8mf4
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8mf2_t
TruncateTo
(
Simd
<
uint8_t
N
-
1
>
d
const
VFromD
<
Simd
<
uint16_t
N
0
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint16m1_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
return
__riscv_vnclipu_wx_u8mf2
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8m1_t
TruncateTo
(
Simd
<
uint8_t
N
0
>
d
const
VFromD
<
Simd
<
uint16_t
N
1
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint16m2_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
return
__riscv_vnclipu_wx_u8m1
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8m2_t
TruncateTo
(
Simd
<
uint8_t
N
1
>
d
const
VFromD
<
Simd
<
uint16_t
N
2
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint16m4_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
return
__riscv_vnclipu_wx_u8m2
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint8m4_t
TruncateTo
(
Simd
<
uint8_t
N
2
>
d
const
VFromD
<
Simd
<
uint16_t
N
3
>
>
v
)
{
const
size_t
avl
=
Lanes
(
d
)
;
const
vuint16m8_t
v1
=
__riscv_vand
(
v
0xFF
avl
)
;
return
__riscv_vnclipu_wx_u8m4
(
v1
0
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
avl
)
)
;
}
HWY_RVV_FOREACH_I16
(
HWY_RVV_DEMOTE
DemoteTo
nclip_wx_
_DEMOTE_VIRT
)
HWY_RVV_FOREACH_I32
(
HWY_RVV_DEMOTE
DemoteTo
nclip_wx_
_DEMOTE_VIRT
)
HWY_RVV_FOREACH_I64
(
HWY_RVV_DEMOTE
DemoteTo
nclip_wx_
_DEMOTE_VIRT
)
template
<
size_t
N
>
HWY_API
vint8mf8_t
DemoteTo
(
Simd
<
int8_t
N
-
3
>
d
const
vint32mf2_t
v
)
{
return
DemoteTo
(
d
DemoteTo
(
Simd
<
int16_t
N
-
2
>
(
)
v
)
)
;
}
template
<
size_t
N
>
HWY_API
vint8mf4_t
DemoteTo
(
Simd
<
int8_t
N
-
2
>
d
const
vint32m1_t
v
)
{
return
DemoteTo
(
d
DemoteTo
(
Simd
<
int16_t
N
-
1
>
(
)
v
)
)
;
}
template
<
size_t
N
>
HWY_API
vint8mf2_t
DemoteTo
(
Simd
<
int8_t
N
-
1
>
d
const
vint32m2_t
v
)
{
return
DemoteTo
(
d
DemoteTo
(
Simd
<
int16_t
N
0
>
(
)
v
)
)
;
}
template
<
size_t
N
>
HWY_API
vint8m1_t
DemoteTo
(
Simd
<
int8_t
N
0
>
d
const
vint32m4_t
v
)
{
return
DemoteTo
(
d
DemoteTo
(
Simd
<
int16_t
N
1
>
(
)
v
)
)
;
}
template
<
size_t
N
>
HWY_API
vint8m2_t
DemoteTo
(
Simd
<
int8_t
N
1
>
d
const
vint32m8_t
v
)
{
return
DemoteTo
(
d
DemoteTo
(
Simd
<
int16_t
N
2
>
(
)
v
)
)
;
}
template
<
size_t
N
int
kPow2
>
HWY_API
VFromD
<
Simd
<
int8_t
N
kPow2
>
>
DemoteTo
(
Simd
<
int8_t
N
kPow2
>
d
VFromD
<
Simd
<
int64_t
N
kPow2
+
3
>
>
v
)
{
return
DemoteTo
(
d
DemoteTo
(
Simd
<
int32_t
N
kPow2
+
2
>
(
)
v
)
)
;
}
template
<
size_t
N
int
kPow2
>
HWY_API
VFromD
<
Simd
<
int16_t
N
kPow2
>
>
DemoteTo
(
Simd
<
int16_t
N
kPow2
>
d
VFromD
<
Simd
<
int64_t
N
kPow2
+
2
>
>
v
)
{
return
DemoteTo
(
d
DemoteTo
(
Simd
<
int32_t
N
kPow2
+
1
>
(
)
v
)
)
;
}
#
undef
HWY_RVV_DEMOTE
#
define
HWY_RVV_DEMOTE_F
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEWH
LMULH
)
NAME
(
\
HWY_RVV_D
(
BASE
SEWH
N
SHIFT
-
1
)
d
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
SEWH
#
#
LMULH
(
v
Lanes
(
d
)
)
;
\
}
#
if
HWY_HAVE_FLOAT16
|
|
HWY_RVV_HAVE_F16C
HWY_RVV_FOREACH_F32
(
HWY_RVV_DEMOTE_F
DemoteTo
fncvt_rod_f_f_w_f
_DEMOTE_VIRT
)
#
endif
HWY_RVV_FOREACH_F64
(
HWY_RVV_DEMOTE_F
DemoteTo
fncvt_rod_f_f_w_f
_DEMOTE_VIRT
)
#
undef
HWY_RVV_DEMOTE_F
template
<
size_t
N
>
HWY_API
vint32mf2_t
DemoteTo
(
Simd
<
int32_t
N
-
2
>
d
const
vfloat64m1_t
v
)
{
return
__riscv_vfncvt_rtz_x_f_w_i32mf2
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vint32mf2_t
DemoteTo
(
Simd
<
int32_t
N
-
1
>
d
const
vfloat64m1_t
v
)
{
return
__riscv_vfncvt_rtz_x_f_w_i32mf2
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vint32m1_t
DemoteTo
(
Simd
<
int32_t
N
0
>
d
const
vfloat64m2_t
v
)
{
return
__riscv_vfncvt_rtz_x_f_w_i32m1
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vint32m2_t
DemoteTo
(
Simd
<
int32_t
N
1
>
d
const
vfloat64m4_t
v
)
{
return
__riscv_vfncvt_rtz_x_f_w_i32m2
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vint32m4_t
DemoteTo
(
Simd
<
int32_t
N
2
>
d
const
vfloat64m8_t
v
)
{
return
__riscv_vfncvt_rtz_x_f_w_i32m4
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint32mf2_t
DemoteTo
(
Simd
<
uint32_t
N
-
2
>
d
const
vfloat64m1_t
v
)
{
return
__riscv_vfncvt_rtz_xu_f_w_u32mf2
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint32mf2_t
DemoteTo
(
Simd
<
uint32_t
N
-
1
>
d
const
vfloat64m1_t
v
)
{
return
__riscv_vfncvt_rtz_xu_f_w_u32mf2
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint32m1_t
DemoteTo
(
Simd
<
uint32_t
N
0
>
d
const
vfloat64m2_t
v
)
{
return
__riscv_vfncvt_rtz_xu_f_w_u32m1
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint32m2_t
DemoteTo
(
Simd
<
uint32_t
N
1
>
d
const
vfloat64m4_t
v
)
{
return
__riscv_vfncvt_rtz_xu_f_w_u32m2
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vuint32m4_t
DemoteTo
(
Simd
<
uint32_t
N
2
>
d
const
vfloat64m8_t
v
)
{
return
__riscv_vfncvt_rtz_xu_f_w_u32m4
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vfloat32mf2_t
DemoteTo
(
Simd
<
float
N
-
2
>
d
const
vint64m1_t
v
)
{
return
__riscv_vfncvt_f_x_w_f32mf2
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vfloat32mf2_t
DemoteTo
(
Simd
<
float
N
-
1
>
d
const
vint64m1_t
v
)
{
return
__riscv_vfncvt_f_x_w_f32mf2
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vfloat32m1_t
DemoteTo
(
Simd
<
float
N
0
>
d
const
vint64m2_t
v
)
{
return
__riscv_vfncvt_f_x_w_f32m1
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vfloat32m2_t
DemoteTo
(
Simd
<
float
N
1
>
d
const
vint64m4_t
v
)
{
return
__riscv_vfncvt_f_x_w_f32m2
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vfloat32m4_t
DemoteTo
(
Simd
<
float
N
2
>
d
const
vint64m8_t
v
)
{
return
__riscv_vfncvt_f_x_w_f32m4
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vfloat32mf2_t
DemoteTo
(
Simd
<
float
N
-
2
>
d
const
vuint64m1_t
v
)
{
return
__riscv_vfncvt_f_xu_w_f32mf2
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vfloat32mf2_t
DemoteTo
(
Simd
<
float
N
-
1
>
d
const
vuint64m1_t
v
)
{
return
__riscv_vfncvt_f_xu_w_f32mf2
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vfloat32m1_t
DemoteTo
(
Simd
<
float
N
0
>
d
const
vuint64m2_t
v
)
{
return
__riscv_vfncvt_f_xu_w_f32m1
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vfloat32m2_t
DemoteTo
(
Simd
<
float
N
1
>
d
const
vuint64m4_t
v
)
{
return
__riscv_vfncvt_f_xu_w_f32m2
(
v
Lanes
(
d
)
)
;
}
template
<
size_t
N
>
HWY_API
vfloat32m4_t
DemoteTo
(
Simd
<
float
N
2
>
d
const
vuint64m8_t
v
)
{
return
__riscv_vfncvt_f_xu_w_f32m4
(
v
Lanes
(
d
)
)
;
}
#
define
HWY_RVV_DEMOTE_TO_SHR_16
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
\
LMULH
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEWH
LMULH
)
NAME
(
\
HWY_RVV_D
(
BASE
SEWH
N
SHIFT
-
1
)
d
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
CHAR
#
#
SEWH
#
#
LMULH
(
\
v
16
HWY_RVV_INSERT_VXRM
(
__RISCV_VXRM_RDN
Lanes
(
d
)
)
)
;
\
}
namespace
detail
{
HWY_RVV_FOREACH_U32
(
HWY_RVV_DEMOTE_TO_SHR_16
DemoteToShr16
nclipu_wx_
_DEMOTE_VIRT
)
}
#
undef
HWY_RVV_DEMOTE_TO_SHR_16
template
<
size_t
N
int
kPow2
>
HWY_API
VFromD
<
Simd
<
hwy
:
:
bfloat16_t
N
kPow2
>
>
DemoteTo
(
Simd
<
hwy
:
:
bfloat16_t
N
kPow2
>
d
VFromD
<
Simd
<
float
N
kPow2
+
1
>
>
v
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du16
;
const
Rebind
<
uint32_t
decltype
(
d
)
>
du32
;
return
BitCast
(
d
detail
:
:
DemoteToShr16
(
du16
BitCast
(
du32
v
)
)
)
;
}
#
define
HWY_RVV_CONVERT
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
ConvertTo
(
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
HWY_RVV_V
(
int
SEW
LMUL
)
v
)
{
\
return
__riscv_vfcvt_f_x_v_f
#
#
SEW
#
#
LMUL
(
v
Lanes
(
d
)
)
;
\
}
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
ConvertTo
(
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
HWY_RVV_V
(
uint
SEW
LMUL
)
v
)
{
\
return
__riscv_vfcvt_f_xu_v_f
#
#
SEW
#
#
LMUL
(
v
Lanes
(
d
)
)
;
\
}
\
/
*
Truncates
(
rounds
toward
zero
)
.
*
/
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
int
SEW
LMUL
)
ConvertTo
(
HWY_RVV_D
(
int
SEW
N
SHIFT
)
d
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_vfcvt_rtz_x_f_v_i
#
#
SEW
#
#
LMUL
(
v
Lanes
(
d
)
)
;
\
}
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
uint
SEW
LMUL
)
ConvertTo
(
\
HWY_RVV_D
(
uint
SEW
N
SHIFT
)
d
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_vfcvt_rtz_xu_f_v_u
#
#
SEW
#
#
LMUL
(
v
Lanes
(
d
)
)
;
\
}
\
/
/
API
only
requires
f32
but
we
provide
f64
for
internal
use
.
HWY_RVV_FOREACH_F
(
HWY_RVV_CONVERT
_
_
_ALL_VIRT
)
#
undef
HWY_RVV_CONVERT
#
define
HWY_RVV_NEAREST
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
int
SEW
LMUL
)
NearestInt
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_vfcvt_x_f_v_i
#
#
SEW
#
#
LMUL
(
v
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH_F
(
HWY_RVV_NEAREST
_
_
_ALL
)
#
undef
HWY_RVV_NEAREST
namespace
detail
{
template
<
typename
T
size_t
N
int
kPow2
>
HWY_INLINE
size_t
LanesPerBlock
(
Simd
<
T
N
kPow2
>
d
)
{
constexpr
size_t
kMinVecBytes
=
ScaleByPower
(
16
HWY_MAX
(
HWY_MIN
(
kPow2
3
)
-
3
)
)
;
constexpr
size_t
kMinVecLanes
=
(
kMinVecBytes
+
sizeof
(
T
)
-
1
)
/
sizeof
(
T
)
;
constexpr
size_t
kMaxLpb
=
HWY_MIN
(
16
/
sizeof
(
T
)
MaxLanes
(
d
)
)
;
if
(
kMaxLpb
<
=
kMinVecLanes
)
return
kMaxLpb
;
const
size_t
lanes_per_vec
=
Lanes
(
d
)
;
return
HWY_MIN
(
lanes_per_vec
kMaxLpb
)
;
}
template
<
class
D
class
V
>
HWY_INLINE
V
OffsetsOf128BitBlocks
(
const
D
d
const
V
iota0
)
{
using
T
=
MakeUnsigned
<
TFromD
<
D
>
>
;
return
AndS
(
iota0
static_cast
<
T
>
(
~
(
LanesPerBlock
(
d
)
-
1
)
)
)
;
}
template
<
size_t
kLanes
class
D
>
HWY_INLINE
MFromD
<
D
>
FirstNPerBlock
(
D
)
{
const
RebindToUnsigned
<
D
>
du
;
const
RebindToSigned
<
D
>
di
;
using
TU
=
TFromD
<
decltype
(
du
)
>
;
const
auto
idx_mod
=
AndS
(
Iota0
(
du
)
static_cast
<
TU
>
(
LanesPerBlock
(
du
)
-
1
)
)
;
return
LtS
(
BitCast
(
di
idx_mod
)
static_cast
<
TFromD
<
decltype
(
di
)
>
>
(
kLanes
)
)
;
}
#
define
HWY_RVV_SLIDE_UP
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
dst
HWY_RVV_V
(
BASE
SEW
LMUL
)
src
\
size_t
lanes
)
{
\
return
__riscv_v
#
#
OP
#
#
_vx_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
dst
src
lanes
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
#
define
HWY_RVV_SLIDE_DOWN
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
src
size_t
lanes
)
{
\
return
__riscv_v
#
#
OP
#
#
_vx_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
src
lanes
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_SLIDE_UP
SlideUp
slideup
_ALL
)
HWY_RVV_FOREACH
(
HWY_RVV_SLIDE_DOWN
SlideDown
slidedown
_ALL
)
#
undef
HWY_RVV_SLIDE_UP
#
undef
HWY_RVV_SLIDE_DOWN
}
template
<
class
D
>
HWY_API
VFromD
<
D
>
SlideUpLanes
(
D
d
VFromD
<
D
>
v
size_t
amt
)
{
return
detail
:
:
SlideUp
(
Zero
(
d
)
v
amt
)
;
}
template
<
class
D
>
HWY_API
VFromD
<
D
>
SlideDownLanes
(
D
d
VFromD
<
D
>
v
size_t
amt
)
{
v
=
detail
:
:
SlideDown
(
v
amt
)
;
if
(
MaxLanes
(
d
)
<
MaxLanes
(
DFromV
<
decltype
(
v
)
>
(
)
)
)
{
v
=
detail
:
:
SlideUp
(
v
Zero
(
d
)
Lanes
(
d
)
-
amt
)
;
}
return
v
;
}
template
<
class
D
class
V
>
HWY_API
V
ConcatUpperLower
(
D
d
const
V
hi
const
V
lo
)
{
const
size_t
half
=
Lanes
(
d
)
/
2
;
const
V
hi_down
=
detail
:
:
SlideDown
(
hi
half
)
;
return
detail
:
:
SlideUp
(
lo
hi_down
half
)
;
}
template
<
class
D
class
V
>
HWY_API
V
ConcatLowerLower
(
D
d
const
V
hi
const
V
lo
)
{
return
detail
:
:
SlideUp
(
lo
hi
Lanes
(
d
)
/
2
)
;
}
template
<
class
D
class
V
>
HWY_API
V
ConcatUpperUpper
(
D
d
const
V
hi
const
V
lo
)
{
const
size_t
half
=
Lanes
(
d
)
/
2
;
const
V
hi_down
=
detail
:
:
SlideDown
(
hi
half
)
;
const
V
lo_down
=
detail
:
:
SlideDown
(
lo
half
)
;
return
detail
:
:
SlideUp
(
lo_down
hi_down
half
)
;
}
template
<
class
D
class
V
>
HWY_API
V
ConcatLowerUpper
(
D
d
const
V
hi
const
V
lo
)
{
const
size_t
half
=
Lanes
(
d
)
/
2
;
const
V
lo_down
=
detail
:
:
SlideDown
(
lo
half
)
;
return
detail
:
:
SlideUp
(
lo_down
hi
half
)
;
}
template
<
class
D2
class
V
>
HWY_API
VFromD
<
D2
>
Combine
(
D2
d2
const
V
hi
const
V
lo
)
{
return
detail
:
:
SlideUp
(
detail
:
:
Ext
(
d2
lo
)
detail
:
:
Ext
(
d2
hi
)
Lanes
(
d2
)
/
2
)
;
}
template
<
class
D2
class
V
>
HWY_API
VFromD
<
D2
>
ZeroExtendVector
(
D2
d2
const
V
lo
)
{
return
Combine
(
d2
Xor
(
lo
lo
)
lo
)
;
}
namespace
detail
{
template
<
class
D
>
constexpr
bool
IsSupportedLMUL
(
D
d
)
{
return
(
size_t
{
1
}
<
<
(
d
.
Pow2
(
)
+
3
)
)
>
=
sizeof
(
TFromD
<
D
>
)
;
}
}
template
<
class
DH
hwy
:
:
EnableIf
<
detail
:
:
IsSupportedLMUL
(
DH
(
)
)
>
*
=
nullptr
>
HWY_API
VFromD
<
DH
>
LowerHalf
(
const
DH
const
VFromD
<
Twice
<
DH
>
>
v
)
{
return
detail
:
:
Trunc
(
v
)
;
}
template
<
class
DH
class
V
hwy
:
:
EnableIf
<
!
detail
:
:
IsSupportedLMUL
(
DH
(
)
)
>
*
=
nullptr
>
HWY_API
V
LowerHalf
(
const
DH
const
V
v
)
{
return
v
;
}
template
<
class
V
>
HWY_API
VFromD
<
Half
<
DFromV
<
V
>
>
>
LowerHalf
(
const
V
v
)
{
return
LowerHalf
(
Half
<
DFromV
<
V
>
>
(
)
v
)
;
}
template
<
class
DH
>
HWY_API
VFromD
<
DH
>
UpperHalf
(
const
DH
d2
const
VFromD
<
Twice
<
DH
>
>
v
)
{
return
LowerHalf
(
d2
detail
:
:
SlideDown
(
v
Lanes
(
d2
)
)
)
;
}
namespace
detail
{
#
define
HWY_RVV_SLIDE1
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
v
0
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_SLIDE1
Slide1Up
slide1up_vx
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_SLIDE1
Slide1Up
fslide1up_vf
_ALL
)
HWY_RVV_FOREACH_UI
(
HWY_RVV_SLIDE1
Slide1Down
slide1down_vx
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_SLIDE1
Slide1Down
fslide1down_vf
_ALL
)
#
undef
HWY_RVV_SLIDE1
}
#
ifdef
HWY_NATIVE_SLIDE1_UP_DOWN
#
undef
HWY_NATIVE_SLIDE1_UP_DOWN
#
else
#
define
HWY_NATIVE_SLIDE1_UP_DOWN
#
endif
template
<
class
D
>
HWY_API
VFromD
<
D
>
Slide1Up
(
D
VFromD
<
D
>
v
)
{
return
detail
:
:
Slide1Up
(
v
)
;
}
template
<
class
D
>
HWY_API
VFromD
<
D
>
Slide1Down
(
D
d
VFromD
<
D
>
v
)
{
v
=
detail
:
:
Slide1Down
(
v
)
;
if
(
MaxLanes
(
d
)
<
MaxLanes
(
DFromV
<
decltype
(
v
)
>
(
)
)
)
{
v
=
detail
:
:
SlideUp
(
v
Zero
(
d
)
Lanes
(
d
)
-
1
)
;
}
return
v
;
}
#
define
HWY_RVV_GET_LANE
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_T
(
BASE
SEW
)
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_s_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_
#
#
CHAR
#
#
SEW
(
v
)
;
/
*
no
AVL
*
/
\
}
HWY_RVV_FOREACH_UI
(
HWY_RVV_GET_LANE
GetLane
mv_x
_ALL
)
HWY_RVV_FOREACH_F
(
HWY_RVV_GET_LANE
GetLane
fmv_f
_ALL
)
#
undef
HWY_RVV_GET_LANE
template
<
class
V
>
HWY_API
TFromV
<
V
>
ExtractLane
(
const
V
v
size_t
i
)
{
return
GetLane
(
detail
:
:
SlideDown
(
v
i
)
)
;
}
HWY_RVV_FOREACH_B
(
HWY_RVV_RETM_ARGM
SetOnlyFirst
sof
)
HWY_RVV_FOREACH_B
(
HWY_RVV_RETM_ARGM
SetBeforeFirst
sbf
)
HWY_RVV_FOREACH_B
(
HWY_RVV_RETM_ARGM
SetAtOrBeforeFirst
sif
)
#
define
HWY_RVV_SET_AT_OR_AFTER_FIRST
(
SEW
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_M
(
MLEN
)
SetAtOrAfterFirst
(
HWY_RVV_M
(
MLEN
)
m
)
{
\
return
Not
(
SetBeforeFirst
(
m
)
)
;
\
}
HWY_RVV_FOREACH_B
(
HWY_RVV_SET_AT_OR_AFTER_FIRST
_
_
)
#
undef
HWY_RVV_SET_AT_OR_AFTER_FIRST
template
<
class
V
typename
T
HWY_IF_NOT_T_SIZE_V
(
V
1
)
>
HWY_API
V
InsertLane
(
const
V
v
size_t
i
T
t
)
{
const
Rebind
<
T
DFromV
<
V
>
>
d
;
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
using
TU
=
TFromD
<
decltype
(
du
)
>
;
const
auto
is_i
=
detail
:
:
EqS
(
detail
:
:
Iota0
(
du
)
static_cast
<
TU
>
(
i
)
)
;
return
IfThenElse
(
RebindMask
(
d
is_i
)
Set
(
d
t
)
v
)
;
}
template
<
class
V
typename
T
HWY_IF_T_SIZE_V
(
V
1
)
>
HWY_API
V
InsertLane
(
const
V
v
size_t
i
T
t
)
{
const
Rebind
<
T
DFromV
<
V
>
>
d
;
const
auto
zero
=
Zero
(
d
)
;
const
auto
one
=
Set
(
d
1
)
;
const
auto
ge_i
=
Eq
(
detail
:
:
SlideUp
(
zero
one
i
)
one
)
;
const
auto
is_i
=
SetOnlyFirst
(
ge_i
)
;
return
IfThenElse
(
RebindMask
(
d
is_i
)
Set
(
d
t
)
v
)
;
}
namespace
detail
{
template
<
class
D
HWY_IF_NOT_T_SIZE_D
(
D
8
)
>
HWY_INLINE
MFromD
<
D
>
IsEven
(
D
d
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
const
RepartitionToWide
<
decltype
(
du
)
>
duw
;
return
RebindMask
(
d
detail
:
:
NeS
(
BitCast
(
du
Set
(
duw
1
)
)
0u
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
8
)
>
HWY_INLINE
MFromD
<
D
>
IsEven
(
D
d
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
return
detail
:
:
EqS
(
detail
:
:
AndS
(
detail
:
:
Iota0
(
du
)
1
)
0
)
;
}
template
<
class
D
HWY_IF_NOT_T_SIZE_D
(
D
8
)
>
HWY_INLINE
MFromD
<
D
>
IsOdd
(
D
d
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
const
RepartitionToWide
<
decltype
(
du
)
>
duw
;
return
RebindMask
(
d
detail
:
:
EqS
(
BitCast
(
du
Set
(
duw
1
)
)
0u
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
8
)
>
HWY_INLINE
MFromD
<
D
>
IsOdd
(
D
d
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
return
detail
:
:
NeS
(
detail
:
:
AndS
(
detail
:
:
Iota0
(
du
)
1
)
0
)
;
}
}
template
<
class
V
>
HWY_API
V
OddEven
(
const
V
a
const
V
b
)
{
return
IfThenElse
(
detail
:
:
IsEven
(
DFromV
<
V
>
(
)
)
b
a
)
;
}
template
<
class
V
>
HWY_API
V
DupEven
(
const
V
v
)
{
const
V
up
=
detail
:
:
Slide1Up
(
v
)
;
return
OddEven
(
up
v
)
;
}
template
<
class
V
>
HWY_API
V
DupOdd
(
const
V
v
)
{
const
V
down
=
detail
:
:
Slide1Down
(
v
)
;
return
OddEven
(
v
down
)
;
}
template
<
class
V
>
HWY_API
V
OddEvenBlocks
(
const
V
a
const
V
b
)
{
const
RebindToUnsigned
<
DFromV
<
V
>
>
du
;
constexpr
size_t
kShift
=
CeilLog2
(
16
/
sizeof
(
TFromV
<
V
>
)
)
;
const
auto
idx_block
=
ShiftRight
<
kShift
>
(
detail
:
:
Iota0
(
du
)
)
;
const
auto
is_even
=
detail
:
:
EqS
(
detail
:
:
AndS
(
idx_block
1
)
0
)
;
return
IfThenElse
(
is_even
b
a
)
;
}
template
<
class
V
>
HWY_API
V
SwapAdjacentBlocks
(
const
V
v
)
{
const
DFromV
<
V
>
d
;
const
size_t
lpb
=
detail
:
:
LanesPerBlock
(
d
)
;
const
V
down
=
detail
:
:
SlideDown
(
v
lpb
)
;
const
V
up
=
detail
:
:
SlideUp
(
v
v
lpb
)
;
return
OddEvenBlocks
(
up
down
)
;
}
template
<
class
D
class
VI
>
HWY_API
VFromD
<
RebindToUnsigned
<
D
>
>
IndicesFromVec
(
D
d
VI
vec
)
{
static_assert
(
sizeof
(
TFromD
<
D
>
)
=
=
sizeof
(
TFromV
<
VI
>
)
"
Index
!
=
lane
"
)
;
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
const
auto
indices
=
BitCast
(
du
vec
)
;
#
if
HWY_IS_DEBUG_BUILD
using
TU
=
TFromD
<
decltype
(
du
)
>
;
const
size_t
twice_num_of_lanes
=
Lanes
(
d
)
*
2
;
HWY_DASSERT
(
AllTrue
(
du
Eq
(
indices
detail
:
:
AndS
(
indices
static_cast
<
TU
>
(
twice_num_of_lanes
-
1
)
)
)
)
)
;
#
endif
return
indices
;
}
template
<
class
D
typename
TI
>
HWY_API
VFromD
<
RebindToUnsigned
<
D
>
>
SetTableIndices
(
D
d
const
TI
*
idx
)
{
static_assert
(
sizeof
(
TFromD
<
D
>
)
=
=
sizeof
(
TI
)
"
Index
size
must
match
lane
"
)
;
return
IndicesFromVec
(
d
LoadU
(
Rebind
<
TI
D
>
(
)
idx
)
)
;
}
#
define
HWY_RVV_TABLE
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
HWY_RVV_V
(
uint
SEW
LMUL
)
idx
)
{
\
return
__riscv_v
#
#
OP
#
#
_vv_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
v
idx
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_TABLE
TableLookupLanes
rgather
_ALL
)
#
undef
HWY_RVV_TABLE
namespace
detail
{
#
define
HWY_RVV_TABLE16
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
HWY_RVV_V
(
uint
SEWD
LMULD
)
idx
)
{
\
return
__riscv_v
#
#
OP
#
#
_vv_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
v
idx
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH_UI08
(
HWY_RVV_TABLE16
TableLookupLanes16
rgatherei16
_EXT
)
#
undef
HWY_RVV_TABLE16
#
define
HWY_RVV_MASKED_TABLE
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_M
(
MLEN
)
mask
HWY_RVV_V
(
BASE
SEW
LMUL
)
maskedoff
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
HWY_RVV_V
(
uint
SEW
LMUL
)
idx
)
{
\
return
__riscv_v
#
#
OP
#
#
_vv_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_mu
(
mask
maskedoff
v
idx
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_MASKED_TABLE
MaskedTableLookupLanes
rgather
_ALL
)
#
undef
HWY_RVV_MASKED_TABLE
#
define
HWY_RVV_MASKED_TABLE16
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
\
LMULH
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_M
(
MLEN
)
mask
HWY_RVV_V
(
BASE
SEW
LMUL
)
maskedoff
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
HWY_RVV_V
(
uint
SEWD
LMULD
)
idx
)
{
\
return
__riscv_v
#
#
OP
#
#
_vv_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_mu
(
mask
maskedoff
v
idx
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH_UI08
(
HWY_RVV_MASKED_TABLE16
MaskedTableLookupLanes16
rgatherei16
_EXT
)
#
undef
HWY_RVV_MASKED_TABLE16
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
1
)
HWY_IF_POW2_LE_D
(
D
2
)
>
HWY_API
VFromD
<
D
>
Reverse
(
D
d
VFromD
<
D
>
v
)
{
const
Rebind
<
uint16_t
decltype
(
d
)
>
du16
;
const
size_t
N
=
Lanes
(
d
)
;
const
auto
idx
=
detail
:
:
ReverseSubS
(
detail
:
:
Iota0
(
du16
)
static_cast
<
uint16_t
>
(
N
-
1
)
)
;
return
detail
:
:
TableLookupLanes16
(
v
idx
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
1
)
HWY_IF_POW2_GT_D
(
D
2
)
>
HWY_API
VFromD
<
D
>
Reverse
(
D
d
VFromD
<
D
>
v
)
{
const
Half
<
decltype
(
d
)
>
dh
;
const
Rebind
<
uint16_t
decltype
(
dh
)
>
du16
;
const
size_t
half_n
=
Lanes
(
dh
)
;
const
auto
idx
=
detail
:
:
ReverseSubS
(
detail
:
:
Iota0
(
du16
)
static_cast
<
uint16_t
>
(
half_n
-
1
)
)
;
const
auto
reversed_lo
=
detail
:
:
TableLookupLanes16
(
LowerHalf
(
dh
v
)
idx
)
;
const
auto
reversed_hi
=
detail
:
:
TableLookupLanes16
(
UpperHalf
(
dh
v
)
idx
)
;
return
Combine
(
d
reversed_lo
reversed_hi
)
;
}
template
<
class
D
HWY_IF_T_SIZE_ONE_OF_D
(
D
(
1
<
<
2
)
|
(
1
<
<
4
)
|
(
1
<
<
8
)
)
>
HWY_API
VFromD
<
D
>
Reverse
(
D
VFromD
<
D
>
v
)
{
const
RebindToUnsigned
<
D
>
du
;
using
TU
=
TFromD
<
decltype
(
du
)
>
;
const
size_t
N
=
Lanes
(
du
)
;
const
auto
idx
=
detail
:
:
ReverseSubS
(
detail
:
:
Iota0
(
du
)
static_cast
<
TU
>
(
N
-
1
)
)
;
return
TableLookupLanes
(
v
idx
)
;
}
namespace
detail
{
template
<
class
D
>
HWY_INLINE
VFromD
<
D
>
ChangeLMUL
(
D
VFromD
<
D
>
v
)
{
return
v
;
}
#
define
HWY_RVV_IF_SAME_T_DV
(
D
V
)
\
hwy
:
:
EnableIf
<
IsSame
<
NativeLaneType
<
TFromD
<
D
>
>
TFromV
<
V
>
>
(
)
>
*
=
nullptr
template
<
class
D
class
V
HWY_IF_POW2_LE_D
(
DFromV
<
VFromD
<
D
>
>
DFromV
<
V
>
(
)
.
Pow2
(
)
-
1
)
>
HWY_INLINE
VFromD
<
D
>
ChangeLMUL
(
D
d
V
v
)
{
const
DFromV
<
V
>
d_from
;
const
Half
<
decltype
(
d_from
)
>
dh_from
;
static_assert
(
DFromV
<
VFromD
<
decltype
(
dh_from
)
>
>
(
)
.
Pow2
(
)
<
DFromV
<
V
>
(
)
.
Pow2
(
)
"
The
LMUL
of
VFromD
<
decltype
(
dh_from
)
>
must
be
less
than
the
LMUL
of
V
"
)
;
static_assert
(
DFromV
<
VFromD
<
D
>
>
(
)
.
Pow2
(
)
<
=
DFromV
<
VFromD
<
decltype
(
dh_from
)
>
>
(
)
.
Pow2
(
)
"
The
LMUL
of
VFromD
<
D
>
must
be
less
than
or
equal
to
the
LMUL
of
"
"
VFromD
<
decltype
(
dh_from
)
>
"
)
;
return
ChangeLMUL
(
d
Trunc
(
v
)
)
;
}
template
<
class
D
class
V
HWY_IF_POW2_GT_D
(
DFromV
<
VFromD
<
D
>
>
DFromV
<
V
>
(
)
.
Pow2
(
)
)
>
HWY_INLINE
VFromD
<
D
>
ChangeLMUL
(
D
d
V
v
)
{
const
DFromV
<
V
>
d_from
;
const
Twice
<
decltype
(
d_from
)
>
dt_from
;
static_assert
(
DFromV
<
VFromD
<
decltype
(
dt_from
)
>
>
(
)
.
Pow2
(
)
>
DFromV
<
V
>
(
)
.
Pow2
(
)
"
The
LMUL
of
VFromD
<
decltype
(
dt_from
)
>
must
be
greater
than
"
"
the
LMUL
of
V
"
)
;
static_assert
(
DFromV
<
VFromD
<
D
>
>
(
)
.
Pow2
(
)
>
=
DFromV
<
VFromD
<
decltype
(
dt_from
)
>
>
(
)
.
Pow2
(
)
"
The
LMUL
of
VFromD
<
D
>
must
be
greater
than
or
equal
to
the
LMUL
of
"
"
VFromD
<
decltype
(
dt_from
)
>
"
)
;
return
ChangeLMUL
(
d
Ext
(
dt_from
v
)
)
;
}
#
undef
HWY_RVV_IF_SAME_T_DV
}
template
<
class
DTo
class
VFrom
>
HWY_API
VFromD
<
DTo
>
ResizeBitCast
(
DTo
VFrom
v
)
{
const
DFromV
<
decltype
(
v
)
>
d_from
;
const
Repartition
<
uint8_t
decltype
(
d_from
)
>
du8_from
;
const
DFromV
<
VFromD
<
DTo
>
>
d_to
;
const
Repartition
<
uint8_t
decltype
(
d_to
)
>
du8_to
;
return
BitCast
(
d_to
detail
:
:
ChangeLMUL
(
du8_to
BitCast
(
du8_from
v
)
)
)
;
}
#
ifdef
HWY_NATIVE_REVERSE2_8
#
undef
HWY_NATIVE_REVERSE2_8
#
else
#
define
HWY_NATIVE_REVERSE2_8
#
endif
template
<
class
D
HWY_IF_T_SIZE_D
(
D
1
)
>
HWY_API
VFromD
<
D
>
Reverse2
(
D
d
const
VFromD
<
D
>
v
)
{
const
detail
:
:
AdjustSimdTagToMinVecPow2
<
Repartition
<
uint16_t
D
>
>
du16
;
return
ResizeBitCast
(
d
RotateRight
<
8
>
(
ResizeBitCast
(
du16
v
)
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
2
)
>
HWY_API
VFromD
<
D
>
Reverse2
(
D
d
const
VFromD
<
D
>
v
)
{
const
detail
:
:
AdjustSimdTagToMinVecPow2
<
Repartition
<
uint32_t
D
>
>
du32
;
return
ResizeBitCast
(
d
RotateRight
<
16
>
(
ResizeBitCast
(
du32
v
)
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
4
)
>
HWY_API
VFromD
<
D
>
Reverse2
(
D
d
const
VFromD
<
D
>
v
)
{
const
detail
:
:
AdjustSimdTagToMinVecPow2
<
Repartition
<
uint64_t
D
>
>
du64
;
return
ResizeBitCast
(
d
RotateRight
<
32
>
(
ResizeBitCast
(
du64
v
)
)
)
;
}
template
<
class
D
class
V
=
VFromD
<
D
>
HWY_IF_T_SIZE_D
(
D
8
)
>
HWY_API
V
Reverse2
(
D
const
V
v
)
{
const
V
up
=
detail
:
:
Slide1Up
(
v
)
;
const
V
down
=
detail
:
:
Slide1Down
(
v
)
;
return
OddEven
(
up
down
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
1
)
>
HWY_API
VFromD
<
D
>
Reverse4
(
D
d
const
VFromD
<
D
>
v
)
{
const
detail
:
:
AdjustSimdTagToMinVecPow2
<
Repartition
<
uint16_t
D
>
>
du16
;
return
ResizeBitCast
(
d
Reverse2
(
du16
ResizeBitCast
(
du16
Reverse2
(
d
v
)
)
)
)
;
}
template
<
class
D
HWY_IF_NOT_T_SIZE_D
(
D
1
)
>
HWY_API
VFromD
<
D
>
Reverse4
(
D
d
const
VFromD
<
D
>
v
)
{
const
RebindToUnsigned
<
D
>
du
;
const
auto
idx
=
detail
:
:
XorS
(
detail
:
:
Iota0
(
du
)
3
)
;
return
BitCast
(
d
TableLookupLanes
(
BitCast
(
du
v
)
idx
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
1
)
>
HWY_API
VFromD
<
D
>
Reverse8
(
D
d
const
VFromD
<
D
>
v
)
{
const
detail
:
:
AdjustSimdTagToMinVecPow2
<
Repartition
<
uint32_t
D
>
>
du32
;
return
ResizeBitCast
(
d
Reverse2
(
du32
ResizeBitCast
(
du32
Reverse4
(
d
v
)
)
)
)
;
}
template
<
class
D
HWY_IF_NOT_T_SIZE_D
(
D
1
)
>
HWY_API
VFromD
<
D
>
Reverse8
(
D
d
const
VFromD
<
D
>
v
)
{
const
RebindToUnsigned
<
D
>
du
;
const
auto
idx
=
detail
:
:
XorS
(
detail
:
:
Iota0
(
du
)
7
)
;
return
BitCast
(
d
TableLookupLanes
(
BitCast
(
du
v
)
idx
)
)
;
}
template
<
class
D
class
V
=
VFromD
<
D
>
>
HWY_API
V
ReverseBlocks
(
D
d
V
v
)
{
const
detail
:
:
AdjustSimdTagToMinVecPow2
<
Repartition
<
uint64_t
D
>
>
du64
;
const
size_t
N
=
Lanes
(
du64
)
;
const
auto
rev
=
detail
:
:
ReverseSubS
(
detail
:
:
Iota0
(
du64
)
static_cast
<
uint64_t
>
(
N
-
1
)
)
;
const
auto
idx
=
detail
:
:
XorS
(
rev
1
)
;
return
ResizeBitCast
(
d
TableLookupLanes
(
ResizeBitCast
(
du64
v
)
idx
)
)
;
}
#
ifdef
HWY_NATIVE_COMPRESS8
#
undef
HWY_NATIVE_COMPRESS8
#
else
#
define
HWY_NATIVE_COMPRESS8
#
endif
template
<
typename
T
>
struct
CompressIsPartition
{
enum
{
value
=
0
}
;
}
;
#
define
HWY_RVV_COMPRESS
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
HWY_RVV_M
(
MLEN
)
mask
)
{
\
return
__riscv_v
#
#
OP
#
#
_vm_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
v
mask
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_COMPRESS
Compress
compress
_ALL
)
#
undef
HWY_RVV_COMPRESS
#
ifdef
HWY_NATIVE_EXPAND
#
undef
HWY_NATIVE_EXPAND
#
else
#
define
HWY_NATIVE_EXPAND
#
endif
template
<
class
V
class
M
HWY_IF_NOT_T_SIZE_V
(
V
1
)
>
HWY_API
V
Expand
(
V
v
const
M
mask
)
{
const
DFromV
<
V
>
d
;
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
const
auto
idx
=
detail
:
:
MaskedIota
(
du
RebindMask
(
du
mask
)
)
;
const
V
zero
=
Zero
(
d
)
;
return
detail
:
:
MaskedTableLookupLanes
(
mask
zero
v
idx
)
;
}
template
<
class
V
class
M
HWY_IF_T_SIZE_V
(
V
1
)
class
D
=
DFromV
<
V
>
HWY_IF_POW2_LE_D
(
D
2
)
>
HWY_API
V
Expand
(
V
v
const
M
mask
)
{
const
D
d
;
const
Rebind
<
uint16_t
decltype
(
d
)
>
du16
;
const
auto
idx
=
detail
:
:
MaskedIota
(
du16
RebindMask
(
du16
mask
)
)
;
const
V
zero
=
Zero
(
d
)
;
return
detail
:
:
MaskedTableLookupLanes16
(
mask
zero
v
idx
)
;
}
template
<
class
V
class
M
HWY_IF_T_SIZE_V
(
V
1
)
class
D
=
DFromV
<
V
>
HWY_IF_POW2_GT_D
(
DFromV
<
V
>
2
)
>
HWY_API
V
Expand
(
V
v
const
M
mask
)
{
const
D
d
;
const
Half
<
D
>
dh
;
const
auto
v0
=
LowerHalf
(
dh
v
)
;
const
V
vmask
=
VecFromMask
(
d
mask
)
;
const
auto
m0
=
MaskFromVec
(
LowerHalf
(
dh
vmask
)
)
;
const
size_t
count
=
CountTrue
(
dh
m0
)
;
const
auto
v1
=
detail
:
:
Trunc
(
detail
:
:
SlideDown
(
v
count
)
)
;
const
auto
m1
=
MaskFromVec
(
UpperHalf
(
dh
vmask
)
)
;
return
Combine
(
d
Expand
(
v1
m1
)
Expand
(
v0
m0
)
)
;
}
template
<
class
D
>
HWY_API
VFromD
<
D
>
LoadExpand
(
MFromD
<
D
>
mask
D
d
const
TFromD
<
D
>
*
HWY_RESTRICT
unaligned
)
{
return
Expand
(
LoadU
(
d
unaligned
)
mask
)
;
}
template
<
class
V
class
M
>
HWY_API
V
CompressNot
(
V
v
const
M
mask
)
{
return
Compress
(
v
Not
(
mask
)
)
;
}
template
<
class
V
class
M
>
HWY_API
V
CompressBlocksNot
(
V
v
const
M
mask
)
{
return
CompressNot
(
v
mask
)
;
}
template
<
class
V
class
M
class
D
>
HWY_API
size_t
CompressStore
(
const
V
v
const
M
mask
const
D
d
TFromD
<
D
>
*
HWY_RESTRICT
unaligned
)
{
StoreU
(
Compress
(
v
mask
)
d
unaligned
)
;
return
CountTrue
(
d
mask
)
;
}
template
<
class
V
class
M
class
D
>
HWY_API
size_t
CompressBlendedStore
(
const
V
v
const
M
mask
const
D
d
TFromD
<
D
>
*
HWY_RESTRICT
unaligned
)
{
const
size_t
count
=
CountTrue
(
d
mask
)
;
StoreN
(
Compress
(
v
mask
)
d
unaligned
count
)
;
return
count
;
}
template
<
class
D
>
HWY_API
intptr_t
FindLastTrue
(
D
d
MFromD
<
D
>
m
)
{
const
RebindToSigned
<
decltype
(
d
)
>
di
;
const
intptr_t
fft_rev_idx
=
FindFirstTrue
(
d
MaskFromVec
(
Reverse
(
di
VecFromMask
(
di
m
)
)
)
)
;
return
(
fft_rev_idx
>
=
0
)
?
(
static_cast
<
intptr_t
>
(
Lanes
(
d
)
-
1
)
-
fft_rev_idx
)
:
intptr_t
{
-
1
}
;
}
template
<
class
D
>
HWY_API
size_t
FindKnownLastTrue
(
D
d
MFromD
<
D
>
m
)
{
const
RebindToSigned
<
decltype
(
d
)
>
di
;
const
size_t
fft_rev_idx
=
FindKnownFirstTrue
(
d
MaskFromVec
(
Reverse
(
di
VecFromMask
(
di
m
)
)
)
)
;
return
Lanes
(
d
)
-
1
-
fft_rev_idx
;
}
namespace
detail
{
#
define
HWY_RVV_NARROW
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
kShift
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
NAME
(
HWY_RVV_V
(
BASE
SEWD
LMULD
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_wx_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
v
kShift
\
HWY_RVV_AVL
(
SEWD
SHIFT
+
1
)
)
;
\
}
HWY_RVV_FOREACH_U08
(
HWY_RVV_NARROW
Narrow
nsrl
_EXT
)
HWY_RVV_FOREACH_U16
(
HWY_RVV_NARROW
Narrow
nsrl
_EXT
)
HWY_RVV_FOREACH_U32
(
HWY_RVV_NARROW
Narrow
nsrl
_EXT
)
#
undef
HWY_RVV_NARROW
}
template
<
class
D
HWY_IF_NOT_T_SIZE_D
(
D
8
)
HWY_IF_POW2_LE_D
(
D
2
)
>
HWY_API
VFromD
<
D
>
ConcatOdd
(
D
d
VFromD
<
D
>
hi
VFromD
<
D
>
lo
)
{
constexpr
size_t
kBits
=
sizeof
(
TFromD
<
D
>
)
*
8
;
const
Twice
<
decltype
(
d
)
>
dt
;
const
RepartitionToWide
<
RebindToUnsigned
<
decltype
(
dt
)
>
>
dtuw
;
const
VFromD
<
decltype
(
dtuw
)
>
hl
=
BitCast
(
dtuw
Combine
(
dt
hi
lo
)
)
;
return
BitCast
(
d
detail
:
:
Narrow
<
kBits
>
(
hl
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
8
)
HWY_IF_POW2_LE_D
(
D
2
)
>
HWY_API
VFromD
<
D
>
ConcatOdd
(
D
d
VFromD
<
D
>
hi
VFromD
<
D
>
lo
)
{
const
Twice
<
decltype
(
d
)
>
dt
;
const
VFromD
<
decltype
(
dt
)
>
hl
=
Combine
(
dt
hi
lo
)
;
return
LowerHalf
(
d
Compress
(
hl
detail
:
:
IsOdd
(
dt
)
)
)
;
}
template
<
class
D
HWY_IF_POW2_GT_D
(
D
2
)
>
HWY_API
VFromD
<
D
>
ConcatOdd
(
D
d
VFromD
<
D
>
hi
VFromD
<
D
>
lo
)
{
const
Half
<
decltype
(
d
)
>
dh
;
const
MFromD
<
D
>
is_odd
=
detail
:
:
IsOdd
(
d
)
;
const
VFromD
<
decltype
(
d
)
>
hi_odd
=
Compress
(
hi
is_odd
)
;
const
VFromD
<
decltype
(
d
)
>
lo_odd
=
Compress
(
lo
is_odd
)
;
return
Combine
(
d
LowerHalf
(
dh
hi_odd
)
LowerHalf
(
dh
lo_odd
)
)
;
}
template
<
class
D
HWY_IF_NOT_T_SIZE_D
(
D
8
)
HWY_IF_POW2_LE_D
(
D
2
)
>
HWY_API
VFromD
<
D
>
ConcatEven
(
D
d
VFromD
<
D
>
hi
VFromD
<
D
>
lo
)
{
const
Twice
<
decltype
(
d
)
>
dt
;
const
RepartitionToWide
<
RebindToUnsigned
<
decltype
(
dt
)
>
>
dtuw
;
const
VFromD
<
decltype
(
dtuw
)
>
hl
=
BitCast
(
dtuw
Combine
(
dt
hi
lo
)
)
;
return
BitCast
(
d
detail
:
:
Narrow
<
0
>
(
hl
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
8
)
HWY_IF_POW2_LE_D
(
D
2
)
>
HWY_API
VFromD
<
D
>
ConcatEven
(
D
d
VFromD
<
D
>
hi
VFromD
<
D
>
lo
)
{
const
Twice
<
decltype
(
d
)
>
dt
;
const
VFromD
<
decltype
(
dt
)
>
hl
=
Combine
(
dt
hi
lo
)
;
return
LowerHalf
(
d
Compress
(
hl
detail
:
:
IsEven
(
dt
)
)
)
;
}
template
<
class
D
HWY_IF_POW2_GT_D
(
D
2
)
>
HWY_API
VFromD
<
D
>
ConcatEven
(
D
d
VFromD
<
D
>
hi
VFromD
<
D
>
lo
)
{
const
Half
<
decltype
(
d
)
>
dh
;
const
MFromD
<
D
>
is_even
=
detail
:
:
IsEven
(
d
)
;
const
VFromD
<
decltype
(
d
)
>
hi_even
=
Compress
(
hi
is_even
)
;
const
VFromD
<
decltype
(
d
)
>
lo_even
=
Compress
(
lo
is_even
)
;
return
Combine
(
d
LowerHalf
(
dh
hi_even
)
LowerHalf
(
dh
lo_even
)
)
;
}
template
<
size_t
kBytes
class
D
class
V
=
VFromD
<
D
>
>
HWY_API
V
CombineShiftRightBytes
(
const
D
d
const
V
hi
V
lo
)
{
const
Repartition
<
uint8_t
decltype
(
d
)
>
d8
;
const
auto
hi8
=
BitCast
(
d8
hi
)
;
const
auto
lo8
=
BitCast
(
d8
lo
)
;
const
auto
hi_up
=
detail
:
:
SlideUp
(
hi8
hi8
16
-
kBytes
)
;
const
auto
lo_down
=
detail
:
:
SlideDown
(
lo8
kBytes
)
;
const
auto
is_lo
=
detail
:
:
FirstNPerBlock
<
16
-
kBytes
>
(
d8
)
;
return
BitCast
(
d
IfThenElse
(
is_lo
lo_down
hi_up
)
)
;
}
template
<
size_t
kLanes
class
D
class
V
=
VFromD
<
D
>
>
HWY_API
V
CombineShiftRightLanes
(
const
D
d
const
V
hi
V
lo
)
{
constexpr
size_t
kLanesUp
=
16
/
sizeof
(
TFromV
<
V
>
)
-
kLanes
;
const
auto
hi_up
=
detail
:
:
SlideUp
(
hi
hi
kLanesUp
)
;
const
auto
lo_down
=
detail
:
:
SlideDown
(
lo
kLanes
)
;
const
auto
is_lo
=
detail
:
:
FirstNPerBlock
<
kLanesUp
>
(
d
)
;
return
IfThenElse
(
is_lo
lo_down
hi_up
)
;
}
template
<
class
V
>
HWY_API
V
Shuffle2301
(
const
V
v
)
{
const
DFromV
<
V
>
d
;
static_assert
(
sizeof
(
TFromD
<
decltype
(
d
)
>
)
=
=
4
"
Defined
for
32
-
bit
types
"
)
;
const
Repartition
<
uint64_t
decltype
(
d
)
>
du64
;
const
auto
v64
=
BitCast
(
du64
v
)
;
return
BitCast
(
d
Or
(
ShiftRight
<
32
>
(
v64
)
ShiftLeft
<
32
>
(
v64
)
)
)
;
}
template
<
class
V
>
HWY_API
V
Shuffle2103
(
const
V
v
)
{
const
DFromV
<
V
>
d
;
static_assert
(
sizeof
(
TFromD
<
decltype
(
d
)
>
)
=
=
4
"
Defined
for
32
-
bit
types
"
)
;
return
CombineShiftRightLanes
<
3
>
(
d
v
v
)
;
}
template
<
class
V
>
HWY_API
V
Shuffle0321
(
const
V
v
)
{
const
DFromV
<
V
>
d
;
static_assert
(
sizeof
(
TFromD
<
decltype
(
d
)
>
)
=
=
4
"
Defined
for
32
-
bit
types
"
)
;
return
CombineShiftRightLanes
<
1
>
(
d
v
v
)
;
}
template
<
class
V
>
HWY_API
V
Shuffle1032
(
const
V
v
)
{
const
DFromV
<
V
>
d
;
static_assert
(
sizeof
(
TFromD
<
decltype
(
d
)
>
)
=
=
4
"
Defined
for
32
-
bit
types
"
)
;
return
CombineShiftRightLanes
<
2
>
(
d
v
v
)
;
}
template
<
class
V
>
HWY_API
V
Shuffle01
(
const
V
v
)
{
const
DFromV
<
V
>
d
;
static_assert
(
sizeof
(
TFromD
<
decltype
(
d
)
>
)
=
=
8
"
Defined
for
64
-
bit
types
"
)
;
return
CombineShiftRightLanes
<
1
>
(
d
v
v
)
;
}
template
<
class
V
>
HWY_API
V
Shuffle0123
(
const
V
v
)
{
return
Shuffle2301
(
Shuffle1032
(
v
)
)
;
}
template
<
class
VT
class
VI
>
HWY_API
VI
TableLookupBytes
(
const
VT
vt
const
VI
vi
)
{
const
DFromV
<
VT
>
dt
;
const
DFromV
<
VI
>
di
;
const
Repartition
<
uint8_t
decltype
(
dt
)
>
dt8
;
const
Repartition
<
uint8_t
decltype
(
di
)
>
di8
;
constexpr
int
kPow2T
=
dt8
.
Pow2
(
)
;
constexpr
int
kPow2I
=
di8
.
Pow2
(
)
;
const
Simd
<
uint8_t
MaxLanes
(
di8
)
HWY_MAX
(
kPow2T
kPow2I
)
>
dm8
;
const
auto
vmt
=
detail
:
:
ChangeLMUL
(
dm8
BitCast
(
dt8
vt
)
)
;
const
auto
vmi
=
detail
:
:
ChangeLMUL
(
dm8
BitCast
(
di8
vi
)
)
;
auto
offsets
=
detail
:
:
OffsetsOf128BitBlocks
(
dm8
detail
:
:
Iota0
(
dm8
)
)
;
if
(
kPow2T
<
kPow2I
)
{
offsets
=
detail
:
:
AndS
(
offsets
static_cast
<
uint8_t
>
(
Lanes
(
dt8
)
-
1
)
)
;
}
const
auto
out
=
TableLookupLanes
(
vmt
Add
(
vmi
offsets
)
)
;
return
BitCast
(
di
detail
:
:
ChangeLMUL
(
di8
out
)
)
;
}
template
<
class
VT
class
VI
>
HWY_API
VI
TableLookupBytesOr0
(
const
VT
vt
const
VI
idx
)
{
const
DFromV
<
VI
>
di
;
const
Repartition
<
int8_t
decltype
(
di
)
>
di8
;
const
auto
idx8
=
BitCast
(
di8
idx
)
;
const
auto
lookup
=
TableLookupBytes
(
vt
idx8
)
;
return
BitCast
(
di
IfThenZeroElse
(
detail
:
:
LtS
(
idx8
0
)
lookup
)
)
;
}
template
<
class
D
HWY_IF_POW2_LE_D
(
D
2
)
>
HWY_API
VFromD
<
D
>
TwoTablesLookupLanes
(
D
d
VFromD
<
D
>
a
VFromD
<
D
>
b
VFromD
<
RebindToUnsigned
<
D
>
>
idx
)
{
const
Twice
<
decltype
(
d
)
>
dt
;
const
RebindToUnsigned
<
decltype
(
dt
)
>
dt_u
;
const
auto
combined_tbl
=
Combine
(
dt
b
a
)
;
const
auto
combined_idx
=
Combine
(
dt_u
idx
idx
)
;
return
LowerHalf
(
d
TableLookupLanes
(
combined_tbl
combined_idx
)
)
;
}
template
<
class
D
HWY_IF_POW2_GT_D
(
D
2
)
>
HWY_API
VFromD
<
D
>
TwoTablesLookupLanes
(
D
d
VFromD
<
D
>
a
VFromD
<
D
>
b
VFromD
<
RebindToUnsigned
<
D
>
>
idx
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
using
TU
=
TFromD
<
decltype
(
du
)
>
;
const
size_t
num_of_lanes
=
Lanes
(
d
)
;
const
auto
idx_mod
=
detail
:
:
AndS
(
idx
static_cast
<
TU
>
(
num_of_lanes
-
1
)
)
;
const
auto
sel_a_mask
=
Ne
(
idx
idx_mod
)
;
const
auto
a_lookup_result
=
TableLookupLanes
(
a
idx_mod
)
;
return
detail
:
:
MaskedTableLookupLanes
(
sel_a_mask
a_lookup_result
b
idx_mod
)
;
}
template
<
class
V
>
HWY_API
V
TwoTablesLookupLanes
(
V
a
V
b
VFromD
<
RebindToUnsigned
<
DFromV
<
V
>
>
>
idx
)
{
const
DFromV
<
decltype
(
a
)
>
d
;
return
TwoTablesLookupLanes
(
d
a
b
idx
)
;
}
template
<
int
kLane
class
V
class
D
=
DFromV
<
V
>
HWY_IF_T_SIZE_D
(
D
1
)
HWY_IF_POW2_LE_D
(
D
2
)
>
HWY_API
V
Broadcast
(
const
V
v
)
{
HWY_DASSERT
(
0
<
=
kLane
&
&
kLane
<
detail
:
:
LanesPerBlock
(
d
)
)
;
const
D
d
;
const
Rebind
<
uint16_t
decltype
(
d
)
>
du16
;
VFromD
<
decltype
(
du16
)
>
idx
=
detail
:
:
OffsetsOf128BitBlocks
(
d
detail
:
:
Iota0
(
du16
)
)
;
if
(
kLane
!
=
0
)
{
idx
=
detail
:
:
AddS
(
idx
kLane
)
;
}
return
detail
:
:
TableLookupLanes16
(
v
idx
)
;
}
template
<
int
kLane
class
V
class
D
=
DFromV
<
V
>
HWY_IF_T_SIZE_D
(
D
1
)
HWY_IF_POW2_GT_D
(
D
2
)
>
HWY_API
V
Broadcast
(
const
V
v
)
{
HWY_DASSERT
(
0
<
=
kLane
&
&
kLane
<
detail
:
:
LanesPerBlock
(
d
)
)
;
const
D
d
;
const
Half
<
decltype
(
d
)
>
dh
;
using
VH
=
VFromD
<
decltype
(
dh
)
>
;
const
Rebind
<
uint16_t
decltype
(
dh
)
>
du16
;
VFromD
<
decltype
(
du16
)
>
idx
=
detail
:
:
OffsetsOf128BitBlocks
(
d
detail
:
:
Iota0
(
du16
)
)
;
if
(
kLane
!
=
0
)
{
idx
=
detail
:
:
AddS
(
idx
kLane
)
;
}
const
VH
lo
=
detail
:
:
TableLookupLanes16
(
LowerHalf
(
dh
v
)
idx
)
;
const
VH
hi
=
detail
:
:
TableLookupLanes16
(
UpperHalf
(
dh
v
)
idx
)
;
return
Combine
(
d
lo
hi
)
;
}
template
<
int
kLane
class
V
class
D
=
DFromV
<
V
>
HWY_IF_T_SIZE_ONE_OF_D
(
D
(
1
<
<
2
)
|
(
1
<
<
4
)
|
(
1
<
<
8
)
)
>
HWY_API
V
Broadcast
(
const
V
v
)
{
HWY_DASSERT
(
0
<
=
kLane
&
&
kLane
<
detail
:
:
LanesPerBlock
(
d
)
)
;
const
D
d
;
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
auto
idx
=
detail
:
:
OffsetsOf128BitBlocks
(
d
detail
:
:
Iota0
(
du
)
)
;
if
(
kLane
!
=
0
)
{
idx
=
detail
:
:
AddS
(
idx
kLane
)
;
}
return
TableLookupLanes
(
v
idx
)
;
}
#
ifdef
HWY_NATIVE_BROADCASTLANE
#
undef
HWY_NATIVE_BROADCASTLANE
#
else
#
define
HWY_NATIVE_BROADCASTLANE
#
endif
namespace
detail
{
#
define
HWY_RVV_BROADCAST_LANE
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
\
LMULH
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
size_t
idx
)
{
\
return
__riscv_v
#
#
OP
#
#
_vx_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
v
idx
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_BROADCAST_LANE
BroadcastLane
rgather
_ALL
)
#
undef
HWY_RVV_BROADCAST_LANE
}
template
<
int
kLane
class
V
>
HWY_API
V
BroadcastLane
(
V
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
HWY_MAX_LANES_V
(
V
)
"
Invalid
lane
"
)
;
return
detail
:
:
BroadcastLane
(
v
static_cast
<
size_t
>
(
kLane
)
)
;
}
#
ifdef
HWY_NATIVE_BLK_INSERT_EXTRACT
#
undef
HWY_NATIVE_BLK_INSERT_EXTRACT
#
else
#
define
HWY_NATIVE_BLK_INSERT_EXTRACT
#
endif
template
<
int
kBlockIdx
class
V
>
HWY_API
V
InsertBlock
(
V
v
VFromD
<
BlockDFromD
<
DFromV
<
V
>
>
>
blk_to_insert
)
{
const
DFromV
<
decltype
(
v
)
>
d
;
using
TU
=
If
<
(
sizeof
(
TFromV
<
V
>
)
=
=
1
&
&
DFromV
<
V
>
(
)
.
Pow2
(
)
>
=
-
2
)
uint16_t
MakeUnsigned
<
TFromV
<
V
>
>
>
;
using
TIdx
=
If
<
sizeof
(
TU
)
=
=
1
uint16_t
TU
>
;
const
Repartition
<
TU
decltype
(
d
)
>
du
;
const
Rebind
<
TIdx
decltype
(
du
)
>
d_idx
;
static_assert
(
0
<
=
kBlockIdx
&
&
kBlockIdx
<
d
.
MaxBlocks
(
)
"
Invalid
block
index
"
)
;
constexpr
size_t
kMaxLanesPerBlock
=
16
/
sizeof
(
TU
)
;
constexpr
size_t
kBlkByteOffset
=
static_cast
<
size_t
>
(
kBlockIdx
)
*
kMaxLanesPerBlock
;
const
auto
vu
=
BitCast
(
du
v
)
;
const
auto
vblk
=
ResizeBitCast
(
du
blk_to_insert
)
;
const
auto
vblk_shifted
=
detail
:
:
SlideUp
(
vblk
vblk
kBlkByteOffset
)
;
const
auto
insert_mask
=
RebindMask
(
du
detail
:
:
LtS
(
detail
:
:
SubS
(
detail
:
:
Iota0
(
d_idx
)
static_cast
<
TIdx
>
(
kBlkByteOffset
)
)
static_cast
<
TIdx
>
(
kMaxLanesPerBlock
)
)
)
;
return
BitCast
(
d
IfThenElse
(
insert_mask
vblk_shifted
vu
)
)
;
}
template
<
int
kBlockIdx
class
V
HWY_IF_POW2_LE_D
(
DFromV
<
V
>
-
3
)
>
HWY_API
V
BroadcastBlock
(
V
v
)
{
const
DFromV
<
decltype
(
v
)
>
d
;
const
Repartition
<
uint8_t
decltype
(
d
)
>
du8
;
const
Rebind
<
uint16_t
decltype
(
d
)
>
du16
;
static_assert
(
0
<
=
kBlockIdx
&
&
kBlockIdx
<
d
.
MaxBlocks
(
)
"
Invalid
block
index
"
)
;
const
auto
idx
=
detail
:
:
AddS
(
detail
:
:
AndS
(
detail
:
:
Iota0
(
du16
)
uint16_t
{
15
}
)
static_cast
<
uint16_t
>
(
kBlockIdx
*
16
)
)
;
return
BitCast
(
d
detail
:
:
TableLookupLanes16
(
BitCast
(
du8
v
)
idx
)
)
;
}
template
<
int
kBlockIdx
class
V
HWY_IF_POW2_GT_D
(
DFromV
<
V
>
-
3
)
>
HWY_API
V
BroadcastBlock
(
V
v
)
{
const
DFromV
<
decltype
(
v
)
>
d
;
using
TU
=
If
<
sizeof
(
TFromV
<
V
>
)
=
=
1
uint16_t
MakeUnsigned
<
TFromV
<
V
>
>
>
;
const
Repartition
<
TU
decltype
(
d
)
>
du
;
static_assert
(
0
<
=
kBlockIdx
&
&
kBlockIdx
<
d
.
MaxBlocks
(
)
"
Invalid
block
index
"
)
;
constexpr
size_t
kMaxLanesPerBlock
=
16
/
sizeof
(
TU
)
;
const
auto
idx
=
detail
:
:
AddS
(
detail
:
:
AndS
(
detail
:
:
Iota0
(
du
)
static_cast
<
TU
>
(
kMaxLanesPerBlock
-
1
)
)
static_cast
<
TU
>
(
static_cast
<
size_t
>
(
kBlockIdx
)
*
kMaxLanesPerBlock
)
)
;
return
BitCast
(
d
TableLookupLanes
(
BitCast
(
du
v
)
idx
)
)
;
}
template
<
int
kBlockIdx
class
V
>
HWY_API
VFromD
<
BlockDFromD
<
DFromV
<
V
>
>
>
ExtractBlock
(
V
v
)
{
const
DFromV
<
decltype
(
v
)
>
d
;
const
BlockDFromD
<
decltype
(
d
)
>
d_block
;
static_assert
(
0
<
=
kBlockIdx
&
&
kBlockIdx
<
d
.
MaxBlocks
(
)
"
Invalid
block
index
"
)
;
constexpr
size_t
kMaxLanesPerBlock
=
16
/
sizeof
(
TFromD
<
decltype
(
d
)
>
)
;
constexpr
size_t
kBlkByteOffset
=
static_cast
<
size_t
>
(
kBlockIdx
)
*
kMaxLanesPerBlock
;
return
ResizeBitCast
(
d_block
detail
:
:
SlideDown
(
v
kBlkByteOffset
)
)
;
}
template
<
size_t
kLanes
class
D
class
V
=
VFromD
<
D
>
>
HWY_API
V
ShiftLeftLanes
(
const
D
d
const
V
v
)
{
const
RebindToSigned
<
decltype
(
d
)
>
di
;
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
using
TI
=
TFromD
<
decltype
(
di
)
>
;
const
auto
shifted
=
detail
:
:
SlideUp
(
v
v
kLanes
)
;
const
auto
idx_mod
=
detail
:
:
AndS
(
BitCast
(
di
detail
:
:
Iota0
(
du
)
)
static_cast
<
TI
>
(
detail
:
:
LanesPerBlock
(
di
)
-
1
)
)
;
const
auto
clear
=
detail
:
:
LtS
(
idx_mod
static_cast
<
TI
>
(
kLanes
)
)
;
return
IfThenZeroElse
(
clear
shifted
)
;
}
template
<
size_t
kLanes
class
V
>
HWY_API
V
ShiftLeftLanes
(
const
V
v
)
{
return
ShiftLeftLanes
<
kLanes
>
(
DFromV
<
V
>
(
)
v
)
;
}
template
<
int
kBytes
class
D
>
HWY_API
VFromD
<
D
>
ShiftLeftBytes
(
D
d
const
VFromD
<
D
>
v
)
{
const
Repartition
<
uint8_t
decltype
(
d
)
>
d8
;
return
BitCast
(
d
ShiftLeftLanes
<
kBytes
>
(
BitCast
(
d8
v
)
)
)
;
}
template
<
int
kBytes
class
V
>
HWY_API
V
ShiftLeftBytes
(
const
V
v
)
{
return
ShiftLeftBytes
<
kBytes
>
(
DFromV
<
V
>
(
)
v
)
;
}
template
<
size_t
kLanes
typename
T
size_t
N
int
kPow2
class
V
=
VFromD
<
Simd
<
T
N
kPow2
>
>
>
HWY_API
V
ShiftRightLanes
(
const
Simd
<
T
N
kPow2
>
d
V
v
)
{
const
RebindToSigned
<
decltype
(
d
)
>
di
;
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
using
TI
=
TFromD
<
decltype
(
di
)
>
;
if
(
N
<
=
16
/
sizeof
(
T
)
)
{
v
=
detail
:
:
SlideUp
(
v
Zero
(
d
)
N
)
;
}
const
auto
shifted
=
detail
:
:
SlideDown
(
v
kLanes
)
;
const
size_t
lpb
=
detail
:
:
LanesPerBlock
(
di
)
;
const
auto
idx_mod
=
detail
:
:
AndS
(
BitCast
(
di
detail
:
:
Iota0
(
du
)
)
static_cast
<
TI
>
(
lpb
-
1
)
)
;
const
auto
keep
=
detail
:
:
LtS
(
idx_mod
static_cast
<
TI
>
(
lpb
-
kLanes
)
)
;
return
IfThenElseZero
(
keep
shifted
)
;
}
template
<
int
kBytes
class
D
class
V
=
VFromD
<
D
>
>
HWY_API
V
ShiftRightBytes
(
const
D
d
const
V
v
)
{
const
Repartition
<
uint8_t
decltype
(
d
)
>
d8
;
return
BitCast
(
d
ShiftRightLanes
<
kBytes
>
(
d8
BitCast
(
d8
v
)
)
)
;
}
#
ifdef
HWY_NATIVE_INTERLEAVE_WHOLE
#
undef
HWY_NATIVE_INTERLEAVE_WHOLE
#
else
#
define
HWY_NATIVE_INTERLEAVE_WHOLE
#
endif
namespace
detail
{
template
<
class
D
HWY_IF_T_SIZE_ONE_OF_D
(
D
(
1
<
<
1
)
|
(
1
<
<
2
)
|
(
1
<
<
4
)
)
HWY_IF_POW2_GT_D
(
D
-
3
)
>
HWY_API
VFromD
<
D
>
InterleaveWhole
(
D
d
VFromD
<
Half
<
D
>
>
a
VFromD
<
Half
<
D
>
>
b
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
using
TW
=
MakeWide
<
TFromD
<
decltype
(
du
)
>
>
;
const
Rebind
<
TW
Half
<
decltype
(
du
)
>
>
dw
;
const
Half
<
decltype
(
du
)
>
duh
;
const
VFromD
<
decltype
(
dw
)
>
aw
=
PromoteTo
(
dw
BitCast
(
duh
a
)
)
;
const
VFromD
<
decltype
(
dw
)
>
bw
=
PromoteTo
(
dw
BitCast
(
duh
b
)
)
;
return
BitCast
(
d
Or
(
aw
BitCast
(
dw
detail
:
:
Slide1Up
(
BitCast
(
du
bw
)
)
)
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
8
)
HWY_IF_POW2_LE_D
(
D
2
)
>
HWY_API
VFromD
<
D
>
InterleaveWhole
(
D
d
VFromD
<
Half
<
D
>
>
a
VFromD
<
Half
<
D
>
>
b
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
const
auto
idx
=
ShiftRight
<
1
>
(
detail
:
:
Iota0
(
du
)
)
;
return
OddEven
(
TableLookupLanes
(
detail
:
:
Ext
(
d
b
)
idx
)
TableLookupLanes
(
detail
:
:
Ext
(
d
a
)
idx
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
8
)
HWY_IF_POW2_GT_D
(
D
2
)
>
HWY_API
VFromD
<
D
>
InterleaveWhole
(
D
d
VFromD
<
Half
<
D
>
>
a
VFromD
<
Half
<
D
>
>
b
)
{
const
Half
<
D
>
dh
;
const
Half
<
decltype
(
dh
)
>
dq
;
const
VFromD
<
decltype
(
dh
)
>
i0
=
InterleaveWhole
(
dh
LowerHalf
(
dq
a
)
LowerHalf
(
dq
b
)
)
;
const
VFromD
<
decltype
(
dh
)
>
i1
=
InterleaveWhole
(
dh
UpperHalf
(
dq
a
)
UpperHalf
(
dq
b
)
)
;
return
Combine
(
d
i1
i0
)
;
}
}
template
<
class
D
HWY_IF_T_SIZE_ONE_OF_D
(
D
(
1
<
<
1
)
|
(
1
<
<
2
)
|
(
1
<
<
4
)
)
>
HWY_API
VFromD
<
D
>
InterleaveWholeLower
(
D
d
VFromD
<
D
>
a
VFromD
<
D
>
b
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
const
detail
:
:
AdjustSimdTagToMinVecPow2
<
RepartitionToWide
<
decltype
(
du
)
>
>
dw
;
const
RepartitionToNarrow
<
decltype
(
dw
)
>
du_src
;
const
VFromD
<
D
>
aw
=
ResizeBitCast
(
d
PromoteLowerTo
(
dw
ResizeBitCast
(
du_src
a
)
)
)
;
const
VFromD
<
D
>
bw
=
ResizeBitCast
(
d
PromoteLowerTo
(
dw
ResizeBitCast
(
du_src
b
)
)
)
;
return
Or
(
aw
detail
:
:
Slide1Up
(
bw
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
8
)
>
HWY_API
VFromD
<
D
>
InterleaveWholeLower
(
D
d
VFromD
<
D
>
a
VFromD
<
D
>
b
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
const
auto
idx
=
ShiftRight
<
1
>
(
detail
:
:
Iota0
(
du
)
)
;
return
OddEven
(
TableLookupLanes
(
b
idx
)
TableLookupLanes
(
a
idx
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_ONE_OF_D
(
D
(
1
<
<
1
)
|
(
1
<
<
2
)
|
(
1
<
<
4
)
)
>
HWY_API
VFromD
<
D
>
InterleaveWholeUpper
(
D
d
VFromD
<
D
>
a
VFromD
<
D
>
b
)
{
const
size_t
half_N
=
Lanes
(
d
)
/
2
;
return
InterleaveWholeLower
(
d
detail
:
:
SlideDown
(
a
half_N
)
detail
:
:
SlideDown
(
b
half_N
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
8
)
>
HWY_API
VFromD
<
D
>
InterleaveWholeUpper
(
D
d
VFromD
<
D
>
a
VFromD
<
D
>
b
)
{
const
size_t
half_N
=
Lanes
(
d
)
/
2
;
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
const
auto
idx
=
detail
:
:
AddS
(
ShiftRight
<
1
>
(
detail
:
:
Iota0
(
du
)
)
static_cast
<
uint64_t
>
(
half_N
)
)
;
return
OddEven
(
TableLookupLanes
(
b
idx
)
TableLookupLanes
(
a
idx
)
)
;
}
namespace
detail
{
template
<
class
D
class
V
HWY_IF_POW2_LE_D
(
D
2
)
>
HWY_INLINE
V
InterleaveLowerBlocks
(
D
d
const
V
a
const
V
b
)
{
static_assert
(
IsSame
<
TFromD
<
D
>
TFromV
<
V
>
>
(
)
"
D
/
V
mismatch
"
)
;
const
Twice
<
D
>
dt
;
const
RebindToUnsigned
<
decltype
(
dt
)
>
dt_u
;
const
VFromD
<
decltype
(
dt
)
>
interleaved
=
detail
:
:
InterleaveWhole
(
dt
a
b
)
;
constexpr
size_t
kShift
=
CeilLog2
(
16
/
sizeof
(
TFromD
<
D
>
)
)
;
const
VFromD
<
decltype
(
dt_u
)
>
idx_block
=
ShiftRight
<
kShift
>
(
detail
:
:
Iota0
(
dt_u
)
)
;
const
MFromD
<
decltype
(
dt_u
)
>
is_even
=
detail
:
:
EqS
(
detail
:
:
AndS
(
idx_block
1
)
0
)
;
return
BitCast
(
d
LowerHalf
(
Compress
(
BitCast
(
dt_u
interleaved
)
is_even
)
)
)
;
}
template
<
class
D
class
V
HWY_IF_POW2_GT_D
(
D
2
)
>
HWY_INLINE
V
InterleaveLowerBlocks
(
D
d
const
V
a
const
V
b
)
{
const
Half
<
D
>
dh
;
const
VFromD
<
decltype
(
dh
)
>
i0
=
InterleaveLowerBlocks
(
dh
LowerHalf
(
dh
a
)
LowerHalf
(
dh
b
)
)
;
const
VFromD
<
decltype
(
dh
)
>
i1
=
InterleaveLowerBlocks
(
dh
UpperHalf
(
dh
a
)
UpperHalf
(
dh
b
)
)
;
return
Combine
(
d
i1
i0
)
;
}
template
<
class
D
class
V
HWY_IF_POW2_LE_D
(
D
2
)
>
HWY_INLINE
V
InterleaveUpperBlocks
(
D
d
const
V
a
const
V
b
)
{
static_assert
(
IsSame
<
TFromD
<
D
>
TFromV
<
V
>
>
(
)
"
D
/
V
mismatch
"
)
;
const
Twice
<
D
>
dt
;
const
RebindToUnsigned
<
decltype
(
dt
)
>
dt_u
;
const
VFromD
<
decltype
(
dt
)
>
interleaved
=
detail
:
:
InterleaveWhole
(
dt
a
b
)
;
constexpr
size_t
kShift
=
CeilLog2
(
16
/
sizeof
(
TFromD
<
D
>
)
)
;
const
VFromD
<
decltype
(
dt_u
)
>
idx_block
=
ShiftRight
<
kShift
>
(
detail
:
:
Iota0
(
dt_u
)
)
;
const
MFromD
<
decltype
(
dt_u
)
>
is_odd
=
detail
:
:
EqS
(
detail
:
:
AndS
(
idx_block
1
)
1
)
;
return
BitCast
(
d
LowerHalf
(
Compress
(
BitCast
(
dt_u
interleaved
)
is_odd
)
)
)
;
}
template
<
class
D
class
V
HWY_IF_POW2_GT_D
(
D
2
)
>
HWY_INLINE
V
InterleaveUpperBlocks
(
D
d
const
V
a
const
V
b
)
{
const
Half
<
D
>
dh
;
const
VFromD
<
decltype
(
dh
)
>
i0
=
InterleaveUpperBlocks
(
dh
LowerHalf
(
dh
a
)
LowerHalf
(
dh
b
)
)
;
const
VFromD
<
decltype
(
dh
)
>
i1
=
InterleaveUpperBlocks
(
dh
UpperHalf
(
dh
a
)
UpperHalf
(
dh
b
)
)
;
return
Combine
(
d
i1
i0
)
;
}
template
<
typename
T
size_t
N
int
kPow2
>
constexpr
bool
IsGE128
(
Simd
<
T
N
kPow2
>
)
{
return
N
*
sizeof
(
T
)
>
=
16
&
&
kPow2
>
=
0
;
}
template
<
typename
T
size_t
N
int
kPow2
>
constexpr
bool
IsLT128
(
Simd
<
T
N
kPow2
>
)
{
return
N
*
sizeof
(
T
)
<
16
;
}
}
#
define
HWY_RVV_IF_GE128_D
(
D
)
hwy
:
:
EnableIf
<
detail
:
:
IsGE128
(
D
(
)
)
>
*
=
nullptr
#
define
HWY_RVV_IF_LT128_D
(
D
)
hwy
:
:
EnableIf
<
detail
:
:
IsLT128
(
D
(
)
)
>
*
=
nullptr
#
define
HWY_RVV_IF_CAN128_D
(
D
)
\
hwy
:
:
EnableIf
<
!
detail
:
:
IsLT128
(
D
(
)
)
&
&
!
detail
:
:
IsGE128
(
D
(
)
)
>
*
=
nullptr
template
<
class
D
class
V
HWY_RVV_IF_GE128_D
(
D
)
>
HWY_API
V
InterleaveLower
(
D
d
const
V
a
const
V
b
)
{
return
detail
:
:
InterleaveLowerBlocks
(
d
a
b
)
;
}
template
<
class
D
class
V
HWY_RVV_IF_LT128_D
(
D
)
>
HWY_API
V
InterleaveLower
(
D
d
const
V
a
const
V
b
)
{
static_assert
(
IsSame
<
TFromD
<
D
>
TFromV
<
V
>
>
(
)
"
D
/
V
mismatch
"
)
;
return
InterleaveWholeLower
(
d
a
b
)
;
}
template
<
class
D
class
V
HWY_RVV_IF_CAN128_D
(
D
)
>
HWY_API
V
InterleaveLower
(
D
d
const
V
a
const
V
b
)
{
if
(
Lanes
(
d
)
*
sizeof
(
TFromD
<
D
>
)
<
=
16
)
{
return
InterleaveWholeLower
(
d
a
b
)
;
}
const
ScalableTag
<
TFromD
<
D
>
HWY_MAX
(
d
.
Pow2
(
)
0
)
>
d1
;
return
ResizeBitCast
(
d
detail
:
:
InterleaveLowerBlocks
(
d1
ResizeBitCast
(
d1
a
)
ResizeBitCast
(
d1
b
)
)
)
;
}
template
<
class
V
>
HWY_API
V
InterleaveLower
(
const
V
a
const
V
b
)
{
return
InterleaveLower
(
DFromV
<
V
>
(
)
a
b
)
;
}
template
<
class
D
class
V
HWY_RVV_IF_GE128_D
(
D
)
>
HWY_API
V
InterleaveUpper
(
D
d
const
V
a
const
V
b
)
{
return
detail
:
:
InterleaveUpperBlocks
(
d
a
b
)
;
}
template
<
class
D
class
V
HWY_RVV_IF_LT128_D
(
D
)
>
HWY_API
V
InterleaveUpper
(
D
d
const
V
a
const
V
b
)
{
static_assert
(
IsSame
<
TFromD
<
D
>
TFromV
<
V
>
>
(
)
"
D
/
V
mismatch
"
)
;
return
InterleaveWholeUpper
(
d
a
b
)
;
}
template
<
class
D
class
V
HWY_RVV_IF_CAN128_D
(
D
)
>
HWY_API
V
InterleaveUpper
(
D
d
const
V
a
const
V
b
)
{
if
(
Lanes
(
d
)
*
sizeof
(
TFromD
<
D
>
)
<
=
16
)
{
return
InterleaveWholeUpper
(
d
a
b
)
;
}
const
ScalableTag
<
TFromD
<
D
>
HWY_MAX
(
d
.
Pow2
(
)
0
)
>
d1
;
return
ResizeBitCast
(
d
detail
:
:
InterleaveUpperBlocks
(
d1
ResizeBitCast
(
d1
a
)
ResizeBitCast
(
d1
b
)
)
)
;
}
template
<
class
V
class
DW
=
RepartitionToWide
<
DFromV
<
V
>
>
>
HWY_API
VFromD
<
DW
>
ZipLower
(
DW
dw
V
a
V
b
)
{
const
RepartitionToNarrow
<
DW
>
dn
;
static_assert
(
IsSame
<
TFromD
<
decltype
(
dn
)
>
TFromV
<
V
>
>
(
)
"
D
/
V
mismatch
"
)
;
return
BitCast
(
dw
InterleaveLower
(
dn
a
b
)
)
;
}
template
<
class
V
class
DW
=
RepartitionToWide
<
DFromV
<
V
>
>
>
HWY_API
VFromD
<
DW
>
ZipLower
(
V
a
V
b
)
{
return
BitCast
(
DW
(
)
InterleaveLower
(
a
b
)
)
;
}
template
<
class
DW
class
V
>
HWY_API
VFromD
<
DW
>
ZipUpper
(
DW
dw
V
a
V
b
)
{
const
RepartitionToNarrow
<
DW
>
dn
;
static_assert
(
IsSame
<
TFromD
<
decltype
(
dn
)
>
TFromV
<
V
>
>
(
)
"
D
/
V
mismatch
"
)
;
return
BitCast
(
dw
InterleaveUpper
(
dn
a
b
)
)
;
}
#
ifdef
HWY_NATIVE_REDUCE_SCALAR
#
undef
HWY_NATIVE_REDUCE_SCALAR
#
else
#
define
HWY_NATIVE_REDUCE_SCALAR
#
endif
#
define
HWY_RVV_REDUCE
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_T
(
BASE
SEW
)
\
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
\
HWY_RVV_V
(
BASE
SEW
m1
)
v0
)
{
\
return
GetLane
(
__riscv_v
#
#
OP
#
#
_vs_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_
#
#
CHAR
#
#
SEW
#
#
m1
(
\
v
v0
Lanes
(
d
)
)
)
;
\
}
#
undef
HWY_IF_REDUCE_D
#
define
HWY_IF_REDUCE_D
(
D
)
hwy
:
:
EnableIf
<
HWY_MAX_LANES_D
(
D
)
!
=
1
>
*
=
nullptr
#
ifdef
HWY_NATIVE_REDUCE_SUM_4_UI8
#
undef
HWY_NATIVE_REDUCE_SUM_4_UI8
#
else
#
define
HWY_NATIVE_REDUCE_SUM_4_UI8
#
endif
#
ifdef
HWY_NATIVE_REDUCE_MINMAX_4_UI8
#
undef
HWY_NATIVE_REDUCE_MINMAX_4_UI8
#
else
#
define
HWY_NATIVE_REDUCE_MINMAX_4_UI8
#
endif
namespace
detail
{
HWY_RVV_FOREACH_UI
(
HWY_RVV_REDUCE
RedSum
redsum
_ALL_VIRT
)
HWY_RVV_FOREACH_F
(
HWY_RVV_REDUCE
RedSum
fredusum
_ALL_VIRT
)
}
template
<
class
D
HWY_IF_REDUCE_D
(
D
)
>
HWY_API
TFromD
<
D
>
ReduceSum
(
D
d
const
VFromD
<
D
>
v
)
{
const
auto
v0
=
Zero
(
ScalableTag
<
TFromD
<
D
>
>
(
)
)
;
return
detail
:
:
RedSum
(
d
v
v0
)
;
}
namespace
detail
{
HWY_RVV_FOREACH_U
(
HWY_RVV_REDUCE
RedMin
redminu
_ALL_VIRT
)
HWY_RVV_FOREACH_I
(
HWY_RVV_REDUCE
RedMin
redmin
_ALL_VIRT
)
HWY_RVV_FOREACH_F
(
HWY_RVV_REDUCE
RedMin
fredmin
_ALL_VIRT
)
}
template
<
class
D
typename
T
=
TFromD
<
D
>
HWY_IF_REDUCE_D
(
D
)
>
HWY_API
T
ReduceMin
(
D
d
const
VFromD
<
D
>
v
)
{
const
ScalableTag
<
T
>
d1
;
return
detail
:
:
RedMin
(
d
v
Set
(
d1
HighestValue
<
T
>
(
)
)
)
;
}
namespace
detail
{
HWY_RVV_FOREACH_U
(
HWY_RVV_REDUCE
RedMax
redmaxu
_ALL_VIRT
)
HWY_RVV_FOREACH_I
(
HWY_RVV_REDUCE
RedMax
redmax
_ALL_VIRT
)
HWY_RVV_FOREACH_F
(
HWY_RVV_REDUCE
RedMax
fredmax
_ALL_VIRT
)
}
template
<
class
D
typename
T
=
TFromD
<
D
>
HWY_IF_REDUCE_D
(
D
)
>
HWY_API
T
ReduceMax
(
D
d
const
VFromD
<
D
>
v
)
{
const
ScalableTag
<
T
>
d1
;
return
detail
:
:
RedMax
(
d
v
Set
(
d1
LowestValue
<
T
>
(
)
)
)
;
}
#
undef
HWY_RVV_REDUCE
template
<
class
D
HWY_IF_LANES_GT_D
(
D
1
)
>
HWY_API
VFromD
<
D
>
SumOfLanes
(
D
d
VFromD
<
D
>
v
)
{
return
Set
(
d
ReduceSum
(
d
v
)
)
;
}
template
<
class
D
HWY_IF_LANES_GT_D
(
D
1
)
>
HWY_API
VFromD
<
D
>
MinOfLanes
(
D
d
VFromD
<
D
>
v
)
{
return
Set
(
d
ReduceMin
(
d
v
)
)
;
}
template
<
class
D
HWY_IF_LANES_GT_D
(
D
1
)
>
HWY_API
VFromD
<
D
>
MaxOfLanes
(
D
d
VFromD
<
D
>
v
)
{
return
Set
(
d
ReduceMax
(
d
v
)
)
;
}
#
ifdef
HWY_NATIVE_LOAD_STORE_INTERLEAVED
#
undef
HWY_NATIVE_LOAD_STORE_INTERLEAVED
#
else
#
define
HWY_NATIVE_LOAD_STORE_INTERLEAVED
#
endif
#
if
HWY_HAVE_TUPLE
#
define
HWY_RVV_GET
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
kIndex
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
#
#
2
(
HWY_RVV_TUP
(
BASE
SEW
LMUL
2
)
tup
)
{
\
return
__riscv_v
#
#
OP
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
x2_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
tup
\
kIndex
)
;
\
}
\
template
<
size_t
kIndex
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
#
#
3
(
HWY_RVV_TUP
(
BASE
SEW
LMUL
3
)
tup
)
{
\
return
__riscv_v
#
#
OP
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
x3_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
tup
\
kIndex
)
;
\
}
\
template
<
size_t
kIndex
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
#
#
4
(
HWY_RVV_TUP
(
BASE
SEW
LMUL
4
)
tup
)
{
\
return
__riscv_v
#
#
OP
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
x4_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
tup
\
kIndex
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_GET
Get
get
_LE2
)
#
undef
HWY_RVV_GET
#
define
HWY_RVV_SET
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
kIndex
>
\
HWY_API
HWY_RVV_TUP
(
BASE
SEW
LMUL
2
)
NAME
#
#
2
(
\
HWY_RVV_TUP
(
BASE
SEW
LMUL
2
)
tup
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
x2
(
\
tup
kIndex
v
)
;
\
}
\
template
<
size_t
kIndex
>
\
HWY_API
HWY_RVV_TUP
(
BASE
SEW
LMUL
3
)
NAME
#
#
3
(
\
HWY_RVV_TUP
(
BASE
SEW
LMUL
3
)
tup
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
x3
(
\
tup
kIndex
v
)
;
\
}
\
template
<
size_t
kIndex
>
\
HWY_API
HWY_RVV_TUP
(
BASE
SEW
LMUL
4
)
NAME
#
#
4
(
\
HWY_RVV_TUP
(
BASE
SEW
LMUL
4
)
tup
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
x4
(
\
tup
kIndex
v
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_SET
Set
set
_LE2
)
#
undef
HWY_RVV_SET
#
define
HWY_RVV_CREATE
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_TUP
(
BASE
SEW
LMUL
2
)
\
NAME
#
#
2
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
/
*
d
*
/
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v0
HWY_RVV_V
(
BASE
SEW
LMUL
)
v1
)
{
\
HWY_RVV_TUP
(
BASE
SEW
LMUL
2
)
tup
{
}
;
\
tup
=
Set2
<
0
>
(
tup
v0
)
;
\
tup
=
Set2
<
1
>
(
tup
v1
)
;
\
return
tup
;
\
}
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_TUP
(
BASE
SEW
LMUL
3
)
NAME
#
#
3
(
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
/
*
d
*
/
HWY_RVV_V
(
BASE
SEW
LMUL
)
v0
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v1
HWY_RVV_V
(
BASE
SEW
LMUL
)
v2
)
{
\
HWY_RVV_TUP
(
BASE
SEW
LMUL
3
)
tup
{
}
;
\
tup
=
Set3
<
0
>
(
tup
v0
)
;
\
tup
=
Set3
<
1
>
(
tup
v1
)
;
\
tup
=
Set3
<
2
>
(
tup
v2
)
;
\
return
tup
;
\
}
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_TUP
(
BASE
SEW
LMUL
4
)
\
NAME
#
#
4
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
/
*
d
*
/
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v0
HWY_RVV_V
(
BASE
SEW
LMUL
)
v1
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v2
HWY_RVV_V
(
BASE
SEW
LMUL
)
v3
)
{
\
HWY_RVV_TUP
(
BASE
SEW
LMUL
4
)
tup
{
}
;
\
tup
=
Set4
<
0
>
(
tup
v0
)
;
\
tup
=
Set4
<
1
>
(
tup
v1
)
;
\
tup
=
Set4
<
2
>
(
tup
v2
)
;
\
tup
=
Set4
<
3
>
(
tup
v3
)
;
\
return
tup
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_CREATE
Create
xx
_LE2_VIRT
)
#
undef
HWY_RVV_CREATE
template
<
class
D
>
using
Vec2
=
decltype
(
Create2
(
D
(
)
Zero
(
D
(
)
)
Zero
(
D
(
)
)
)
)
;
template
<
class
D
>
using
Vec3
=
decltype
(
Create3
(
D
(
)
Zero
(
D
(
)
)
Zero
(
D
(
)
)
Zero
(
D
(
)
)
)
)
;
template
<
class
D
>
using
Vec4
=
decltype
(
Create4
(
D
(
)
Zero
(
D
(
)
)
Zero
(
D
(
)
)
Zero
(
D
(
)
)
Zero
(
D
(
)
)
)
)
;
#
define
HWY_RVV_LOAD2
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
void
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
const
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
unaligned
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
&
v0
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
&
v1
)
{
\
const
HWY_RVV_TUP
(
BASE
SEW
LMUL
2
)
tup
=
\
__riscv_v
#
#
OP
#
#
e
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
x2
(
unaligned
Lanes
(
d
)
)
;
\
v0
=
Get2
<
0
>
(
tup
)
;
\
v1
=
Get2
<
1
>
(
tup
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_LOAD2
LoadInterleaved2
lseg2
_LE2_VIRT
)
#
undef
HWY_RVV_LOAD2
#
define
HWY_RVV_LOAD3
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
void
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
const
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
unaligned
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
&
v0
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
&
v1
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
&
v2
)
{
\
const
HWY_RVV_TUP
(
BASE
SEW
LMUL
3
)
tup
=
\
__riscv_v
#
#
OP
#
#
e
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
x3
(
unaligned
Lanes
(
d
)
)
;
\
v0
=
Get3
<
0
>
(
tup
)
;
\
v1
=
Get3
<
1
>
(
tup
)
;
\
v2
=
Get3
<
2
>
(
tup
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_LOAD3
LoadInterleaved3
lseg3
_LE2_VIRT
)
#
undef
HWY_RVV_LOAD3
#
define
HWY_RVV_LOAD4
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
void
NAME
(
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
const
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
unaligned
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
&
v0
HWY_RVV_V
(
BASE
SEW
LMUL
)
&
v1
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
&
v2
HWY_RVV_V
(
BASE
SEW
LMUL
)
&
v3
)
{
\
const
HWY_RVV_TUP
(
BASE
SEW
LMUL
4
)
tup
=
\
__riscv_v
#
#
OP
#
#
e
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
x4
(
unaligned
Lanes
(
d
)
)
;
\
v0
=
Get4
<
0
>
(
tup
)
;
\
v1
=
Get4
<
1
>
(
tup
)
;
\
v2
=
Get4
<
2
>
(
tup
)
;
\
v3
=
Get4
<
3
>
(
tup
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_LOAD4
LoadInterleaved4
lseg4
_LE2_VIRT
)
#
undef
HWY_RVV_LOAD4
#
define
HWY_RVV_STORE2
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
void
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v0
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v1
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
unaligned
)
{
\
const
HWY_RVV_TUP
(
BASE
SEW
LMUL
2
)
tup
=
Create2
(
d
v0
v1
)
;
\
__riscv_v
#
#
OP
#
#
e
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
x2
(
unaligned
tup
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_STORE2
StoreInterleaved2
sseg2
_LE2_VIRT
)
#
undef
HWY_RVV_STORE2
#
define
HWY_RVV_STORE3
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
void
NAME
(
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v0
HWY_RVV_V
(
BASE
SEW
LMUL
)
v1
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v2
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
unaligned
)
{
\
const
HWY_RVV_TUP
(
BASE
SEW
LMUL
3
)
tup
=
Create3
(
d
v0
v1
v2
)
;
\
__riscv_v
#
#
OP
#
#
e
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
x3
(
unaligned
tup
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_STORE3
StoreInterleaved3
sseg3
_LE2_VIRT
)
#
undef
HWY_RVV_STORE3
#
define
HWY_RVV_STORE4
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
SHIFT
\
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
void
NAME
(
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v0
HWY_RVV_V
(
BASE
SEW
LMUL
)
v1
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
v2
HWY_RVV_V
(
BASE
SEW
LMUL
)
v3
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
unaligned
)
{
\
const
HWY_RVV_TUP
(
BASE
SEW
LMUL
4
)
tup
=
Create4
(
d
v0
v1
v2
v3
)
;
\
__riscv_v
#
#
OP
#
#
e
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
#
#
x4
(
unaligned
tup
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_STORE4
StoreInterleaved4
sseg4
_LE2_VIRT
)
#
undef
HWY_RVV_STORE4
#
else
template
<
class
D
typename
T
=
TFromD
<
D
>
>
HWY_API
void
LoadInterleaved2
(
D
d
const
T
*
HWY_RESTRICT
unaligned
VFromD
<
D
>
&
v0
VFromD
<
D
>
&
v1
)
{
const
VFromD
<
D
>
A
=
LoadU
(
d
unaligned
)
;
const
VFromD
<
D
>
B
=
LoadU
(
d
unaligned
+
Lanes
(
d
)
)
;
v0
=
ConcatEven
(
d
B
A
)
;
v1
=
ConcatOdd
(
d
B
A
)
;
}
namespace
detail
{
#
define
HWY_RVV_LOAD_STRIDED
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
\
NAME
(
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
const
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
p
size_t
stride
)
{
\
return
__riscv_v
#
#
OP
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
\
p
static_cast
<
ptrdiff_t
>
(
stride
)
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_LOAD_STRIDED
LoadStrided
lse
_ALL_VIRT
)
#
undef
HWY_RVV_LOAD_STRIDED
}
template
<
class
D
typename
T
=
TFromD
<
D
>
>
HWY_API
void
LoadInterleaved3
(
D
d
const
TFromD
<
D
>
*
HWY_RESTRICT
unaligned
VFromD
<
D
>
&
v0
VFromD
<
D
>
&
v1
VFromD
<
D
>
&
v2
)
{
v0
=
detail
:
:
LoadStrided
(
d
unaligned
+
0
3
*
sizeof
(
T
)
)
;
v1
=
detail
:
:
LoadStrided
(
d
unaligned
+
1
3
*
sizeof
(
T
)
)
;
v2
=
detail
:
:
LoadStrided
(
d
unaligned
+
2
3
*
sizeof
(
T
)
)
;
}
template
<
class
D
typename
T
=
TFromD
<
D
>
>
HWY_API
void
LoadInterleaved4
(
D
d
const
TFromD
<
D
>
*
HWY_RESTRICT
unaligned
VFromD
<
D
>
&
v0
VFromD
<
D
>
&
v1
VFromD
<
D
>
&
v2
VFromD
<
D
>
&
v3
)
{
v0
=
detail
:
:
LoadStrided
(
d
unaligned
+
0
4
*
sizeof
(
T
)
)
;
v1
=
detail
:
:
LoadStrided
(
d
unaligned
+
1
4
*
sizeof
(
T
)
)
;
v2
=
detail
:
:
LoadStrided
(
d
unaligned
+
2
4
*
sizeof
(
T
)
)
;
v3
=
detail
:
:
LoadStrided
(
d
unaligned
+
3
4
*
sizeof
(
T
)
)
;
}
template
<
class
D
typename
T
=
TFromD
<
D
>
HWY_IF_NOT_T_SIZE_D
(
D
8
)
HWY_IF_POW2_LE_D
(
D
2
)
>
HWY_API
void
StoreInterleaved2
(
VFromD
<
D
>
v0
VFromD
<
D
>
v1
D
d
T
*
HWY_RESTRICT
unaligned
)
{
const
RebindToUnsigned
<
D
>
du
;
const
Twice
<
RepartitionToWide
<
decltype
(
du
)
>
>
duw
;
const
Twice
<
decltype
(
d
)
>
dt
;
const
VFromD
<
decltype
(
dt
)
>
w0
=
BitCast
(
dt
PromoteTo
(
duw
BitCast
(
du
v0
)
)
)
;
const
VFromD
<
decltype
(
dt
)
>
w1
=
BitCast
(
dt
PromoteTo
(
duw
BitCast
(
du
v1
)
)
)
;
StoreU
(
Or
(
w0
detail
:
:
Slide1Up
(
w1
)
)
dt
unaligned
)
;
}
template
<
class
D
typename
T
=
TFromD
<
D
>
HWY_IF_NOT_T_SIZE_D
(
D
8
)
HWY_IF_POW2_GT_D
(
D
2
)
>
HWY_API
void
StoreInterleaved2
(
VFromD
<
D
>
v0
VFromD
<
D
>
v1
D
d
T
*
HWY_RESTRICT
unaligned
)
{
const
Half
<
decltype
(
d
)
>
dh
;
StoreInterleaved2
(
LowerHalf
(
dh
v0
)
LowerHalf
(
dh
v1
)
d
unaligned
)
;
StoreInterleaved2
(
UpperHalf
(
dh
v0
)
UpperHalf
(
dh
v1
)
d
unaligned
+
Lanes
(
d
)
)
;
}
namespace
detail
{
#
define
HWY_RVV_STORE_STRIDED
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
void
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
\
HWY_RVV_D
(
BASE
SEW
N
SHIFT
)
d
\
HWY_RVV_T
(
BASE
SEW
)
*
HWY_RESTRICT
p
size_t
stride
)
{
\
return
__riscv_v
#
#
OP
#
#
SEW
#
#
_v_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
\
p
static_cast
<
ptrdiff_t
>
(
stride
)
v
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH
(
HWY_RVV_STORE_STRIDED
StoreStrided
sse
_ALL_VIRT
)
#
undef
HWY_RVV_STORE_STRIDED
}
template
<
class
D
typename
T
=
TFromD
<
D
>
HWY_IF_T_SIZE_D
(
D
8
)
>
HWY_API
void
StoreInterleaved2
(
VFromD
<
D
>
v0
VFromD
<
D
>
v1
D
d
T
*
HWY_RESTRICT
unaligned
)
{
detail
:
:
StoreStrided
(
v0
d
unaligned
+
0
2
*
sizeof
(
T
)
)
;
detail
:
:
StoreStrided
(
v1
d
unaligned
+
1
2
*
sizeof
(
T
)
)
;
}
template
<
class
D
typename
T
=
TFromD
<
D
>
>
HWY_API
void
StoreInterleaved3
(
VFromD
<
D
>
v0
VFromD
<
D
>
v1
VFromD
<
D
>
v2
D
d
T
*
HWY_RESTRICT
unaligned
)
{
detail
:
:
StoreStrided
(
v0
d
unaligned
+
0
3
*
sizeof
(
T
)
)
;
detail
:
:
StoreStrided
(
v1
d
unaligned
+
1
3
*
sizeof
(
T
)
)
;
detail
:
:
StoreStrided
(
v2
d
unaligned
+
2
3
*
sizeof
(
T
)
)
;
}
template
<
class
D
typename
T
=
TFromD
<
D
>
>
HWY_API
void
StoreInterleaved4
(
VFromD
<
D
>
v0
VFromD
<
D
>
v1
VFromD
<
D
>
v2
VFromD
<
D
>
v3
D
d
T
*
HWY_RESTRICT
unaligned
)
{
detail
:
:
StoreStrided
(
v0
d
unaligned
+
0
4
*
sizeof
(
T
)
)
;
detail
:
:
StoreStrided
(
v1
d
unaligned
+
1
4
*
sizeof
(
T
)
)
;
detail
:
:
StoreStrided
(
v2
d
unaligned
+
2
4
*
sizeof
(
T
)
)
;
detail
:
:
StoreStrided
(
v3
d
unaligned
+
3
4
*
sizeof
(
T
)
)
;
}
#
endif
template
<
class
D
HWY_IF_T_SIZE_D
(
D
8
)
HWY_IF_LANES_D
(
D
1
)
>
HWY_API
VFromD
<
D
>
Dup128VecFromValues
(
D
d
TFromD
<
D
>
t0
TFromD
<
D
>
)
{
return
Set
(
d
t0
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
8
)
HWY_IF_LANES_GT_D
(
D
1
)
>
HWY_API
VFromD
<
D
>
Dup128VecFromValues
(
D
d
TFromD
<
D
>
t0
TFromD
<
D
>
t1
)
{
const
auto
even_lanes
=
Set
(
d
t0
)
;
#
if
HWY_COMPILER_GCC
&
&
!
HWY_IS_DEBUG_BUILD
if
(
__builtin_constant_p
(
BitCastScalar
<
uint64_t
>
(
t0
)
=
=
BitCastScalar
<
uint64_t
>
(
t1
)
)
&
&
(
BitCastScalar
<
uint64_t
>
(
t0
)
=
=
BitCastScalar
<
uint64_t
>
(
t1
)
)
)
{
return
even_lanes
;
}
#
endif
const
auto
odd_lanes
=
Set
(
d
t1
)
;
return
OddEven
(
odd_lanes
even_lanes
)
;
}
namespace
detail
{
#
pragma
pack
(
push
1
)
template
<
class
T
>
struct
alignas
(
8
)
Vec64ValsWrapper
{
static_assert
(
sizeof
(
T
)
>
=
1
"
sizeof
(
T
)
>
=
1
must
be
true
"
)
;
static_assert
(
sizeof
(
T
)
<
=
8
"
sizeof
(
T
)
<
=
8
must
be
true
"
)
;
T
vals
[
8
/
sizeof
(
T
)
]
;
}
;
#
pragma
pack
(
pop
)
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
1
)
>
HWY_API
VFromD
<
D
>
Dup128VecFromValues
(
D
d
TFromD
<
D
>
t0
TFromD
<
D
>
t1
TFromD
<
D
>
t2
TFromD
<
D
>
t3
TFromD
<
D
>
t4
TFromD
<
D
>
t5
TFromD
<
D
>
t6
TFromD
<
D
>
t7
TFromD
<
D
>
t8
TFromD
<
D
>
t9
TFromD
<
D
>
t10
TFromD
<
D
>
t11
TFromD
<
D
>
t12
TFromD
<
D
>
t13
TFromD
<
D
>
t14
TFromD
<
D
>
t15
)
{
const
detail
:
:
AdjustSimdTagToMinVecPow2
<
Repartition
<
uint64_t
D
>
>
du64
;
return
ResizeBitCast
(
d
Dup128VecFromValues
(
du64
BitCastScalar
<
uint64_t
>
(
detail
:
:
Vec64ValsWrapper
<
TFromD
<
D
>
>
{
{
t0
t1
t2
t3
t4
t5
t6
t7
}
}
)
BitCastScalar
<
uint64_t
>
(
detail
:
:
Vec64ValsWrapper
<
TFromD
<
D
>
>
{
{
t8
t9
t10
t11
t12
t13
t14
t15
}
}
)
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
2
)
>
HWY_API
VFromD
<
D
>
Dup128VecFromValues
(
D
d
TFromD
<
D
>
t0
TFromD
<
D
>
t1
TFromD
<
D
>
t2
TFromD
<
D
>
t3
TFromD
<
D
>
t4
TFromD
<
D
>
t5
TFromD
<
D
>
t6
TFromD
<
D
>
t7
)
{
const
detail
:
:
AdjustSimdTagToMinVecPow2
<
Repartition
<
uint64_t
D
>
>
du64
;
return
ResizeBitCast
(
d
Dup128VecFromValues
(
du64
BitCastScalar
<
uint64_t
>
(
detail
:
:
Vec64ValsWrapper
<
TFromD
<
D
>
>
{
{
t0
t1
t2
t3
}
}
)
BitCastScalar
<
uint64_t
>
(
detail
:
:
Vec64ValsWrapper
<
TFromD
<
D
>
>
{
{
t4
t5
t6
t7
}
}
)
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
4
)
>
HWY_API
VFromD
<
D
>
Dup128VecFromValues
(
D
d
TFromD
<
D
>
t0
TFromD
<
D
>
t1
TFromD
<
D
>
t2
TFromD
<
D
>
t3
)
{
const
detail
:
:
AdjustSimdTagToMinVecPow2
<
Repartition
<
uint64_t
D
>
>
du64
;
return
ResizeBitCast
(
d
Dup128VecFromValues
(
du64
BitCastScalar
<
uint64_t
>
(
detail
:
:
Vec64ValsWrapper
<
TFromD
<
D
>
>
{
{
t0
t1
}
}
)
BitCastScalar
<
uint64_t
>
(
detail
:
:
Vec64ValsWrapper
<
TFromD
<
D
>
>
{
{
t2
t3
}
}
)
)
)
;
}
template
<
typename
V
class
D
=
DFromV
<
V
>
HWY_IF_U8_D
(
D
)
hwy
:
:
EnableIf
<
D
(
)
.
Pow2
(
)
<
1
|
|
D
(
)
.
MaxLanes
(
)
<
16
>
*
=
nullptr
>
HWY_API
V
PopulationCount
(
V
v
)
{
v
=
Sub
(
v
detail
:
:
AndS
(
ShiftRight
<
1
>
(
v
)
0x55
)
)
;
v
=
Add
(
detail
:
:
AndS
(
ShiftRight
<
2
>
(
v
)
0x33
)
detail
:
:
AndS
(
v
0x33
)
)
;
return
detail
:
:
AndS
(
Add
(
v
ShiftRight
<
4
>
(
v
)
)
0x0F
)
;
}
template
<
class
D
>
HWY_API
VFromD
<
D
>
LoadDup128
(
D
d
const
TFromD
<
D
>
*
const
HWY_RESTRICT
p
)
{
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
constexpr
int
kLoadPow2
=
d
.
Pow2
(
)
;
constexpr
size_t
kMaxLanesToLoad
=
HWY_MIN
(
HWY_MAX_LANES_D
(
D
)
16
/
sizeof
(
TFromD
<
D
>
)
)
;
constexpr
size_t
kLoadN
=
D
:
:
template
NewN
<
kLoadPow2
kMaxLanesToLoad
>
(
)
;
const
Simd
<
TFromD
<
D
>
kLoadN
kLoadPow2
>
d_load
;
static_assert
(
d_load
.
MaxBytes
(
)
<
=
16
"
d_load
.
MaxBytes
(
)
<
=
16
must
be
true
"
)
;
static_assert
(
(
d
.
MaxBytes
(
)
<
16
)
|
|
(
d_load
.
MaxBytes
(
)
=
=
16
)
"
d_load
.
MaxBytes
(
)
=
=
16
must
be
true
if
d
.
MaxBytes
(
)
>
=
16
is
"
"
true
"
)
;
static_assert
(
(
d
.
MaxBytes
(
)
>
=
16
)
|
|
(
d_load
.
MaxBytes
(
)
=
=
d
.
MaxBytes
(
)
)
"
d_load
.
MaxBytes
(
)
=
=
d
.
MaxBytes
(
)
must
be
true
if
"
"
d
.
MaxBytes
(
)
<
16
is
true
"
)
;
const
VFromD
<
D
>
loaded
=
Load
(
d_load
p
)
;
if
(
d
.
MaxBytes
(
)
<
=
16
)
return
loaded
;
using
TU
=
TFromD
<
decltype
(
du
)
>
;
const
TU
mask
=
static_cast
<
TU
>
(
detail
:
:
LanesPerBlock
(
d
)
-
1
)
;
const
VFromD
<
RebindToUnsigned
<
D
>
>
idx
=
detail
:
:
AndS
(
detail
:
:
Iota0
(
du
)
mask
)
;
return
TableLookupLanes
(
loaded
idx
)
;
}
namespace
detail
{
template
<
class
D
>
using
MaskTag
=
hwy
:
:
SizeTag
<
HWY_MIN
(
64
detail
:
:
ScaleByPower
(
8
*
sizeof
(
TFromD
<
D
>
)
-
D
(
)
.
Pow2
(
)
)
)
>
;
#
define
HWY_RVV_LOAD_MASK_BITS
(
SEW
SHIFT
MLEN
NAME
OP
)
\
HWY_INLINE
HWY_RVV_M
(
MLEN
)
\
NAME
(
hwy
:
:
SizeTag
<
MLEN
>
/
*
tag
*
/
const
uint8_t
*
bits
size_t
N
)
{
\
return
__riscv_v
#
#
OP
#
#
_v_b
#
#
MLEN
(
bits
N
)
;
\
}
HWY_RVV_FOREACH_B
(
HWY_RVV_LOAD_MASK_BITS
LoadMaskBits
lm
)
#
undef
HWY_RVV_LOAD_MASK_BITS
}
template
<
class
D
class
MT
=
detail
:
:
MaskTag
<
D
>
>
HWY_API
auto
LoadMaskBits
(
D
d
const
uint8_t
*
bits
)
-
>
decltype
(
detail
:
:
LoadMaskBits
(
MT
(
)
bits
Lanes
(
d
)
)
)
{
return
detail
:
:
LoadMaskBits
(
MT
(
)
bits
Lanes
(
d
)
)
;
}
#
define
HWY_RVV_STORE_MASK_BITS
(
SEW
SHIFT
MLEN
NAME
OP
)
\
template
<
class
D
>
\
HWY_API
size_t
NAME
(
D
d
HWY_RVV_M
(
MLEN
)
m
uint8_t
*
bits
)
{
\
const
size_t
N
=
Lanes
(
d
)
;
\
__riscv_v
#
#
OP
#
#
_v_b
#
#
MLEN
(
bits
m
N
)
;
\
/
*
Non
-
full
byte
need
to
clear
the
undefined
upper
bits
.
*
/
\
/
*
Use
MaxLanes
and
sizeof
(
T
)
to
move
some
checks
to
compile
-
time
.
*
/
\
constexpr
bool
kLessThan8
=
\
detail
:
:
ScaleByPower
(
16
/
sizeof
(
TFromD
<
D
>
)
d
.
Pow2
(
)
)
<
8
;
\
if
(
MaxLanes
(
d
)
<
8
|
|
(
kLessThan8
&
&
N
<
8
)
)
{
\
const
int
mask
=
(
1
<
<
N
)
-
1
;
\
bits
[
0
]
=
static_cast
<
uint8_t
>
(
bits
[
0
]
&
mask
)
;
\
}
\
return
(
N
+
7
)
/
8
;
\
}
HWY_RVV_FOREACH_B
(
HWY_RVV_STORE_MASK_BITS
StoreMaskBits
sm
)
#
undef
HWY_RVV_STORE_MASK_BITS
template
<
class
V
>
HWY_INLINE
V
CompressBits
(
V
v
const
uint8_t
*
HWY_RESTRICT
bits
)
{
return
Compress
(
v
LoadMaskBits
(
DFromV
<
V
>
(
)
bits
)
)
;
}
template
<
class
D
>
HWY_API
size_t
CompressBitsStore
(
VFromD
<
D
>
v
const
uint8_t
*
HWY_RESTRICT
bits
D
d
TFromD
<
D
>
*
HWY_RESTRICT
unaligned
)
{
return
CompressStore
(
v
LoadMaskBits
(
d
bits
)
d
unaligned
)
;
}
template
<
class
D
HWY_IF_NOT_T_SIZE_D
(
D
1
)
>
HWY_API
MFromD
<
D
>
FirstN
(
const
D
d
const
size_t
n
)
{
const
RebindToUnsigned
<
D
>
du
;
using
TU
=
TFromD
<
decltype
(
du
)
>
;
return
RebindMask
(
d
detail
:
:
LtS
(
detail
:
:
Iota0
(
du
)
static_cast
<
TU
>
(
n
)
)
)
;
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
1
)
>
HWY_API
MFromD
<
D
>
FirstN
(
const
D
d
const
size_t
n
)
{
const
auto
zero
=
Zero
(
d
)
;
const
auto
one
=
Set
(
d
1
)
;
return
Eq
(
detail
:
:
SlideUp
(
one
zero
n
)
one
)
;
}
#
if
HWY_COMPILER_CLANG
>
=
1700
|
|
HWY_COMPILER_GCC_ACTUAL
>
=
1400
namespace
detail
{
HWY_INLINE
vuint8m1_t
MaskToU8MaskBitsVec
(
vbool1_t
m
)
{
return
__riscv_vreinterpret_v_b1_u8m1
(
m
)
;
}
HWY_INLINE
vuint8m1_t
MaskToU8MaskBitsVec
(
vbool2_t
m
)
{
return
__riscv_vreinterpret_v_b2_u8m1
(
m
)
;
}
HWY_INLINE
vuint8m1_t
MaskToU8MaskBitsVec
(
vbool4_t
m
)
{
return
__riscv_vreinterpret_v_b4_u8m1
(
m
)
;
}
HWY_INLINE
vuint8m1_t
MaskToU8MaskBitsVec
(
vbool8_t
m
)
{
return
__riscv_vreinterpret_v_b8_u8m1
(
m
)
;
}
HWY_INLINE
vuint8m1_t
MaskToU8MaskBitsVec
(
vbool16_t
m
)
{
return
__riscv_vreinterpret_v_b16_u8m1
(
m
)
;
}
HWY_INLINE
vuint8m1_t
MaskToU8MaskBitsVec
(
vbool32_t
m
)
{
return
__riscv_vreinterpret_v_b32_u8m1
(
m
)
;
}
HWY_INLINE
vuint8m1_t
MaskToU8MaskBitsVec
(
vbool64_t
m
)
{
return
__riscv_vreinterpret_v_b64_u8m1
(
m
)
;
}
template
<
class
D
hwy
:
:
EnableIf
<
IsSame
<
MFromD
<
D
>
vbool1_t
>
(
)
>
*
=
nullptr
>
HWY_INLINE
MFromD
<
D
>
U8MaskBitsVecToMask
(
D
vuint8m1_t
v
)
{
return
__riscv_vreinterpret_v_u8m1_b1
(
v
)
;
}
template
<
class
D
hwy
:
:
EnableIf
<
IsSame
<
MFromD
<
D
>
vbool2_t
>
(
)
>
*
=
nullptr
>
HWY_INLINE
MFromD
<
D
>
U8MaskBitsVecToMask
(
D
vuint8m1_t
v
)
{
return
__riscv_vreinterpret_v_u8m1_b2
(
v
)
;
}
template
<
class
D
hwy
:
:
EnableIf
<
IsSame
<
MFromD
<
D
>
vbool4_t
>
(
)
>
*
=
nullptr
>
HWY_INLINE
MFromD
<
D
>
U8MaskBitsVecToMask
(
D
vuint8m1_t
v
)
{
return
__riscv_vreinterpret_v_u8m1_b4
(
v
)
;
}
template
<
class
D
hwy
:
:
EnableIf
<
IsSame
<
MFromD
<
D
>
vbool8_t
>
(
)
>
*
=
nullptr
>
HWY_INLINE
MFromD
<
D
>
U8MaskBitsVecToMask
(
D
vuint8m1_t
v
)
{
return
__riscv_vreinterpret_v_u8m1_b8
(
v
)
;
}
template
<
class
D
hwy
:
:
EnableIf
<
IsSame
<
MFromD
<
D
>
vbool16_t
>
(
)
>
*
=
nullptr
>
HWY_INLINE
MFromD
<
D
>
U8MaskBitsVecToMask
(
D
vuint8m1_t
v
)
{
return
__riscv_vreinterpret_v_u8m1_b16
(
v
)
;
}
template
<
class
D
hwy
:
:
EnableIf
<
IsSame
<
MFromD
<
D
>
vbool32_t
>
(
)
>
*
=
nullptr
>
HWY_INLINE
MFromD
<
D
>
U8MaskBitsVecToMask
(
D
vuint8m1_t
v
)
{
return
__riscv_vreinterpret_v_u8m1_b32
(
v
)
;
}
template
<
class
D
hwy
:
:
EnableIf
<
IsSame
<
MFromD
<
D
>
vbool64_t
>
(
)
>
*
=
nullptr
>
HWY_INLINE
MFromD
<
D
>
U8MaskBitsVecToMask
(
D
vuint8m1_t
v
)
{
return
__riscv_vreinterpret_v_u8m1_b64
(
v
)
;
}
}
#
ifdef
HWY_NATIVE_LOWER_HALF_OF_MASK
#
undef
HWY_NATIVE_LOWER_HALF_OF_MASK
#
else
#
define
HWY_NATIVE_LOWER_HALF_OF_MASK
#
endif
template
<
class
D
>
HWY_API
MFromD
<
D
>
LowerHalfOfMask
(
D
d
MFromD
<
Twice
<
D
>
>
m
)
{
return
detail
:
:
U8MaskBitsVecToMask
(
d
detail
:
:
MaskToU8MaskBitsVec
(
m
)
)
;
}
#
ifdef
HWY_NATIVE_UPPER_HALF_OF_MASK
#
undef
HWY_NATIVE_UPPER_HALF_OF_MASK
#
else
#
define
HWY_NATIVE_UPPER_HALF_OF_MASK
#
endif
template
<
class
D
>
HWY_API
MFromD
<
D
>
UpperHalfOfMask
(
D
d
MFromD
<
Twice
<
D
>
>
m
)
{
const
size_t
N
=
Lanes
(
d
)
;
vuint8m1_t
mask_bits
=
detail
:
:
MaskToU8MaskBitsVec
(
m
)
;
mask_bits
=
ShiftRightSame
(
mask_bits
static_cast
<
int
>
(
N
&
7
)
)
;
if
(
HWY_MAX_LANES_D
(
D
)
>
=
8
)
{
mask_bits
=
SlideDownLanes
(
ScalableTag
<
uint8_t
>
(
)
mask_bits
N
/
8
)
;
}
return
detail
:
:
U8MaskBitsVecToMask
(
d
mask_bits
)
;
}
#
ifdef
HWY_NATIVE_COMBINE_MASKS
#
undef
HWY_NATIVE_COMBINE_MASKS
#
else
#
define
HWY_NATIVE_COMBINE_MASKS
#
endif
template
<
class
D
>
HWY_API
MFromD
<
D
>
CombineMasks
(
D
d
MFromD
<
Half
<
D
>
>
hi
MFromD
<
Half
<
D
>
>
lo
)
{
const
Half
<
decltype
(
d
)
>
dh
;
const
size_t
half_N
=
Lanes
(
dh
)
;
const
auto
ext_lo_mask
=
And
(
detail
:
:
U8MaskBitsVecToMask
(
d
detail
:
:
MaskToU8MaskBitsVec
(
lo
)
)
FirstN
(
d
half_N
)
)
;
vuint8m1_t
hi_mask_bits
=
detail
:
:
MaskToU8MaskBitsVec
(
hi
)
;
hi_mask_bits
=
ShiftLeftSame
(
hi_mask_bits
static_cast
<
int
>
(
half_N
&
7
)
)
;
if
(
HWY_MAX_LANES_D
(
D
)
>
=
8
)
{
hi_mask_bits
=
SlideUpLanes
(
ScalableTag
<
uint8_t
>
(
)
hi_mask_bits
half_N
/
8
)
;
}
return
Or
(
ext_lo_mask
detail
:
:
U8MaskBitsVecToMask
(
d
hi_mask_bits
)
)
;
}
#
ifdef
HWY_NATIVE_ORDERED_DEMOTE_2_MASKS_TO
#
undef
HWY_NATIVE_ORDERED_DEMOTE_2_MASKS_TO
#
else
#
define
HWY_NATIVE_ORDERED_DEMOTE_2_MASKS_TO
#
endif
template
<
class
DTo
class
DFrom
HWY_IF_T_SIZE_D
(
DTo
sizeof
(
TFromD
<
DFrom
>
)
/
2
)
class
DTo_2
=
Repartition
<
TFromD
<
DTo
>
DFrom
>
hwy
:
:
EnableIf
<
IsSame
<
MFromD
<
DTo
>
MFromD
<
DTo_2
>
>
(
)
>
*
=
nullptr
>
HWY_API
MFromD
<
DTo
>
OrderedDemote2MasksTo
(
DTo
d_to
DFrom
MFromD
<
DFrom
>
a
MFromD
<
DFrom
>
b
)
{
return
CombineMasks
(
d_to
b
a
)
;
}
#
endif
namespace
detail
{
template
<
size_t
kN
HWY_IF_LANES_LE
(
kN
31
)
>
constexpr
unsigned
MaxMaskBits
(
)
{
return
(
1u
<
<
kN
)
-
1
;
}
template
<
size_t
kN
HWY_IF_LANES_GT
(
kN
31
)
>
constexpr
unsigned
MaxMaskBits
(
)
{
return
~
0u
;
}
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
1
)
HWY_IF_LANES_LE_D
(
D
8
)
>
HWY_API
MFromD
<
D
>
Dup128MaskFromMaskBits
(
D
d
unsigned
mask_bits
)
{
constexpr
size_t
kN
=
MaxLanes
(
d
)
;
if
(
kN
<
8
)
mask_bits
&
=
detail
:
:
MaxMaskBits
<
kN
>
(
)
;
#
if
HWY_COMPILER_CLANG
>
=
1700
|
|
HWY_COMPILER_GCC_ACTUAL
>
=
1400
return
detail
:
:
U8MaskBitsVecToMask
(
d
Set
(
ScalableTag
<
uint8_t
>
(
)
static_cast
<
uint8_t
>
(
mask_bits
)
)
)
;
#
else
const
RebindToUnsigned
<
decltype
(
d
)
>
du8
;
const
detail
:
:
AdjustSimdTagToMinVecPow2
<
Repartition
<
uint64_t
decltype
(
du8
)
>
>
du64
;
const
auto
bytes
=
ResizeBitCast
(
du8
detail
:
:
AndS
(
ResizeBitCast
(
du64
Set
(
du8
static_cast
<
uint8_t
>
(
mask_bits
)
)
)
uint64_t
{
0x8040201008040201u
}
)
)
;
return
detail
:
:
NeS
(
bytes
uint8_t
{
0
}
)
;
#
endif
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
1
)
HWY_IF_LANES_GT_D
(
D
8
)
>
HWY_API
MFromD
<
D
>
Dup128MaskFromMaskBits
(
D
d
unsigned
mask_bits
)
{
#
if
HWY_COMPILER_CLANG
>
=
1700
|
|
HWY_COMPILER_GCC_ACTUAL
>
=
1400
const
ScalableTag
<
uint8_t
>
du8
;
const
ScalableTag
<
uint16_t
>
du16
;
return
detail
:
:
U8MaskBitsVecToMask
(
d
BitCast
(
du8
Set
(
du16
static_cast
<
uint16_t
>
(
mask_bits
)
)
)
)
;
#
else
const
RebindToUnsigned
<
decltype
(
d
)
>
du8
;
const
Repartition
<
uint16_t
decltype
(
du8
)
>
du16
;
const
detail
:
:
AdjustSimdTagToMinVecPow2
<
Repartition
<
uint64_t
decltype
(
du8
)
>
>
du64
;
const
auto
bytes
=
BitCast
(
du8
Set
(
du16
static_cast
<
uint16_t
>
(
mask_bits
)
)
)
;
const
auto
rep8
=
TableLookupLanes
(
bytes
ShiftRight
<
3
>
(
detail
:
:
Iota0
(
du8
)
)
)
;
const
auto
masked_out_rep8
=
ResizeBitCast
(
du8
detail
:
:
AndS
(
ResizeBitCast
(
du64
rep8
)
uint64_t
{
0x8040201008040201u
}
)
)
;
return
detail
:
:
NeS
(
masked_out_rep8
uint8_t
{
0
}
)
;
#
endif
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
2
)
>
HWY_API
MFromD
<
D
>
Dup128MaskFromMaskBits
(
D
d
unsigned
mask_bits
)
{
constexpr
size_t
kN
=
MaxLanes
(
d
)
;
if
(
kN
<
8
)
mask_bits
&
=
detail
:
:
MaxMaskBits
<
kN
>
(
)
;
#
if
HWY_COMPILER_CLANG
>
=
1700
|
|
HWY_COMPILER_GCC_ACTUAL
>
=
1400
const
ScalableTag
<
uint8_t
>
du8
;
return
detail
:
:
U8MaskBitsVecToMask
(
d
Set
(
du8
static_cast
<
uint8_t
>
(
mask_bits
)
)
)
;
#
else
const
RebindToUnsigned
<
D
>
du
;
const
VFromD
<
decltype
(
du
)
>
bits
=
Shl
(
Set
(
du
uint16_t
{
1
}
)
Iota
(
du
uint16_t
{
0
}
)
)
;
return
TestBit
(
Set
(
du
static_cast
<
uint16_t
>
(
mask_bits
)
)
bits
)
;
#
endif
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
4
)
>
HWY_API
MFromD
<
D
>
Dup128MaskFromMaskBits
(
D
d
unsigned
mask_bits
)
{
constexpr
size_t
kN
=
MaxLanes
(
d
)
;
if
(
kN
<
4
)
mask_bits
&
=
detail
:
:
MaxMaskBits
<
kN
>
(
)
;
#
if
HWY_COMPILER_CLANG
>
=
1700
|
|
HWY_COMPILER_GCC_ACTUAL
>
=
1400
const
ScalableTag
<
uint8_t
>
du8
;
return
detail
:
:
U8MaskBitsVecToMask
(
d
Set
(
du8
static_cast
<
uint8_t
>
(
mask_bits
*
0x11
)
)
)
;
#
else
const
RebindToUnsigned
<
D
>
du
;
const
VFromD
<
decltype
(
du
)
>
bits
=
Shl
(
Set
(
du
uint32_t
{
1
}
)
Iota
(
du
uint32_t
{
0
}
)
)
;
return
TestBit
(
Set
(
du
static_cast
<
uint32_t
>
(
mask_bits
)
)
bits
)
;
#
endif
}
template
<
class
D
HWY_IF_T_SIZE_D
(
D
8
)
>
HWY_API
MFromD
<
D
>
Dup128MaskFromMaskBits
(
D
d
unsigned
mask_bits
)
{
constexpr
size_t
kN
=
MaxLanes
(
d
)
;
if
(
kN
<
2
)
mask_bits
&
=
detail
:
:
MaxMaskBits
<
kN
>
(
)
;
#
if
HWY_COMPILER_CLANG
>
=
1700
|
|
HWY_COMPILER_GCC_ACTUAL
>
=
1400
const
ScalableTag
<
uint8_t
>
du8
;
return
detail
:
:
U8MaskBitsVecToMask
(
d
Set
(
du8
static_cast
<
uint8_t
>
(
mask_bits
*
0x55
)
)
)
;
#
else
const
RebindToUnsigned
<
D
>
du
;
const
VFromD
<
decltype
(
du
)
>
bits
=
Dup128VecFromValues
(
du
0
1
)
;
return
TestBit
(
Set
(
du
static_cast
<
uint64_t
>
(
mask_bits
)
)
bits
)
;
#
endif
}
template
<
class
V
HWY_IF_SIGNED_V
(
V
)
>
HWY_API
V
Neg
(
const
V
v
)
{
return
detail
:
:
ReverseSubS
(
v
0
)
;
}
#
define
HWY_RVV_RETV_ARGV2
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
HWY_API
HWY_RVV_V
(
BASE
SEW
LMUL
)
NAME
(
HWY_RVV_V
(
BASE
SEW
LMUL
)
v
)
{
\
return
__riscv_v
#
#
OP
#
#
_vv_
#
#
CHAR
#
#
SEW
#
#
LMUL
(
v
v
\
HWY_RVV_AVL
(
SEW
SHIFT
)
)
;
\
}
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGV2
Neg
fsgnjn
_ALL
)
#
if
!
HWY_HAVE_FLOAT16
template
<
class
V
HWY_IF_U16_D
(
DFromV
<
V
>
)
>
HWY_API
V
Neg
(
V
v
)
{
const
DFromV
<
decltype
(
v
)
>
d
;
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
using
TU
=
TFromD
<
decltype
(
du
)
>
;
return
BitCast
(
d
Xor
(
BitCast
(
du
v
)
Set
(
du
SignMask
<
TU
>
(
)
)
)
)
;
}
#
endif
template
<
class
V
HWY_IF_SIGNED_V
(
V
)
>
HWY_API
V
Abs
(
const
V
v
)
{
return
Max
(
v
Neg
(
v
)
)
;
}
HWY_RVV_FOREACH_F
(
HWY_RVV_RETV_ARGV2
Abs
fsgnjx
_ALL
)
#
undef
HWY_RVV_RETV_ARGV2
template
<
class
V
HWY_IF_FLOAT_V
(
V
)
>
HWY_API
V
AbsDiff
(
const
V
a
const
V
b
)
{
return
Abs
(
Sub
(
a
b
)
)
;
}
namespace
detail
{
enum
RoundingModes
{
kNear
kTrunc
kDown
kUp
}
;
template
<
class
V
>
HWY_INLINE
auto
UseInt
(
const
V
v
)
-
>
decltype
(
MaskFromVec
(
v
)
)
{
return
detail
:
:
LtS
(
Abs
(
v
)
MantissaEnd
<
TFromV
<
V
>
>
(
)
)
;
}
}
template
<
class
V
>
HWY_API
V
Round
(
const
V
v
)
{
const
DFromV
<
V
>
df
;
const
auto
integer
=
NearestInt
(
v
)
;
const
auto
int_f
=
ConvertTo
(
df
integer
)
;
return
IfThenElse
(
detail
:
:
UseInt
(
v
)
CopySign
(
int_f
v
)
v
)
;
}
template
<
class
V
>
HWY_API
V
Trunc
(
const
V
v
)
{
const
DFromV
<
V
>
df
;
const
RebindToSigned
<
decltype
(
df
)
>
di
;
const
auto
integer
=
ConvertTo
(
di
v
)
;
const
auto
int_f
=
ConvertTo
(
df
integer
)
;
return
IfThenElse
(
detail
:
:
UseInt
(
v
)
CopySign
(
int_f
v
)
v
)
;
}
template
<
class
V
>
HWY_API
V
Ceil
(
const
V
v
)
{
asm
volatile
(
"
fsrm
%
0
"
:
:
"
r
"
(
detail
:
:
kUp
)
)
;
const
auto
ret
=
Round
(
v
)
;
asm
volatile
(
"
fsrm
%
0
"
:
:
"
r
"
(
detail
:
:
kNear
)
)
;
return
ret
;
}
template
<
class
V
>
HWY_API
V
Floor
(
const
V
v
)
{
asm
volatile
(
"
fsrm
%
0
"
:
:
"
r
"
(
detail
:
:
kDown
)
)
;
const
auto
ret
=
Round
(
v
)
;
asm
volatile
(
"
fsrm
%
0
"
:
:
"
r
"
(
detail
:
:
kNear
)
)
;
return
ret
;
}
template
<
class
V
>
HWY_API
MFromD
<
DFromV
<
V
>
>
IsNaN
(
const
V
v
)
{
return
Ne
(
v
v
)
;
}
#
ifdef
HWY_NATIVE_ISINF
#
undef
HWY_NATIVE_ISINF
#
else
#
define
HWY_NATIVE_ISINF
#
endif
template
<
class
V
class
D
=
DFromV
<
V
>
>
HWY_API
MFromD
<
D
>
IsInf
(
const
V
v
)
{
const
D
d
;
const
RebindToSigned
<
decltype
(
d
)
>
di
;
using
T
=
TFromD
<
D
>
;
const
VFromD
<
decltype
(
di
)
>
vi
=
BitCast
(
di
v
)
;
return
RebindMask
(
d
detail
:
:
EqS
(
Add
(
vi
vi
)
hwy
:
:
MaxExponentTimes2
<
T
>
(
)
)
)
;
}
template
<
class
V
class
D
=
DFromV
<
V
>
>
HWY_API
MFromD
<
D
>
IsFinite
(
const
V
v
)
{
const
D
d
;
const
RebindToUnsigned
<
decltype
(
d
)
>
du
;
const
RebindToSigned
<
decltype
(
d
)
>
di
;
using
T
=
TFromD
<
D
>
;
const
VFromD
<
decltype
(
du
)
>
vu
=
BitCast
(
du
v
)
;
const
VFromD
<
decltype
(
di
)
>
exp
=
BitCast
(
di
ShiftRight
<
hwy
:
:
MantissaBits
<
T
>
(
)
+
1
>
(
Add
(
vu
vu
)
)
)
;
return
RebindMask
(
d
detail
:
:
LtS
(
exp
hwy
:
:
MaxExponentField
<
T
>
(
)
)
)
;
}
template
<
class
D
typename
T2
HWY_IF_UNSIGNED_D
(
D
)
>
HWY_API
VFromD
<
D
>
Iota
(
const
D
d
T2
first
)
{
return
detail
:
:
AddS
(
detail
:
:
Iota0
(
d
)
static_cast
<
TFromD
<
D
>
>
(
first
)
)
;
}
template
<
class
D
typename
T2
HWY_IF_SIGNED_D
(
D
)
>
HWY_API
VFromD
<
D
>
Iota
(
const
D
d
T2
first
)
{
const
RebindToUnsigned
<
D
>
du
;
return
detail
:
:
AddS
(
BitCast
(
d
detail
:
:
Iota0
(
du
)
)
static_cast
<
TFromD
<
D
>
>
(
first
)
)
;
}
template
<
class
D
typename
T2
HWY_IF_FLOAT_D
(
D
)
>
HWY_API
VFromD
<
D
>
Iota
(
const
D
d
T2
first
)
{
const
RebindToUnsigned
<
D
>
du
;
const
RebindToSigned
<
D
>
di
;
return
detail
:
:
AddS
(
ConvertTo
(
d
BitCast
(
di
detail
:
:
Iota0
(
du
)
)
)
ConvertScalarTo
<
TFromD
<
D
>
>
(
first
)
)
;
}
template
<
class
V
HWY_IF_T_SIZE_ONE_OF_V
(
V
(
1
<
<
1
)
|
(
1
<
<
2
)
|
(
1
<
<
4
)
)
class
D
=
DFromV
<
V
>
class
DW
=
RepartitionToWide
<
D
>
>
HWY_API
VFromD
<
DW
>
MulEven
(
const
V
a
const
V
b
)
{
const
auto
lo
=
Mul
(
a
b
)
;
const
auto
hi
=
detail
:
:
MulHigh
(
a
b
)
;
return
BitCast
(
DW
(
)
OddEven
(
detail
:
:
Slide1Up
(
hi
)
lo
)
)
;
}
template
<
class
V
HWY_IF_T_SIZE_ONE_OF_V
(
V
(
1
<
<
1
)
|
(
1
<
<
2
)
|
(
1
<
<
4
)
)
class
D
=
DFromV
<
V
>
class
DW
=
RepartitionToWide
<
D
>
>
HWY_API
VFromD
<
DW
>
MulOdd
(
const
V
a
const
V
b
)
{
const
auto
lo
=
Mul
(
a
b
)
;
const
auto
hi
=
detail
:
:
MulHigh
(
a
b
)
;
return
BitCast
(
DW
(
)
OddEven
(
hi
detail
:
:
Slide1Down
(
lo
)
)
)
;
}
template
<
class
V
HWY_IF_T_SIZE_V
(
V
8
)
>
HWY_INLINE
V
MulEven
(
const
V
a
const
V
b
)
{
const
auto
lo
=
Mul
(
a
b
)
;
const
auto
hi
=
detail
:
:
MulHigh
(
a
b
)
;
return
OddEven
(
detail
:
:
Slide1Up
(
hi
)
lo
)
;
}
template
<
class
V
HWY_IF_T_SIZE_V
(
V
8
)
>
HWY_INLINE
V
MulOdd
(
const
V
a
const
V
b
)
{
const
auto
lo
=
Mul
(
a
b
)
;
const
auto
hi
=
detail
:
:
MulHigh
(
a
b
)
;
return
OddEven
(
hi
detail
:
:
Slide1Down
(
lo
)
)
;
}
template
<
size_t
N
int
kPow2
>
HWY_API
VFromD
<
Simd
<
hwy
:
:
bfloat16_t
N
kPow2
>
>
ReorderDemote2To
(
Simd
<
hwy
:
:
bfloat16_t
N
kPow2
>
dbf16
VFromD
<
RepartitionToWide
<
decltype
(
dbf16
)
>
>
a
VFromD
<
RepartitionToWide
<
decltype
(
dbf16
)
>
>
b
)
{
const
RebindToUnsigned
<
decltype
(
dbf16
)
>
du16
;
const
RebindToUnsigned
<
DFromV
<
decltype
(
a
)
>
>
du32
;
const
VFromD
<
decltype
(
du32
)
>
b_in_even
=
ShiftRight
<
16
>
(
BitCast
(
du32
b
)
)
;
return
BitCast
(
dbf16
OddEven
(
BitCast
(
du16
a
)
BitCast
(
du16
b_in_even
)
)
)
;
}
template
<
class
DN
HWY_IF_NOT_FLOAT_NOR_SPECIAL
(
TFromD
<
DN
>
)
HWY_IF_POW2_LE_D
(
DN
2
)
class
V
HWY_IF_SIGNED_V
(
V
)
HWY_IF_T_SIZE_V
(
V
sizeof
(
TFromD
<
DN
>
)
*
2
)
class
V2
=
VFromD
<
Repartition
<
TFromV
<
V
>
DN
>
>
hwy
:
:
EnableIf
<
DFromV
<
V
>
(
)
.
Pow2
(
)
=
=
DFromV
<
V2
>
(
)
.
Pow2
(
)
>
*
=
nullptr
>
HWY_API
VFromD
<
DN
>
ReorderDemote2To
(
DN
dn
V
a
V
b
)
{
const
Rebind
<
TFromV
<
V
>
DN
>
dt
;
const
VFromD
<
decltype
(
dt
)
>
ab
=
Combine
(
dt
b
a
)
;
return
DemoteTo
(
dn
ab
)
;
}
template
<
class
DN
HWY_IF_UNSIGNED_D
(
DN
)
HWY_IF_POW2_LE_D
(
DN
2
)
class
V
HWY_IF_UNSIGNED_V
(
V
)
HWY_IF_T_SIZE_V
(
V
sizeof
(
TFromD
<
DN
>
)
*
2
)
class
V2
=
VFromD
<
Repartition
<
TFromV
<
V
>
DN
>
>
hwy
:
:
EnableIf
<
DFromV
<
V
>
(
)
.
Pow2
(
)
=
=
DFromV
<
V2
>
(
)
.
Pow2
(
)
>
*
=
nullptr
>
HWY_API
VFromD
<
DN
>
ReorderDemote2To
(
DN
dn
V
a
V
b
)
{
const
Rebind
<
TFromV
<
V
>
DN
>
dt
;
const
VFromD
<
decltype
(
dt
)
>
ab
=
Combine
(
dt
b
a
)
;
return
DemoteTo
(
dn
ab
)
;
}
template
<
class
DN
HWY_IF_NOT_FLOAT_NOR_SPECIAL
(
TFromD
<
DN
>
)
HWY_IF_POW2_GT_D
(
DN
2
)
class
V
HWY_IF_SIGNED_V
(
V
)
HWY_IF_T_SIZE_V
(
V
sizeof
(
TFromD
<
DN
>
)
*
2
)
class
V2
=
VFromD
<
Repartition
<
TFromV
<
V
>
DN
>
>
hwy
:
:
EnableIf
<
DFromV
<
V
>
(
)
.
Pow2
(
)
=
=
DFromV
<
V2
>
(
)
.
Pow2
(
)
>
*
=
nullptr
>
HWY_API
VFromD
<
DN
>
ReorderDemote2To
(
DN
dn
V
a
V
b
)
{
const
Half
<
decltype
(
dn
)
>
dnh
;
const
VFromD
<
decltype
(
dnh
)
>
demoted_a
=
DemoteTo
(
dnh
a
)
;
const
VFromD
<
decltype
(
dnh
)
>
demoted_b
=
DemoteTo
(
dnh
b
)
;
return
Combine
(
dn
demoted_b
demoted_a
)
;
}
template
<
class
DN
HWY_IF_UNSIGNED_D
(
DN
)
HWY_IF_POW2_GT_D
(
DN
2
)
class
V
HWY_IF_UNSIGNED_V
(
V
)
HWY_IF_T_SIZE_V
(
V
sizeof
(
TFromD
<
DN
>
)
*
2
)
class
V2
=
VFromD
<
Repartition
<
TFromV
<
V
>
DN
>
>
hwy
:
:
EnableIf
<
DFromV
<
V
>
(
)
.
Pow2
(
)
=
=
DFromV
<
V2
>
(
)
.
Pow2
(
)
>
*
=
nullptr
>
HWY_API
VFromD
<
DN
>
ReorderDemote2To
(
DN
dn
V
a
V
b
)
{
const
Half
<
decltype
(
dn
)
>
dnh
;
const
VFromD
<
decltype
(
dnh
)
>
demoted_a
=
DemoteTo
(
dnh
a
)
;
const
VFromD
<
decltype
(
dnh
)
>
demoted_b
=
DemoteTo
(
dnh
b
)
;
return
Combine
(
dn
demoted_b
demoted_a
)
;
}
template
<
class
DN
HWY_IF_SPECIAL_FLOAT_D
(
DN
)
HWY_IF_POW2_LE_D
(
DN
2
)
class
V
HWY_IF_F32_D
(
DFromV
<
V
>
)
class
V2
=
VFromD
<
Repartition
<
TFromV
<
V
>
DN
>
>
hwy
:
:
EnableIf
<
DFromV
<
V
>
(
)
.
Pow2
(
)
=
=
DFromV
<
V2
>
(
)
.
Pow2
(
)
>
*
=
nullptr
>
HWY_API
VFromD
<
DN
>
OrderedDemote2To
(
DN
dn
V
a
V
b
)
{
const
Rebind
<
TFromV
<
V
>
DN
>
dt
;
const
VFromD
<
decltype
(
dt
)
>
ab
=
Combine
(
dt
b
a
)
;
return
DemoteTo
(
dn
ab
)
;
}
template
<
class
DN
HWY_IF_SPECIAL_FLOAT_D
(
DN
)
HWY_IF_POW2_GT_D
(
DN
2
)
class
V
HWY_IF_F32_D
(
DFromV
<
V
>
)
class
V2
=
VFromD
<
Repartition
<
TFromV
<
V
>
DN
>
>
hwy
:
:
EnableIf
<
DFromV
<
V
>
(
)
.
Pow2
(
)
=
=
DFromV
<
V2
>
(
)
.
Pow2
(
)
>
*
=
nullptr
>
HWY_API
VFromD
<
DN
>
OrderedDemote2To
(
DN
dn
V
a
V
b
)
{
const
Half
<
decltype
(
dn
)
>
dnh
;
const
RebindToUnsigned
<
decltype
(
dn
)
>
dn_u
;
const
RebindToUnsigned
<
decltype
(
dnh
)
>
dnh_u
;
const
auto
demoted_a
=
BitCast
(
dnh_u
DemoteTo
(
dnh
a
)
)
;
const
auto
demoted_b
=
BitCast
(
dnh_u
DemoteTo
(
dnh
b
)
)
;
return
BitCast
(
dn
Combine
(
dn_u
demoted_b
demoted_a
)
)
;
}
template
<
class
DN
HWY_IF_NOT_FLOAT_NOR_SPECIAL
(
TFromD
<
DN
>
)
class
V
HWY_IF_NOT_FLOAT_NOR_SPECIAL_V
(
V
)
HWY_IF_T_SIZE_V
(
V
sizeof
(
TFromD
<
DN
>
)
*
2
)
class
V2
=
VFromD
<
Repartition
<
TFromV
<
V
>
DN
>
>
hwy
:
:
EnableIf
<
DFromV
<
V
>
(
)
.
Pow2
(
)
=
=
DFromV
<
V2
>
(
)
.
Pow2
(
)
>
*
=
nullptr
>
HWY_API
VFromD
<
DN
>
OrderedDemote2To
(
DN
dn
V
a
V
b
)
{
return
ReorderDemote2To
(
dn
a
b
)
;
}
template
<
class
D32
HWY_IF_F32_D
(
D32
)
class
V16
=
VFromD
<
Repartition
<
hwy
:
:
bfloat16_t
D32
>
>
>
HWY_API
VFromD
<
D32
>
WidenMulPairwiseAdd
(
D32
df32
V16
a
V16
b
)
{
const
RebindToUnsigned
<
decltype
(
df32
)
>
du32
;
using
VU32
=
VFromD
<
decltype
(
du32
)
>
;
const
VU32
odd
=
Set
(
du32
0xFFFF0000u
)
;
const
VU32
ae
=
ShiftLeft
<
16
>
(
BitCast
(
du32
a
)
)
;
const
VU32
ao
=
And
(
BitCast
(
du32
a
)
odd
)
;
const
VU32
be
=
ShiftLeft
<
16
>
(
BitCast
(
du32
b
)
)
;
const
VU32
bo
=
And
(
BitCast
(
du32
b
)
odd
)
;
return
MulAdd
(
BitCast
(
df32
ae
)
BitCast
(
df32
be
)
Mul
(
BitCast
(
df32
ao
)
BitCast
(
df32
bo
)
)
)
;
}
template
<
class
D
HWY_IF_I32_D
(
D
)
class
VI16
>
HWY_API
VFromD
<
D
>
WidenMulPairwiseAdd
(
D
d32
VI16
a
VI16
b
)
{
using
VI32
=
VFromD
<
decltype
(
d32
)
>
;
const
VI32
ae
=
ShiftRight
<
16
>
(
ShiftLeft
<
16
>
(
BitCast
(
d32
a
)
)
)
;
const
VI32
be
=
ShiftRight
<
16
>
(
ShiftLeft
<
16
>
(
BitCast
(
d32
b
)
)
)
;
const
VI32
ao
=
ShiftRight
<
16
>
(
BitCast
(
d32
a
)
)
;
const
VI32
bo
=
ShiftRight
<
16
>
(
BitCast
(
d32
b
)
)
;
return
Add
(
Mul
(
ae
be
)
Mul
(
ao
bo
)
)
;
}
template
<
class
D
HWY_IF_U32_D
(
D
)
class
VI16
>
HWY_API
VFromD
<
D
>
WidenMulPairwiseAdd
(
D
du32
VI16
a
VI16
b
)
{
using
VU32
=
VFromD
<
decltype
(
du32
)
>
;
const
VU32
ae
=
detail
:
:
AndS
(
BitCast
(
du32
a
)
uint32_t
{
0x0000FFFFu
}
)
;
const
VU32
be
=
detail
:
:
AndS
(
BitCast
(
du32
b
)
uint32_t
{
0x0000FFFFu
}
)
;
const
VU32
ao
=
ShiftRight
<
16
>
(
BitCast
(
du32
a
)
)
;
const
VU32
bo
=
ShiftRight
<
16
>
(
BitCast
(
du32
b
)
)
;
return
Add
(
Mul
(
ae
be
)
Mul
(
ao
bo
)
)
;
}
namespace
detail
{
template
<
size_t
N
int
kPow2
class
DF32
=
Simd
<
float
N
kPow2
>
class
VF32
=
VFromD
<
DF32
>
class
DBF16
=
Repartition
<
hwy
:
:
bfloat16_t
Simd
<
float
N
kPow2
>
>
>
HWY_API
VF32
ReorderWidenMulAccumulateBF16
(
Simd
<
float
N
kPow2
>
df32
VFromD
<
DBF16
>
a
VFromD
<
DBF16
>
b
const
VF32
sum0
VF32
&
sum1
)
{
const
RebindToUnsigned
<
DF32
>
du32
;
using
VU32
=
VFromD
<
decltype
(
du32
)
>
;
const
VU32
odd
=
Set
(
du32
0xFFFF0000u
)
;
const
VU32
ae
=
ShiftLeft
<
16
>
(
BitCast
(
du32
a
)
)
;
const
VU32
ao
=
And
(
BitCast
(
du32
a
)
odd
)
;
const
VU32
be
=
ShiftLeft
<
16
>
(
BitCast
(
du32
b
)
)
;
const
VU32
bo
=
And
(
BitCast
(
du32
b
)
odd
)
;
sum1
=
MulAdd
(
BitCast
(
df32
ao
)
BitCast
(
df32
bo
)
sum1
)
;
return
MulAdd
(
BitCast
(
df32
ae
)
BitCast
(
df32
be
)
sum0
)
;
}
#
define
HWY_RVV_WIDEN_MACC
(
BASE
CHAR
SEW
SEWD
SEWH
LMUL
LMULD
LMULH
\
SHIFT
MLEN
NAME
OP
)
\
template
<
size_t
N
>
\
HWY_API
HWY_RVV_V
(
BASE
SEWD
LMULD
)
NAME
(
\
HWY_RVV_D
(
BASE
SEWD
N
SHIFT
+
1
)
d
HWY_RVV_V
(
BASE
SEWD
LMULD
)
sum
\
HWY_RVV_V
(
BASE
SEW
LMUL
)
a
HWY_RVV_V
(
BASE
SEW
LMUL
)
b
)
{
\
return
__riscv_v
#
#
OP
#
#
CHAR
#
#
SEWD
#
#
LMULD
(
sum
a
b
Lanes
(
d
)
)
;
\
}
HWY_RVV_FOREACH_I16
(
HWY_RVV_WIDEN_MACC
WidenMulAcc
wmacc_vv_
_EXT_VIRT
)
HWY_RVV_FOREACH_U16
(
HWY_RVV_WIDEN_MACC
WidenMulAcc
wmaccu_vv_
_EXT_VIRT
)
#
undef
HWY_RVV_WIDEN_MACC
template
<
class
D32
HWY_IF_POW2_LE_D
(
D32
2
)
class
V32
=
VFromD
<
D32
>
class
D16
=
RepartitionToNarrow
<
D32
>
>
HWY_API
VFromD
<
D32
>
ReorderWidenMulAccumulateI16
(
D32
d32
VFromD
<
D16
>
a
VFromD
<
D16
>
b
const
V32
sum0
V32
&
sum1
)
{
const
Twice
<
decltype
(
d32
)
>
d32t
;
using
V32T
=
VFromD
<
decltype
(
d32t
)
>
;
V32T
sum
=
Combine
(
d32t
sum1
sum0
)
;
sum
=
detail
:
:
WidenMulAcc
(
d32t
sum
a
b
)
;
sum1
=
UpperHalf
(
d32
sum
)
;
return
LowerHalf
(
d32
sum
)
;
}
template
<
class
D32
HWY_IF_POW2_GT_D
(
D32
2
)
class
V32
=
VFromD
<
D32
>
class
D16
=
RepartitionToNarrow
<
D32
>
>
HWY_API
VFromD
<
D32
>
ReorderWidenMulAccumulateI16
(
D32
d32
VFromD
<
D16
>
a
VFromD
<
D16
>
b
const
V32
sum0
V32
&
sum1
)
{
const
Half
<
D16
>
d16h
;
using
V16H
=
VFromD
<
decltype
(
d16h
)
>
;
const
V16H
a0
=
LowerHalf
(
d16h
a
)
;
const
V16H
a1
=
UpperHalf
(
d16h
a
)
;
const
V16H
b0
=
LowerHalf
(
d16h
b
)
;
const
V16H
b1
=
UpperHalf
(
d16h
b
)
;
sum1
=
detail
:
:
WidenMulAcc
(
d32
sum1
a1
b1
)
;
return
detail
:
:
WidenMulAcc
(
d32
sum0
a0
b0
)
;
}
template
<
class
D32
HWY_IF_POW2_LE_D
(
D32
2
)
class
V32
=
VFromD
<
D32
>
class
D16
=
RepartitionToNarrow
<
D32
>
>
HWY_API
VFromD
<
D32
>
ReorderWidenMulAccumulateU16
(
D32
d32
VFromD
<
D16
>
a
VFromD
<
D16
>
b
const
V32
sum0
V32
&
sum1
)
{
const
Twice
<
decltype
(
d32
)
>
d32t
;
using
V32T
=
VFromD
<
decltype
(
d32t
)
>
;
V32T
sum
=
Combine
(
d32t
sum1
sum0
)
;
sum
=
detail
:
:
WidenMulAcc
(
d32t
sum
a
b
)
;
sum1
=
UpperHalf
(
d32
sum
)
;
return
LowerHalf
(
d32
sum
)
;
}
template
<
class
D32
HWY_IF_POW2_GT_D
(
D32
2
)
class
V32
=
VFromD
<
D32
>
class
D16
=
RepartitionToNarrow
<
D32
>
>
HWY_API
VFromD
<
D32
>
ReorderWidenMulAccumulateU16
(
D32
d32
VFromD
<
D16
>
a
VFromD
<
D16
>
b
const
V32
sum0
V32
&
sum1
)
{
const
Half
<
D16
>
d16h
;
using
V16H
=
VFromD
<
decltype
(
d16h
)
>
;
const
V16H
a0
=
LowerHalf
(
d16h
a
)
;
const
V16H
a1
=
UpperHalf
(
d16h
a
)
;
const
V16H
b0
=
LowerHalf
(
d16h
b
)
;
const
V16H
b1
=
UpperHalf
(
d16h
b
)
;
sum1
=
detail
:
:
WidenMulAcc
(
d32
sum1
a1
b1
)
;
return
detail
:
:
WidenMulAcc
(
d32
sum0
a0
b0
)
;
}
}
template
<
size_t
N
int
kPow2
class
VN
class
VW
>
HWY_API
VW
ReorderWidenMulAccumulate
(
Simd
<
float
N
kPow2
>
d32
VN
a
VN
b
const
VW
sum0
VW
&
sum1
)
{
return
detail
:
:
ReorderWidenMulAccumulateBF16
(
d32
a
b
sum0
sum1
)
;
}
template
<
size_t
N
int
kPow2
class
VN
class
VW
>
HWY_API
VW
ReorderWidenMulAccumulate
(
Simd
<
int32_t
N
kPow2
>
d32
VN
a
VN
b
const
VW
sum0
VW
&
sum1
)
{
return
detail
:
:
ReorderWidenMulAccumulateI16
(
d32
a
b
sum0
sum1
)
;
}
template
<
size_t
N
int
kPow2
class
VN
class
VW
>
HWY_API
VW
ReorderWidenMulAccumulate
(
Simd
<
uint32_t
N
kPow2
>
d32
VN
a
VN
b
const
VW
sum0
VW
&
sum1
)
{
return
detail
:
:
ReorderWidenMulAccumulateU16
(
d32
a
b
sum0
sum1
)
;
}
template
<
class
VW
HWY_IF_SIGNED_V
(
VW
)
>
HWY_API
VW
RearrangeToOddPlusEven
(
const
VW
sum0
const
VW
sum1
)
{
const
DFromV
<
VW
>
di32
;
const
RebindToUnsigned
<
decltype
(
di32
)
>
du32
;
const
Twice
<
decltype
(
di32
)
>
di32x2
;
const
RepartitionToWide
<
decltype
(
di32x2
)
>
di64x2
;
const
RebindToUnsigned
<
decltype
(
di64x2
)
>
du64x2
;
const
auto
combined
=
BitCast
(
di64x2
Combine
(
di32x2
sum1
sum0
)
)
;
const
auto
even
=
ShiftRight
<
32
>
(
ShiftLeft
<
32
>
(
combined
)
)
;
const
auto
odd
=
ShiftRight
<
32
>
(
combined
)
;
return
BitCast
(
di32
TruncateTo
(
du32
BitCast
(
du64x2
Add
(
even
odd
)
)
)
)
;
}
HWY_API
vint32m8_t
RearrangeToOddPlusEven
(
vint32m8_t
sum0
vint32m8_t
sum1
)
{
const
DFromV
<
vint32m8_t
>
d
;
const
Half
<
decltype
(
d
)
>
dh
;
const
vint32m4_t
lo
=
RearrangeToOddPlusEven
(
LowerHalf
(
sum0
)
UpperHalf
(
dh
sum0
)
)
;
const
vint32m4_t
hi
=
RearrangeToOddPlusEven
(
LowerHalf
(
sum1
)
UpperHalf
(
dh
sum1
)
)
;
return
Combine
(
d
hi
lo
)
;
}
template
<
class
VW
HWY_IF_UNSIGNED_V
(
VW
)
>
HWY_API
VW
RearrangeToOddPlusEven
(
const
VW
sum0
const
VW
sum1
)
{
const
DFromV
<
VW
>
du32
;
const
Twice
<
decltype
(
du32
)
>
du32x2
;
const
RepartitionToWide
<
decltype
(
du32x2
)
>
du64x2
;
const
auto
combined
=
BitCast
(
du64x2
Combine
(
du32x2
sum1
sum0
)
)
;
const
auto
even
=
detail
:
:
AndS
(
combined
uint64_t
{
0xFFFFFFFFu
}
)
;
const
auto
odd
=
ShiftRight
<
32
>
(
combined
)
;
return
TruncateTo
(
du32
Add
(
even
odd
)
)
;
}
HWY_API
vuint32m8_t
RearrangeToOddPlusEven
(
vuint32m8_t
sum0
vuint32m8_t
sum1
)
{
const
DFromV
<
vuint32m8_t
>
d
;
const
Half
<
decltype
(
d
)
>
dh
;
const
vuint32m4_t
lo
=
RearrangeToOddPlusEven
(
LowerHalf
(
sum0
)
UpperHalf
(
dh
sum0
)
)
;
const
vuint32m4_t
hi
=
RearrangeToOddPlusEven
(
LowerHalf
(
sum1
)
UpperHalf
(
dh
sum1
)
)
;
return
Combine
(
d
hi
lo
)
;
}
template
<
class
VW
HWY_IF_FLOAT_V
(
VW
)
>
HWY_API
VW
RearrangeToOddPlusEven
(
const
VW
sum0
const
VW
sum1
)
{
return
Add
(
sum0
sum1
)
;
}
template
<
class
D
>
HWY_INLINE
MFromD
<
D
>
Lt128
(
D
d
const
VFromD
<
D
>
a
const
VFromD
<
D
>
b
)
{
static_assert
(
IsSame
<
TFromD
<
D
>
uint64_t
>
(
)
"
D
must
be
u64
"
)
;
const
VFromD
<
D
>
eqHL
=
VecFromMask
(
d
Eq
(
a
b
)
)
;
const
VFromD
<
D
>
ltHL
=
VecFromMask
(
d
Lt
(
a
b
)
)
;
const
VFromD
<
D
>
ltLx
=
detail
:
:
Slide1Up
(
ltHL
)
;
const
VFromD
<
D
>
vecHx
=
OrAnd
(
ltHL
eqHL
ltLx
)
;
return
MaskFromVec
(
OddEven
(
vecHx
detail
:
:
Slide1Down
(
vecHx
)
)
)
;
}
template
<
class
D
>
HWY_INLINE
MFromD
<
D
>
Lt128Upper
(
D
d
const
VFromD
<
D
>
a
const
VFromD
<
D
>
b
)
{
static_assert
(
IsSame
<
TFromD
<
D
>
uint64_t
>
(
)
"
D
must
be
u64
"
)
;
const
VFromD
<
D
>
ltHL
=
VecFromMask
(
d
Lt
(
a
b
)
)
;
const
VFromD
<
D
>
down
=
detail
:
:
Slide1Down
(
ltHL
)
;
asm
volatile
(
"
"
:
:
"
r
m
"
(
GetLane
(
down
)
)
:
"
memory
"
)
;
return
MaskFromVec
(
OddEven
(
ltHL
down
)
)
;
}
template
<
class
D
>
HWY_INLINE
MFromD
<
D
>
Eq128
(
D
d
const
VFromD
<
D
>
a
const
VFromD
<
D
>
b
)
{
static_assert
(
IsSame
<
TFromD
<
D
>
uint64_t
>
(
)
"
D
must
be
u64
"
)
;
const
VFromD
<
D
>
eqHL
=
VecFromMask
(
d
Eq
(
a
b
)
)
;
const
VFromD
<
D
>
eqLH
=
Reverse2
(
d
eqHL
)
;
const
VFromD
<
D
>
eq
=
And
(
eqHL
eqLH
)
;
asm
volatile
(
"
"
:
:
"
r
m
"
(
GetLane
(
eq
)
)
:
"
memory
"
)
;
return
MaskFromVec
(
eq
)
;
}
template
<
class
D
>
HWY_INLINE
MFromD
<
D
>
Eq128Upper
(
D
d
const
VFromD
<
D
>
a
const
VFromD
<
D
>
b
)
{
static_assert
(
IsSame
<
TFromD
<
D
>
uint64_t
>
(
)
"
D
must
be
u64
"
)
;
const
VFromD
<
D
>
eqHL
=
VecFromMask
(
d
Eq
(
a
b
)
)
;
return
MaskFromVec
(
OddEven
(
eqHL
detail
:
:
Slide1Down
(
eqHL
)
)
)
;
}
template
<
class
D
>
HWY_INLINE
MFromD
<
D
>
Ne128
(
D
d
const
VFromD
<
D
>
a
const
VFromD
<
D
>
b
)
{
static_assert
(
IsSame
<
TFromD
<
D
>
uint64_t
>
(
)
"
D
must
be
u64
"
)
;
const
VFromD
<
D
>
neHL
=
VecFromMask
(
d
Ne
(
a
b
)
)
;
const
VFromD
<
D
>
neLH
=
Reverse2
(
d
neHL
)
;
asm
volatile
(
"
"
:
:
"
r
m
"
(
GetLane
(
neLH
)
)
:
"
memory
"
)
;
return
MaskFromVec
(
Or
(
neHL
neLH
)
)
;
}
template
<
class
D
>
HWY_INLINE
MFromD
<
D
>
Ne128Upper
(
D
d
const
VFromD
<
D
>
a
const
VFromD
<
D
>
b
)
{
static_assert
(
IsSame
<
TFromD
<
D
>
uint64_t
>
(
)
"
D
must
be
u64
"
)
;
const
VFromD
<
D
>
neHL
=
VecFromMask
(
d
Ne
(
a
b
)
)
;
const
VFromD
<
D
>
down
=
detail
:
:
Slide1Down
(
neHL
)
;
asm
volatile
(
"
"
:
:
"
r
m
"
(
GetLane
(
down
)
)
:
"
memory
"
)
;
return
MaskFromVec
(
OddEven
(
neHL
down
)
)
;
}
template
<
class
D
>
HWY_INLINE
VFromD
<
D
>
Min128
(
D
const
VFromD
<
D
>
a
const
VFromD
<
D
>
b
)
{
const
VFromD
<
D
>
aXH
=
detail
:
:
Slide1Down
(
a
)
;
const
VFromD
<
D
>
bXH
=
detail
:
:
Slide1Down
(
b
)
;
const
VFromD
<
D
>
minHL
=
Min
(
a
b
)
;
const
MFromD
<
D
>
ltXH
=
Lt
(
aXH
bXH
)
;
const
MFromD
<
D
>
eqXH
=
Eq
(
aXH
bXH
)
;
const
VFromD
<
D
>
lo
=
IfThenElse
(
ltXH
a
b
)
;
return
OddEven
(
minHL
IfThenElse
(
eqXH
minHL
lo
)
)
;
}
template
<
class
D
>
HWY_INLINE
VFromD
<
D
>
Max128
(
D
const
VFromD
<
D
>
a
const
VFromD
<
D
>
b
)
{
const
VFromD
<
D
>
aXH
=
detail
:
:
Slide1Down
(
a
)
;
const
VFromD
<
D
>
bXH
=
detail
:
:
Slide1Down
(
b
)
;
const
VFromD
<
D
>
maxHL
=
Max
(
a
b
)
;
const
MFromD
<
D
>
ltXH
=
Lt
(
aXH
bXH
)
;
const
MFromD
<
D
>
eqXH
=
Eq
(
aXH
bXH
)
;
const
VFromD
<
D
>
lo
=
IfThenElse
(
ltXH
b
a
)
;
return
OddEven
(
maxHL
IfThenElse
(
eqXH
maxHL
lo
)
)
;
}
template
<
class
D
>
HWY_INLINE
VFromD
<
D
>
Min128Upper
(
D
d
VFromD
<
D
>
a
VFromD
<
D
>
b
)
{
return
IfThenElse
(
Lt128Upper
(
d
a
b
)
a
b
)
;
}
template
<
class
D
>
HWY_INLINE
VFromD
<
D
>
Max128Upper
(
D
d
VFromD
<
D
>
a
VFromD
<
D
>
b
)
{
return
IfThenElse
(
Lt128Upper
(
d
b
a
)
a
b
)
;
}
#
undef
HWY_RVV_AVL
#
undef
HWY_RVV_D
#
undef
HWY_RVV_FOREACH
#
undef
HWY_RVV_FOREACH_08_ALL
#
undef
HWY_RVV_FOREACH_08_ALL_VIRT
#
undef
HWY_RVV_FOREACH_08_DEMOTE
#
undef
HWY_RVV_FOREACH_08_DEMOTE_VIRT
#
undef
HWY_RVV_FOREACH_08_EXT
#
undef
HWY_RVV_FOREACH_08_EXT_VIRT
#
undef
HWY_RVV_FOREACH_08_TRUNC
#
undef
HWY_RVV_FOREACH_08_VIRT
#
undef
HWY_RVV_FOREACH_16_ALL
#
undef
HWY_RVV_FOREACH_16_ALL_VIRT
#
undef
HWY_RVV_FOREACH_16_DEMOTE
#
undef
HWY_RVV_FOREACH_16_DEMOTE_VIRT
#
undef
HWY_RVV_FOREACH_16_EXT
#
undef
HWY_RVV_FOREACH_16_EXT_VIRT
#
undef
HWY_RVV_FOREACH_16_TRUNC
#
undef
HWY_RVV_FOREACH_16_VIRT
#
undef
HWY_RVV_FOREACH_32_ALL
#
undef
HWY_RVV_FOREACH_32_ALL_VIRT
#
undef
HWY_RVV_FOREACH_32_DEMOTE
#
undef
HWY_RVV_FOREACH_32_DEMOTE_VIRT
#
undef
HWY_RVV_FOREACH_32_EXT
#
undef
HWY_RVV_FOREACH_32_EXT_VIRT
#
undef
HWY_RVV_FOREACH_32_TRUNC
#
undef
HWY_RVV_FOREACH_32_VIRT
#
undef
HWY_RVV_FOREACH_64_ALL
#
undef
HWY_RVV_FOREACH_64_ALL_VIRT
#
undef
HWY_RVV_FOREACH_64_DEMOTE
#
undef
HWY_RVV_FOREACH_64_DEMOTE_VIRT
#
undef
HWY_RVV_FOREACH_64_EXT
#
undef
HWY_RVV_FOREACH_64_EXT_VIRT
#
undef
HWY_RVV_FOREACH_64_TRUNC
#
undef
HWY_RVV_FOREACH_64_VIRT
#
undef
HWY_RVV_FOREACH_B
#
undef
HWY_RVV_FOREACH_F
#
undef
HWY_RVV_FOREACH_F16
#
undef
HWY_RVV_FOREACH_F32
#
undef
HWY_RVV_FOREACH_F3264
#
undef
HWY_RVV_FOREACH_F64
#
undef
HWY_RVV_FOREACH_I
#
undef
HWY_RVV_FOREACH_I08
#
undef
HWY_RVV_FOREACH_I16
#
undef
HWY_RVV_FOREACH_I163264
#
undef
HWY_RVV_FOREACH_I32
#
undef
HWY_RVV_FOREACH_I64
#
undef
HWY_RVV_FOREACH_U
#
undef
HWY_RVV_FOREACH_U08
#
undef
HWY_RVV_FOREACH_U16
#
undef
HWY_RVV_FOREACH_U163264
#
undef
HWY_RVV_FOREACH_U32
#
undef
HWY_RVV_FOREACH_U64
#
undef
HWY_RVV_FOREACH_UI
#
undef
HWY_RVV_FOREACH_UI08
#
undef
HWY_RVV_FOREACH_UI16
#
undef
HWY_RVV_FOREACH_UI163264
#
undef
HWY_RVV_FOREACH_UI32
#
undef
HWY_RVV_FOREACH_UI3264
#
undef
HWY_RVV_FOREACH_UI64
#
undef
HWY_RVV_IF_EMULATED_D
#
undef
HWY_RVV_IF_CAN128_D
#
undef
HWY_RVV_IF_GE128_D
#
undef
HWY_RVV_IF_LT128_D
#
undef
HWY_RVV_INSERT_VXRM
#
undef
HWY_RVV_M
#
undef
HWY_RVV_RETM_ARGM
#
undef
HWY_RVV_RETV_ARGMVV
#
undef
HWY_RVV_RETV_ARGV
#
undef
HWY_RVV_RETV_ARGVS
#
undef
HWY_RVV_RETV_ARGVV
#
undef
HWY_RVV_T
#
undef
HWY_RVV_V
}
}
HWY_AFTER_NAMESPACE
(
)
;
