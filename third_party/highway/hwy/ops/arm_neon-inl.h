#
include
<
arm_neon
.
h
>
#
include
<
stddef
.
h
>
#
include
<
stdint
.
h
>
#
include
"
hwy
/
base
.
h
"
#
include
"
hwy
/
ops
/
shared
-
inl
.
h
"
HWY_BEFORE_NAMESPACE
(
)
;
namespace
hwy
{
namespace
HWY_NAMESPACE
{
#
define
HWY_NEON_BUILD_TPL_1
#
define
HWY_NEON_BUILD_TPL_2
#
define
HWY_NEON_BUILD_TPL_3
#
define
HWY_NEON_BUILD_RET_1
(
type
size
)
Vec128
<
type
size
>
#
define
HWY_NEON_BUILD_RET_2
(
type
size
)
Vec128
<
type
size
>
#
define
HWY_NEON_BUILD_RET_3
(
type
size
)
Vec128
<
type
size
>
#
define
HWY_NEON_BUILD_PARAM_1
(
type
size
)
const
Vec128
<
type
size
>
a
#
define
HWY_NEON_BUILD_PARAM_2
(
type
size
)
\
const
Vec128
<
type
size
>
a
const
Vec128
<
type
size
>
b
#
define
HWY_NEON_BUILD_PARAM_3
(
type
size
)
\
const
Vec128
<
type
size
>
a
const
Vec128
<
type
size
>
b
\
const
Vec128
<
type
size
>
c
#
define
HWY_NEON_BUILD_ARG_1
a
.
raw
#
define
HWY_NEON_BUILD_ARG_2
a
.
raw
b
.
raw
#
define
HWY_NEON_BUILD_ARG_3
a
.
raw
b
.
raw
c
.
raw
#
define
HWY_NEON_EVAL
(
func
.
.
.
)
func
(
__VA_ARGS__
)
#
define
HWY_NEON_DEF_FUNCTION
(
type
size
name
prefix
infix
suffix
args
)
\
HWY_CONCAT
(
HWY_NEON_BUILD_TPL_
args
)
\
HWY_INLINE
HWY_CONCAT
(
HWY_NEON_BUILD_RET_
args
)
(
type
size
)
\
name
(
HWY_CONCAT
(
HWY_NEON_BUILD_PARAM_
args
)
(
type
size
)
)
{
\
return
HWY_CONCAT
(
HWY_NEON_BUILD_RET_
args
)
(
type
size
)
(
\
HWY_NEON_EVAL
(
prefix
#
#
infix
#
#
suffix
HWY_NEON_BUILD_ARG_
#
#
args
)
)
;
\
}
#
define
HWY_NEON_DEF_FUNCTION_UINT_8
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION
(
uint8_t
16
name
prefix
#
#
q
infix
u8
args
)
\
HWY_NEON_DEF_FUNCTION
(
uint8_t
8
name
prefix
infix
u8
args
)
\
HWY_NEON_DEF_FUNCTION
(
uint8_t
4
name
prefix
infix
u8
args
)
\
HWY_NEON_DEF_FUNCTION
(
uint8_t
2
name
prefix
infix
u8
args
)
\
HWY_NEON_DEF_FUNCTION
(
uint8_t
1
name
prefix
infix
u8
args
)
#
define
HWY_NEON_DEF_FUNCTION_INT_8
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION
(
int8_t
16
name
prefix
#
#
q
infix
s8
args
)
\
HWY_NEON_DEF_FUNCTION
(
int8_t
8
name
prefix
infix
s8
args
)
\
HWY_NEON_DEF_FUNCTION
(
int8_t
4
name
prefix
infix
s8
args
)
\
HWY_NEON_DEF_FUNCTION
(
int8_t
2
name
prefix
infix
s8
args
)
\
HWY_NEON_DEF_FUNCTION
(
int8_t
1
name
prefix
infix
s8
args
)
#
define
HWY_NEON_DEF_FUNCTION_UINT_16
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION
(
uint16_t
8
name
prefix
#
#
q
infix
u16
args
)
\
HWY_NEON_DEF_FUNCTION
(
uint16_t
4
name
prefix
infix
u16
args
)
\
HWY_NEON_DEF_FUNCTION
(
uint16_t
2
name
prefix
infix
u16
args
)
\
HWY_NEON_DEF_FUNCTION
(
uint16_t
1
name
prefix
infix
u16
args
)
#
define
HWY_NEON_DEF_FUNCTION_INT_16
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION
(
int16_t
8
name
prefix
#
#
q
infix
s16
args
)
\
HWY_NEON_DEF_FUNCTION
(
int16_t
4
name
prefix
infix
s16
args
)
\
HWY_NEON_DEF_FUNCTION
(
int16_t
2
name
prefix
infix
s16
args
)
\
HWY_NEON_DEF_FUNCTION
(
int16_t
1
name
prefix
infix
s16
args
)
#
define
HWY_NEON_DEF_FUNCTION_UINT_32
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION
(
uint32_t
4
name
prefix
#
#
q
infix
u32
args
)
\
HWY_NEON_DEF_FUNCTION
(
uint32_t
2
name
prefix
infix
u32
args
)
\
HWY_NEON_DEF_FUNCTION
(
uint32_t
1
name
prefix
infix
u32
args
)
#
define
HWY_NEON_DEF_FUNCTION_INT_32
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION
(
int32_t
4
name
prefix
#
#
q
infix
s32
args
)
\
HWY_NEON_DEF_FUNCTION
(
int32_t
2
name
prefix
infix
s32
args
)
\
HWY_NEON_DEF_FUNCTION
(
int32_t
1
name
prefix
infix
s32
args
)
#
define
HWY_NEON_DEF_FUNCTION_UINT_64
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION
(
uint64_t
2
name
prefix
#
#
q
infix
u64
args
)
\
HWY_NEON_DEF_FUNCTION
(
uint64_t
1
name
prefix
infix
u64
args
)
#
define
HWY_NEON_DEF_FUNCTION_INT_64
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION
(
int64_t
2
name
prefix
#
#
q
infix
s64
args
)
\
HWY_NEON_DEF_FUNCTION
(
int64_t
1
name
prefix
infix
s64
args
)
#
if
HWY_ARCH_ARM_A64
#
define
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION
(
float
4
name
prefix
#
#
q
infix
f32
args
)
\
HWY_NEON_DEF_FUNCTION
(
float
2
name
prefix
infix
f32
args
)
\
HWY_NEON_DEF_FUNCTION
(
float
1
name
prefix
infix
f32
args
)
\
HWY_NEON_DEF_FUNCTION
(
double
2
name
prefix
#
#
q
infix
f64
args
)
\
HWY_NEON_DEF_FUNCTION
(
double
1
name
prefix
infix
f64
args
)
#
else
#
define
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION
(
float
4
name
prefix
#
#
q
infix
f32
args
)
\
HWY_NEON_DEF_FUNCTION
(
float
2
name
prefix
infix
f32
args
)
\
HWY_NEON_DEF_FUNCTION
(
float
1
name
prefix
infix
f32
args
)
#
endif
#
define
HWY_NEON_DEF_FUNCTION_UINT_8_16_32
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION_UINT_8
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION_UINT_16
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION_UINT_32
(
name
prefix
infix
args
)
#
define
HWY_NEON_DEF_FUNCTION_INT_8_16_32
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION_INT_8
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION_INT_16
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION_INT_32
(
name
prefix
infix
args
)
#
define
HWY_NEON_DEF_FUNCTION_UINTS
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION_UINT_8_16_32
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION_UINT_64
(
name
prefix
infix
args
)
#
define
HWY_NEON_DEF_FUNCTION_INTS
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION_INT_8_16_32
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION_INT_64
(
name
prefix
infix
args
)
#
define
HWY_NEON_DEF_FUNCTION_INTS_UINTS
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION_INTS
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION_UINTS
(
name
prefix
infix
args
)
#
define
HWY_NEON_DEF_FUNCTION_ALL_TYPES
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION_INTS_UINTS
(
name
prefix
infix
args
)
\
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
name
prefix
infix
args
)
#
if
HWY_ARCH_ARM_V7
#
define
vuzp1_s8
(
x
y
)
vuzp_s8
(
x
y
)
.
val
[
0
]
#
define
vuzp1_u8
(
x
y
)
vuzp_u8
(
x
y
)
.
val
[
0
]
#
define
vuzp1_s16
(
x
y
)
vuzp_s16
(
x
y
)
.
val
[
0
]
#
define
vuzp1_u16
(
x
y
)
vuzp_u16
(
x
y
)
.
val
[
0
]
#
define
vuzp1_s32
(
x
y
)
vuzp_s32
(
x
y
)
.
val
[
0
]
#
define
vuzp1_u32
(
x
y
)
vuzp_u32
(
x
y
)
.
val
[
0
]
#
define
vuzp1_f32
(
x
y
)
vuzp_f32
(
x
y
)
.
val
[
0
]
#
define
vuzp1q_s8
(
x
y
)
vuzpq_s8
(
x
y
)
.
val
[
0
]
#
define
vuzp1q_u8
(
x
y
)
vuzpq_u8
(
x
y
)
.
val
[
0
]
#
define
vuzp1q_s16
(
x
y
)
vuzpq_s16
(
x
y
)
.
val
[
0
]
#
define
vuzp1q_u16
(
x
y
)
vuzpq_u16
(
x
y
)
.
val
[
0
]
#
define
vuzp1q_s32
(
x
y
)
vuzpq_s32
(
x
y
)
.
val
[
0
]
#
define
vuzp1q_u32
(
x
y
)
vuzpq_u32
(
x
y
)
.
val
[
0
]
#
define
vuzp1q_f32
(
x
y
)
vuzpq_f32
(
x
y
)
.
val
[
0
]
#
define
vuzp2_s8
(
x
y
)
vuzp_s8
(
x
y
)
.
val
[
1
]
#
define
vuzp2_u8
(
x
y
)
vuzp_u8
(
x
y
)
.
val
[
1
]
#
define
vuzp2_s16
(
x
y
)
vuzp_s16
(
x
y
)
.
val
[
1
]
#
define
vuzp2_u16
(
x
y
)
vuzp_u16
(
x
y
)
.
val
[
1
]
#
define
vuzp2_s32
(
x
y
)
vuzp_s32
(
x
y
)
.
val
[
1
]
#
define
vuzp2_u32
(
x
y
)
vuzp_u32
(
x
y
)
.
val
[
1
]
#
define
vuzp2_f32
(
x
y
)
vuzp_f32
(
x
y
)
.
val
[
1
]
#
define
vuzp2q_s8
(
x
y
)
vuzpq_s8
(
x
y
)
.
val
[
1
]
#
define
vuzp2q_u8
(
x
y
)
vuzpq_u8
(
x
y
)
.
val
[
1
]
#
define
vuzp2q_s16
(
x
y
)
vuzpq_s16
(
x
y
)
.
val
[
1
]
#
define
vuzp2q_u16
(
x
y
)
vuzpq_u16
(
x
y
)
.
val
[
1
]
#
define
vuzp2q_s32
(
x
y
)
vuzpq_s32
(
x
y
)
.
val
[
1
]
#
define
vuzp2q_u32
(
x
y
)
vuzpq_u32
(
x
y
)
.
val
[
1
]
#
define
vuzp2q_f32
(
x
y
)
vuzpq_f32
(
x
y
)
.
val
[
1
]
#
define
vzip1_s8
(
x
y
)
vzip_s8
(
x
y
)
.
val
[
0
]
#
define
vzip1_u8
(
x
y
)
vzip_u8
(
x
y
)
.
val
[
0
]
#
define
vzip1_s16
(
x
y
)
vzip_s16
(
x
y
)
.
val
[
0
]
#
define
vzip1_u16
(
x
y
)
vzip_u16
(
x
y
)
.
val
[
0
]
#
define
vzip1_f32
(
x
y
)
vzip_f32
(
x
y
)
.
val
[
0
]
#
define
vzip1_u32
(
x
y
)
vzip_u32
(
x
y
)
.
val
[
0
]
#
define
vzip1_s32
(
x
y
)
vzip_s32
(
x
y
)
.
val
[
0
]
#
define
vzip1q_s8
(
x
y
)
vzipq_s8
(
x
y
)
.
val
[
0
]
#
define
vzip1q_u8
(
x
y
)
vzipq_u8
(
x
y
)
.
val
[
0
]
#
define
vzip1q_s16
(
x
y
)
vzipq_s16
(
x
y
)
.
val
[
0
]
#
define
vzip1q_u16
(
x
y
)
vzipq_u16
(
x
y
)
.
val
[
0
]
#
define
vzip1q_s32
(
x
y
)
vzipq_s32
(
x
y
)
.
val
[
0
]
#
define
vzip1q_u32
(
x
y
)
vzipq_u32
(
x
y
)
.
val
[
0
]
#
define
vzip1q_f32
(
x
y
)
vzipq_f32
(
x
y
)
.
val
[
0
]
#
define
vzip2_s8
(
x
y
)
vzip_s8
(
x
y
)
.
val
[
1
]
#
define
vzip2_u8
(
x
y
)
vzip_u8
(
x
y
)
.
val
[
1
]
#
define
vzip2_s16
(
x
y
)
vzip_s16
(
x
y
)
.
val
[
1
]
#
define
vzip2_u16
(
x
y
)
vzip_u16
(
x
y
)
.
val
[
1
]
#
define
vzip2_s32
(
x
y
)
vzip_s32
(
x
y
)
.
val
[
1
]
#
define
vzip2_u32
(
x
y
)
vzip_u32
(
x
y
)
.
val
[
1
]
#
define
vzip2_f32
(
x
y
)
vzip_f32
(
x
y
)
.
val
[
1
]
#
define
vzip2q_s8
(
x
y
)
vzipq_s8
(
x
y
)
.
val
[
1
]
#
define
vzip2q_u8
(
x
y
)
vzipq_u8
(
x
y
)
.
val
[
1
]
#
define
vzip2q_s16
(
x
y
)
vzipq_s16
(
x
y
)
.
val
[
1
]
#
define
vzip2q_u16
(
x
y
)
vzipq_u16
(
x
y
)
.
val
[
1
]
#
define
vzip2q_s32
(
x
y
)
vzipq_s32
(
x
y
)
.
val
[
1
]
#
define
vzip2q_u32
(
x
y
)
vzipq_u32
(
x
y
)
.
val
[
1
]
#
define
vzip2q_f32
(
x
y
)
vzipq_f32
(
x
y
)
.
val
[
1
]
#
endif
template
<
typename
T
size_t
N
>
struct
Raw128
;
template
<
>
struct
Raw128
<
uint8_t
16
>
{
using
type
=
uint8x16_t
;
}
;
template
<
>
struct
Raw128
<
uint16_t
8
>
{
using
type
=
uint16x8_t
;
}
;
template
<
>
struct
Raw128
<
uint32_t
4
>
{
using
type
=
uint32x4_t
;
}
;
template
<
>
struct
Raw128
<
uint64_t
2
>
{
using
type
=
uint64x2_t
;
}
;
template
<
>
struct
Raw128
<
int8_t
16
>
{
using
type
=
int8x16_t
;
}
;
template
<
>
struct
Raw128
<
int16_t
8
>
{
using
type
=
int16x8_t
;
}
;
template
<
>
struct
Raw128
<
int32_t
4
>
{
using
type
=
int32x4_t
;
}
;
template
<
>
struct
Raw128
<
int64_t
2
>
{
using
type
=
int64x2_t
;
}
;
template
<
>
struct
Raw128
<
float16_t
8
>
{
using
type
=
uint16x8_t
;
}
;
template
<
>
struct
Raw128
<
float
4
>
{
using
type
=
float32x4_t
;
}
;
#
if
HWY_ARCH_ARM_A64
template
<
>
struct
Raw128
<
double
2
>
{
using
type
=
float64x2_t
;
}
;
#
endif
template
<
>
struct
Raw128
<
uint8_t
8
>
{
using
type
=
uint8x8_t
;
}
;
template
<
>
struct
Raw128
<
uint16_t
4
>
{
using
type
=
uint16x4_t
;
}
;
template
<
>
struct
Raw128
<
uint32_t
2
>
{
using
type
=
uint32x2_t
;
}
;
template
<
>
struct
Raw128
<
uint64_t
1
>
{
using
type
=
uint64x1_t
;
}
;
template
<
>
struct
Raw128
<
int8_t
8
>
{
using
type
=
int8x8_t
;
}
;
template
<
>
struct
Raw128
<
int16_t
4
>
{
using
type
=
int16x4_t
;
}
;
template
<
>
struct
Raw128
<
int32_t
2
>
{
using
type
=
int32x2_t
;
}
;
template
<
>
struct
Raw128
<
int64_t
1
>
{
using
type
=
int64x1_t
;
}
;
template
<
>
struct
Raw128
<
float16_t
4
>
{
using
type
=
uint16x4_t
;
}
;
template
<
>
struct
Raw128
<
float
2
>
{
using
type
=
float32x2_t
;
}
;
#
if
HWY_ARCH_ARM_A64
template
<
>
struct
Raw128
<
double
1
>
{
using
type
=
float64x1_t
;
}
;
#
endif
template
<
>
struct
Raw128
<
uint8_t
4
>
{
using
type
=
uint8x8_t
;
}
;
template
<
>
struct
Raw128
<
uint16_t
2
>
{
using
type
=
uint16x4_t
;
}
;
template
<
>
struct
Raw128
<
uint32_t
1
>
{
using
type
=
uint32x2_t
;
}
;
template
<
>
struct
Raw128
<
int8_t
4
>
{
using
type
=
int8x8_t
;
}
;
template
<
>
struct
Raw128
<
int16_t
2
>
{
using
type
=
int16x4_t
;
}
;
template
<
>
struct
Raw128
<
int32_t
1
>
{
using
type
=
int32x2_t
;
}
;
template
<
>
struct
Raw128
<
float16_t
2
>
{
using
type
=
uint16x4_t
;
}
;
template
<
>
struct
Raw128
<
float
1
>
{
using
type
=
float32x2_t
;
}
;
template
<
>
struct
Raw128
<
uint8_t
2
>
{
using
type
=
uint8x8_t
;
}
;
template
<
>
struct
Raw128
<
uint16_t
1
>
{
using
type
=
uint16x4_t
;
}
;
template
<
>
struct
Raw128
<
int8_t
2
>
{
using
type
=
int8x8_t
;
}
;
template
<
>
struct
Raw128
<
int16_t
1
>
{
using
type
=
int16x4_t
;
}
;
template
<
>
struct
Raw128
<
float16_t
1
>
{
using
type
=
uint16x4_t
;
}
;
template
<
>
struct
Raw128
<
uint8_t
1
>
{
using
type
=
uint8x8_t
;
}
;
template
<
>
struct
Raw128
<
int8_t
1
>
{
using
type
=
int8x8_t
;
}
;
template
<
typename
T
>
using
Full128
=
Simd
<
T
16
/
sizeof
(
T
)
>
;
template
<
typename
T
size_t
N
=
16
/
sizeof
(
T
)
>
class
Vec128
{
using
Raw
=
typename
Raw128
<
T
N
>
:
:
type
;
public
:
HWY_INLINE
Vec128
(
)
{
}
Vec128
(
const
Vec128
&
)
=
default
;
Vec128
&
operator
=
(
const
Vec128
&
)
=
default
;
HWY_INLINE
explicit
Vec128
(
const
Raw
raw
)
:
raw
(
raw
)
{
}
HWY_INLINE
Vec128
&
operator
*
=
(
const
Vec128
other
)
{
return
*
this
=
(
*
this
*
other
)
;
}
HWY_INLINE
Vec128
&
operator
/
=
(
const
Vec128
other
)
{
return
*
this
=
(
*
this
/
other
)
;
}
HWY_INLINE
Vec128
&
operator
+
=
(
const
Vec128
other
)
{
return
*
this
=
(
*
this
+
other
)
;
}
HWY_INLINE
Vec128
&
operator
-
=
(
const
Vec128
other
)
{
return
*
this
=
(
*
this
-
other
)
;
}
HWY_INLINE
Vec128
&
operator
&
=
(
const
Vec128
other
)
{
return
*
this
=
(
*
this
&
other
)
;
}
HWY_INLINE
Vec128
&
operator
|
=
(
const
Vec128
other
)
{
return
*
this
=
(
*
this
|
other
)
;
}
HWY_INLINE
Vec128
&
operator
^
=
(
const
Vec128
other
)
{
return
*
this
=
(
*
this
^
other
)
;
}
Raw
raw
;
}
;
template
<
typename
T
size_t
N
=
16
/
sizeof
(
T
)
>
class
Mask128
{
using
Raw
=
typename
Raw128
<
T
N
>
:
:
type
;
public
:
HWY_INLINE
Mask128
(
)
{
}
Mask128
(
const
Mask128
&
)
=
default
;
Mask128
&
operator
=
(
const
Mask128
&
)
=
default
;
HWY_INLINE
explicit
Mask128
(
const
Raw
raw
)
:
raw
(
raw
)
{
}
Raw
raw
;
}
;
namespace
detail
{
#
define
HWY_NEON_BUILD_TPL_HWY_CAST_TO_U8
#
define
HWY_NEON_BUILD_RET_HWY_CAST_TO_U8
(
type
size
)
\
Vec128
<
uint8_t
size
*
sizeof
(
type
)
>
#
define
HWY_NEON_BUILD_PARAM_HWY_CAST_TO_U8
(
type
size
)
Vec128
<
type
size
>
v
#
define
HWY_NEON_BUILD_ARG_HWY_CAST_TO_U8
v
.
raw
template
<
size_t
N
>
HWY_INLINE
Vec128
<
uint8_t
N
>
BitCastToByte
(
Vec128
<
uint8_t
N
>
v
)
{
return
v
;
}
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
BitCastToByte
vreinterpret
_u8_
HWY_CAST_TO_U8
)
HWY_NEON_DEF_FUNCTION_INTS
(
BitCastToByte
vreinterpret
_u8_
HWY_CAST_TO_U8
)
HWY_NEON_DEF_FUNCTION_UINT_16
(
BitCastToByte
vreinterpret
_u8_
HWY_CAST_TO_U8
)
HWY_NEON_DEF_FUNCTION_UINT_32
(
BitCastToByte
vreinterpret
_u8_
HWY_CAST_TO_U8
)
HWY_NEON_DEF_FUNCTION_UINT_64
(
BitCastToByte
vreinterpret
_u8_
HWY_CAST_TO_U8
)
template
<
size_t
N
>
HWY_INLINE
Vec128
<
uint8_t
N
*
2
>
BitCastToByte
(
Vec128
<
float16_t
N
>
v
)
{
return
BitCastToByte
(
Vec128
<
uint16_t
N
>
(
v
.
raw
)
)
;
}
#
undef
HWY_NEON_BUILD_TPL_HWY_CAST_TO_U8
#
undef
HWY_NEON_BUILD_RET_HWY_CAST_TO_U8
#
undef
HWY_NEON_BUILD_PARAM_HWY_CAST_TO_U8
#
undef
HWY_NEON_BUILD_ARG_HWY_CAST_TO_U8
template
<
size_t
N
>
HWY_INLINE
Vec128
<
uint8_t
N
>
BitCastFromByte
(
Simd
<
uint8_t
N
>
Vec128
<
uint8_t
N
>
v
)
{
return
v
;
}
template
<
size_t
N
HWY_IF_LE64
(
int8_t
N
)
>
HWY_INLINE
Vec128
<
int8_t
N
>
BitCastFromByte
(
Simd
<
int8_t
N
>
Vec128
<
uint8_t
N
>
v
)
{
return
Vec128
<
int8_t
N
>
(
vreinterpret_s8_u8
(
v
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint16_t
N
)
>
HWY_INLINE
Vec128
<
uint16_t
N
>
BitCastFromByte
(
Simd
<
uint16_t
N
>
Vec128
<
uint8_t
N
*
2
>
v
)
{
return
Vec128
<
uint16_t
N
>
(
vreinterpret_u16_u8
(
v
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int16_t
N
)
>
HWY_INLINE
Vec128
<
int16_t
N
>
BitCastFromByte
(
Simd
<
int16_t
N
>
Vec128
<
uint8_t
N
*
2
>
v
)
{
return
Vec128
<
int16_t
N
>
(
vreinterpret_s16_u8
(
v
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint32_t
N
)
>
HWY_INLINE
Vec128
<
uint32_t
N
>
BitCastFromByte
(
Simd
<
uint32_t
N
>
Vec128
<
uint8_t
N
*
4
>
v
)
{
return
Vec128
<
uint32_t
N
>
(
vreinterpret_u32_u8
(
v
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
int32_t
N
>
BitCastFromByte
(
Simd
<
int32_t
N
>
Vec128
<
uint8_t
N
*
4
>
v
)
{
return
Vec128
<
int32_t
N
>
(
vreinterpret_s32_u8
(
v
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
float
N
)
>
HWY_INLINE
Vec128
<
float
N
>
BitCastFromByte
(
Simd
<
float
N
>
Vec128
<
uint8_t
N
*
4
>
v
)
{
return
Vec128
<
float
N
>
(
vreinterpret_f32_u8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
1
>
BitCastFromByte
(
Simd
<
uint64_t
1
>
Vec128
<
uint8_t
1
*
8
>
v
)
{
return
Vec128
<
uint64_t
1
>
(
vreinterpret_u64_u8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
1
>
BitCastFromByte
(
Simd
<
int64_t
1
>
Vec128
<
uint8_t
1
*
8
>
v
)
{
return
Vec128
<
int64_t
1
>
(
vreinterpret_s64_u8
(
v
.
raw
)
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
double
1
>
BitCastFromByte
(
Simd
<
double
1
>
Vec128
<
uint8_t
1
*
8
>
v
)
{
return
Vec128
<
double
1
>
(
vreinterpret_f64_u8
(
v
.
raw
)
)
;
}
#
endif
HWY_INLINE
Vec128
<
int8_t
>
BitCastFromByte
(
Full128
<
int8_t
>
Vec128
<
uint8_t
>
v
)
{
return
Vec128
<
int8_t
>
(
vreinterpretq_s8_u8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint16_t
>
BitCastFromByte
(
Full128
<
uint16_t
>
Vec128
<
uint8_t
>
v
)
{
return
Vec128
<
uint16_t
>
(
vreinterpretq_u16_u8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
>
BitCastFromByte
(
Full128
<
int16_t
>
Vec128
<
uint8_t
>
v
)
{
return
Vec128
<
int16_t
>
(
vreinterpretq_s16_u8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
>
BitCastFromByte
(
Full128
<
uint32_t
>
Vec128
<
uint8_t
>
v
)
{
return
Vec128
<
uint32_t
>
(
vreinterpretq_u32_u8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
BitCastFromByte
(
Full128
<
int32_t
>
Vec128
<
uint8_t
>
v
)
{
return
Vec128
<
int32_t
>
(
vreinterpretq_s32_u8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
float
>
BitCastFromByte
(
Full128
<
float
>
Vec128
<
uint8_t
>
v
)
{
return
Vec128
<
float
>
(
vreinterpretq_f32_u8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
>
BitCastFromByte
(
Full128
<
uint64_t
>
Vec128
<
uint8_t
>
v
)
{
return
Vec128
<
uint64_t
>
(
vreinterpretq_u64_u8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
BitCastFromByte
(
Full128
<
int64_t
>
Vec128
<
uint8_t
>
v
)
{
return
Vec128
<
int64_t
>
(
vreinterpretq_s64_u8
(
v
.
raw
)
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
double
>
BitCastFromByte
(
Full128
<
double
>
Vec128
<
uint8_t
>
v
)
{
return
Vec128
<
double
>
(
vreinterpretq_f64_u8
(
v
.
raw
)
)
;
}
#
endif
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float16_t
N
>
BitCastFromByte
(
Simd
<
float16_t
N
>
Vec128
<
uint8_t
N
*
2
>
v
)
{
return
Vec128
<
float16_t
N
>
(
BitCastFromByte
(
Simd
<
uint16_t
N
>
(
)
v
)
.
raw
)
;
}
}
template
<
typename
T
size_t
N
typename
FromT
>
HWY_INLINE
Vec128
<
T
N
>
BitCast
(
Simd
<
T
N
>
d
Vec128
<
FromT
N
*
sizeof
(
T
)
/
sizeof
(
FromT
)
>
v
)
{
return
detail
:
:
BitCastFromByte
(
d
detail
:
:
BitCastToByte
(
v
)
)
;
}
#
define
HWY_NEON_BUILD_TPL_HWY_SET1
#
define
HWY_NEON_BUILD_RET_HWY_SET1
(
type
size
)
Vec128
<
type
size
>
#
define
HWY_NEON_BUILD_PARAM_HWY_SET1
(
type
size
)
\
Simd
<
type
size
>
/
*
tag
*
/
const
type
t
#
define
HWY_NEON_BUILD_ARG_HWY_SET1
t
HWY_NEON_DEF_FUNCTION_ALL_TYPES
(
Set
vdup
_n_
HWY_SET1
)
#
undef
HWY_NEON_BUILD_TPL_HWY_SET1
#
undef
HWY_NEON_BUILD_RET_HWY_SET1
#
undef
HWY_NEON_BUILD_PARAM_HWY_SET1
#
undef
HWY_NEON_BUILD_ARG_HWY_SET1
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
Zero
(
Simd
<
T
N
>
d
)
{
return
Set
(
d
0
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
Undefined
(
Simd
<
T
N
>
)
{
HWY_DIAGNOSTICS
(
push
)
HWY_DIAGNOSTICS_OFF
(
disable
:
4701
ignored
"
-
Wuninitialized
"
)
typename
Raw128
<
T
N
>
:
:
type
a
;
return
Vec128
<
T
N
>
(
a
)
;
HWY_DIAGNOSTICS
(
pop
)
}
HWY_INLINE
uint8_t
GetLane
(
const
Vec128
<
uint8_t
16
>
v
)
{
return
vget_lane_u8
(
vget_low_u8
(
v
.
raw
)
0
)
;
}
template
<
size_t
N
>
HWY_INLINE
uint8_t
GetLane
(
const
Vec128
<
uint8_t
N
>
v
)
{
return
vget_lane_u8
(
v
.
raw
0
)
;
}
HWY_INLINE
int8_t
GetLane
(
const
Vec128
<
int8_t
16
>
v
)
{
return
vget_lane_s8
(
vget_low_s8
(
v
.
raw
)
0
)
;
}
template
<
size_t
N
>
HWY_INLINE
int8_t
GetLane
(
const
Vec128
<
int8_t
N
>
v
)
{
return
vget_lane_s8
(
v
.
raw
0
)
;
}
HWY_INLINE
uint16_t
GetLane
(
const
Vec128
<
uint16_t
8
>
v
)
{
return
vget_lane_u16
(
vget_low_u16
(
v
.
raw
)
0
)
;
}
template
<
size_t
N
>
HWY_INLINE
uint16_t
GetLane
(
const
Vec128
<
uint16_t
N
>
v
)
{
return
vget_lane_u16
(
v
.
raw
0
)
;
}
HWY_INLINE
int16_t
GetLane
(
const
Vec128
<
int16_t
8
>
v
)
{
return
vget_lane_s16
(
vget_low_s16
(
v
.
raw
)
0
)
;
}
template
<
size_t
N
>
HWY_INLINE
int16_t
GetLane
(
const
Vec128
<
int16_t
N
>
v
)
{
return
vget_lane_s16
(
v
.
raw
0
)
;
}
HWY_INLINE
uint32_t
GetLane
(
const
Vec128
<
uint32_t
4
>
v
)
{
return
vget_lane_u32
(
vget_low_u32
(
v
.
raw
)
0
)
;
}
template
<
size_t
N
>
HWY_INLINE
uint32_t
GetLane
(
const
Vec128
<
uint32_t
N
>
v
)
{
return
vget_lane_u32
(
v
.
raw
0
)
;
}
HWY_INLINE
int32_t
GetLane
(
const
Vec128
<
int32_t
4
>
v
)
{
return
vget_lane_s32
(
vget_low_s32
(
v
.
raw
)
0
)
;
}
template
<
size_t
N
>
HWY_INLINE
int32_t
GetLane
(
const
Vec128
<
int32_t
N
>
v
)
{
return
vget_lane_s32
(
v
.
raw
0
)
;
}
HWY_INLINE
uint64_t
GetLane
(
const
Vec128
<
uint64_t
2
>
v
)
{
return
vget_lane_u64
(
vget_low_u64
(
v
.
raw
)
0
)
;
}
HWY_INLINE
uint64_t
GetLane
(
const
Vec128
<
uint64_t
1
>
v
)
{
return
vget_lane_u64
(
v
.
raw
0
)
;
}
HWY_INLINE
int64_t
GetLane
(
const
Vec128
<
int64_t
2
>
v
)
{
return
vget_lane_s64
(
vget_low_s64
(
v
.
raw
)
0
)
;
}
HWY_INLINE
int64_t
GetLane
(
const
Vec128
<
int64_t
1
>
v
)
{
return
vget_lane_s64
(
v
.
raw
0
)
;
}
HWY_INLINE
float
GetLane
(
const
Vec128
<
float
4
>
v
)
{
return
vget_lane_f32
(
vget_low_f32
(
v
.
raw
)
0
)
;
}
HWY_INLINE
float
GetLane
(
const
Vec128
<
float
2
>
v
)
{
return
vget_lane_f32
(
v
.
raw
0
)
;
}
HWY_INLINE
float
GetLane
(
const
Vec128
<
float
1
>
v
)
{
return
vget_lane_f32
(
v
.
raw
0
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
double
GetLane
(
const
Vec128
<
double
2
>
v
)
{
return
vget_lane_f64
(
vget_low_f64
(
v
.
raw
)
0
)
;
}
HWY_INLINE
double
GetLane
(
const
Vec128
<
double
1
>
v
)
{
return
vget_lane_f64
(
v
.
raw
0
)
;
}
#
endif
HWY_NEON_DEF_FUNCTION_ALL_TYPES
(
operator
+
vadd
_
2
)
HWY_NEON_DEF_FUNCTION_ALL_TYPES
(
operator
-
vsub
_
2
)
HWY_NEON_DEF_FUNCTION_INT_8
(
SaturatedAdd
vqadd
_
2
)
HWY_NEON_DEF_FUNCTION_INT_16
(
SaturatedAdd
vqadd
_
2
)
HWY_NEON_DEF_FUNCTION_UINT_8
(
SaturatedAdd
vqadd
_
2
)
HWY_NEON_DEF_FUNCTION_UINT_16
(
SaturatedAdd
vqadd
_
2
)
HWY_NEON_DEF_FUNCTION_INT_8
(
SaturatedSub
vqsub
_
2
)
HWY_NEON_DEF_FUNCTION_INT_16
(
SaturatedSub
vqsub
_
2
)
HWY_NEON_DEF_FUNCTION_UINT_8
(
SaturatedSub
vqsub
_
2
)
HWY_NEON_DEF_FUNCTION_UINT_16
(
SaturatedSub
vqsub
_
2
)
namespace
detail
{
HWY_NEON_DEF_FUNCTION_UINT_32
(
SaturatedSub
vqsub
_
2
)
HWY_NEON_DEF_FUNCTION_UINT_64
(
SaturatedSub
vqsub
_
2
)
HWY_NEON_DEF_FUNCTION_INT_32
(
SaturatedSub
vqsub
_
2
)
HWY_NEON_DEF_FUNCTION_INT_64
(
SaturatedSub
vqsub
_
2
)
}
HWY_NEON_DEF_FUNCTION_UINT_8
(
AverageRound
vrhadd
_
2
)
HWY_NEON_DEF_FUNCTION_UINT_16
(
AverageRound
vrhadd
_
2
)
HWY_INLINE
Vec128
<
int8_t
>
Abs
(
const
Vec128
<
int8_t
>
v
)
{
return
Vec128
<
int8_t
>
(
vabsq_s8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
>
Abs
(
const
Vec128
<
int16_t
>
v
)
{
return
Vec128
<
int16_t
>
(
vabsq_s16
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
Abs
(
const
Vec128
<
int32_t
>
v
)
{
return
Vec128
<
int32_t
>
(
vabsq_s32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
float
>
Abs
(
const
Vec128
<
float
>
v
)
{
return
Vec128
<
float
>
(
vabsq_f32
(
v
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int8_t
N
)
>
HWY_INLINE
Vec128
<
int8_t
N
>
Abs
(
const
Vec128
<
int8_t
N
>
v
)
{
return
Vec128
<
int8_t
N
>
(
vabs_s8
(
v
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int16_t
N
)
>
HWY_INLINE
Vec128
<
int16_t
N
>
Abs
(
const
Vec128
<
int16_t
N
>
v
)
{
return
Vec128
<
int16_t
N
>
(
vabs_s16
(
v
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
int32_t
N
>
Abs
(
const
Vec128
<
int32_t
N
>
v
)
{
return
Vec128
<
int32_t
N
>
(
vabs_s32
(
v
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
float
N
)
>
HWY_INLINE
Vec128
<
float
N
>
Abs
(
const
Vec128
<
float
N
>
v
)
{
return
Vec128
<
float
N
>
(
vabs_f32
(
v
.
raw
)
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
double
>
Abs
(
const
Vec128
<
double
>
v
)
{
return
Vec128
<
double
>
(
vabsq_f64
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
double
1
>
Abs
(
const
Vec128
<
double
1
>
v
)
{
return
Vec128
<
double
1
>
(
vabs_f64
(
v
.
raw
)
)
;
}
#
endif
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
Neg
vneg
_
1
)
HWY_NEON_DEF_FUNCTION_INT_8_16_32
(
Neg
vneg
_
1
)
HWY_INLINE
Vec128
<
int64_t
1
>
Neg
(
const
Vec128
<
int64_t
1
>
v
)
{
#
if
HWY_ARCH_ARM_A64
return
Vec128
<
int64_t
1
>
(
vneg_s64
(
v
.
raw
)
)
;
#
else
return
Zero
(
Simd
<
int64_t
1
>
(
)
)
-
v
;
#
endif
}
HWY_INLINE
Vec128
<
int64_t
>
Neg
(
const
Vec128
<
int64_t
>
v
)
{
#
if
HWY_ARCH_ARM_A64
return
Vec128
<
int64_t
>
(
vnegq_s64
(
v
.
raw
)
)
;
#
else
return
Zero
(
Full128
<
int64_t
>
(
)
)
-
v
;
#
endif
}
#
pragma
push_macro
(
"
HWY_NEON_DEF_FUNCTION
"
)
#
undef
HWY_NEON_DEF_FUNCTION
#
define
HWY_NEON_DEF_FUNCTION
(
type
size
name
prefix
infix
suffix
args
)
\
template
<
int
kBits
>
\
HWY_INLINE
Vec128
<
type
size
>
name
(
const
Vec128
<
type
size
>
v
)
{
\
return
kBits
=
=
0
?
v
\
:
Vec128
<
type
size
>
(
HWY_NEON_EVAL
(
\
prefix
#
#
infix
#
#
suffix
v
.
raw
HWY_MAX
(
1
kBits
)
)
)
;
\
}
HWY_NEON_DEF_FUNCTION_INTS_UINTS
(
ShiftLeft
vshl
_n_
HWY_SHIFT
)
HWY_NEON_DEF_FUNCTION_UINTS
(
ShiftRight
vshr
_n_
HWY_SHIFT
)
HWY_NEON_DEF_FUNCTION_INTS
(
ShiftRight
vshr
_n_
HWY_SHIFT
)
#
pragma
pop_macro
(
"
HWY_NEON_DEF_FUNCTION
"
)
HWY_INLINE
Vec128
<
uint8_t
>
operator
<
<
(
const
Vec128
<
uint8_t
>
v
const
Vec128
<
uint8_t
>
bits
)
{
return
Vec128
<
uint8_t
>
(
vshlq_u8
(
v
.
raw
vreinterpretq_s8_u8
(
bits
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint8_t
N
)
>
HWY_INLINE
Vec128
<
uint8_t
N
>
operator
<
<
(
const
Vec128
<
uint8_t
N
>
v
const
Vec128
<
uint8_t
N
>
bits
)
{
return
Vec128
<
uint8_t
N
>
(
vshl_u8
(
v
.
raw
vreinterpret_s8_u8
(
bits
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
uint16_t
>
operator
<
<
(
const
Vec128
<
uint16_t
>
v
const
Vec128
<
uint16_t
>
bits
)
{
return
Vec128
<
uint16_t
>
(
vshlq_u16
(
v
.
raw
vreinterpretq_s16_u16
(
bits
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint16_t
N
)
>
HWY_INLINE
Vec128
<
uint16_t
N
>
operator
<
<
(
const
Vec128
<
uint16_t
N
>
v
const
Vec128
<
uint16_t
N
>
bits
)
{
return
Vec128
<
uint16_t
N
>
(
vshl_u16
(
v
.
raw
vreinterpret_s16_u16
(
bits
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
>
operator
<
<
(
const
Vec128
<
uint32_t
>
v
const
Vec128
<
uint32_t
>
bits
)
{
return
Vec128
<
uint32_t
>
(
vshlq_u32
(
v
.
raw
vreinterpretq_s32_u32
(
bits
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint32_t
N
)
>
HWY_INLINE
Vec128
<
uint32_t
N
>
operator
<
<
(
const
Vec128
<
uint32_t
N
>
v
const
Vec128
<
uint32_t
N
>
bits
)
{
return
Vec128
<
uint32_t
N
>
(
vshl_u32
(
v
.
raw
vreinterpret_s32_u32
(
bits
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
>
operator
<
<
(
const
Vec128
<
uint64_t
>
v
const
Vec128
<
uint64_t
>
bits
)
{
return
Vec128
<
uint64_t
>
(
vshlq_u64
(
v
.
raw
vreinterpretq_s64_u64
(
bits
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
1
>
operator
<
<
(
const
Vec128
<
uint64_t
1
>
v
const
Vec128
<
uint64_t
1
>
bits
)
{
return
Vec128
<
uint64_t
1
>
(
vshl_u64
(
v
.
raw
vreinterpret_s64_u64
(
bits
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
int8_t
>
operator
<
<
(
const
Vec128
<
int8_t
>
v
const
Vec128
<
int8_t
>
bits
)
{
return
Vec128
<
int8_t
>
(
vshlq_s8
(
v
.
raw
bits
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int8_t
N
)
>
HWY_INLINE
Vec128
<
int8_t
N
>
operator
<
<
(
const
Vec128
<
int8_t
N
>
v
const
Vec128
<
int8_t
N
>
bits
)
{
return
Vec128
<
int8_t
N
>
(
vshl_s8
(
v
.
raw
bits
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
>
operator
<
<
(
const
Vec128
<
int16_t
>
v
const
Vec128
<
int16_t
>
bits
)
{
return
Vec128
<
int16_t
>
(
vshlq_s16
(
v
.
raw
bits
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int16_t
N
)
>
HWY_INLINE
Vec128
<
int16_t
N
>
operator
<
<
(
const
Vec128
<
int16_t
N
>
v
const
Vec128
<
int16_t
N
>
bits
)
{
return
Vec128
<
int16_t
N
>
(
vshl_s16
(
v
.
raw
bits
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
operator
<
<
(
const
Vec128
<
int32_t
>
v
const
Vec128
<
int32_t
>
bits
)
{
return
Vec128
<
int32_t
>
(
vshlq_s32
(
v
.
raw
bits
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
int32_t
N
>
operator
<
<
(
const
Vec128
<
int32_t
N
>
v
const
Vec128
<
int32_t
N
>
bits
)
{
return
Vec128
<
int32_t
N
>
(
vshl_s32
(
v
.
raw
bits
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
operator
<
<
(
const
Vec128
<
int64_t
>
v
const
Vec128
<
int64_t
>
bits
)
{
return
Vec128
<
int64_t
>
(
vshlq_s64
(
v
.
raw
bits
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
1
>
operator
<
<
(
const
Vec128
<
int64_t
1
>
v
const
Vec128
<
int64_t
1
>
bits
)
{
return
Vec128
<
int64_t
1
>
(
vshl_s64
(
v
.
raw
bits
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint8_t
>
operator
>
>
(
const
Vec128
<
uint8_t
>
v
const
Vec128
<
uint8_t
>
bits
)
{
const
int8x16_t
neg_bits
=
Neg
(
BitCast
(
Full128
<
int8_t
>
(
)
bits
)
)
.
raw
;
return
Vec128
<
uint8_t
>
(
vshlq_u8
(
v
.
raw
neg_bits
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint8_t
N
)
>
HWY_INLINE
Vec128
<
uint8_t
N
>
operator
>
>
(
const
Vec128
<
uint8_t
N
>
v
const
Vec128
<
uint8_t
N
>
bits
)
{
const
int8x8_t
neg_bits
=
Neg
(
BitCast
(
Simd
<
int8_t
N
>
(
)
bits
)
)
.
raw
;
return
Vec128
<
uint8_t
N
>
(
vshl_u8
(
v
.
raw
neg_bits
)
)
;
}
HWY_INLINE
Vec128
<
uint16_t
>
operator
>
>
(
const
Vec128
<
uint16_t
>
v
const
Vec128
<
uint16_t
>
bits
)
{
const
int16x8_t
neg_bits
=
Neg
(
BitCast
(
Full128
<
int16_t
>
(
)
bits
)
)
.
raw
;
return
Vec128
<
uint16_t
>
(
vshlq_u16
(
v
.
raw
neg_bits
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint16_t
N
)
>
HWY_INLINE
Vec128
<
uint16_t
N
>
operator
>
>
(
const
Vec128
<
uint16_t
N
>
v
const
Vec128
<
uint16_t
N
>
bits
)
{
const
int16x4_t
neg_bits
=
Neg
(
BitCast
(
Simd
<
int16_t
N
>
(
)
bits
)
)
.
raw
;
return
Vec128
<
uint16_t
N
>
(
vshl_u16
(
v
.
raw
neg_bits
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
>
operator
>
>
(
const
Vec128
<
uint32_t
>
v
const
Vec128
<
uint32_t
>
bits
)
{
const
int32x4_t
neg_bits
=
Neg
(
BitCast
(
Full128
<
int32_t
>
(
)
bits
)
)
.
raw
;
return
Vec128
<
uint32_t
>
(
vshlq_u32
(
v
.
raw
neg_bits
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint32_t
N
)
>
HWY_INLINE
Vec128
<
uint32_t
N
>
operator
>
>
(
const
Vec128
<
uint32_t
N
>
v
const
Vec128
<
uint32_t
N
>
bits
)
{
const
int32x2_t
neg_bits
=
Neg
(
BitCast
(
Simd
<
int32_t
N
>
(
)
bits
)
)
.
raw
;
return
Vec128
<
uint32_t
N
>
(
vshl_u32
(
v
.
raw
neg_bits
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
>
operator
>
>
(
const
Vec128
<
uint64_t
>
v
const
Vec128
<
uint64_t
>
bits
)
{
const
int64x2_t
neg_bits
=
Neg
(
BitCast
(
Full128
<
int64_t
>
(
)
bits
)
)
.
raw
;
return
Vec128
<
uint64_t
>
(
vshlq_u64
(
v
.
raw
neg_bits
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
1
>
operator
>
>
(
const
Vec128
<
uint64_t
1
>
v
const
Vec128
<
uint64_t
1
>
bits
)
{
const
int64x1_t
neg_bits
=
Neg
(
BitCast
(
Simd
<
int64_t
1
>
(
)
bits
)
)
.
raw
;
return
Vec128
<
uint64_t
1
>
(
vshl_u64
(
v
.
raw
neg_bits
)
)
;
}
HWY_INLINE
Vec128
<
int8_t
>
operator
>
>
(
const
Vec128
<
int8_t
>
v
const
Vec128
<
int8_t
>
bits
)
{
return
Vec128
<
int8_t
>
(
vshlq_s8
(
v
.
raw
Neg
(
bits
)
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int8_t
N
)
>
HWY_INLINE
Vec128
<
int8_t
N
>
operator
>
>
(
const
Vec128
<
int8_t
N
>
v
const
Vec128
<
int8_t
N
>
bits
)
{
return
Vec128
<
int8_t
N
>
(
vshl_s8
(
v
.
raw
Neg
(
bits
)
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
>
operator
>
>
(
const
Vec128
<
int16_t
>
v
const
Vec128
<
int16_t
>
bits
)
{
return
Vec128
<
int16_t
>
(
vshlq_s16
(
v
.
raw
Neg
(
bits
)
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int16_t
N
)
>
HWY_INLINE
Vec128
<
int16_t
N
>
operator
>
>
(
const
Vec128
<
int16_t
N
>
v
const
Vec128
<
int16_t
N
>
bits
)
{
return
Vec128
<
int16_t
N
>
(
vshl_s16
(
v
.
raw
Neg
(
bits
)
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
operator
>
>
(
const
Vec128
<
int32_t
>
v
const
Vec128
<
int32_t
>
bits
)
{
return
Vec128
<
int32_t
>
(
vshlq_s32
(
v
.
raw
Neg
(
bits
)
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
int32_t
N
>
operator
>
>
(
const
Vec128
<
int32_t
N
>
v
const
Vec128
<
int32_t
N
>
bits
)
{
return
Vec128
<
int32_t
N
>
(
vshl_s32
(
v
.
raw
Neg
(
bits
)
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
operator
>
>
(
const
Vec128
<
int64_t
>
v
const
Vec128
<
int64_t
>
bits
)
{
return
Vec128
<
int64_t
>
(
vshlq_s64
(
v
.
raw
Neg
(
bits
)
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
1
>
operator
>
>
(
const
Vec128
<
int64_t
1
>
v
const
Vec128
<
int64_t
1
>
bits
)
{
return
Vec128
<
int64_t
1
>
(
vshl_s64
(
v
.
raw
Neg
(
bits
)
.
raw
)
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
ShiftLeftSame
(
const
Vec128
<
T
N
>
v
int
bits
)
{
return
v
<
<
Set
(
Simd
<
T
N
>
(
)
bits
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
ShiftRightSame
(
const
Vec128
<
T
N
>
v
int
bits
)
{
return
v
>
>
Set
(
Simd
<
T
N
>
(
)
bits
)
;
}
HWY_INLINE
Vec128
<
uint16_t
>
operator
*
(
const
Vec128
<
uint16_t
>
a
const
Vec128
<
uint16_t
>
b
)
{
return
Vec128
<
uint16_t
>
(
vmulq_u16
(
a
.
raw
b
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
>
operator
*
(
const
Vec128
<
uint32_t
>
a
const
Vec128
<
uint32_t
>
b
)
{
return
Vec128
<
uint32_t
>
(
vmulq_u32
(
a
.
raw
b
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint16_t
N
)
>
HWY_INLINE
Vec128
<
uint16_t
N
>
operator
*
(
const
Vec128
<
uint16_t
N
>
a
const
Vec128
<
uint16_t
N
>
b
)
{
return
Vec128
<
uint16_t
N
>
(
vmul_u16
(
a
.
raw
b
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint32_t
N
)
>
HWY_INLINE
Vec128
<
uint32_t
N
>
operator
*
(
const
Vec128
<
uint32_t
N
>
a
const
Vec128
<
uint32_t
N
>
b
)
{
return
Vec128
<
uint32_t
N
>
(
vmul_u32
(
a
.
raw
b
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
>
operator
*
(
const
Vec128
<
int16_t
>
a
const
Vec128
<
int16_t
>
b
)
{
return
Vec128
<
int16_t
>
(
vmulq_s16
(
a
.
raw
b
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
operator
*
(
const
Vec128
<
int32_t
>
a
const
Vec128
<
int32_t
>
b
)
{
return
Vec128
<
int32_t
>
(
vmulq_s32
(
a
.
raw
b
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint16_t
N
)
>
HWY_INLINE
Vec128
<
int16_t
N
>
operator
*
(
const
Vec128
<
int16_t
N
>
a
const
Vec128
<
int16_t
N
>
b
)
{
return
Vec128
<
int16_t
N
>
(
vmul_s16
(
a
.
raw
b
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
int32_t
N
>
operator
*
(
const
Vec128
<
int32_t
N
>
a
const
Vec128
<
int32_t
N
>
b
)
{
return
Vec128
<
int32_t
N
>
(
vmul_s32
(
a
.
raw
b
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
>
MulHigh
(
const
Vec128
<
int16_t
>
a
const
Vec128
<
int16_t
>
b
)
{
int32x4_t
rlo
=
vmull_s16
(
vget_low_s16
(
a
.
raw
)
vget_low_s16
(
b
.
raw
)
)
;
#
if
HWY_ARCH_ARM_A64
int32x4_t
rhi
=
vmull_high_s16
(
a
.
raw
b
.
raw
)
;
#
else
int32x4_t
rhi
=
vmull_s16
(
vget_high_s16
(
a
.
raw
)
vget_high_s16
(
b
.
raw
)
)
;
#
endif
return
Vec128
<
int16_t
>
(
vuzp2q_s16
(
vreinterpretq_s16_s32
(
rlo
)
vreinterpretq_s16_s32
(
rhi
)
)
)
;
}
HWY_INLINE
Vec128
<
uint16_t
>
MulHigh
(
const
Vec128
<
uint16_t
>
a
const
Vec128
<
uint16_t
>
b
)
{
uint32x4_t
rlo
=
vmull_u16
(
vget_low_u16
(
a
.
raw
)
vget_low_u16
(
b
.
raw
)
)
;
#
if
HWY_ARCH_ARM_A64
uint32x4_t
rhi
=
vmull_high_u16
(
a
.
raw
b
.
raw
)
;
#
else
uint32x4_t
rhi
=
vmull_u16
(
vget_high_u16
(
a
.
raw
)
vget_high_u16
(
b
.
raw
)
)
;
#
endif
return
Vec128
<
uint16_t
>
(
vuzp2q_u16
(
vreinterpretq_u16_u32
(
rlo
)
vreinterpretq_u16_u32
(
rhi
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int16_t
N
)
>
HWY_INLINE
Vec128
<
int16_t
N
>
MulHigh
(
const
Vec128
<
int16_t
N
>
a
const
Vec128
<
int16_t
N
>
b
)
{
int16x8_t
hi_lo
=
vreinterpretq_s16_s32
(
vmull_s16
(
a
.
raw
b
.
raw
)
)
;
return
Vec128
<
int16_t
N
>
(
vget_low_s16
(
vuzp2q_s16
(
hi_lo
hi_lo
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint16_t
N
)
>
HWY_INLINE
Vec128
<
uint16_t
N
>
MulHigh
(
const
Vec128
<
uint16_t
N
>
a
const
Vec128
<
uint16_t
N
>
b
)
{
uint16x8_t
hi_lo
=
vreinterpretq_u16_u32
(
vmull_u16
(
a
.
raw
b
.
raw
)
)
;
return
Vec128
<
uint16_t
N
>
(
vget_low_u16
(
vuzp2q_u16
(
hi_lo
hi_lo
)
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
MulEven
(
const
Vec128
<
int32_t
>
a
const
Vec128
<
int32_t
>
b
)
{
int32x4_t
a_packed
=
vuzp1q_s32
(
a
.
raw
a
.
raw
)
;
int32x4_t
b_packed
=
vuzp1q_s32
(
b
.
raw
b
.
raw
)
;
return
Vec128
<
int64_t
>
(
vmull_s32
(
vget_low_s32
(
a_packed
)
vget_low_s32
(
b_packed
)
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
>
MulEven
(
const
Vec128
<
uint32_t
>
a
const
Vec128
<
uint32_t
>
b
)
{
uint32x4_t
a_packed
=
vuzp1q_u32
(
a
.
raw
a
.
raw
)
;
uint32x4_t
b_packed
=
vuzp1q_u32
(
b
.
raw
b
.
raw
)
;
return
Vec128
<
uint64_t
>
(
vmull_u32
(
vget_low_u32
(
a_packed
)
vget_low_u32
(
b_packed
)
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
int64_t
(
N
+
1
)
/
2
>
MulEven
(
const
Vec128
<
int32_t
N
>
a
const
Vec128
<
int32_t
N
>
b
)
{
int32x2_t
a_packed
=
vuzp1_s32
(
a
.
raw
a
.
raw
)
;
int32x2_t
b_packed
=
vuzp1_s32
(
b
.
raw
b
.
raw
)
;
return
Vec128
<
int64_t
(
N
+
1
)
/
2
>
(
vget_low_s64
(
vmull_s32
(
a_packed
b_packed
)
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
uint64_t
(
N
+
1
)
/
2
>
MulEven
(
const
Vec128
<
uint32_t
N
>
a
const
Vec128
<
uint32_t
N
>
b
)
{
uint32x2_t
a_packed
=
vuzp1_u32
(
a
.
raw
a
.
raw
)
;
uint32x2_t
b_packed
=
vuzp1_u32
(
b
.
raw
b
.
raw
)
;
return
Vec128
<
uint64_t
(
N
+
1
)
/
2
>
(
vget_low_u64
(
vmull_u32
(
a_packed
b_packed
)
)
)
;
}
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
operator
*
vmul
_
2
)
HWY_INLINE
Vec128
<
float
>
ApproximateReciprocal
(
const
Vec128
<
float
>
v
)
{
return
Vec128
<
float
>
(
vrecpeq_f32
(
v
.
raw
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float
N
>
ApproximateReciprocal
(
const
Vec128
<
float
N
>
v
)
{
return
Vec128
<
float
N
>
(
vrecpe_f32
(
v
.
raw
)
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
operator
/
vdiv
_
2
)
#
else
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float
N
>
operator
/
(
const
Vec128
<
float
N
>
a
const
Vec128
<
float
N
>
b
)
{
auto
x
=
ApproximateReciprocal
(
b
)
;
const
auto
two
=
Set
(
Simd
<
float
N
>
(
)
2
)
;
x
=
x
*
(
two
-
b
*
x
)
;
x
=
x
*
(
two
-
b
*
x
)
;
x
=
x
*
(
two
-
b
*
x
)
;
return
a
*
x
;
}
#
endif
HWY_INLINE
Vec128
<
float
>
AbsDiff
(
const
Vec128
<
float
>
a
const
Vec128
<
float
>
b
)
{
return
Vec128
<
float
>
(
vabdq_f32
(
a
.
raw
b
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
float
N
)
>
HWY_INLINE
Vec128
<
float
N
>
AbsDiff
(
const
Vec128
<
float
N
>
a
const
Vec128
<
float
N
>
b
)
{
return
Vec128
<
float
N
>
(
vabd_f32
(
a
.
raw
b
.
raw
)
)
;
}
#
if
defined
(
__ARM_VFPV4__
)
|
|
HWY_ARCH_ARM_A64
template
<
size_t
N
HWY_IF_LE64
(
float
N
)
>
HWY_INLINE
Vec128
<
float
N
>
MulAdd
(
const
Vec128
<
float
N
>
mul
const
Vec128
<
float
N
>
x
const
Vec128
<
float
N
>
add
)
{
return
Vec128
<
float
N
>
(
vfma_f32
(
add
.
raw
mul
.
raw
x
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
float
>
MulAdd
(
const
Vec128
<
float
>
mul
const
Vec128
<
float
>
x
const
Vec128
<
float
>
add
)
{
return
Vec128
<
float
>
(
vfmaq_f32
(
add
.
raw
mul
.
raw
x
.
raw
)
)
;
}
#
else
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float
N
>
MulAdd
(
const
Vec128
<
float
N
>
mul
const
Vec128
<
float
N
>
x
const
Vec128
<
float
N
>
add
)
{
return
mul
*
x
+
add
;
}
#
endif
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
double
1
>
MulAdd
(
const
Vec128
<
double
1
>
mul
const
Vec128
<
double
1
>
x
const
Vec128
<
double
1
>
add
)
{
return
Vec128
<
double
1
>
(
vfma_f64
(
add
.
raw
mul
.
raw
x
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
double
>
MulAdd
(
const
Vec128
<
double
>
mul
const
Vec128
<
double
>
x
const
Vec128
<
double
>
add
)
{
return
Vec128
<
double
>
(
vfmaq_f64
(
add
.
raw
mul
.
raw
x
.
raw
)
)
;
}
#
endif
#
if
defined
(
__ARM_VFPV4__
)
|
|
HWY_ARCH_ARM_A64
template
<
size_t
N
HWY_IF_LE64
(
float
N
)
>
HWY_INLINE
Vec128
<
float
N
>
NegMulAdd
(
const
Vec128
<
float
N
>
mul
const
Vec128
<
float
N
>
x
const
Vec128
<
float
N
>
add
)
{
return
Vec128
<
float
N
>
(
vfms_f32
(
add
.
raw
mul
.
raw
x
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
float
>
NegMulAdd
(
const
Vec128
<
float
>
mul
const
Vec128
<
float
>
x
const
Vec128
<
float
>
add
)
{
return
Vec128
<
float
>
(
vfmsq_f32
(
add
.
raw
mul
.
raw
x
.
raw
)
)
;
}
#
else
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float
N
>
NegMulAdd
(
const
Vec128
<
float
N
>
mul
const
Vec128
<
float
N
>
x
const
Vec128
<
float
N
>
add
)
{
return
add
-
mul
*
x
;
}
#
endif
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
double
1
>
NegMulAdd
(
const
Vec128
<
double
1
>
mul
const
Vec128
<
double
1
>
x
const
Vec128
<
double
1
>
add
)
{
return
Vec128
<
double
1
>
(
vfms_f64
(
add
.
raw
mul
.
raw
x
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
double
>
NegMulAdd
(
const
Vec128
<
double
>
mul
const
Vec128
<
double
>
x
const
Vec128
<
double
>
add
)
{
return
Vec128
<
double
>
(
vfmsq_f64
(
add
.
raw
mul
.
raw
x
.
raw
)
)
;
}
#
endif
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float
N
>
MulSub
(
const
Vec128
<
float
N
>
mul
const
Vec128
<
float
N
>
x
const
Vec128
<
float
N
>
sub
)
{
return
MulAdd
(
mul
x
Neg
(
sub
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float
N
>
NegMulSub
(
const
Vec128
<
float
N
>
mul
const
Vec128
<
float
N
>
x
const
Vec128
<
float
N
>
sub
)
{
return
Neg
(
MulAdd
(
mul
x
sub
)
)
;
}
#
if
HWY_ARCH_ARM_A64
template
<
size_t
N
>
HWY_INLINE
Vec128
<
double
N
>
MulSub
(
const
Vec128
<
double
N
>
mul
const
Vec128
<
double
N
>
x
const
Vec128
<
double
N
>
sub
)
{
return
MulAdd
(
mul
x
Neg
(
sub
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
double
N
>
NegMulSub
(
const
Vec128
<
double
N
>
mul
const
Vec128
<
double
N
>
x
const
Vec128
<
double
N
>
sub
)
{
return
Neg
(
MulAdd
(
mul
x
sub
)
)
;
}
#
endif
HWY_INLINE
Vec128
<
float
>
ApproximateReciprocalSqrt
(
const
Vec128
<
float
>
v
)
{
return
Vec128
<
float
>
(
vrsqrteq_f32
(
v
.
raw
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float
N
>
ApproximateReciprocalSqrt
(
const
Vec128
<
float
N
>
v
)
{
return
Vec128
<
float
N
>
(
vrsqrte_f32
(
v
.
raw
)
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
Sqrt
vsqrt
_
1
)
#
else
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float
N
>
Sqrt
(
const
Vec128
<
float
N
>
v
)
{
auto
b
=
v
;
auto
Y
=
ApproximateReciprocalSqrt
(
v
)
;
auto
x
=
v
*
Y
;
const
auto
half
=
Set
(
Simd
<
float
N
>
(
)
0
.
5
)
;
const
auto
oneandhalf
=
Set
(
Simd
<
float
N
>
(
)
1
.
5
)
;
for
(
size_t
i
=
0
;
i
<
3
;
i
+
+
)
{
b
=
b
*
Y
*
Y
;
Y
=
oneandhalf
-
half
*
b
;
x
=
x
*
Y
;
}
return
IfThenZeroElse
(
v
=
=
Zero
(
Simd
<
float
N
>
(
)
)
x
)
;
}
#
endif
template
<
typename
TFrom
typename
TTo
size_t
N
>
HWY_API
Mask128
<
TTo
N
>
RebindMask
(
Simd
<
TTo
N
>
Mask128
<
TFrom
N
>
m
)
{
static_assert
(
sizeof
(
TFrom
)
=
=
sizeof
(
TTo
)
"
Must
have
same
size
"
)
;
return
Mask128
<
TTo
N
>
{
m
.
raw
}
;
}
#
define
HWY_NEON_BUILD_TPL_HWY_COMPARE
#
define
HWY_NEON_BUILD_RET_HWY_COMPARE
(
type
size
)
Mask128
<
type
size
>
#
define
HWY_NEON_BUILD_PARAM_HWY_COMPARE
(
type
size
)
\
const
Vec128
<
type
size
>
a
const
Vec128
<
type
size
>
b
#
define
HWY_NEON_BUILD_ARG_HWY_COMPARE
a
.
raw
b
.
raw
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
operator
=
=
vceq
_
HWY_COMPARE
)
#
if
HWY_ARCH_ARM_A64
HWY_NEON_DEF_FUNCTION_INTS_UINTS
(
operator
=
=
vceq
_
HWY_COMPARE
)
#
else
HWY_NEON_DEF_FUNCTION_INT_8_16_32
(
operator
=
=
vceq
_
HWY_COMPARE
)
HWY_NEON_DEF_FUNCTION_UINT_8_16_32
(
operator
=
=
vceq
_
HWY_COMPARE
)
#
endif
#
if
HWY_ARCH_ARM_A64
HWY_NEON_DEF_FUNCTION_INTS
(
operator
<
vclt
_
HWY_COMPARE
)
#
else
HWY_NEON_DEF_FUNCTION_INT_8_16_32
(
operator
<
vclt
_
HWY_COMPARE
)
#
endif
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
operator
<
vclt
_
HWY_COMPARE
)
#
if
HWY_ARCH_ARM_A64
HWY_NEON_DEF_FUNCTION_INTS
(
operator
>
vcgt
_
HWY_COMPARE
)
#
else
HWY_NEON_DEF_FUNCTION_INT_8_16_32
(
operator
>
vcgt
_
HWY_COMPARE
)
#
endif
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
operator
>
vcgt
_
HWY_COMPARE
)
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
operator
<
=
vcle
_
HWY_COMPARE
)
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
operator
>
=
vcge
_
HWY_COMPARE
)
#
undef
HWY_NEON_BUILD_TPL_HWY_COMPARE
#
undef
HWY_NEON_BUILD_RET_HWY_COMPARE
#
undef
HWY_NEON_BUILD_PARAM_HWY_COMPARE
#
undef
HWY_NEON_BUILD_ARG_HWY_COMPARE
template
<
typename
T
>
HWY_INLINE
Vec128
<
T
>
Not
(
const
Vec128
<
T
>
v
)
{
const
Full128
<
uint8_t
>
d8
;
return
Vec128
<
T
>
(
vmvnq_u8
(
BitCast
(
d8
v
)
.
raw
)
)
;
}
template
<
typename
T
size_t
N
HWY_IF_LE64
(
T
N
)
>
HWY_INLINE
Vec128
<
T
N
>
Not
(
const
Vec128
<
T
N
>
v
)
{
const
Repartition
<
uint8_t
Simd
<
T
N
>
>
d8
;
return
Vec128
<
T
N
>
(
vmvn_u8
(
BitCast
(
d8
v
)
.
raw
)
)
;
}
HWY_NEON_DEF_FUNCTION_INTS_UINTS
(
And
vand
_
2
)
template
<
typename
T
size_t
N
HWY_IF_FLOAT
(
T
)
>
HWY_INLINE
Vec128
<
T
N
>
And
(
const
Vec128
<
T
N
>
a
const
Vec128
<
T
N
>
b
)
{
const
Simd
<
MakeUnsigned
<
T
>
N
>
d
;
return
BitCast
(
Simd
<
T
N
>
(
)
BitCast
(
d
a
)
&
BitCast
(
d
b
)
)
;
}
namespace
internal
{
HWY_NEON_DEF_FUNCTION_INTS_UINTS
(
reversed_andnot
vbic
_
2
)
}
template
<
typename
T
size_t
N
HWY_IF_NOT_FLOAT
(
T
)
>
HWY_INLINE
Vec128
<
T
N
>
AndNot
(
const
Vec128
<
T
N
>
not_mask
const
Vec128
<
T
N
>
mask
)
{
return
internal
:
:
reversed_andnot
(
mask
not_mask
)
;
}
template
<
typename
T
size_t
N
HWY_IF_FLOAT
(
T
)
>
HWY_INLINE
Vec128
<
T
N
>
AndNot
(
const
Vec128
<
T
N
>
not_mask
const
Vec128
<
T
N
>
mask
)
{
const
Simd
<
MakeUnsigned
<
T
>
N
>
du
;
Vec128
<
MakeUnsigned
<
T
>
N
>
ret
=
internal
:
:
reversed_andnot
(
BitCast
(
du
mask
)
BitCast
(
du
not_mask
)
)
;
return
BitCast
(
Simd
<
T
N
>
(
)
ret
)
;
}
HWY_NEON_DEF_FUNCTION_INTS_UINTS
(
Or
vorr
_
2
)
template
<
typename
T
size_t
N
HWY_IF_FLOAT
(
T
)
>
HWY_INLINE
Vec128
<
T
N
>
Or
(
const
Vec128
<
T
N
>
a
const
Vec128
<
T
N
>
b
)
{
const
Simd
<
MakeUnsigned
<
T
>
N
>
d
;
return
BitCast
(
Simd
<
T
N
>
(
)
BitCast
(
d
a
)
|
BitCast
(
d
b
)
)
;
}
HWY_NEON_DEF_FUNCTION_INTS_UINTS
(
Xor
veor
_
2
)
template
<
typename
T
size_t
N
HWY_IF_FLOAT
(
T
)
>
HWY_INLINE
Vec128
<
T
N
>
Xor
(
const
Vec128
<
T
N
>
a
const
Vec128
<
T
N
>
b
)
{
const
Simd
<
MakeUnsigned
<
T
>
N
>
d
;
return
BitCast
(
Simd
<
T
N
>
(
)
BitCast
(
d
a
)
^
BitCast
(
d
b
)
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
operator
&
(
const
Vec128
<
T
N
>
a
const
Vec128
<
T
N
>
b
)
{
return
And
(
a
b
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
operator
|
(
const
Vec128
<
T
N
>
a
const
Vec128
<
T
N
>
b
)
{
return
Or
(
a
b
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
operator
^
(
const
Vec128
<
T
N
>
a
const
Vec128
<
T
N
>
b
)
{
return
Xor
(
a
b
)
;
}
template
<
typename
T
size_t
N
>
HWY_API
Vec128
<
T
N
>
CopySign
(
const
Vec128
<
T
N
>
magn
const
Vec128
<
T
N
>
sign
)
{
static_assert
(
IsFloat
<
T
>
(
)
"
Only
makes
sense
for
floating
-
point
"
)
;
const
auto
msb
=
SignBit
(
Simd
<
T
N
>
(
)
)
;
return
Or
(
AndNot
(
msb
magn
)
And
(
msb
sign
)
)
;
}
template
<
typename
T
size_t
N
>
HWY_API
Vec128
<
T
N
>
CopySignToAbs
(
const
Vec128
<
T
N
>
abs
const
Vec128
<
T
N
>
sign
)
{
static_assert
(
IsFloat
<
T
>
(
)
"
Only
makes
sense
for
floating
-
point
"
)
;
return
Or
(
abs
And
(
SignBit
(
Simd
<
T
N
>
(
)
)
sign
)
)
;
}
template
<
typename
T
size_t
N
>
HWY_API
Vec128
<
T
N
>
BroadcastSignBit
(
const
Vec128
<
T
N
>
v
)
{
return
ShiftRight
<
sizeof
(
T
)
*
8
-
1
>
(
v
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Mask128
<
T
N
>
TestBit
(
Vec128
<
T
N
>
v
Vec128
<
T
N
>
bit
)
{
static_assert
(
!
hwy
:
:
IsFloat
<
T
>
(
)
"
Only
integer
vectors
supported
"
)
;
return
(
v
&
bit
)
=
=
bit
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Mask128
<
T
N
>
MaskFromVec
(
const
Vec128
<
T
N
>
v
)
{
return
Mask128
<
T
N
>
(
v
.
raw
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
VecFromMask
(
const
Mask128
<
T
N
>
v
)
{
return
Vec128
<
T
N
>
(
v
.
raw
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
VecFromMask
(
Simd
<
T
N
>
const
Mask128
<
T
N
>
v
)
{
return
Vec128
<
T
N
>
(
v
.
raw
)
;
}
#
define
HWY_NEON_BUILD_TPL_HWY_IF
#
define
HWY_NEON_BUILD_RET_HWY_IF
(
type
size
)
Vec128
<
type
size
>
#
define
HWY_NEON_BUILD_PARAM_HWY_IF
(
type
size
)
\
const
Mask128
<
type
size
>
mask
const
Vec128
<
type
size
>
yes
\
const
Vec128
<
type
size
>
no
#
define
HWY_NEON_BUILD_ARG_HWY_IF
mask
.
raw
yes
.
raw
no
.
raw
HWY_NEON_DEF_FUNCTION_ALL_TYPES
(
IfThenElse
vbsl
_
HWY_IF
)
#
undef
HWY_NEON_BUILD_TPL_HWY_IF
#
undef
HWY_NEON_BUILD_RET_HWY_IF
#
undef
HWY_NEON_BUILD_PARAM_HWY_IF
#
undef
HWY_NEON_BUILD_ARG_HWY_IF
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
IfThenElseZero
(
const
Mask128
<
T
N
>
mask
const
Vec128
<
T
N
>
yes
)
{
return
yes
&
VecFromMask
(
Simd
<
T
N
>
(
)
mask
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
IfThenZeroElse
(
const
Mask128
<
T
N
>
mask
const
Vec128
<
T
N
>
no
)
{
return
AndNot
(
VecFromMask
(
Simd
<
T
N
>
(
)
mask
)
no
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
ZeroIfNegative
(
Vec128
<
T
N
>
v
)
{
const
auto
zero
=
Zero
(
Simd
<
T
N
>
(
)
)
;
return
Max
(
zero
v
)
;
}
template
<
typename
T
size_t
N
>
HWY_API
Mask128
<
T
N
>
Not
(
const
Mask128
<
T
N
>
m
)
{
const
Simd
<
T
N
>
d
;
return
MaskFromVec
(
Not
(
VecFromMask
(
d
m
)
)
)
;
}
template
<
typename
T
size_t
N
>
HWY_API
Mask128
<
T
N
>
And
(
const
Mask128
<
T
N
>
a
Mask128
<
T
N
>
b
)
{
const
Simd
<
T
N
>
d
;
return
MaskFromVec
(
And
(
VecFromMask
(
d
a
)
VecFromMask
(
d
b
)
)
)
;
}
template
<
typename
T
size_t
N
>
HWY_API
Mask128
<
T
N
>
AndNot
(
const
Mask128
<
T
N
>
a
Mask128
<
T
N
>
b
)
{
const
Simd
<
T
N
>
d
;
return
MaskFromVec
(
AndNot
(
VecFromMask
(
d
a
)
VecFromMask
(
d
b
)
)
)
;
}
template
<
typename
T
size_t
N
>
HWY_API
Mask128
<
T
N
>
Or
(
const
Mask128
<
T
N
>
a
Mask128
<
T
N
>
b
)
{
const
Simd
<
T
N
>
d
;
return
MaskFromVec
(
Or
(
VecFromMask
(
d
a
)
VecFromMask
(
d
b
)
)
)
;
}
template
<
typename
T
size_t
N
>
HWY_API
Mask128
<
T
N
>
Xor
(
const
Mask128
<
T
N
>
a
Mask128
<
T
N
>
b
)
{
const
Simd
<
T
N
>
d
;
return
MaskFromVec
(
Xor
(
VecFromMask
(
d
a
)
VecFromMask
(
d
b
)
)
)
;
}
namespace
detail
{
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
uint64_t
>
Gt
(
Vec128
<
uint64_t
>
a
Vec128
<
uint64_t
>
b
)
{
return
Vec128
<
uint64_t
>
(
vcgtq_u64
(
a
.
raw
b
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
1
>
Gt
(
Vec128
<
uint64_t
1
>
a
Vec128
<
uint64_t
1
>
b
)
{
return
Vec128
<
uint64_t
1
>
(
vcgt_u64
(
a
.
raw
b
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
Gt
(
Vec128
<
int64_t
>
a
Vec128
<
int64_t
>
b
)
{
return
Vec128
<
int64_t
>
(
vcgtq_s64
(
a
.
raw
b
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
1
>
Gt
(
Vec128
<
int64_t
1
>
a
Vec128
<
int64_t
1
>
b
)
{
return
Vec128
<
int64_t
1
>
(
vcgt_s64
(
a
.
raw
b
.
raw
)
)
;
}
#
endif
}
HWY_NEON_DEF_FUNCTION_UINT_8_16_32
(
Min
vmin
_
2
)
template
<
size_t
N
>
HWY_INLINE
Vec128
<
uint64_t
N
>
Min
(
const
Vec128
<
uint64_t
N
>
a
const
Vec128
<
uint64_t
N
>
b
)
{
#
if
HWY_ARCH_ARM_A64
return
IfThenElse
(
MaskFromVec
(
detail
:
:
Gt
(
a
b
)
)
b
a
)
;
#
else
const
Simd
<
uint64_t
N
>
du
;
const
Simd
<
int64_t
N
>
di
;
return
BitCast
(
du
BitCast
(
di
a
)
-
BitCast
(
di
detail
:
:
SaturatedSub
(
a
b
)
)
)
;
#
endif
}
HWY_NEON_DEF_FUNCTION_INT_8_16_32
(
Min
vmin
_
2
)
template
<
size_t
N
>
HWY_INLINE
Vec128
<
int64_t
N
>
Min
(
const
Vec128
<
int64_t
N
>
a
const
Vec128
<
int64_t
N
>
b
)
{
#
if
HWY_ARCH_ARM_A64
return
IfThenElse
(
MaskFromVec
(
detail
:
:
Gt
(
a
b
)
)
b
a
)
;
#
else
const
Vec128
<
int64_t
N
>
sign
=
detail
:
:
SaturatedSub
(
a
b
)
;
return
IfThenElse
(
MaskFromVec
(
BroadcastSignBit
(
sign
)
)
a
b
)
;
#
endif
}
#
if
HWY_ARCH_ARM_A64
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
Min
vminnm
_
2
)
#
else
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
Min
vmin
_
2
)
#
endif
HWY_NEON_DEF_FUNCTION_UINT_8_16_32
(
Max
vmax
_
2
)
template
<
size_t
N
>
HWY_INLINE
Vec128
<
uint64_t
N
>
Max
(
const
Vec128
<
uint64_t
N
>
a
const
Vec128
<
uint64_t
N
>
b
)
{
#
if
HWY_ARCH_ARM_A64
return
IfThenElse
(
MaskFromVec
(
detail
:
:
Gt
(
a
b
)
)
a
b
)
;
#
else
const
Simd
<
uint64_t
N
>
du
;
const
Simd
<
int64_t
N
>
di
;
return
BitCast
(
du
BitCast
(
di
b
)
+
BitCast
(
di
detail
:
:
SaturatedSub
(
a
b
)
)
)
;
#
endif
}
HWY_NEON_DEF_FUNCTION_INT_8_16_32
(
Max
vmax
_
2
)
template
<
size_t
N
>
HWY_INLINE
Vec128
<
int64_t
N
>
Max
(
const
Vec128
<
int64_t
N
>
a
const
Vec128
<
int64_t
N
>
b
)
{
#
if
HWY_ARCH_ARM_A64
return
IfThenElse
(
MaskFromVec
(
detail
:
:
Gt
(
a
b
)
)
a
b
)
;
#
else
const
Vec128
<
int64_t
N
>
sign
=
detail
:
:
SaturatedSub
(
a
b
)
;
return
IfThenElse
(
MaskFromVec
(
BroadcastSignBit
(
sign
)
)
b
a
)
;
#
endif
}
#
if
HWY_ARCH_ARM_A64
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
Max
vmaxnm
_
2
)
#
else
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
Max
vmax
_
2
)
#
endif
HWY_INLINE
Vec128
<
uint8_t
>
LoadU
(
Full128
<
uint8_t
>
const
uint8_t
*
HWY_RESTRICT
aligned
)
{
return
Vec128
<
uint8_t
>
(
vld1q_u8
(
aligned
)
)
;
}
HWY_INLINE
Vec128
<
uint16_t
>
LoadU
(
Full128
<
uint16_t
>
const
uint16_t
*
HWY_RESTRICT
aligned
)
{
return
Vec128
<
uint16_t
>
(
vld1q_u16
(
aligned
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
>
LoadU
(
Full128
<
uint32_t
>
const
uint32_t
*
HWY_RESTRICT
aligned
)
{
return
Vec128
<
uint32_t
>
(
vld1q_u32
(
aligned
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
>
LoadU
(
Full128
<
uint64_t
>
const
uint64_t
*
HWY_RESTRICT
aligned
)
{
return
Vec128
<
uint64_t
>
(
vld1q_u64
(
aligned
)
)
;
}
HWY_INLINE
Vec128
<
int8_t
>
LoadU
(
Full128
<
int8_t
>
const
int8_t
*
HWY_RESTRICT
aligned
)
{
return
Vec128
<
int8_t
>
(
vld1q_s8
(
aligned
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
>
LoadU
(
Full128
<
int16_t
>
const
int16_t
*
HWY_RESTRICT
aligned
)
{
return
Vec128
<
int16_t
>
(
vld1q_s16
(
aligned
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
LoadU
(
Full128
<
int32_t
>
const
int32_t
*
HWY_RESTRICT
aligned
)
{
return
Vec128
<
int32_t
>
(
vld1q_s32
(
aligned
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
LoadU
(
Full128
<
int64_t
>
const
int64_t
*
HWY_RESTRICT
aligned
)
{
return
Vec128
<
int64_t
>
(
vld1q_s64
(
aligned
)
)
;
}
HWY_INLINE
Vec128
<
float
>
LoadU
(
Full128
<
float
>
const
float
*
HWY_RESTRICT
aligned
)
{
return
Vec128
<
float
>
(
vld1q_f32
(
aligned
)
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
double
>
LoadU
(
Full128
<
double
>
const
double
*
HWY_RESTRICT
aligned
)
{
return
Vec128
<
double
>
(
vld1q_f64
(
aligned
)
)
;
}
#
endif
HWY_INLINE
Vec128
<
uint8_t
8
>
LoadU
(
Simd
<
uint8_t
8
>
const
uint8_t
*
HWY_RESTRICT
p
)
{
return
Vec128
<
uint8_t
8
>
(
vld1_u8
(
p
)
)
;
}
HWY_INLINE
Vec128
<
uint16_t
4
>
LoadU
(
Simd
<
uint16_t
4
>
const
uint16_t
*
HWY_RESTRICT
p
)
{
return
Vec128
<
uint16_t
4
>
(
vld1_u16
(
p
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
2
>
LoadU
(
Simd
<
uint32_t
2
>
const
uint32_t
*
HWY_RESTRICT
p
)
{
return
Vec128
<
uint32_t
2
>
(
vld1_u32
(
p
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
1
>
LoadU
(
Simd
<
uint64_t
1
>
const
uint64_t
*
HWY_RESTRICT
p
)
{
return
Vec128
<
uint64_t
1
>
(
vld1_u64
(
p
)
)
;
}
HWY_INLINE
Vec128
<
int8_t
8
>
LoadU
(
Simd
<
int8_t
8
>
const
int8_t
*
HWY_RESTRICT
p
)
{
return
Vec128
<
int8_t
8
>
(
vld1_s8
(
p
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
4
>
LoadU
(
Simd
<
int16_t
4
>
const
int16_t
*
HWY_RESTRICT
p
)
{
return
Vec128
<
int16_t
4
>
(
vld1_s16
(
p
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
2
>
LoadU
(
Simd
<
int32_t
2
>
const
int32_t
*
HWY_RESTRICT
p
)
{
return
Vec128
<
int32_t
2
>
(
vld1_s32
(
p
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
1
>
LoadU
(
Simd
<
int64_t
1
>
const
int64_t
*
HWY_RESTRICT
p
)
{
return
Vec128
<
int64_t
1
>
(
vld1_s64
(
p
)
)
;
}
HWY_INLINE
Vec128
<
float
2
>
LoadU
(
Simd
<
float
2
>
const
float
*
HWY_RESTRICT
p
)
{
return
Vec128
<
float
2
>
(
vld1_f32
(
p
)
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
double
1
>
LoadU
(
Simd
<
double
1
>
const
double
*
HWY_RESTRICT
p
)
{
return
Vec128
<
double
1
>
(
vld1_f64
(
p
)
)
;
}
#
endif
HWY_INLINE
Vec128
<
uint8_t
4
>
LoadU
(
Simd
<
uint8_t
4
>
d
const
uint8_t
*
HWY_RESTRICT
p
)
{
uint32x2_t
a
=
Undefined
(
d
)
.
raw
;
uint32x2_t
b
=
vld1_lane_u32
(
reinterpret_cast
<
const
uint32_t
*
>
(
p
)
a
0
)
;
return
Vec128
<
uint8_t
4
>
(
vreinterpret_u8_u32
(
b
)
)
;
}
HWY_INLINE
Vec128
<
uint16_t
2
>
LoadU
(
Simd
<
uint16_t
2
>
d
const
uint16_t
*
HWY_RESTRICT
p
)
{
uint32x2_t
a
=
Undefined
(
d
)
.
raw
;
uint32x2_t
b
=
vld1_lane_u32
(
reinterpret_cast
<
const
uint32_t
*
>
(
p
)
a
0
)
;
return
Vec128
<
uint16_t
2
>
(
vreinterpret_u16_u32
(
b
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
1
>
LoadU
(
Simd
<
uint32_t
1
>
d
const
uint32_t
*
HWY_RESTRICT
p
)
{
uint32x2_t
a
=
Undefined
(
d
)
.
raw
;
uint32x2_t
b
=
vld1_lane_u32
(
p
a
0
)
;
return
Vec128
<
uint32_t
1
>
(
b
)
;
}
HWY_INLINE
Vec128
<
int8_t
4
>
LoadU
(
Simd
<
int8_t
4
>
d
const
int8_t
*
HWY_RESTRICT
p
)
{
int32x2_t
a
=
Undefined
(
d
)
.
raw
;
int32x2_t
b
=
vld1_lane_s32
(
reinterpret_cast
<
const
int32_t
*
>
(
p
)
a
0
)
;
return
Vec128
<
int8_t
4
>
(
vreinterpret_s8_s32
(
b
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
2
>
LoadU
(
Simd
<
int16_t
2
>
d
const
int16_t
*
HWY_RESTRICT
p
)
{
int32x2_t
a
=
Undefined
(
d
)
.
raw
;
int32x2_t
b
=
vld1_lane_s32
(
reinterpret_cast
<
const
int32_t
*
>
(
p
)
a
0
)
;
return
Vec128
<
int16_t
2
>
(
vreinterpret_s16_s32
(
b
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
1
>
LoadU
(
Simd
<
int32_t
1
>
d
const
int32_t
*
HWY_RESTRICT
p
)
{
int32x2_t
a
=
Undefined
(
d
)
.
raw
;
int32x2_t
b
=
vld1_lane_s32
(
p
a
0
)
;
return
Vec128
<
int32_t
1
>
(
b
)
;
}
HWY_INLINE
Vec128
<
float
1
>
LoadU
(
Simd
<
float
1
>
d
const
float
*
HWY_RESTRICT
p
)
{
float32x2_t
a
=
Undefined
(
d
)
.
raw
;
float32x2_t
b
=
vld1_lane_f32
(
p
a
0
)
;
return
Vec128
<
float
1
>
(
b
)
;
}
HWY_INLINE
Vec128
<
uint8_t
2
>
LoadU
(
Simd
<
uint8_t
2
>
d
const
uint8_t
*
HWY_RESTRICT
p
)
{
uint16x4_t
a
=
Undefined
(
d
)
.
raw
;
uint16x4_t
b
=
vld1_lane_u16
(
reinterpret_cast
<
const
uint16_t
*
>
(
p
)
a
0
)
;
return
Vec128
<
uint8_t
2
>
(
vreinterpret_u8_u16
(
b
)
)
;
}
HWY_INLINE
Vec128
<
uint16_t
1
>
LoadU
(
Simd
<
uint16_t
1
>
d
const
uint16_t
*
HWY_RESTRICT
p
)
{
uint16x4_t
a
=
Undefined
(
d
)
.
raw
;
uint16x4_t
b
=
vld1_lane_u16
(
p
a
0
)
;
return
Vec128
<
uint16_t
1
>
(
b
)
;
}
HWY_INLINE
Vec128
<
int8_t
2
>
LoadU
(
Simd
<
int8_t
2
>
d
const
int8_t
*
HWY_RESTRICT
p
)
{
int16x4_t
a
=
Undefined
(
d
)
.
raw
;
int16x4_t
b
=
vld1_lane_s16
(
reinterpret_cast
<
const
int16_t
*
>
(
p
)
a
0
)
;
return
Vec128
<
int8_t
2
>
(
vreinterpret_s8_s16
(
b
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
1
>
LoadU
(
Simd
<
int16_t
1
>
d
const
int16_t
*
HWY_RESTRICT
p
)
{
int16x4_t
a
=
Undefined
(
d
)
.
raw
;
int16x4_t
b
=
vld1_lane_s16
(
p
a
0
)
;
return
Vec128
<
int16_t
1
>
(
b
)
;
}
HWY_INLINE
Vec128
<
uint8_t
1
>
LoadU
(
Simd
<
uint8_t
1
>
d
const
uint8_t
*
HWY_RESTRICT
p
)
{
uint8x8_t
a
=
Undefined
(
d
)
.
raw
;
uint8x8_t
b
=
vld1_lane_u8
(
p
a
0
)
;
return
Vec128
<
uint8_t
1
>
(
b
)
;
}
HWY_INLINE
Vec128
<
int8_t
1
>
LoadU
(
Simd
<
int8_t
1
>
d
const
int8_t
*
HWY_RESTRICT
p
)
{
int8x8_t
a
=
Undefined
(
d
)
.
raw
;
int8x8_t
b
=
vld1_lane_s8
(
p
a
0
)
;
return
Vec128
<
int8_t
1
>
(
b
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float16_t
N
>
LoadU
(
Simd
<
float16_t
N
>
const
float16_t
*
HWY_RESTRICT
p
)
{
const
Simd
<
uint16_t
N
>
du16
;
const
auto
pu16
=
reinterpret_cast
<
const
uint16_t
*
>
(
p
)
;
return
Vec128
<
float16_t
N
>
(
LoadU
(
du16
pu16
)
.
raw
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
Load
(
Simd
<
T
N
>
d
const
T
*
HWY_RESTRICT
p
)
{
return
LoadU
(
d
p
)
;
}
template
<
typename
T
size_t
N
HWY_IF_LE128
(
T
N
)
>
HWY_INLINE
Vec128
<
T
N
>
LoadDup128
(
Simd
<
T
N
>
d
const
T
*
const
HWY_RESTRICT
p
)
{
return
LoadU
(
d
p
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
uint8_t
>
v
Full128
<
uint8_t
>
uint8_t
*
HWY_RESTRICT
aligned
)
{
vst1q_u8
(
aligned
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
uint16_t
>
v
Full128
<
uint16_t
>
uint16_t
*
HWY_RESTRICT
aligned
)
{
vst1q_u16
(
aligned
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
uint32_t
>
v
Full128
<
uint32_t
>
uint32_t
*
HWY_RESTRICT
aligned
)
{
vst1q_u32
(
aligned
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
uint64_t
>
v
Full128
<
uint64_t
>
uint64_t
*
HWY_RESTRICT
aligned
)
{
vst1q_u64
(
aligned
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
int8_t
>
v
Full128
<
int8_t
>
int8_t
*
HWY_RESTRICT
aligned
)
{
vst1q_s8
(
aligned
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
int16_t
>
v
Full128
<
int16_t
>
int16_t
*
HWY_RESTRICT
aligned
)
{
vst1q_s16
(
aligned
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
int32_t
>
v
Full128
<
int32_t
>
int32_t
*
HWY_RESTRICT
aligned
)
{
vst1q_s32
(
aligned
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
int64_t
>
v
Full128
<
int64_t
>
int64_t
*
HWY_RESTRICT
aligned
)
{
vst1q_s64
(
aligned
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
float
>
v
Full128
<
float
>
float
*
HWY_RESTRICT
aligned
)
{
vst1q_f32
(
aligned
v
.
raw
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
void
StoreU
(
const
Vec128
<
double
>
v
Full128
<
double
>
double
*
HWY_RESTRICT
aligned
)
{
vst1q_f64
(
aligned
v
.
raw
)
;
}
#
endif
HWY_INLINE
void
StoreU
(
const
Vec128
<
uint8_t
8
>
v
Simd
<
uint8_t
8
>
uint8_t
*
HWY_RESTRICT
p
)
{
vst1_u8
(
p
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
uint16_t
4
>
v
Simd
<
uint16_t
4
>
uint16_t
*
HWY_RESTRICT
p
)
{
vst1_u16
(
p
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
uint32_t
2
>
v
Simd
<
uint32_t
2
>
uint32_t
*
HWY_RESTRICT
p
)
{
vst1_u32
(
p
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
uint64_t
1
>
v
Simd
<
uint64_t
1
>
uint64_t
*
HWY_RESTRICT
p
)
{
vst1_u64
(
p
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
int8_t
8
>
v
Simd
<
int8_t
8
>
int8_t
*
HWY_RESTRICT
p
)
{
vst1_s8
(
p
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
int16_t
4
>
v
Simd
<
int16_t
4
>
int16_t
*
HWY_RESTRICT
p
)
{
vst1_s16
(
p
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
int32_t
2
>
v
Simd
<
int32_t
2
>
int32_t
*
HWY_RESTRICT
p
)
{
vst1_s32
(
p
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
int64_t
1
>
v
Simd
<
int64_t
1
>
int64_t
*
HWY_RESTRICT
p
)
{
vst1_s64
(
p
v
.
raw
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
float
2
>
v
Simd
<
float
2
>
float
*
HWY_RESTRICT
p
)
{
vst1_f32
(
p
v
.
raw
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
void
StoreU
(
const
Vec128
<
double
1
>
v
Simd
<
double
1
>
double
*
HWY_RESTRICT
p
)
{
vst1_f64
(
p
v
.
raw
)
;
}
#
endif
HWY_INLINE
void
StoreU
(
const
Vec128
<
uint8_t
4
>
v
Simd
<
uint8_t
4
>
uint8_t
*
HWY_RESTRICT
p
)
{
uint32x2_t
a
=
vreinterpret_u32_u8
(
v
.
raw
)
;
vst1_lane_u32
(
p
a
0
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
uint16_t
2
>
v
Simd
<
uint16_t
2
>
uint16_t
*
HWY_RESTRICT
p
)
{
uint32x2_t
a
=
vreinterpret_u32_u16
(
v
.
raw
)
;
vst1_lane_u32
(
p
a
0
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
uint32_t
1
>
v
Simd
<
uint32_t
1
>
uint32_t
*
HWY_RESTRICT
p
)
{
vst1_lane_u32
(
p
v
.
raw
0
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
int8_t
4
>
v
Simd
<
int8_t
4
>
int8_t
*
HWY_RESTRICT
p
)
{
int32x2_t
a
=
vreinterpret_s32_s8
(
v
.
raw
)
;
vst1_lane_s32
(
p
a
0
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
int16_t
2
>
v
Simd
<
int16_t
2
>
int16_t
*
HWY_RESTRICT
p
)
{
int32x2_t
a
=
vreinterpret_s32_s16
(
v
.
raw
)
;
vst1_lane_s32
(
p
a
0
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
int32_t
1
>
v
Simd
<
int32_t
1
>
int32_t
*
HWY_RESTRICT
p
)
{
vst1_lane_s32
(
p
v
.
raw
0
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
float
1
>
v
Simd
<
float
1
>
float
*
HWY_RESTRICT
p
)
{
vst1_lane_f32
(
p
v
.
raw
0
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
uint8_t
2
>
v
Simd
<
uint8_t
2
>
uint8_t
*
HWY_RESTRICT
p
)
{
uint16x4_t
a
=
vreinterpret_u16_u8
(
v
.
raw
)
;
vst1_lane_u16
(
p
a
0
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
uint16_t
1
>
v
Simd
<
uint16_t
1
>
uint16_t
*
HWY_RESTRICT
p
)
{
vst1_lane_u16
(
p
v
.
raw
0
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
int8_t
2
>
v
Simd
<
int8_t
2
>
int8_t
*
HWY_RESTRICT
p
)
{
int16x4_t
a
=
vreinterpret_s16_s8
(
v
.
raw
)
;
vst1_lane_s16
(
p
a
0
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
int16_t
1
>
v
Simd
<
int16_t
1
>
int16_t
*
HWY_RESTRICT
p
)
{
vst1_lane_s16
(
p
v
.
raw
0
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
uint8_t
1
>
v
Simd
<
uint8_t
1
>
uint8_t
*
HWY_RESTRICT
p
)
{
vst1_lane_u8
(
p
v
.
raw
0
)
;
}
HWY_INLINE
void
StoreU
(
const
Vec128
<
int8_t
1
>
v
Simd
<
int8_t
1
>
int8_t
*
HWY_RESTRICT
p
)
{
vst1_lane_s8
(
p
v
.
raw
0
)
;
}
template
<
size_t
N
>
HWY_API
void
StoreU
(
Vec128
<
float16_t
N
>
v
Simd
<
float16_t
N
>
float16_t
*
HWY_RESTRICT
p
)
{
const
Simd
<
uint16_t
N
>
du16
;
const
auto
pu16
=
reinterpret_cast
<
uint16_t
*
>
(
p
)
;
return
StoreU
(
Vec128
<
uint16_t
N
>
(
v
.
raw
)
du16
pu16
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
void
Store
(
Vec128
<
T
N
>
v
Simd
<
T
N
>
d
T
*
HWY_RESTRICT
p
)
{
StoreU
(
v
d
p
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
void
Stream
(
const
Vec128
<
T
N
>
v
Simd
<
T
N
>
d
T
*
HWY_RESTRICT
aligned
)
{
Store
(
v
d
aligned
)
;
}
HWY_INLINE
Vec128
<
uint16_t
>
PromoteTo
(
Full128
<
uint16_t
>
const
Vec128
<
uint8_t
8
>
v
)
{
return
Vec128
<
uint16_t
>
(
vmovl_u8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
>
PromoteTo
(
Full128
<
uint32_t
>
const
Vec128
<
uint8_t
4
>
v
)
{
uint16x8_t
a
=
vmovl_u8
(
v
.
raw
)
;
return
Vec128
<
uint32_t
>
(
vmovl_u16
(
vget_low_u16
(
a
)
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
>
PromoteTo
(
Full128
<
uint32_t
>
const
Vec128
<
uint16_t
4
>
v
)
{
return
Vec128
<
uint32_t
>
(
vmovl_u16
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
>
PromoteTo
(
Full128
<
uint64_t
>
const
Vec128
<
uint32_t
2
>
v
)
{
return
Vec128
<
uint64_t
>
(
vmovl_u32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
>
PromoteTo
(
Full128
<
int16_t
>
const
Vec128
<
uint8_t
8
>
v
)
{
return
Vec128
<
int16_t
>
(
vmovl_u8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
PromoteTo
(
Full128
<
int32_t
>
const
Vec128
<
uint8_t
4
>
v
)
{
uint16x8_t
a
=
vmovl_u8
(
v
.
raw
)
;
return
Vec128
<
int32_t
>
(
vreinterpretq_s32_u16
(
vmovl_u16
(
vget_low_u16
(
a
)
)
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
PromoteTo
(
Full128
<
int32_t
>
const
Vec128
<
uint16_t
4
>
v
)
{
return
Vec128
<
int32_t
>
(
vmovl_u16
(
v
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint16_t
N
)
>
HWY_INLINE
Vec128
<
uint16_t
N
>
PromoteTo
(
Simd
<
uint16_t
N
>
const
Vec128
<
uint8_t
N
>
v
)
{
return
Vec128
<
uint16_t
N
>
(
vget_low_u16
(
vmovl_u8
(
v
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint32_t
N
)
>
HWY_INLINE
Vec128
<
uint32_t
N
>
PromoteTo
(
Simd
<
uint32_t
N
>
const
Vec128
<
uint8_t
N
>
v
)
{
uint16x8_t
a
=
vmovl_u8
(
v
.
raw
)
;
return
Vec128
<
uint32_t
N
>
(
vget_low_u32
(
vmovl_u16
(
vget_low_u16
(
a
)
)
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
uint32_t
N
>
PromoteTo
(
Simd
<
uint32_t
N
>
const
Vec128
<
uint16_t
N
>
v
)
{
return
Vec128
<
uint32_t
N
>
(
vget_low_u32
(
vmovl_u16
(
v
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint64_t
N
)
>
HWY_INLINE
Vec128
<
uint64_t
N
>
PromoteTo
(
Simd
<
uint64_t
N
>
const
Vec128
<
uint32_t
N
>
v
)
{
return
Vec128
<
uint64_t
N
>
(
vget_low_u64
(
vmovl_u32
(
v
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int16_t
N
)
>
HWY_INLINE
Vec128
<
int16_t
N
>
PromoteTo
(
Simd
<
int16_t
N
>
const
Vec128
<
uint8_t
N
>
v
)
{
return
Vec128
<
int16_t
N
>
(
vget_low_s16
(
vmovl_u8
(
v
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
int32_t
N
>
PromoteTo
(
Simd
<
int32_t
N
>
const
Vec128
<
uint8_t
N
>
v
)
{
uint16x8_t
a
=
vmovl_u8
(
v
.
raw
)
;
uint32x4_t
b
=
vmovl_u16
(
vget_low_u16
(
a
)
)
;
return
Vec128
<
int32_t
N
>
(
vget_low_s32
(
vreinterpretq_s32_u32
(
b
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
int32_t
N
>
PromoteTo
(
Simd
<
int32_t
N
>
const
Vec128
<
uint16_t
N
>
v
)
{
uint32x4_t
a
=
vmovl_u16
(
v
.
raw
)
;
return
Vec128
<
int32_t
N
>
(
vget_low_s32
(
vreinterpretq_s32_u32
(
a
)
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
>
PromoteTo
(
Full128
<
int16_t
>
const
Vec128
<
int8_t
8
>
v
)
{
return
Vec128
<
int16_t
>
(
vmovl_s8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
PromoteTo
(
Full128
<
int32_t
>
const
Vec128
<
int8_t
4
>
v
)
{
int16x8_t
a
=
vmovl_s8
(
v
.
raw
)
;
return
Vec128
<
int32_t
>
(
vmovl_s16
(
vget_low_s16
(
a
)
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
PromoteTo
(
Full128
<
int32_t
>
const
Vec128
<
int16_t
4
>
v
)
{
return
Vec128
<
int32_t
>
(
vmovl_s16
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
PromoteTo
(
Full128
<
int64_t
>
const
Vec128
<
int32_t
2
>
v
)
{
return
Vec128
<
int64_t
>
(
vmovl_s32
(
v
.
raw
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
int16_t
N
>
PromoteTo
(
Simd
<
int16_t
N
>
const
Vec128
<
int8_t
N
>
v
)
{
return
Vec128
<
int16_t
N
>
(
vget_low_s16
(
vmovl_s8
(
v
.
raw
)
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
int32_t
N
>
PromoteTo
(
Simd
<
int32_t
N
>
const
Vec128
<
int8_t
N
>
v
)
{
int16x8_t
a
=
vmovl_s8
(
v
.
raw
)
;
int32x4_t
b
=
vmovl_s16
(
vget_low_s16
(
a
)
)
;
return
Vec128
<
int32_t
N
>
(
vget_low_s32
(
b
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
int32_t
N
>
PromoteTo
(
Simd
<
int32_t
N
>
const
Vec128
<
int16_t
N
>
v
)
{
return
Vec128
<
int32_t
N
>
(
vget_low_s32
(
vmovl_s16
(
v
.
raw
)
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
int64_t
N
>
PromoteTo
(
Simd
<
int64_t
N
>
const
Vec128
<
int32_t
N
>
v
)
{
return
Vec128
<
int64_t
N
>
(
vget_low_s64
(
vmovl_s32
(
v
.
raw
)
)
)
;
}
#
if
__ARM_FP
&
2
HWY_INLINE
Vec128
<
float
>
PromoteTo
(
Full128
<
float
>
const
Vec128
<
float16_t
4
>
v
)
{
return
Vec128
<
float
>
(
vcvt_f32_f16
(
vreinterpret_f16_u16
(
v
.
raw
)
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float
N
>
PromoteTo
(
Simd
<
float
N
>
const
Vec128
<
float16_t
N
>
v
)
{
return
Vec128
<
float
N
>
(
vget_low_f32
(
vcvt_f32_f16
(
v
.
raw
)
)
)
;
}
#
else
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float
N
>
PromoteTo
(
Simd
<
float
N
>
const
Vec128
<
float16_t
N
>
v
)
{
const
Simd
<
int32_t
N
>
di32
;
const
Simd
<
uint32_t
N
>
du32
;
const
Simd
<
float
N
>
df32
;
const
auto
bits16
=
PromoteTo
(
du32
Vec128
<
uint16_t
N
>
{
v
.
raw
}
)
;
const
auto
sign
=
ShiftRight
<
15
>
(
bits16
)
;
const
auto
biased_exp
=
ShiftRight
<
10
>
(
bits16
)
&
Set
(
du32
0x1F
)
;
const
auto
mantissa
=
bits16
&
Set
(
du32
0x3FF
)
;
const
auto
subnormal
=
BitCast
(
du32
ConvertTo
(
df32
BitCast
(
di32
mantissa
)
)
*
Set
(
df32
1
.
0f
/
16384
/
1024
)
)
;
const
auto
biased_exp32
=
biased_exp
+
Set
(
du32
127
-
15
)
;
const
auto
mantissa32
=
ShiftLeft
<
23
-
10
>
(
mantissa
)
;
const
auto
normal
=
ShiftLeft
<
23
>
(
biased_exp32
)
|
mantissa32
;
const
auto
bits32
=
IfThenElse
(
biased_exp
=
=
Zero
(
du32
)
subnormal
normal
)
;
return
BitCast
(
df32
ShiftLeft
<
31
>
(
sign
)
|
bits32
)
;
}
#
endif
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
double
>
PromoteTo
(
Full128
<
double
>
const
Vec128
<
float
2
>
v
)
{
return
Vec128
<
double
>
(
vcvt_f64_f32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
double
1
>
PromoteTo
(
Simd
<
double
1
>
const
Vec128
<
float
1
>
v
)
{
return
Vec128
<
double
1
>
(
vget_low_f64
(
vcvt_f64_f32
(
v
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
double
>
PromoteTo
(
Full128
<
double
>
const
Vec128
<
int32_t
2
>
v
)
{
const
int64x2_t
i64
=
vmovl_s32
(
v
.
raw
)
;
return
Vec128
<
double
>
(
vcvtq_f64_s64
(
i64
)
)
;
}
HWY_INLINE
Vec128
<
double
1
>
PromoteTo
(
Simd
<
double
1
>
const
Vec128
<
int32_t
1
>
v
)
{
const
int64x1_t
i64
=
vget_low_s64
(
vmovl_s32
(
v
.
raw
)
)
;
return
Vec128
<
double
1
>
(
vcvt_f64_s64
(
i64
)
)
;
}
#
endif
HWY_INLINE
Vec128
<
uint16_t
4
>
DemoteTo
(
Simd
<
uint16_t
4
>
const
Vec128
<
int32_t
>
v
)
{
return
Vec128
<
uint16_t
4
>
(
vqmovun_s32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
4
>
DemoteTo
(
Simd
<
int16_t
4
>
const
Vec128
<
int32_t
>
v
)
{
return
Vec128
<
int16_t
4
>
(
vqmovn_s32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint8_t
4
>
DemoteTo
(
Simd
<
uint8_t
4
>
const
Vec128
<
int32_t
>
v
)
{
const
uint16x4_t
a
=
vqmovun_s32
(
v
.
raw
)
;
return
Vec128
<
uint8_t
4
>
(
vqmovn_u16
(
vcombine_u16
(
a
a
)
)
)
;
}
HWY_INLINE
Vec128
<
uint8_t
8
>
DemoteTo
(
Simd
<
uint8_t
8
>
const
Vec128
<
int16_t
>
v
)
{
return
Vec128
<
uint8_t
8
>
(
vqmovun_s16
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int8_t
4
>
DemoteTo
(
Simd
<
int8_t
4
>
const
Vec128
<
int32_t
>
v
)
{
const
int16x4_t
a
=
vqmovn_s32
(
v
.
raw
)
;
return
Vec128
<
int8_t
4
>
(
vqmovn_s16
(
vcombine_s16
(
a
a
)
)
)
;
}
HWY_INLINE
Vec128
<
int8_t
8
>
DemoteTo
(
Simd
<
int8_t
8
>
const
Vec128
<
int16_t
>
v
)
{
return
Vec128
<
int8_t
8
>
(
vqmovn_s16
(
v
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
uint16_t
N
>
DemoteTo
(
Simd
<
uint16_t
N
>
const
Vec128
<
int32_t
N
>
v
)
{
return
Vec128
<
uint16_t
N
>
(
vqmovun_s32
(
vcombine_s32
(
v
.
raw
v
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
int16_t
N
>
DemoteTo
(
Simd
<
int16_t
N
>
const
Vec128
<
int32_t
N
>
v
)
{
return
Vec128
<
int16_t
N
>
(
vqmovn_s32
(
vcombine_s32
(
v
.
raw
v
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
uint8_t
N
>
DemoteTo
(
Simd
<
uint8_t
N
>
const
Vec128
<
int32_t
N
>
v
)
{
const
uint16x4_t
a
=
vqmovun_s32
(
vcombine_s32
(
v
.
raw
v
.
raw
)
)
;
return
Vec128
<
uint8_t
N
>
(
vqmovn_u16
(
vcombine_u16
(
a
a
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int16_t
N
)
>
HWY_INLINE
Vec128
<
uint8_t
N
>
DemoteTo
(
Simd
<
uint8_t
N
>
const
Vec128
<
int16_t
N
>
v
)
{
return
Vec128
<
uint8_t
N
>
(
vqmovun_s16
(
vcombine_s16
(
v
.
raw
v
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
int8_t
N
>
DemoteTo
(
Simd
<
int8_t
N
>
const
Vec128
<
int32_t
N
>
v
)
{
const
int16x4_t
a
=
vqmovn_s32
(
vcombine_s32
(
v
.
raw
v
.
raw
)
)
;
return
Vec128
<
int8_t
N
>
(
vqmovn_s16
(
vcombine_s16
(
a
a
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int16_t
N
)
>
HWY_INLINE
Vec128
<
int8_t
N
>
DemoteTo
(
Simd
<
int8_t
N
>
const
Vec128
<
int16_t
N
>
v
)
{
return
Vec128
<
int8_t
N
>
(
vqmovn_s16
(
vcombine_s16
(
v
.
raw
v
.
raw
)
)
)
;
}
#
if
__ARM_FP
&
2
HWY_INLINE
Vec128
<
float16_t
4
>
DemoteTo
(
Simd
<
float16_t
4
>
const
Vec128
<
float
>
v
)
{
return
Vec128
<
float16_t
4
>
{
vreinterpret_u16_f16
(
vcvt_f16_f32
(
v
.
raw
)
)
}
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float16_t
N
>
DemoteTo
(
Simd
<
float16_t
N
>
const
Vec128
<
float
N
>
v
)
{
return
Vec128
<
float16_t
N
>
{
vcvt_f16_f32
(
vcombine_f32
(
v
.
raw
v
.
raw
)
)
}
;
}
#
else
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float16_t
N
>
DemoteTo
(
Simd
<
float16_t
N
>
const
Vec128
<
float
N
>
v
)
{
const
Simd
<
int32_t
N
>
di
;
const
Simd
<
uint32_t
N
>
du
;
const
Simd
<
uint16_t
N
>
du16
;
const
auto
bits32
=
BitCast
(
du
v
)
;
const
auto
sign
=
ShiftRight
<
31
>
(
bits32
)
;
const
auto
biased_exp32
=
ShiftRight
<
23
>
(
bits32
)
&
Set
(
du
0xFF
)
;
const
auto
mantissa32
=
bits32
&
Set
(
du
0x7FFFFF
)
;
const
auto
k15
=
Set
(
di
15
)
;
const
auto
exp
=
Min
(
BitCast
(
di
biased_exp32
)
-
Set
(
di
127
)
k15
)
;
const
auto
is_tiny
=
exp
<
Set
(
di
-
24
)
;
const
auto
is_subnormal
=
exp
<
Set
(
di
-
14
)
;
const
auto
biased_exp16
=
BitCast
(
du
IfThenZeroElse
(
is_subnormal
exp
+
k15
)
)
;
const
auto
sub_exp
=
BitCast
(
du
Set
(
di
-
14
)
-
exp
)
;
const
auto
sub_m
=
(
Set
(
du
1
)
<
<
(
Set
(
du
10
)
-
sub_exp
)
)
+
(
mantissa32
>
>
(
Set
(
du
13
)
+
sub_exp
)
)
;
const
auto
mantissa16
=
IfThenElse
(
RebindMask
(
du
is_subnormal
)
sub_m
ShiftRight
<
13
>
(
mantissa32
)
)
;
const
auto
sign16
=
ShiftLeft
<
15
>
(
sign
)
;
const
auto
normal16
=
sign16
|
ShiftLeft
<
10
>
(
biased_exp16
)
|
mantissa16
;
const
auto
bits16
=
IfThenZeroElse
(
is_tiny
BitCast
(
di
normal16
)
)
;
return
Vec128
<
float16_t
N
>
(
DemoteTo
(
du16
bits16
)
.
raw
)
;
}
#
endif
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
float
2
>
DemoteTo
(
Simd
<
float
2
>
const
Vec128
<
double
>
v
)
{
return
Vec128
<
float
2
>
(
vcvt_f32_f64
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
float
1
>
DemoteTo
(
Simd
<
float
1
>
const
Vec128
<
double
1
>
v
)
{
return
Vec128
<
float
1
>
(
vcvt_f32_f64
(
vcombine_f64
(
v
.
raw
v
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
2
>
DemoteTo
(
Simd
<
int32_t
2
>
const
Vec128
<
double
>
v
)
{
const
int64x2_t
i64
=
vcvtq_s64_f64
(
v
.
raw
)
;
return
Vec128
<
int32_t
2
>
(
vqmovn_s64
(
i64
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
1
>
DemoteTo
(
Simd
<
int32_t
1
>
const
Vec128
<
double
1
>
v
)
{
const
int64x1_t
i64
=
vcvt_s64_f64
(
v
.
raw
)
;
const
int64x2_t
i64x2
=
vcombine_s64
(
i64
i64
)
;
return
Vec128
<
int32_t
1
>
(
vqmovn_s64
(
i64x2
)
)
;
}
#
endif
HWY_API
Vec128
<
uint8_t
4
>
U8FromU32
(
const
Vec128
<
uint32_t
>
v
)
{
const
uint8x16_t
org_v
=
detail
:
:
BitCastToByte
(
v
)
.
raw
;
const
uint8x16_t
w
=
vuzp1q_u8
(
org_v
org_v
)
;
return
Vec128
<
uint8_t
4
>
(
vget_low_u8
(
vuzp1q_u8
(
w
w
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint32_t
N
)
>
HWY_API
Vec128
<
uint8_t
N
>
U8FromU32
(
const
Vec128
<
uint32_t
N
>
v
)
{
const
uint8x8_t
org_v
=
detail
:
:
BitCastToByte
(
v
)
.
raw
;
const
uint8x8_t
w
=
vuzp1_u8
(
org_v
org_v
)
;
return
Vec128
<
uint8_t
N
>
(
vuzp1_u8
(
w
w
)
)
;
}
HWY_DIAGNOSTICS
(
push
)
HWY_DIAGNOSTICS_OFF
(
disable
:
4701
ignored
"
-
Wuninitialized
"
)
template
<
size_t
N
>
HWY_INLINE
Vec128
<
uint8_t
N
>
DemoteTo
(
Simd
<
uint8_t
N
>
const
Vec128
<
int32_t
>
v
)
{
Vec128
<
uint16_t
N
>
a
=
DemoteTo
(
Simd
<
uint16_t
N
>
(
)
v
)
;
Vec128
<
uint16_t
N
>
b
;
uint16x8_t
c
=
vcombine_u16
(
a
.
raw
b
.
raw
)
;
return
Vec128
<
uint8_t
N
>
(
vqmovn_u16
(
c
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
int8_t
N
>
DemoteTo
(
Simd
<
int8_t
N
>
const
Vec128
<
int32_t
>
v
)
{
Vec128
<
int16_t
N
>
a
=
DemoteTo
(
Simd
<
int16_t
N
>
(
)
v
)
;
Vec128
<
int16_t
N
>
b
;
int16x8_t
c
=
vcombine_s16
(
a
.
raw
b
.
raw
)
;
return
Vec128
<
int8_t
N
>
(
vqmovn_s16
(
c
)
)
;
}
HWY_DIAGNOSTICS
(
pop
)
HWY_INLINE
Vec128
<
float
>
ConvertTo
(
Full128
<
float
>
const
Vec128
<
int32_t
>
v
)
{
return
Vec128
<
float
>
(
vcvtq_f32_s32
(
v
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
float
N
>
ConvertTo
(
Simd
<
float
N
>
const
Vec128
<
int32_t
N
>
v
)
{
return
Vec128
<
float
N
>
(
vcvt_f32_s32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
ConvertTo
(
Full128
<
int32_t
>
const
Vec128
<
float
>
v
)
{
return
Vec128
<
int32_t
>
(
vcvtq_s32_f32
(
v
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
float
N
)
>
HWY_INLINE
Vec128
<
int32_t
N
>
ConvertTo
(
Simd
<
int32_t
N
>
const
Vec128
<
float
N
>
v
)
{
return
Vec128
<
int32_t
N
>
(
vcvt_s32_f32
(
v
.
raw
)
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
double
>
ConvertTo
(
Full128
<
double
>
const
Vec128
<
int64_t
>
v
)
{
return
Vec128
<
double
>
(
vcvtq_f64_s64
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
double
1
>
ConvertTo
(
Simd
<
double
1
>
const
Vec128
<
int64_t
1
>
v
)
{
return
Vec128
<
double
1
>
(
vcvt_f64_s64
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
ConvertTo
(
Full128
<
int64_t
>
const
Vec128
<
double
>
v
)
{
return
Vec128
<
int64_t
>
(
vcvtq_s64_f64
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
1
>
ConvertTo
(
Simd
<
int64_t
1
>
const
Vec128
<
double
1
>
v
)
{
return
Vec128
<
int64_t
1
>
(
vcvt_s64_f64
(
v
.
raw
)
)
;
}
#
endif
#
if
HWY_ARCH_ARM_A64
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
Round
vrndn
_
1
)
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
Trunc
vrnd
_
1
)
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
Ceil
vrndp
_
1
)
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
(
Floor
vrndm
_
1
)
#
else
namespace
detail
{
template
<
size_t
N
>
HWY_API
Mask128
<
float
N
>
UseInt
(
const
Vec128
<
float
N
>
v
)
{
return
Abs
(
v
)
<
Set
(
Simd
<
float
N
>
(
)
MantissaEnd
<
float
>
(
)
)
;
}
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float
N
>
Trunc
(
const
Vec128
<
float
N
>
v
)
{
const
Simd
<
float
N
>
df
;
const
RebindToSigned
<
decltype
(
df
)
>
di
;
const
auto
integer
=
ConvertTo
(
di
v
)
;
const
auto
int_f
=
ConvertTo
(
df
integer
)
;
return
IfThenElse
(
detail
:
:
UseInt
(
v
)
int_f
v
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float
N
>
Round
(
const
Vec128
<
float
N
>
v
)
{
const
Simd
<
float
N
>
df
;
const
auto
max
=
Set
(
df
MantissaEnd
<
float
>
(
)
)
;
const
auto
large
=
CopySignToAbs
(
max
v
)
;
const
auto
added
=
large
+
v
;
const
auto
rounded
=
added
-
large
;
return
IfThenElse
(
Abs
(
v
)
<
max
rounded
v
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float
N
>
Ceil
(
const
Vec128
<
float
N
>
v
)
{
const
Simd
<
float
N
>
df
;
const
RebindToSigned
<
decltype
(
df
)
>
di
;
const
auto
integer
=
ConvertTo
(
di
v
)
;
const
auto
int_f
=
ConvertTo
(
df
integer
)
;
const
auto
neg1
=
ConvertTo
(
df
VecFromMask
(
di
RebindMask
(
di
int_f
<
v
)
)
)
;
return
IfThenElse
(
detail
:
:
UseInt
(
v
)
int_f
-
neg1
v
)
;
}
template
<
size_t
N
>
HWY_INLINE
Vec128
<
float
N
>
Floor
(
const
Vec128
<
float
N
>
v
)
{
const
Simd
<
float
N
>
df
;
const
Simd
<
int32_t
N
>
di
;
const
auto
integer
=
ConvertTo
(
di
v
)
;
const
auto
int_f
=
ConvertTo
(
df
integer
)
;
const
auto
neg1
=
ConvertTo
(
df
VecFromMask
(
di
RebindMask
(
di
int_f
>
v
)
)
)
;
return
IfThenElse
(
detail
:
:
UseInt
(
v
)
int_f
+
neg1
v
)
;
}
#
endif
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
int32_t
>
NearestInt
(
const
Vec128
<
float
>
v
)
{
return
Vec128
<
int32_t
>
(
vcvtnq_s32_f32
(
v
.
raw
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
float
N
)
>
HWY_INLINE
Vec128
<
int32_t
N
>
NearestInt
(
const
Vec128
<
float
N
>
v
)
{
return
Vec128
<
int32_t
N
>
(
vcvtn_s32_f32
(
v
.
raw
)
)
;
}
#
else
template
<
size_t
N
>
HWY_INLINE
Vec128
<
int32_t
N
>
NearestInt
(
const
Vec128
<
float
N
>
v
)
{
const
Simd
<
int32_t
N
>
di
;
return
ConvertTo
(
di
Round
(
v
)
)
;
}
#
endif
template
<
typename
T
size_t
N
HWY_IF_LE64
(
uint8_t
N
)
>
HWY_INLINE
Vec128
<
T
N
/
2
>
LowerHalf
(
const
Vec128
<
T
N
>
v
)
{
return
Vec128
<
T
N
/
2
>
(
v
.
raw
)
;
}
HWY_INLINE
Vec128
<
uint8_t
8
>
LowerHalf
(
const
Vec128
<
uint8_t
>
v
)
{
return
Vec128
<
uint8_t
8
>
(
vget_low_u8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint16_t
4
>
LowerHalf
(
const
Vec128
<
uint16_t
>
v
)
{
return
Vec128
<
uint16_t
4
>
(
vget_low_u16
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
2
>
LowerHalf
(
const
Vec128
<
uint32_t
>
v
)
{
return
Vec128
<
uint32_t
2
>
(
vget_low_u32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
1
>
LowerHalf
(
const
Vec128
<
uint64_t
>
v
)
{
return
Vec128
<
uint64_t
1
>
(
vget_low_u64
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int8_t
8
>
LowerHalf
(
const
Vec128
<
int8_t
>
v
)
{
return
Vec128
<
int8_t
8
>
(
vget_low_s8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
4
>
LowerHalf
(
const
Vec128
<
int16_t
>
v
)
{
return
Vec128
<
int16_t
4
>
(
vget_low_s16
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
2
>
LowerHalf
(
const
Vec128
<
int32_t
>
v
)
{
return
Vec128
<
int32_t
2
>
(
vget_low_s32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
1
>
LowerHalf
(
const
Vec128
<
int64_t
>
v
)
{
return
Vec128
<
int64_t
1
>
(
vget_low_s64
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
float
2
>
LowerHalf
(
const
Vec128
<
float
>
v
)
{
return
Vec128
<
float
2
>
(
vget_low_f32
(
v
.
raw
)
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
double
1
>
LowerHalf
(
const
Vec128
<
double
>
v
)
{
return
Vec128
<
double
1
>
(
vget_low_f64
(
v
.
raw
)
)
;
}
#
endif
HWY_INLINE
Vec128
<
uint8_t
8
>
UpperHalf
(
const
Vec128
<
uint8_t
>
v
)
{
return
Vec128
<
uint8_t
8
>
(
vget_high_u8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint16_t
4
>
UpperHalf
(
const
Vec128
<
uint16_t
>
v
)
{
return
Vec128
<
uint16_t
4
>
(
vget_high_u16
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
2
>
UpperHalf
(
const
Vec128
<
uint32_t
>
v
)
{
return
Vec128
<
uint32_t
2
>
(
vget_high_u32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
1
>
UpperHalf
(
const
Vec128
<
uint64_t
>
v
)
{
return
Vec128
<
uint64_t
1
>
(
vget_high_u64
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int8_t
8
>
UpperHalf
(
const
Vec128
<
int8_t
>
v
)
{
return
Vec128
<
int8_t
8
>
(
vget_high_s8
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
4
>
UpperHalf
(
const
Vec128
<
int16_t
>
v
)
{
return
Vec128
<
int16_t
4
>
(
vget_high_s16
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
2
>
UpperHalf
(
const
Vec128
<
int32_t
>
v
)
{
return
Vec128
<
int32_t
2
>
(
vget_high_s32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
1
>
UpperHalf
(
const
Vec128
<
int64_t
>
v
)
{
return
Vec128
<
int64_t
1
>
(
vget_high_s64
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
float
2
>
UpperHalf
(
const
Vec128
<
float
>
v
)
{
return
Vec128
<
float
2
>
(
vget_high_f32
(
v
.
raw
)
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
double
1
>
UpperHalf
(
const
Vec128
<
double
>
v
)
{
return
Vec128
<
double
1
>
(
vget_high_f64
(
v
.
raw
)
)
;
}
#
endif
template
<
int
kBytes
typename
T
>
HWY_INLINE
Vec128
<
T
>
CombineShiftRightBytes
(
const
Vec128
<
T
>
hi
const
Vec128
<
T
>
lo
)
{
static_assert
(
0
<
kBytes
&
&
kBytes
<
16
"
kBytes
must
be
in
[
1
15
]
"
)
;
const
Full128
<
uint8_t
>
d8
;
return
BitCast
(
Full128
<
T
>
(
)
Vec128
<
uint8_t
>
(
vextq_u8
(
BitCast
(
d8
lo
)
.
raw
BitCast
(
d8
hi
)
.
raw
kBytes
)
)
)
;
}
namespace
detail
{
template
<
int
kBytes
>
struct
ShiftLeftBytesT
{
template
<
class
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
operator
(
)
(
const
Vec128
<
T
N
>
v
)
{
return
CombineShiftRightBytes
<
16
-
kBytes
>
(
v
Zero
(
Full128
<
T
>
(
)
)
)
;
}
}
;
template
<
>
struct
ShiftLeftBytesT
<
0
>
{
template
<
class
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
operator
(
)
(
const
Vec128
<
T
N
>
v
)
{
return
v
;
}
}
;
template
<
int
kBytes
>
struct
ShiftRightBytesT
{
template
<
class
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
operator
(
)
(
const
Vec128
<
T
N
>
v
)
{
return
CombineShiftRightBytes
<
kBytes
>
(
Zero
(
Full128
<
T
>
(
)
)
v
)
;
}
}
;
template
<
>
struct
ShiftRightBytesT
<
0
>
{
template
<
class
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
operator
(
)
(
const
Vec128
<
T
N
>
v
)
{
return
v
;
}
}
;
}
template
<
int
kBytes
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
ShiftLeftBytes
(
const
Vec128
<
T
N
>
v
)
{
return
detail
:
:
ShiftLeftBytesT
<
kBytes
>
(
)
(
v
)
;
}
template
<
int
kLanes
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
ShiftLeftLanes
(
const
Vec128
<
T
N
>
v
)
{
const
Simd
<
uint8_t
N
*
sizeof
(
T
)
>
d8
;
const
Simd
<
T
N
>
d
;
return
BitCast
(
d
ShiftLeftBytes
<
kLanes
*
sizeof
(
T
)
>
(
BitCast
(
d8
v
)
)
)
;
}
template
<
int
kBytes
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
ShiftRightBytes
(
const
Vec128
<
T
N
>
v
)
{
return
detail
:
:
ShiftRightBytesT
<
kBytes
>
(
)
(
v
)
;
}
template
<
int
kLanes
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
ShiftRightLanes
(
const
Vec128
<
T
N
>
v
)
{
const
Simd
<
uint8_t
N
*
sizeof
(
T
)
>
d8
;
const
Simd
<
T
N
>
d
;
return
BitCast
(
d
ShiftRightBytes
<
kLanes
*
sizeof
(
T
)
>
(
BitCast
(
d8
v
)
)
)
;
}
#
if
HWY_ARCH_ARM_A64
template
<
int
kLane
>
HWY_INLINE
Vec128
<
uint16_t
>
Broadcast
(
const
Vec128
<
uint16_t
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
8
"
Invalid
lane
"
)
;
return
Vec128
<
uint16_t
>
(
vdupq_laneq_u16
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
size_t
N
HWY_IF_LE64
(
uint16_t
N
)
>
HWY_INLINE
Vec128
<
uint16_t
N
>
Broadcast
(
const
Vec128
<
uint16_t
N
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
N
"
Invalid
lane
"
)
;
return
Vec128
<
uint16_t
N
>
(
vdup_lane_u16
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
uint32_t
>
Broadcast
(
const
Vec128
<
uint32_t
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
4
"
Invalid
lane
"
)
;
return
Vec128
<
uint32_t
>
(
vdupq_laneq_u32
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
size_t
N
HWY_IF_LE64
(
uint32_t
N
)
>
HWY_INLINE
Vec128
<
uint32_t
N
>
Broadcast
(
const
Vec128
<
uint32_t
N
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
N
"
Invalid
lane
"
)
;
return
Vec128
<
uint32_t
N
>
(
vdup_lane_u32
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
uint64_t
>
Broadcast
(
const
Vec128
<
uint64_t
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
2
"
Invalid
lane
"
)
;
return
Vec128
<
uint64_t
>
(
vdupq_laneq_u64
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
int16_t
>
Broadcast
(
const
Vec128
<
int16_t
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
8
"
Invalid
lane
"
)
;
return
Vec128
<
int16_t
>
(
vdupq_laneq_s16
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
size_t
N
HWY_IF_LE64
(
int16_t
N
)
>
HWY_INLINE
Vec128
<
int16_t
N
>
Broadcast
(
const
Vec128
<
int16_t
N
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
N
"
Invalid
lane
"
)
;
return
Vec128
<
int16_t
N
>
(
vdup_lane_s16
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
int32_t
>
Broadcast
(
const
Vec128
<
int32_t
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
4
"
Invalid
lane
"
)
;
return
Vec128
<
int32_t
>
(
vdupq_laneq_s32
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
int32_t
N
>
Broadcast
(
const
Vec128
<
int32_t
N
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
N
"
Invalid
lane
"
)
;
return
Vec128
<
int32_t
N
>
(
vdup_lane_s32
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
int64_t
>
Broadcast
(
const
Vec128
<
int64_t
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
2
"
Invalid
lane
"
)
;
return
Vec128
<
int64_t
>
(
vdupq_laneq_s64
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
float
>
Broadcast
(
const
Vec128
<
float
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
4
"
Invalid
lane
"
)
;
return
Vec128
<
float
>
(
vdupq_laneq_f32
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
size_t
N
HWY_IF_LE64
(
float
N
)
>
HWY_INLINE
Vec128
<
float
N
>
Broadcast
(
const
Vec128
<
float
N
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
N
"
Invalid
lane
"
)
;
return
Vec128
<
float
N
>
(
vdup_lane_f32
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
double
>
Broadcast
(
const
Vec128
<
double
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
2
"
Invalid
lane
"
)
;
return
Vec128
<
double
>
(
vdupq_laneq_f64
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
double
1
>
Broadcast
(
const
Vec128
<
double
1
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
1
"
Invalid
lane
"
)
;
return
v
;
}
#
else
template
<
int
kLane
>
HWY_INLINE
Vec128
<
uint16_t
>
Broadcast
(
const
Vec128
<
uint16_t
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
8
"
Invalid
lane
"
)
;
return
Vec128
<
uint16_t
>
(
vdupq_n_u16
(
vgetq_lane_u16
(
v
.
raw
kLane
)
)
)
;
}
template
<
int
kLane
size_t
N
HWY_IF_LE64
(
uint16_t
N
)
>
HWY_INLINE
Vec128
<
uint16_t
N
>
Broadcast
(
const
Vec128
<
uint16_t
N
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
N
"
Invalid
lane
"
)
;
return
Vec128
<
uint16_t
N
>
(
vdup_lane_u16
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
uint32_t
>
Broadcast
(
const
Vec128
<
uint32_t
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
4
"
Invalid
lane
"
)
;
return
Vec128
<
uint32_t
>
(
vdupq_n_u32
(
vgetq_lane_u32
(
v
.
raw
kLane
)
)
)
;
}
template
<
int
kLane
size_t
N
HWY_IF_LE64
(
uint32_t
N
)
>
HWY_INLINE
Vec128
<
uint32_t
N
>
Broadcast
(
const
Vec128
<
uint32_t
N
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
N
"
Invalid
lane
"
)
;
return
Vec128
<
uint32_t
N
>
(
vdup_lane_u32
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
uint64_t
>
Broadcast
(
const
Vec128
<
uint64_t
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
2
"
Invalid
lane
"
)
;
return
Vec128
<
uint64_t
>
(
vdupq_n_u64
(
vgetq_lane_u64
(
v
.
raw
kLane
)
)
)
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
int16_t
>
Broadcast
(
const
Vec128
<
int16_t
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
8
"
Invalid
lane
"
)
;
return
Vec128
<
int16_t
>
(
vdupq_n_s16
(
vgetq_lane_s16
(
v
.
raw
kLane
)
)
)
;
}
template
<
int
kLane
size_t
N
HWY_IF_LE64
(
int16_t
N
)
>
HWY_INLINE
Vec128
<
int16_t
N
>
Broadcast
(
const
Vec128
<
int16_t
N
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
N
"
Invalid
lane
"
)
;
return
Vec128
<
int16_t
N
>
(
vdup_lane_s16
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
int32_t
>
Broadcast
(
const
Vec128
<
int32_t
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
4
"
Invalid
lane
"
)
;
return
Vec128
<
int32_t
>
(
vdupq_n_s32
(
vgetq_lane_s32
(
v
.
raw
kLane
)
)
)
;
}
template
<
int
kLane
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
int32_t
N
>
Broadcast
(
const
Vec128
<
int32_t
N
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
N
"
Invalid
lane
"
)
;
return
Vec128
<
int32_t
N
>
(
vdup_lane_s32
(
v
.
raw
kLane
)
)
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
int64_t
>
Broadcast
(
const
Vec128
<
int64_t
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
2
"
Invalid
lane
"
)
;
return
Vec128
<
int64_t
>
(
vdupq_n_s64
(
vgetq_lane_s64
(
v
.
raw
kLane
)
)
)
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
float
>
Broadcast
(
const
Vec128
<
float
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
4
"
Invalid
lane
"
)
;
return
Vec128
<
float
>
(
vdupq_n_f32
(
vgetq_lane_f32
(
v
.
raw
kLane
)
)
)
;
}
template
<
int
kLane
size_t
N
HWY_IF_LE64
(
float
N
)
>
HWY_INLINE
Vec128
<
float
N
>
Broadcast
(
const
Vec128
<
float
N
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
N
"
Invalid
lane
"
)
;
return
Vec128
<
float
N
>
(
vdup_lane_f32
(
v
.
raw
kLane
)
)
;
}
#
endif
template
<
int
kLane
>
HWY_INLINE
Vec128
<
uint64_t
1
>
Broadcast
(
const
Vec128
<
uint64_t
1
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
1
"
Invalid
lane
"
)
;
return
v
;
}
template
<
int
kLane
>
HWY_INLINE
Vec128
<
int64_t
1
>
Broadcast
(
const
Vec128
<
int64_t
1
>
v
)
{
static_assert
(
0
<
=
kLane
&
&
kLane
<
1
"
Invalid
lane
"
)
;
return
v
;
}
template
<
typename
T
>
HWY_API
Vec128
<
T
>
TableLookupBytes
(
const
Vec128
<
T
>
bytes
const
Vec128
<
T
>
from
)
{
const
Full128
<
T
>
d
;
const
Repartition
<
uint8_t
decltype
(
d
)
>
d8
;
#
if
HWY_ARCH_ARM_A64
return
BitCast
(
d
Vec128
<
uint8_t
>
(
vqtbl1q_u8
(
BitCast
(
d8
bytes
)
.
raw
BitCast
(
d8
from
)
.
raw
)
)
)
;
#
else
uint8x16_t
table0
=
BitCast
(
d8
bytes
)
.
raw
;
uint8x8x2_t
table
;
table
.
val
[
0
]
=
vget_low_u8
(
table0
)
;
table
.
val
[
1
]
=
vget_high_u8
(
table0
)
;
uint8x16_t
idx
=
BitCast
(
d8
from
)
.
raw
;
uint8x8_t
low
=
vtbl2_u8
(
table
vget_low_u8
(
idx
)
)
;
uint8x8_t
hi
=
vtbl2_u8
(
table
vget_high_u8
(
idx
)
)
;
return
BitCast
(
d
Vec128
<
uint8_t
>
(
vcombine_u8
(
low
hi
)
)
)
;
#
endif
}
template
<
typename
T
size_t
N
typename
TI
HWY_IF_LE64
(
T
N
)
>
HWY_INLINE
Vec128
<
T
N
>
TableLookupBytes
(
const
Vec128
<
T
N
>
bytes
const
Vec128
<
TI
N
*
sizeof
(
T
)
/
sizeof
(
TI
)
>
from
)
{
const
Simd
<
T
N
>
d
;
const
Repartition
<
uint8_t
decltype
(
d
)
>
d8
;
return
BitCast
(
d
decltype
(
Zero
(
d8
)
)
(
vtbl1_u8
(
BitCast
(
d8
bytes
)
.
raw
BitCast
(
d8
from
)
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
2
>
Shuffle2301
(
const
Vec128
<
uint32_t
2
>
v
)
{
return
Vec128
<
uint32_t
2
>
(
vrev64_u32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
2
>
Shuffle2301
(
const
Vec128
<
int32_t
2
>
v
)
{
return
Vec128
<
int32_t
2
>
(
vrev64_s32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
float
2
>
Shuffle2301
(
const
Vec128
<
float
2
>
v
)
{
return
Vec128
<
float
2
>
(
vrev64_f32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
>
Shuffle2301
(
const
Vec128
<
uint32_t
>
v
)
{
return
Vec128
<
uint32_t
>
(
vrev64q_u32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
Shuffle2301
(
const
Vec128
<
int32_t
>
v
)
{
return
Vec128
<
int32_t
>
(
vrev64q_s32
(
v
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
float
>
Shuffle2301
(
const
Vec128
<
float
>
v
)
{
return
Vec128
<
float
>
(
vrev64q_f32
(
v
.
raw
)
)
;
}
template
<
typename
T
>
HWY_INLINE
Vec128
<
T
>
Shuffle1032
(
const
Vec128
<
T
>
v
)
{
return
CombineShiftRightBytes
<
8
>
(
v
v
)
;
}
template
<
typename
T
>
HWY_INLINE
Vec128
<
T
>
Shuffle01
(
const
Vec128
<
T
>
v
)
{
return
CombineShiftRightBytes
<
8
>
(
v
v
)
;
}
template
<
typename
T
>
HWY_INLINE
Vec128
<
T
>
Shuffle0321
(
const
Vec128
<
T
>
v
)
{
return
CombineShiftRightBytes
<
4
>
(
v
v
)
;
}
template
<
typename
T
>
HWY_INLINE
Vec128
<
T
>
Shuffle2103
(
const
Vec128
<
T
>
v
)
{
return
CombineShiftRightBytes
<
12
>
(
v
v
)
;
}
template
<
typename
T
>
HWY_INLINE
Vec128
<
T
>
Shuffle0123
(
const
Vec128
<
T
>
v
)
{
static_assert
(
sizeof
(
T
)
=
=
4
"
Shuffle0123
should
only
be
applied
to
32
-
bit
types
"
)
;
static
constexpr
uint8_t
bytes
[
16
]
=
{
12
13
14
15
8
9
10
11
4
5
6
7
0
1
2
3
}
;
const
Full128
<
uint8_t
>
d8
;
const
Full128
<
T
>
d
;
return
TableLookupBytes
(
v
BitCast
(
d
Load
(
d8
bytes
)
)
)
;
}
template
<
typename
T
>
struct
Indices128
{
typename
Raw128
<
T
16
/
sizeof
(
T
)
>
:
:
type
raw
;
}
;
template
<
typename
T
>
HWY_INLINE
Indices128
<
T
>
SetTableIndices
(
const
Full128
<
T
>
const
int32_t
*
idx
)
{
#
if
!
defined
(
NDEBUG
)
|
|
defined
(
ADDRESS_SANITIZER
)
const
size_t
N
=
16
/
sizeof
(
T
)
;
for
(
size_t
i
=
0
;
i
<
N
;
+
+
i
)
{
HWY_DASSERT
(
0
<
=
idx
[
i
]
&
&
idx
[
i
]
<
static_cast
<
int32_t
>
(
N
)
)
;
}
#
endif
const
Full128
<
uint8_t
>
d8
;
alignas
(
16
)
uint8_t
control
[
16
]
;
for
(
size_t
idx_byte
=
0
;
idx_byte
<
16
;
+
+
idx_byte
)
{
const
size_t
idx_lane
=
idx_byte
/
sizeof
(
T
)
;
const
size_t
mod
=
idx_byte
%
sizeof
(
T
)
;
control
[
idx_byte
]
=
idx
[
idx_lane
]
*
sizeof
(
T
)
+
mod
;
}
return
Indices128
<
T
>
{
BitCast
(
Full128
<
T
>
(
)
Load
(
d8
control
)
)
.
raw
}
;
}
HWY_INLINE
Vec128
<
uint32_t
>
TableLookupLanes
(
const
Vec128
<
uint32_t
>
v
const
Indices128
<
uint32_t
>
idx
)
{
return
TableLookupBytes
(
v
Vec128
<
uint32_t
>
(
idx
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
TableLookupLanes
(
const
Vec128
<
int32_t
>
v
const
Indices128
<
int32_t
>
idx
)
{
return
TableLookupBytes
(
v
Vec128
<
int32_t
>
(
idx
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
float
>
TableLookupLanes
(
const
Vec128
<
float
>
v
const
Indices128
<
float
>
idx
)
{
const
Full128
<
int32_t
>
di
;
const
Full128
<
float
>
df
;
return
BitCast
(
df
TableLookupBytes
(
BitCast
(
di
v
)
Vec128
<
int32_t
>
(
idx
.
raw
)
)
)
;
}
HWY_NEON_DEF_FUNCTION_INT_8_16_32
(
InterleaveLower
vzip1
_
2
)
HWY_NEON_DEF_FUNCTION_UINT_8_16_32
(
InterleaveLower
vzip1
_
2
)
HWY_NEON_DEF_FUNCTION_INT_8_16_32
(
InterleaveUpper
vzip2
_
2
)
HWY_NEON_DEF_FUNCTION_UINT_8_16_32
(
InterleaveUpper
vzip2
_
2
)
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
uint64_t
>
InterleaveLower
(
const
Vec128
<
uint64_t
>
a
const
Vec128
<
uint64_t
>
b
)
{
return
Vec128
<
uint64_t
>
(
vzip1q_u64
(
a
.
raw
b
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
InterleaveLower
(
const
Vec128
<
int64_t
>
a
const
Vec128
<
int64_t
>
b
)
{
return
Vec128
<
int64_t
>
(
vzip1q_s64
(
a
.
raw
b
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
>
InterleaveUpper
(
const
Vec128
<
uint64_t
>
a
const
Vec128
<
uint64_t
>
b
)
{
return
Vec128
<
uint64_t
>
(
vzip2q_u64
(
a
.
raw
b
.
raw
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
InterleaveUpper
(
const
Vec128
<
int64_t
>
a
const
Vec128
<
int64_t
>
b
)
{
return
Vec128
<
int64_t
>
(
vzip2q_s64
(
a
.
raw
b
.
raw
)
)
;
}
#
else
HWY_INLINE
Vec128
<
uint64_t
>
InterleaveLower
(
const
Vec128
<
uint64_t
>
a
const
Vec128
<
uint64_t
>
b
)
{
auto
flip
=
CombineShiftRightBytes
<
8
>
(
a
a
)
;
return
CombineShiftRightBytes
<
8
>
(
b
flip
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
InterleaveLower
(
const
Vec128
<
int64_t
>
a
const
Vec128
<
int64_t
>
b
)
{
auto
flip
=
CombineShiftRightBytes
<
8
>
(
a
a
)
;
return
CombineShiftRightBytes
<
8
>
(
b
flip
)
;
}
HWY_INLINE
Vec128
<
uint64_t
>
InterleaveUpper
(
const
Vec128
<
uint64_t
>
a
const
Vec128
<
uint64_t
>
b
)
{
auto
flip
=
CombineShiftRightBytes
<
8
>
(
b
b
)
;
return
CombineShiftRightBytes
<
8
>
(
flip
a
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
InterleaveUpper
(
const
Vec128
<
int64_t
>
a
const
Vec128
<
int64_t
>
b
)
{
auto
flip
=
CombineShiftRightBytes
<
8
>
(
b
b
)
;
return
CombineShiftRightBytes
<
8
>
(
flip
a
)
;
}
#
endif
HWY_INLINE
Vec128
<
float
>
InterleaveLower
(
const
Vec128
<
float
>
a
const
Vec128
<
float
>
b
)
{
return
Vec128
<
float
>
(
vzip1q_f32
(
a
.
raw
b
.
raw
)
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
double
>
InterleaveLower
(
const
Vec128
<
double
>
a
const
Vec128
<
double
>
b
)
{
return
Vec128
<
double
>
(
vzip1q_f64
(
a
.
raw
b
.
raw
)
)
;
}
#
endif
HWY_INLINE
Vec128
<
float
>
InterleaveUpper
(
const
Vec128
<
float
>
a
const
Vec128
<
float
>
b
)
{
return
Vec128
<
float
>
(
vzip2q_f32
(
a
.
raw
b
.
raw
)
)
;
}
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
double
>
InterleaveUpper
(
const
Vec128
<
double
>
a
const
Vec128
<
double
>
b
)
{
return
Vec128
<
double
>
(
vzip2q_f64
(
a
.
raw
b
.
raw
)
)
;
}
#
endif
HWY_INLINE
Vec128
<
uint16_t
>
ZipLower
(
const
Vec128
<
uint8_t
>
a
const
Vec128
<
uint8_t
>
b
)
{
return
Vec128
<
uint16_t
>
(
vreinterpretq_u16_u8
(
vzip1q_u8
(
a
.
raw
b
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
>
ZipLower
(
const
Vec128
<
uint16_t
>
a
const
Vec128
<
uint16_t
>
b
)
{
return
Vec128
<
uint32_t
>
(
vreinterpretq_u32_u16
(
vzip1q_u16
(
a
.
raw
b
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
>
ZipLower
(
const
Vec128
<
uint32_t
>
a
const
Vec128
<
uint32_t
>
b
)
{
return
Vec128
<
uint64_t
>
(
vreinterpretq_u64_u32
(
vzip1q_u32
(
a
.
raw
b
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
>
ZipLower
(
const
Vec128
<
int8_t
>
a
const
Vec128
<
int8_t
>
b
)
{
return
Vec128
<
int16_t
>
(
vreinterpretq_s16_s8
(
vzip1q_s8
(
a
.
raw
b
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
ZipLower
(
const
Vec128
<
int16_t
>
a
const
Vec128
<
int16_t
>
b
)
{
return
Vec128
<
int32_t
>
(
vreinterpretq_s32_s16
(
vzip1q_s16
(
a
.
raw
b
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
ZipLower
(
const
Vec128
<
int32_t
>
a
const
Vec128
<
int32_t
>
b
)
{
return
Vec128
<
int64_t
>
(
vreinterpretq_s64_s32
(
vzip1q_s32
(
a
.
raw
b
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
uint16_t
>
ZipUpper
(
const
Vec128
<
uint8_t
>
a
const
Vec128
<
uint8_t
>
b
)
{
return
Vec128
<
uint16_t
>
(
vreinterpretq_u16_u8
(
vzip2q_u8
(
a
.
raw
b
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
uint32_t
>
ZipUpper
(
const
Vec128
<
uint16_t
>
a
const
Vec128
<
uint16_t
>
b
)
{
return
Vec128
<
uint32_t
>
(
vreinterpretq_u32_u16
(
vzip2q_u16
(
a
.
raw
b
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
>
ZipUpper
(
const
Vec128
<
uint32_t
>
a
const
Vec128
<
uint32_t
>
b
)
{
return
Vec128
<
uint64_t
>
(
vreinterpretq_u64_u32
(
vzip2q_u32
(
a
.
raw
b
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
int16_t
>
ZipUpper
(
const
Vec128
<
int8_t
>
a
const
Vec128
<
int8_t
>
b
)
{
return
Vec128
<
int16_t
>
(
vreinterpretq_s16_s8
(
vzip2q_s8
(
a
.
raw
b
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
ZipUpper
(
const
Vec128
<
int16_t
>
a
const
Vec128
<
int16_t
>
b
)
{
return
Vec128
<
int32_t
>
(
vreinterpretq_s32_s16
(
vzip2q_s16
(
a
.
raw
b
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
ZipUpper
(
const
Vec128
<
int32_t
>
a
const
Vec128
<
int32_t
>
b
)
{
return
Vec128
<
int64_t
>
(
vreinterpretq_s64_s32
(
vzip2q_s32
(
a
.
raw
b
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint8_t
N
)
>
HWY_INLINE
Vec128
<
uint16_t
(
N
+
1
)
/
2
>
ZipLower
(
const
Vec128
<
uint8_t
N
>
a
const
Vec128
<
uint8_t
N
>
b
)
{
return
Vec128
<
uint16_t
(
N
+
1
)
/
2
>
(
vreinterpret_u16_u8
(
vzip1_u8
(
a
.
raw
b
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint16_t
N
)
>
HWY_INLINE
Vec128
<
uint32_t
(
N
+
1
)
/
2
>
ZipLower
(
const
Vec128
<
uint16_t
N
>
a
const
Vec128
<
uint16_t
N
>
b
)
{
return
Vec128
<
uint32_t
(
N
+
1
)
/
2
>
(
vreinterpret_u32_u16
(
vzip1_u16
(
a
.
raw
b
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint32_t
N
)
>
HWY_INLINE
Vec128
<
uint64_t
(
N
+
1
)
/
2
>
ZipLower
(
const
Vec128
<
uint32_t
N
>
a
const
Vec128
<
uint32_t
N
>
b
)
{
return
Vec128
<
uint64_t
(
N
+
1
)
/
2
>
(
vreinterpret_u64_u32
(
vzip1_u32
(
a
.
raw
b
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int8_t
N
)
>
HWY_INLINE
Vec128
<
int16_t
(
N
+
1
)
/
2
>
ZipLower
(
const
Vec128
<
int8_t
N
>
a
const
Vec128
<
int8_t
N
>
b
)
{
return
Vec128
<
int16_t
(
N
+
1
)
/
2
>
(
vreinterpret_s16_s8
(
vzip1_s8
(
a
.
raw
b
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int16_t
N
)
>
HWY_INLINE
Vec128
<
int32_t
(
N
+
1
)
/
2
>
ZipLower
(
const
Vec128
<
int16_t
N
>
a
const
Vec128
<
int16_t
N
>
b
)
{
return
Vec128
<
int32_t
(
N
+
1
)
/
2
>
(
vreinterpret_s32_s16
(
vzip1_s16
(
a
.
raw
b
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
int64_t
(
N
+
1
)
/
2
>
ZipLower
(
const
Vec128
<
int32_t
N
>
a
const
Vec128
<
int32_t
N
>
b
)
{
return
Vec128
<
int64_t
(
N
+
1
)
/
2
>
(
vreinterpret_s64_s32
(
vzip1_s32
(
a
.
raw
b
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint8_t
N
)
>
HWY_INLINE
Vec128
<
uint16_t
N
/
2
>
ZipUpper
(
const
Vec128
<
uint8_t
N
>
a
const
Vec128
<
uint8_t
N
>
b
)
{
return
Vec128
<
uint16_t
N
/
2
>
(
vreinterpret_u16_u8
(
vzip2_u8
(
a
.
raw
b
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint16_t
N
)
>
HWY_INLINE
Vec128
<
uint32_t
N
/
2
>
ZipUpper
(
const
Vec128
<
uint16_t
N
>
a
const
Vec128
<
uint16_t
N
>
b
)
{
return
Vec128
<
uint32_t
N
/
2
>
(
vreinterpret_u32_u16
(
vzip2_u16
(
a
.
raw
b
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint32_t
N
)
>
HWY_INLINE
Vec128
<
uint64_t
N
/
2
>
ZipUpper
(
const
Vec128
<
uint32_t
N
>
a
const
Vec128
<
uint32_t
N
>
b
)
{
return
Vec128
<
uint64_t
N
/
2
>
(
vreinterpret_u64_u32
(
vzip2_u32
(
a
.
raw
b
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int8_t
N
)
>
HWY_INLINE
Vec128
<
int16_t
N
/
2
>
ZipUpper
(
const
Vec128
<
int8_t
N
>
a
const
Vec128
<
int8_t
N
>
b
)
{
return
Vec128
<
int16_t
N
/
2
>
(
vreinterpret_s16_s8
(
vzip2_s8
(
a
.
raw
b
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int16_t
N
)
>
HWY_INLINE
Vec128
<
int32_t
N
/
2
>
ZipUpper
(
const
Vec128
<
int16_t
N
>
a
const
Vec128
<
int16_t
N
>
b
)
{
return
Vec128
<
int32_t
N
/
2
>
(
vreinterpret_s32_s16
(
vzip2_s16
(
a
.
raw
b
.
raw
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
int32_t
N
)
>
HWY_INLINE
Vec128
<
int64_t
N
/
2
>
ZipUpper
(
const
Vec128
<
int32_t
N
>
a
const
Vec128
<
int32_t
N
>
b
)
{
return
Vec128
<
int64_t
N
/
2
>
(
vreinterpret_s64_s32
(
vzip2_s32
(
a
.
raw
b
.
raw
)
)
)
;
}
template
<
typename
T
>
HWY_INLINE
Vec128
<
T
>
ConcatLowerLower
(
const
Vec128
<
T
>
hi
const
Vec128
<
T
>
lo
)
{
const
Full128
<
uint64_t
>
d64
;
return
BitCast
(
Full128
<
T
>
(
)
InterleaveLower
(
BitCast
(
d64
lo
)
BitCast
(
d64
hi
)
)
)
;
}
template
<
typename
T
>
HWY_INLINE
Vec128
<
T
>
ConcatUpperUpper
(
const
Vec128
<
T
>
hi
const
Vec128
<
T
>
lo
)
{
const
Full128
<
uint64_t
>
d64
;
return
BitCast
(
Full128
<
T
>
(
)
InterleaveUpper
(
BitCast
(
d64
lo
)
BitCast
(
d64
hi
)
)
)
;
}
template
<
typename
T
>
HWY_INLINE
Vec128
<
T
>
ConcatLowerUpper
(
const
Vec128
<
T
>
hi
const
Vec128
<
T
>
lo
)
{
return
CombineShiftRightBytes
<
8
>
(
hi
lo
)
;
}
template
<
typename
T
>
HWY_INLINE
Vec128
<
T
>
ConcatUpperLower
(
const
Vec128
<
T
>
hi
const
Vec128
<
T
>
lo
)
{
alignas
(
16
)
const
uint8_t
kBytes
[
16
]
=
{
0xFF
0xFF
0xFF
0xFF
0xFF
0xFF
0xFF
0xFF
0
0
0
0
0
0
0
0
}
;
const
auto
vec
=
BitCast
(
Full128
<
T
>
(
)
Load
(
Full128
<
uint8_t
>
(
)
kBytes
)
)
;
return
IfThenElse
(
MaskFromVec
(
vec
)
lo
hi
)
;
}
template
<
typename
T
>
HWY_INLINE
Vec128
<
T
>
OddEven
(
const
Vec128
<
T
>
a
const
Vec128
<
T
>
b
)
{
alignas
(
16
)
constexpr
uint8_t
kBytes
[
16
]
=
{
(
(
0
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
1
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
2
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
3
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
4
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
5
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
6
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
7
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
8
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
9
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
10
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
11
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
12
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
13
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
14
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
(
(
15
/
sizeof
(
T
)
)
&
1
)
?
0
:
0xFF
}
;
const
auto
vec
=
BitCast
(
Full128
<
T
>
(
)
Load
(
Full128
<
uint8_t
>
(
)
kBytes
)
)
;
return
IfThenElse
(
MaskFromVec
(
vec
)
b
a
)
;
}
template
<
typename
T
size_t
N
typename
T2
>
Vec128
<
T
N
>
Iota
(
const
Simd
<
T
N
>
d
const
T2
first
)
{
HWY_ALIGN
T
lanes
[
16
/
sizeof
(
T
)
]
;
for
(
size_t
i
=
0
;
i
<
16
/
sizeof
(
T
)
;
+
+
i
)
{
lanes
[
i
]
=
static_cast
<
T
>
(
first
+
static_cast
<
T2
>
(
i
)
)
;
}
return
Load
(
d
lanes
)
;
}
template
<
typename
T
size_t
N
typename
Offset
HWY_IF_LE128
(
T
N
)
>
HWY_API
void
ScatterOffset
(
Vec128
<
T
N
>
v
Simd
<
T
N
>
d
T
*
HWY_RESTRICT
base
const
Vec128
<
Offset
N
>
offset
)
{
static_assert
(
sizeof
(
T
)
=
=
sizeof
(
Offset
)
"
Must
match
for
portability
"
)
;
alignas
(
16
)
T
lanes
[
N
]
;
Store
(
v
d
lanes
)
;
alignas
(
16
)
Offset
offset_lanes
[
N
]
;
Store
(
offset
Simd
<
Offset
N
>
(
)
offset_lanes
)
;
uint8_t
*
base_bytes
=
reinterpret_cast
<
uint8_t
*
>
(
base
)
;
for
(
size_t
i
=
0
;
i
<
N
;
+
+
i
)
{
CopyBytes
<
sizeof
(
T
)
>
(
&
lanes
[
i
]
base_bytes
+
offset_lanes
[
i
]
)
;
}
}
template
<
typename
T
size_t
N
typename
Index
HWY_IF_LE128
(
T
N
)
>
HWY_API
void
ScatterIndex
(
Vec128
<
T
N
>
v
Simd
<
T
N
>
d
T
*
HWY_RESTRICT
base
const
Vec128
<
Index
N
>
index
)
{
static_assert
(
sizeof
(
T
)
=
=
sizeof
(
Index
)
"
Must
match
for
portability
"
)
;
alignas
(
16
)
T
lanes
[
N
]
;
Store
(
v
d
lanes
)
;
alignas
(
16
)
Index
index_lanes
[
N
]
;
Store
(
index
Simd
<
Index
N
>
(
)
index_lanes
)
;
for
(
size_t
i
=
0
;
i
<
N
;
+
+
i
)
{
base
[
index_lanes
[
i
]
]
=
lanes
[
i
]
;
}
}
template
<
typename
T
size_t
N
typename
Offset
>
HWY_API
Vec128
<
T
N
>
GatherOffset
(
const
Simd
<
T
N
>
d
const
T
*
HWY_RESTRICT
base
const
Vec128
<
Offset
N
>
offset
)
{
static_assert
(
sizeof
(
T
)
=
=
sizeof
(
Offset
)
"
Must
match
for
portability
"
)
;
alignas
(
16
)
Offset
offset_lanes
[
N
]
;
Store
(
offset
Simd
<
Offset
N
>
(
)
offset_lanes
)
;
alignas
(
16
)
T
lanes
[
N
]
;
const
uint8_t
*
base_bytes
=
reinterpret_cast
<
const
uint8_t
*
>
(
base
)
;
for
(
size_t
i
=
0
;
i
<
N
;
+
+
i
)
{
CopyBytes
<
sizeof
(
T
)
>
(
base_bytes
+
offset_lanes
[
i
]
&
lanes
[
i
]
)
;
}
return
Load
(
d
lanes
)
;
}
template
<
typename
T
size_t
N
typename
Index
>
HWY_API
Vec128
<
T
N
>
GatherIndex
(
const
Simd
<
T
N
>
d
const
T
*
HWY_RESTRICT
base
const
Vec128
<
Index
N
>
index
)
{
static_assert
(
sizeof
(
T
)
=
=
sizeof
(
Index
)
"
Must
match
for
portability
"
)
;
alignas
(
16
)
Index
index_lanes
[
N
]
;
Store
(
index
Simd
<
Index
N
>
(
)
index_lanes
)
;
alignas
(
16
)
T
lanes
[
N
]
;
for
(
size_t
i
=
0
;
i
<
N
;
+
+
i
)
{
lanes
[
i
]
=
base
[
index_lanes
[
i
]
]
;
}
return
Load
(
d
lanes
)
;
}
#
if
HWY_ARCH_ARM_V7
template
<
size_t
N
>
HWY_INLINE
Mask128
<
int64_t
N
>
operator
=
=
(
const
Vec128
<
int64_t
N
>
a
const
Vec128
<
int64_t
N
>
b
)
{
const
Simd
<
int32_t
N
*
2
>
d32
;
const
Simd
<
int64_t
N
>
d64
;
const
auto
cmp32
=
VecFromMask
(
d32
BitCast
(
d32
a
)
=
=
BitCast
(
d32
b
)
)
;
const
auto
cmp64
=
cmp32
&
Shuffle2301
(
cmp32
)
;
return
MaskFromVec
(
BitCast
(
d64
cmp64
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Mask128
<
uint64_t
N
>
operator
=
=
(
const
Vec128
<
uint64_t
N
>
a
const
Vec128
<
uint64_t
N
>
b
)
{
const
Simd
<
uint32_t
N
*
2
>
d32
;
const
Simd
<
uint64_t
N
>
d64
;
const
auto
cmp32
=
VecFromMask
(
d32
BitCast
(
d32
a
)
=
=
BitCast
(
d32
b
)
)
;
const
auto
cmp64
=
cmp32
&
Shuffle2301
(
cmp32
)
;
return
MaskFromVec
(
BitCast
(
d64
cmp64
)
)
;
}
HWY_INLINE
Mask128
<
int64_t
>
operator
<
(
const
Vec128
<
int64_t
>
a
const
Vec128
<
int64_t
>
b
)
{
const
int64x2_t
sub
=
vqsubq_s64
(
a
.
raw
b
.
raw
)
;
return
MaskFromVec
(
BroadcastSignBit
(
Vec128
<
int64_t
>
(
sub
)
)
)
;
}
HWY_INLINE
Mask128
<
int64_t
1
>
operator
<
(
const
Vec128
<
int64_t
1
>
a
const
Vec128
<
int64_t
1
>
b
)
{
const
int64x1_t
sub
=
vqsub_s64
(
a
.
raw
b
.
raw
)
;
return
MaskFromVec
(
BroadcastSignBit
(
Vec128
<
int64_t
1
>
(
sub
)
)
)
;
}
template
<
size_t
N
>
HWY_INLINE
Mask128
<
int64_t
N
>
operator
>
(
const
Vec128
<
int64_t
N
>
a
const
Vec128
<
int64_t
N
>
b
)
{
return
b
<
a
;
}
#
endif
#
if
HWY_ARCH_ARM_A64
HWY_INLINE
Vec128
<
uint32_t
>
SumOfLanes
(
const
Vec128
<
uint32_t
>
v
)
{
return
Vec128
<
uint32_t
>
(
vdupq_n_u32
(
vaddvq_u32
(
v
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
SumOfLanes
(
const
Vec128
<
int32_t
>
v
)
{
return
Vec128
<
int32_t
>
(
vdupq_n_s32
(
vaddvq_s32
(
v
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
float
>
SumOfLanes
(
const
Vec128
<
float
>
v
)
{
return
Vec128
<
float
>
(
vdupq_n_f32
(
vaddvq_f32
(
v
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
>
SumOfLanes
(
const
Vec128
<
uint64_t
>
v
)
{
return
Vec128
<
uint64_t
>
(
vdupq_n_u64
(
vaddvq_u64
(
v
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
SumOfLanes
(
const
Vec128
<
int64_t
>
v
)
{
return
Vec128
<
int64_t
>
(
vdupq_n_s64
(
vaddvq_s64
(
v
.
raw
)
)
)
;
}
HWY_INLINE
Vec128
<
double
>
SumOfLanes
(
const
Vec128
<
double
>
v
)
{
return
Vec128
<
double
>
(
vdupq_n_f64
(
vaddvq_f64
(
v
.
raw
)
)
)
;
}
#
else
HWY_INLINE
Vec128
<
uint32_t
>
SumOfLanes
(
const
Vec128
<
uint32_t
>
v
)
{
uint32x4x2_t
v0
=
vuzpq_u32
(
v
.
raw
v
.
raw
)
;
uint32x4_t
c0
=
vaddq_u32
(
v0
.
val
[
0
]
v0
.
val
[
1
]
)
;
uint32x4x2_t
v1
=
vuzpq_u32
(
c0
c0
)
;
return
Vec128
<
uint32_t
>
(
vaddq_u32
(
v1
.
val
[
0
]
v1
.
val
[
1
]
)
)
;
}
HWY_INLINE
Vec128
<
int32_t
>
SumOfLanes
(
const
Vec128
<
int32_t
>
v
)
{
int32x4x2_t
v0
=
vuzpq_s32
(
v
.
raw
v
.
raw
)
;
int32x4_t
c0
=
vaddq_s32
(
v0
.
val
[
0
]
v0
.
val
[
1
]
)
;
int32x4x2_t
v1
=
vuzpq_s32
(
c0
c0
)
;
return
Vec128
<
int32_t
>
(
vaddq_s32
(
v1
.
val
[
0
]
v1
.
val
[
1
]
)
)
;
}
HWY_INLINE
Vec128
<
float
>
SumOfLanes
(
const
Vec128
<
float
>
v
)
{
float32x4x2_t
v0
=
vuzpq_f32
(
v
.
raw
v
.
raw
)
;
float32x4_t
c0
=
vaddq_f32
(
v0
.
val
[
0
]
v0
.
val
[
1
]
)
;
float32x4x2_t
v1
=
vuzpq_f32
(
c0
c0
)
;
return
Vec128
<
float
>
(
vaddq_f32
(
v1
.
val
[
0
]
v1
.
val
[
1
]
)
)
;
}
HWY_INLINE
Vec128
<
uint64_t
>
SumOfLanes
(
const
Vec128
<
uint64_t
>
v
)
{
return
v
+
CombineShiftRightBytes
<
8
>
(
v
v
)
;
}
HWY_INLINE
Vec128
<
int64_t
>
SumOfLanes
(
const
Vec128
<
int64_t
>
v
)
{
return
v
+
CombineShiftRightBytes
<
8
>
(
v
v
)
;
}
#
endif
namespace
detail
{
template
<
typename
T
size_t
N
>
HWY_API
Vec128
<
T
N
>
MinOfLanes
(
hwy
:
:
SizeTag
<
4
>
const
Vec128
<
T
N
>
v3210
)
{
const
Vec128
<
T
>
v1032
=
Shuffle1032
(
v3210
)
;
const
Vec128
<
T
>
v31_20_31_20
=
Min
(
v3210
v1032
)
;
const
Vec128
<
T
>
v20_31_20_31
=
Shuffle0321
(
v31_20_31_20
)
;
return
Min
(
v20_31_20_31
v31_20_31_20
)
;
}
template
<
typename
T
size_t
N
>
HWY_API
Vec128
<
T
N
>
MaxOfLanes
(
hwy
:
:
SizeTag
<
4
>
const
Vec128
<
T
N
>
v3210
)
{
const
Vec128
<
T
>
v1032
=
Shuffle1032
(
v3210
)
;
const
Vec128
<
T
>
v31_20_31_20
=
Max
(
v3210
v1032
)
;
const
Vec128
<
T
>
v20_31_20_31
=
Shuffle0321
(
v31_20_31_20
)
;
return
Max
(
v20_31_20_31
v31_20_31_20
)
;
}
template
<
typename
T
size_t
N
>
HWY_API
Vec128
<
T
N
>
MinOfLanes
(
hwy
:
:
SizeTag
<
8
>
const
Vec128
<
T
N
>
v10
)
{
const
Vec128
<
T
>
v01
=
Shuffle01
(
v10
)
;
return
Min
(
v10
v01
)
;
}
template
<
typename
T
size_t
N
>
HWY_API
Vec128
<
T
N
>
MaxOfLanes
(
hwy
:
:
SizeTag
<
8
>
const
Vec128
<
T
N
>
v10
)
{
const
Vec128
<
T
>
v01
=
Shuffle01
(
v10
)
;
return
Max
(
v10
v01
)
;
}
}
template
<
typename
T
size_t
N
>
HWY_API
Vec128
<
T
N
>
MinOfLanes
(
const
Vec128
<
T
N
>
v
)
{
return
detail
:
:
MinOfLanes
(
hwy
:
:
SizeTag
<
sizeof
(
T
)
>
(
)
v
)
;
}
template
<
typename
T
size_t
N
>
HWY_API
Vec128
<
T
N
>
MaxOfLanes
(
const
Vec128
<
T
N
>
v
)
{
return
detail
:
:
MaxOfLanes
(
hwy
:
:
SizeTag
<
sizeof
(
T
)
>
(
)
v
)
;
}
namespace
detail
{
template
<
typename
T
>
HWY_INLINE
uint64_t
BitsFromMask
(
hwy
:
:
SizeTag
<
1
>
const
Mask128
<
T
>
mask
)
{
alignas
(
16
)
constexpr
uint8_t
kSliceLanes
[
16
]
=
{
1
2
4
8
0x10
0x20
0x40
0x80
1
2
4
8
0x10
0x20
0x40
0x80
}
;
const
Full128
<
uint8_t
>
du
;
const
Vec128
<
uint8_t
>
values
=
BitCast
(
du
VecFromMask
(
Full128
<
T
>
(
)
mask
)
)
&
Load
(
du
kSliceLanes
)
;
#
if
HWY_ARCH_ARM_A64
const
uint8x8_t
x2
=
vget_low_u8
(
vpaddq_u8
(
values
.
raw
values
.
raw
)
)
;
const
uint8x8_t
x4
=
vpadd_u8
(
x2
x2
)
;
const
uint8x8_t
x8
=
vpadd_u8
(
x4
x4
)
;
return
vreinterpret_u16_u8
(
x8
)
[
0
]
;
#
else
const
uint16x8_t
x2
=
vpaddlq_u8
(
values
.
raw
)
;
const
uint32x4_t
x4
=
vpaddlq_u16
(
x2
)
;
const
uint64x2_t
x8
=
vpaddlq_u32
(
x4
)
;
return
(
uint64_t
(
x8
[
1
]
)
<
<
8
)
|
x8
[
0
]
;
#
endif
}
template
<
typename
T
size_t
N
HWY_IF_LE64
(
T
N
)
>
HWY_INLINE
uint64_t
BitsFromMask
(
hwy
:
:
SizeTag
<
1
>
const
Mask128
<
T
N
>
mask
)
{
alignas
(
8
)
constexpr
uint8_t
kSliceLanes
[
8
]
=
{
1
2
4
8
0x10
0x20
0x40
0x80
}
;
const
Simd
<
T
N
>
d
;
const
Simd
<
uint8_t
N
>
du
;
const
Vec128
<
uint8_t
N
>
slice
(
Load
(
Simd
<
uint8_t
8
>
(
)
kSliceLanes
)
.
raw
)
;
const
Vec128
<
uint8_t
N
>
values
=
BitCast
(
du
VecFromMask
(
d
mask
)
)
&
slice
;
#
if
HWY_ARCH_ARM_A64
return
vaddv_u8
(
values
.
raw
)
;
#
else
const
uint16x4_t
x2
=
vpaddl_u8
(
values
.
raw
)
;
const
uint32x2_t
x4
=
vpaddl_u16
(
x2
)
;
const
uint64x1_t
x8
=
vpaddl_u32
(
x4
)
;
return
vget_lane_u64
(
x8
0
)
;
#
endif
}
template
<
typename
T
>
HWY_INLINE
uint64_t
BitsFromMask
(
hwy
:
:
SizeTag
<
2
>
const
Mask128
<
T
>
mask
)
{
alignas
(
16
)
constexpr
uint16_t
kSliceLanes
[
8
]
=
{
1
2
4
8
0x10
0x20
0x40
0x80
}
;
const
Full128
<
T
>
d
;
const
Full128
<
uint16_t
>
du
;
const
Vec128
<
uint16_t
>
values
=
BitCast
(
du
VecFromMask
(
d
mask
)
)
&
Load
(
du
kSliceLanes
)
;
#
if
HWY_ARCH_ARM_A64
return
vaddvq_u16
(
values
.
raw
)
;
#
else
const
uint32x4_t
x2
=
vpaddlq_u16
(
values
.
raw
)
;
const
uint64x2_t
x4
=
vpaddlq_u32
(
x2
)
;
return
vgetq_lane_u64
(
x4
0
)
+
vgetq_lane_u64
(
x4
1
)
;
#
endif
}
template
<
typename
T
size_t
N
HWY_IF_LE64
(
T
N
)
>
HWY_INLINE
uint64_t
BitsFromMask
(
hwy
:
:
SizeTag
<
2
>
const
Mask128
<
T
N
>
mask
)
{
alignas
(
8
)
constexpr
uint16_t
kSliceLanes
[
4
]
=
{
1
2
4
8
}
;
const
Simd
<
T
N
>
d
;
const
Simd
<
uint16_t
N
>
du
;
const
Vec128
<
uint16_t
N
>
slice
(
Load
(
Simd
<
uint16_t
4
>
(
)
kSliceLanes
)
.
raw
)
;
const
Vec128
<
uint16_t
N
>
values
=
BitCast
(
du
VecFromMask
(
d
mask
)
)
&
slice
;
#
if
HWY_ARCH_ARM_A64
return
vaddv_u16
(
values
.
raw
)
;
#
else
const
uint32x2_t
x2
=
vpaddl_u16
(
values
.
raw
)
;
const
uint64x1_t
x4
=
vpaddl_u32
(
x2
)
;
return
vget_lane_u64
(
x4
0
)
;
#
endif
}
template
<
typename
T
>
HWY_INLINE
uint64_t
BitsFromMask
(
hwy
:
:
SizeTag
<
4
>
const
Mask128
<
T
>
mask
)
{
alignas
(
16
)
constexpr
uint32_t
kSliceLanes
[
4
]
=
{
1
2
4
8
}
;
const
Full128
<
T
>
d
;
const
Full128
<
uint32_t
>
du
;
const
Vec128
<
uint32_t
>
values
=
BitCast
(
du
VecFromMask
(
d
mask
)
)
&
Load
(
du
kSliceLanes
)
;
#
if
HWY_ARCH_ARM_A64
return
vaddvq_u32
(
values
.
raw
)
;
#
else
const
uint64x2_t
x2
=
vpaddlq_u32
(
values
.
raw
)
;
return
vgetq_lane_u64
(
x2
0
)
+
vgetq_lane_u64
(
x2
1
)
;
#
endif
}
template
<
typename
T
size_t
N
HWY_IF_LE64
(
T
N
)
>
HWY_INLINE
uint64_t
BitsFromMask
(
hwy
:
:
SizeTag
<
4
>
const
Mask128
<
T
N
>
mask
)
{
alignas
(
8
)
constexpr
uint32_t
kSliceLanes
[
2
]
=
{
1
2
}
;
const
Simd
<
T
N
>
d
;
const
Simd
<
uint32_t
N
>
du
;
const
Vec128
<
uint32_t
N
>
slice
(
Load
(
Simd
<
uint32_t
2
>
(
)
kSliceLanes
)
.
raw
)
;
const
Vec128
<
uint32_t
N
>
values
=
BitCast
(
du
VecFromMask
(
d
mask
)
)
&
slice
;
#
if
HWY_ARCH_ARM_A64
return
vaddv_u32
(
values
.
raw
)
;
#
else
const
uint64x1_t
x2
=
vpaddl_u32
(
values
.
raw
)
;
return
vget_lane_u64
(
x2
0
)
;
#
endif
}
template
<
typename
T
>
HWY_INLINE
uint64_t
BitsFromMask
(
hwy
:
:
SizeTag
<
8
>
const
Mask128
<
T
>
m
)
{
alignas
(
16
)
constexpr
uint64_t
kSliceLanes
[
2
]
=
{
1
2
}
;
const
Full128
<
T
>
d
;
const
Full128
<
uint64_t
>
du
;
const
Vec128
<
uint64_t
>
values
=
BitCast
(
du
VecFromMask
(
d
m
)
)
&
Load
(
du
kSliceLanes
)
;
#
if
HWY_ARCH_ARM_A64
return
vaddvq_u64
(
values
.
raw
)
;
#
else
return
vgetq_lane_u64
(
values
.
raw
0
)
+
vgetq_lane_u64
(
values
.
raw
1
)
;
#
endif
}
template
<
typename
T
>
HWY_INLINE
uint64_t
BitsFromMask
(
hwy
:
:
SizeTag
<
8
>
const
Mask128
<
T
1
>
m
)
{
const
Simd
<
T
1
>
d
;
const
Simd
<
uint64_t
1
>
du
;
const
Vec128
<
uint64_t
1
>
values
=
BitCast
(
du
VecFromMask
(
d
m
)
)
&
Set
(
du
1
)
;
return
vget_lane_u64
(
values
.
raw
0
)
;
}
template
<
typename
T
size_t
N
>
constexpr
uint64_t
OnlyActive
(
uint64_t
bits
)
{
return
(
(
N
*
sizeof
(
T
)
)
>
=
8
)
?
bits
:
(
bits
&
(
(
1ull
<
<
N
)
-
1
)
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
uint64_t
BitsFromMask
(
const
Mask128
<
T
N
>
mask
)
{
return
OnlyActive
<
T
N
>
(
BitsFromMask
(
hwy
:
:
SizeTag
<
sizeof
(
T
)
>
(
)
mask
)
)
;
}
template
<
typename
T
>
HWY_INLINE
size_t
CountTrue
(
hwy
:
:
SizeTag
<
1
>
const
Mask128
<
T
>
mask
)
{
const
Full128
<
int8_t
>
di
;
const
int8x16_t
ones
=
vnegq_s8
(
BitCast
(
di
VecFromMask
(
Full128
<
T
>
(
)
mask
)
)
.
raw
)
;
#
if
HWY_ARCH_ARM_A64
return
vaddvq_s8
(
ones
)
;
#
else
const
int16x8_t
x2
=
vpaddlq_s8
(
ones
)
;
const
int32x4_t
x4
=
vpaddlq_s16
(
x2
)
;
const
int64x2_t
x8
=
vpaddlq_s32
(
x4
)
;
return
x8
[
0
]
+
x8
[
1
]
;
#
endif
}
template
<
typename
T
>
HWY_INLINE
size_t
CountTrue
(
hwy
:
:
SizeTag
<
2
>
const
Mask128
<
T
>
mask
)
{
const
Full128
<
int16_t
>
di
;
const
int16x8_t
ones
=
vnegq_s16
(
BitCast
(
di
VecFromMask
(
Full128
<
T
>
(
)
mask
)
)
.
raw
)
;
#
if
HWY_ARCH_ARM_A64
return
vaddvq_s16
(
ones
)
;
#
else
const
int32x4_t
x2
=
vpaddlq_s16
(
ones
)
;
const
int64x2_t
x4
=
vpaddlq_s32
(
x2
)
;
return
x4
[
0
]
+
x4
[
1
]
;
#
endif
}
template
<
typename
T
>
HWY_INLINE
size_t
CountTrue
(
hwy
:
:
SizeTag
<
4
>
const
Mask128
<
T
>
mask
)
{
const
Full128
<
int32_t
>
di
;
const
int32x4_t
ones
=
vnegq_s32
(
BitCast
(
di
VecFromMask
(
Full128
<
T
>
(
)
mask
)
)
.
raw
)
;
#
if
HWY_ARCH_ARM_A64
return
vaddvq_s32
(
ones
)
;
#
else
const
int64x2_t
x2
=
vpaddlq_s32
(
ones
)
;
return
x2
[
0
]
+
x2
[
1
]
;
#
endif
}
template
<
typename
T
>
HWY_INLINE
size_t
CountTrue
(
hwy
:
:
SizeTag
<
8
>
const
Mask128
<
T
>
mask
)
{
#
if
HWY_ARCH_ARM_A64
const
Full128
<
int64_t
>
di
;
const
int64x2_t
ones
=
vnegq_s64
(
BitCast
(
di
VecFromMask
(
Full128
<
T
>
(
)
mask
)
)
.
raw
)
;
return
vaddvq_s64
(
ones
)
;
#
else
const
Full128
<
int64_t
>
di
;
const
int64x2_t
ones
=
vshrq_n_u64
(
BitCast
(
di
VecFromMask
(
Full128
<
T
>
(
)
mask
)
)
.
raw
63
)
;
return
ones
[
0
]
+
ones
[
1
]
;
#
endif
}
}
template
<
typename
T
>
HWY_INLINE
size_t
CountTrue
(
const
Mask128
<
T
>
mask
)
{
return
detail
:
:
CountTrue
(
hwy
:
:
SizeTag
<
sizeof
(
T
)
>
(
)
mask
)
;
}
template
<
typename
T
size_t
N
HWY_IF_LE64
(
T
N
)
>
HWY_INLINE
size_t
CountTrue
(
const
Mask128
<
T
N
>
mask
)
{
return
PopCount
(
detail
:
:
BitsFromMask
(
mask
)
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
size_t
StoreMaskBits
(
const
Mask128
<
T
N
>
mask
uint8_t
*
p
)
{
const
uint64_t
bits
=
detail
:
:
BitsFromMask
(
mask
)
;
const
size_t
kNumBytes
=
(
N
+
7
)
/
8
;
CopyBytes
<
kNumBytes
>
(
&
bits
p
)
;
return
kNumBytes
;
}
template
<
typename
T
>
HWY_INLINE
bool
AllFalse
(
const
Mask128
<
T
>
m
)
{
#
if
HWY_ARCH_ARM_A64
return
(
vmaxvq_u32
(
m
.
raw
)
=
=
0
)
;
#
else
const
auto
v64
=
BitCast
(
Full128
<
uint64_t
>
(
)
VecFromMask
(
Full128
<
T
>
(
)
m
)
)
;
uint32x2_t
a
=
vqmovn_u64
(
v64
.
raw
)
;
return
vreinterpret_u64_u32
(
a
)
[
0
]
=
=
0
;
#
endif
}
template
<
typename
T
size_t
N
HWY_IF_LE64
(
T
N
)
>
HWY_INLINE
bool
AllFalse
(
const
Mask128
<
T
N
>
m
)
{
return
detail
:
:
BitsFromMask
(
m
)
=
=
0
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
bool
AllTrue
(
const
Mask128
<
T
N
>
m
)
{
const
Simd
<
T
N
>
d
;
return
AllFalse
(
VecFromMask
(
d
m
)
=
=
Zero
(
d
)
)
;
}
namespace
detail
{
HWY_INLINE
Vec128
<
uint8_t
>
Load8Bytes
(
Full128
<
uint8_t
>
const
uint8_t
*
bytes
)
{
return
Vec128
<
uint8_t
>
(
vreinterpretq_u8_u64
(
vld1q_dup_u64
(
reinterpret_cast
<
const
uint64_t
*
>
(
bytes
)
)
)
)
;
}
template
<
size_t
N
HWY_IF_LE64
(
uint8_t
N
)
>
HWY_INLINE
Vec128
<
uint8_t
N
>
Load8Bytes
(
Simd
<
uint8_t
N
>
d
const
uint8_t
*
bytes
)
{
return
Load
(
d
bytes
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
IdxFromBits
(
hwy
:
:
SizeTag
<
2
>
const
uint64_t
mask_bits
)
{
HWY_DASSERT
(
mask_bits
<
256
)
;
const
Simd
<
T
N
>
d
;
const
Repartition
<
uint8_t
decltype
(
d
)
>
d8
;
const
Simd
<
uint16_t
N
>
du
;
alignas
(
16
)
constexpr
uint8_t
table
[
256
*
8
]
=
{
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
4
0
0
0
0
0
0
0
0
4
0
0
0
0
0
0
2
4
0
0
0
0
0
0
0
2
4
0
0
0
0
0
6
0
0
0
0
0
0
0
0
6
0
0
0
0
0
0
2
6
0
0
0
0
0
0
0
2
6
0
0
0
0
0
4
6
0
0
0
0
0
0
0
4
6
0
0
0
0
0
2
4
6
0
0
0
0
0
0
2
4
6
0
0
0
0
8
0
0
0
0
0
0
0
0
8
0
0
0
0
0
0
2
8
0
0
0
0
0
0
0
2
8
0
0
0
0
0
4
8
0
0
0
0
0
0
0
4
8
0
0
0
0
0
2
4
8
0
0
0
0
0
0
2
4
8
0
0
0
0
6
8
0
0
0
0
0
0
0
6
8
0
0
0
0
0
2
6
8
0
0
0
0
0
0
2
6
8
0
0
0
0
4
6
8
0
0
0
0
0
0
4
6
8
0
0
0
0
2
4
6
8
0
0
0
0
0
2
4
6
8
0
0
0
10
0
0
0
0
0
0
0
0
10
0
0
0
0
0
0
2
10
0
0
0
0
0
0
0
2
10
0
0
0
0
0
4
10
0
0
0
0
0
0
0
4
10
0
0
0
0
0
2
4
10
0
0
0
0
0
0
2
4
10
0
0
0
0
6
10
0
0
0
0
0
0
0
6
10
0
0
0
0
0
2
6
10
0
0
0
0
0
0
2
6
10
0
0
0
0
4
6
10
0
0
0
0
0
0
4
6
10
0
0
0
0
2
4
6
10
0
0
0
0
0
2
4
6
10
0
0
0
8
10
0
0
0
0
0
0
0
8
10
0
0
0
0
0
2
8
10
0
0
0
0
0
0
2
8
10
0
0
0
0
4
8
10
0
0
0
0
0
0
4
8
10
0
0
0
0
2
4
8
10
0
0
0
0
0
2
4
8
10
0
0
0
6
8
10
0
0
0
0
0
0
6
8
10
0
0
0
0
2
6
8
10
0
0
0
0
0
2
6
8
10
0
0
0
4
6
8
10
0
0
0
0
0
4
6
8
10
0
0
0
2
4
6
8
10
0
0
0
0
2
4
6
8
10
0
0
12
0
0
0
0
0
0
0
0
12
0
0
0
0
0
0
2
12
0
0
0
0
0
0
0
2
12
0
0
0
0
0
4
12
0
0
0
0
0
0
0
4
12
0
0
0
0
0
2
4
12
0
0
0
0
0
0
2
4
12
0
0
0
0
6
12
0
0
0
0
0
0
0
6
12
0
0
0
0
0
2
6
12
0
0
0
0
0
0
2
6
12
0
0
0
0
4
6
12
0
0
0
0
0
0
4
6
12
0
0
0
0
2
4
6
12
0
0
0
0
0
2
4
6
12
0
0
0
8
12
0
0
0
0
0
0
0
8
12
0
0
0
0
0
2
8
12
0
0
0
0
0
0
2
8
12
0
0
0
0
4
8
12
0
0
0
0
0
0
4
8
12
0
0
0
0
2
4
8
12
0
0
0
0
0
2
4
8
12
0
0
0
6
8
12
0
0
0
0
0
0
6
8
12
0
0
0
0
2
6
8
12
0
0
0
0
0
2
6
8
12
0
0
0
4
6
8
12
0
0
0
0
0
4
6
8
12
0
0
0
2
4
6
8
12
0
0
0
0
2
4
6
8
12
0
0
10
12
0
0
0
0
0
0
0
10
12
0
0
0
0
0
2
10
12
0
0
0
0
0
0
2
10
12
0
0
0
0
4
10
12
0
0
0
0
0
0
4
10
12
0
0
0
0
2
4
10
12
0
0
0
0
0
2
4
10
12
0
0
0
6
10
12
0
0
0
0
0
0
6
10
12
0
0
0
0
2
6
10
12
0
0
0
0
0
2
6
10
12
0
0
0
4
6
10
12
0
0
0
0
0
4
6
10
12
0
0
0
2
4
6
10
12
0
0
0
0
2
4
6
10
12
0
0
8
10
12
0
0
0
0
0
0
8
10
12
0
0
0
0
2
8
10
12
0
0
0
0
0
2
8
10
12
0
0
0
4
8
10
12
0
0
0
0
0
4
8
10
12
0
0
0
2
4
8
10
12
0
0
0
0
2
4
8
10
12
0
0
6
8
10
12
0
0
0
0
0
6
8
10
12
0
0
0
2
6
8
10
12
0
0
0
0
2
6
8
10
12
0
0
4
6
8
10
12
0
0
0
0
4
6
8
10
12
0
0
2
4
6
8
10
12
0
0
0
2
4
6
8
10
12
0
14
0
0
0
0
0
0
0
0
14
0
0
0
0
0
0
2
14
0
0
0
0
0
0
0
2
14
0
0
0
0
0
4
14
0
0
0
0
0
0
0
4
14
0
0
0
0
0
2
4
14
0
0
0
0
0
0
2
4
14
0
0
0
0
6
14
0
0
0
0
0
0
0
6
14
0
0
0
0
0
2
6
14
0
0
0
0
0
0
2
6
14
0
0
0
0
4
6
14
0
0
0
0
0
0
4
6
14
0
0
0
0
2
4
6
14
0
0
0
0
0
2
4
6
14
0
0
0
8
14
0
0
0
0
0
0
0
8
14
0
0
0
0
0
2
8
14
0
0
0
0
0
0
2
8
14
0
0
0
0
4
8
14
0
0
0
0
0
0
4
8
14
0
0
0
0
2
4
8
14
0
0
0
0
0
2
4
8
14
0
0
0
6
8
14
0
0
0
0
0
0
6
8
14
0
0
0
0
2
6
8
14
0
0
0
0
0
2
6
8
14
0
0
0
4
6
8
14
0
0
0
0
0
4
6
8
14
0
0
0
2
4
6
8
14
0
0
0
0
2
4
6
8
14
0
0
10
14
0
0
0
0
0
0
0
10
14
0
0
0
0
0
2
10
14
0
0
0
0
0
0
2
10
14
0
0
0
0
4
10
14
0
0
0
0
0
0
4
10
14
0
0
0
0
2
4
10
14
0
0
0
0
0
2
4
10
14
0
0
0
6
10
14
0
0
0
0
0
0
6
10
14
0
0
0
0
2
6
10
14
0
0
0
0
0
2
6
10
14
0
0
0
4
6
10
14
0
0
0
0
0
4
6
10
14
0
0
0
2
4
6
10
14
0
0
0
0
2
4
6
10
14
0
0
8
10
14
0
0
0
0
0
0
8
10
14
0
0
0
0
2
8
10
14
0
0
0
0
0
2
8
10
14
0
0
0
4
8
10
14
0
0
0
0
0
4
8
10
14
0
0
0
2
4
8
10
14
0
0
0
0
2
4
8
10
14
0
0
6
8
10
14
0
0
0
0
0
6
8
10
14
0
0
0
2
6
8
10
14
0
0
0
0
2
6
8
10
14
0
0
4
6
8
10
14
0
0
0
0
4
6
8
10
14
0
0
2
4
6
8
10
14
0
0
0
2
4
6
8
10
14
0
12
14
0
0
0
0
0
0
0
12
14
0
0
0
0
0
2
12
14
0
0
0
0
0
0
2
12
14
0
0
0
0
4
12
14
0
0
0
0
0
0
4
12
14
0
0
0
0
2
4
12
14
0
0
0
0
0
2
4
12
14
0
0
0
6
12
14
0
0
0
0
0
0
6
12
14
0
0
0
0
2
6
12
14
0
0
0
0
0
2
6
12
14
0
0
0
4
6
12
14
0
0
0
0
0
4
6
12
14
0
0
0
2
4
6
12
14
0
0
0
0
2
4
6
12
14
0
0
8
12
14
0
0
0
0
0
0
8
12
14
0
0
0
0
2
8
12
14
0
0
0
0
0
2
8
12
14
0
0
0
4
8
12
14
0
0
0
0
0
4
8
12
14
0
0
0
2
4
8
12
14
0
0
0
0
2
4
8
12
14
0
0
6
8
12
14
0
0
0
0
0
6
8
12
14
0
0
0
2
6
8
12
14
0
0
0
0
2
6
8
12
14
0
0
4
6
8
12
14
0
0
0
0
4
6
8
12
14
0
0
2
4
6
8
12
14
0
0
0
2
4
6
8
12
14
0
10
12
14
0
0
0
0
0
0
10
12
14
0
0
0
0
2
10
12
14
0
0
0
0
0
2
10
12
14
0
0
0
4
10
12
14
0
0
0
0
0
4
10
12
14
0
0
0
2
4
10
12
14
0
0
0
0
2
4
10
12
14
0
0
6
10
12
14
0
0
0
0
0
6
10
12
14
0
0
0
2
6
10
12
14
0
0
0
0
2
6
10
12
14
0
0
4
6
10
12
14
0
0
0
0
4
6
10
12
14
0
0
2
4
6
10
12
14
0
0
0
2
4
6
10
12
14
0
8
10
12
14
0
0
0
0
0
8
10
12
14
0
0
0
2
8
10
12
14
0
0
0
0
2
8
10
12
14
0
0
4
8
10
12
14
0
0
0
0
4
8
10
12
14
0
0
2
4
8
10
12
14
0
0
0
2
4
8
10
12
14
0
6
8
10
12
14
0
0
0
0
6
8
10
12
14
0
0
2
6
8
10
12
14
0
0
0
2
6
8
10
12
14
0
4
6
8
10
12
14
0
0
0
4
6
8
10
12
14
0
2
4
6
8
10
12
14
0
0
2
4
6
8
10
12
14
}
;
const
Vec128
<
uint8_t
2
*
N
>
byte_idx
=
Load8Bytes
(
d8
table
+
mask_bits
*
8
)
;
const
Vec128
<
uint16_t
N
>
pairs
=
ZipLower
(
byte_idx
byte_idx
)
;
return
BitCast
(
d
pairs
+
Set
(
du
0x0100
)
)
;
}
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
IdxFromBits
(
hwy
:
:
SizeTag
<
4
>
const
uint64_t
mask_bits
)
{
HWY_DASSERT
(
mask_bits
<
16
)
;
alignas
(
16
)
constexpr
uint8_t
packed_array
[
16
*
16
]
=
{
0
1
2
3
0
1
2
3
0
1
2
3
0
1
2
3
0
1
2
3
0
1
2
3
0
1
2
3
0
1
2
3
4
5
6
7
0
1
2
3
0
1
2
3
0
1
2
3
0
1
2
3
4
5
6
7
0
1
2
3
0
1
2
3
8
9
10
11
0
1
2
3
0
1
2
3
0
1
2
3
0
1
2
3
8
9
10
11
0
1
2
3
0
1
2
3
4
5
6
7
8
9
10
11
0
1
2
3
0
1
2
3
0
1
2
3
4
5
6
7
8
9
10
11
0
1
2
3
12
13
14
15
0
1
2
3
0
1
2
3
0
1
2
3
0
1
2
3
12
13
14
15
0
1
2
3
0
1
2
3
4
5
6
7
12
13
14
15
0
1
2
3
0
1
2
3
0
1
2
3
4
5
6
7
12
13
14
15
0
1
2
3
8
9
10
11
12
13
14
15
0
1
2
3
0
1
2
3
0
1
2
3
8
9
10
11
12
13
14
15
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
0
1
2
3
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
}
;
const
Simd
<
T
N
>
d
;
const
Repartition
<
uint8_t
decltype
(
d
)
>
d8
;
return
BitCast
(
d
Load
(
d8
packed_array
+
16
*
mask_bits
)
)
;
}
#
if
HWY_CAP_INTEGER64
|
|
HWY_CAP_FLOAT64
template
<
typename
T
size_t
N
>
HWY_INLINE
Vec128
<
T
N
>
IdxFromBits
(
hwy
:
:
SizeTag
<
8
>
const
uint64_t
mask_bits
)
{
HWY_DASSERT
(
mask_bits
<
4
)
;
alignas
(
16
)
constexpr
uint8_t
packed_array
[
4
*
16
]
=
{
0
1
2
3
4
5
6
7
0
1
2
3
4
5
6
7
0
1
2
3
4
5
6
7
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
0
1
2
3
4
5
6
7
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
}
;
const
Simd
<
T
N
>
d
;
const
Repartition
<
uint8_t
decltype
(
d
)
>
d8
;
return
BitCast
(
d
Load
(
d8
packed_array
+
16
*
mask_bits
)
)
;
}
#
endif
template
<
typename
T
size_t
N
>
HWY_API
Vec128
<
T
N
>
Compress
(
Vec128
<
T
N
>
v
const
uint64_t
mask_bits
)
{
const
auto
idx
=
detail
:
:
IdxFromBits
<
T
N
>
(
hwy
:
:
SizeTag
<
sizeof
(
T
)
>
(
)
mask_bits
)
;
using
D
=
Simd
<
T
N
>
;
const
RebindToSigned
<
D
>
di
;
return
BitCast
(
D
(
)
TableLookupBytes
(
BitCast
(
di
v
)
BitCast
(
di
idx
)
)
)
;
}
}
template
<
typename
T
size_t
N
>
HWY_API
Vec128
<
T
N
>
Compress
(
Vec128
<
T
N
>
v
const
Mask128
<
T
N
>
mask
)
{
return
detail
:
:
Compress
(
v
detail
:
:
BitsFromMask
(
mask
)
)
;
}
template
<
typename
T
size_t
N
>
HWY_API
size_t
CompressStore
(
Vec128
<
T
N
>
v
const
Mask128
<
T
N
>
mask
Simd
<
T
N
>
d
T
*
HWY_RESTRICT
aligned
)
{
const
uint64_t
mask_bits
=
detail
:
:
BitsFromMask
(
mask
)
;
Store
(
detail
:
:
Compress
(
v
mask_bits
)
d
aligned
)
;
return
PopCount
(
mask_bits
)
;
}
HWY_API
void
StoreInterleaved3
(
const
Vec128
<
uint8_t
>
v0
const
Vec128
<
uint8_t
>
v1
const
Vec128
<
uint8_t
>
v2
Full128
<
uint8_t
>
uint8_t
*
HWY_RESTRICT
unaligned
)
{
const
uint8x16x3_t
triple
=
{
v0
.
raw
v1
.
raw
v2
.
raw
}
;
vst3q_u8
(
unaligned
triple
)
;
}
HWY_API
void
StoreInterleaved3
(
const
Vec128
<
uint8_t
8
>
v0
const
Vec128
<
uint8_t
8
>
v1
const
Vec128
<
uint8_t
8
>
v2
Simd
<
uint8_t
8
>
uint8_t
*
HWY_RESTRICT
unaligned
)
{
const
uint8x8x3_t
triple
=
{
v0
.
raw
v1
.
raw
v2
.
raw
}
;
vst3_u8
(
unaligned
triple
)
;
}
template
<
size_t
N
HWY_IF_LE32
(
uint8_t
N
)
>
HWY_API
void
StoreInterleaved3
(
const
Vec128
<
uint8_t
N
>
v0
const
Vec128
<
uint8_t
N
>
v1
const
Vec128
<
uint8_t
N
>
v2
Simd
<
uint8_t
N
>
uint8_t
*
HWY_RESTRICT
unaligned
)
{
alignas
(
16
)
uint8_t
buf
[
24
]
;
const
uint8x8x3_t
triple
=
{
v0
.
raw
v1
.
raw
v2
.
raw
}
;
vst3_u8
(
buf
triple
)
;
CopyBytes
<
N
*
3
>
(
buf
unaligned
)
;
}
HWY_API
void
StoreInterleaved4
(
const
Vec128
<
uint8_t
>
v0
const
Vec128
<
uint8_t
>
v1
const
Vec128
<
uint8_t
>
v2
const
Vec128
<
uint8_t
>
v3
Full128
<
uint8_t
>
uint8_t
*
HWY_RESTRICT
unaligned
)
{
const
uint8x16x4_t
quad
=
{
v0
.
raw
v1
.
raw
v2
.
raw
v3
.
raw
}
;
vst4q_u8
(
unaligned
quad
)
;
}
HWY_API
void
StoreInterleaved4
(
const
Vec128
<
uint8_t
8
>
v0
const
Vec128
<
uint8_t
8
>
v1
const
Vec128
<
uint8_t
8
>
v2
const
Vec128
<
uint8_t
8
>
v3
Simd
<
uint8_t
8
>
uint8_t
*
HWY_RESTRICT
unaligned
)
{
const
uint8x8x4_t
quad
=
{
v0
.
raw
v1
.
raw
v2
.
raw
v3
.
raw
}
;
vst4_u8
(
unaligned
quad
)
;
}
template
<
size_t
N
HWY_IF_LE32
(
uint8_t
N
)
>
HWY_API
void
StoreInterleaved4
(
const
Vec128
<
uint8_t
N
>
v0
const
Vec128
<
uint8_t
N
>
v1
const
Vec128
<
uint8_t
N
>
v2
const
Vec128
<
uint8_t
N
>
v3
Simd
<
uint8_t
N
>
uint8_t
*
HWY_RESTRICT
unaligned
)
{
alignas
(
16
)
uint8_t
buf
[
32
]
;
const
uint8x8x4_t
quad
=
{
v0
.
raw
v1
.
raw
v2
.
raw
v3
.
raw
}
;
vst4_u8
(
buf
quad
)
;
CopyBytes
<
N
*
4
>
(
buf
unaligned
)
;
}
template
<
class
V
>
HWY_API
V
Add
(
V
a
V
b
)
{
return
a
+
b
;
}
template
<
class
V
>
HWY_API
V
Sub
(
V
a
V
b
)
{
return
a
-
b
;
}
template
<
class
V
>
HWY_API
V
Mul
(
V
a
V
b
)
{
return
a
*
b
;
}
template
<
class
V
>
HWY_API
V
Div
(
V
a
V
b
)
{
return
a
/
b
;
}
template
<
class
V
>
V
Shl
(
V
a
V
b
)
{
return
a
<
<
b
;
}
template
<
class
V
>
V
Shr
(
V
a
V
b
)
{
return
a
>
>
b
;
}
template
<
class
V
>
HWY_API
auto
Eq
(
V
a
V
b
)
-
>
decltype
(
a
=
=
b
)
{
return
a
=
=
b
;
}
template
<
class
V
>
HWY_API
auto
Lt
(
V
a
V
b
)
-
>
decltype
(
a
=
=
b
)
{
return
a
<
b
;
}
template
<
class
V
>
HWY_API
auto
Gt
(
V
a
V
b
)
-
>
decltype
(
a
=
=
b
)
{
return
a
>
b
;
}
template
<
class
V
>
HWY_API
auto
Ge
(
V
a
V
b
)
-
>
decltype
(
a
=
=
b
)
{
return
a
>
=
b
;
}
template
<
class
V
>
HWY_API
auto
Le
(
V
a
V
b
)
-
>
decltype
(
a
=
=
b
)
{
return
a
<
=
b
;
}
#
if
HWY_ARCH_ARM_V7
#
undef
vuzp1_s8
#
undef
vuzp1_u8
#
undef
vuzp1_s16
#
undef
vuzp1_u16
#
undef
vuzp1_s32
#
undef
vuzp1_u32
#
undef
vuzp1_f32
#
undef
vuzp1q_s8
#
undef
vuzp1q_u8
#
undef
vuzp1q_s16
#
undef
vuzp1q_u16
#
undef
vuzp1q_s32
#
undef
vuzp1q_u32
#
undef
vuzp1q_f32
#
undef
vuzp2_s8
#
undef
vuzp2_u8
#
undef
vuzp2_s16
#
undef
vuzp2_u16
#
undef
vuzp2_s32
#
undef
vuzp2_u32
#
undef
vuzp2_f32
#
undef
vuzp2q_s8
#
undef
vuzp2q_u8
#
undef
vuzp2q_s16
#
undef
vuzp2q_u16
#
undef
vuzp2q_s32
#
undef
vuzp2q_u32
#
undef
vuzp2q_f32
#
undef
vzip1_s8
#
undef
vzip1_u8
#
undef
vzip1_s16
#
undef
vzip1_u16
#
undef
vzip1_s32
#
undef
vzip1_u32
#
undef
vzip1_f32
#
undef
vzip1q_s8
#
undef
vzip1q_u8
#
undef
vzip1q_s16
#
undef
vzip1q_u16
#
undef
vzip1q_s32
#
undef
vzip1q_u32
#
undef
vzip1q_f32
#
undef
vzip2_s8
#
undef
vzip2_u8
#
undef
vzip2_s16
#
undef
vzip2_u16
#
undef
vzip2_s32
#
undef
vzip2_u32
#
undef
vzip2_f32
#
undef
vzip2q_s8
#
undef
vzip2q_u8
#
undef
vzip2q_s16
#
undef
vzip2q_u16
#
undef
vzip2q_s32
#
undef
vzip2q_u32
#
undef
vzip2q_f32
#
endif
#
undef
HWY_NEON_BUILD_ARG_1
#
undef
HWY_NEON_BUILD_ARG_2
#
undef
HWY_NEON_BUILD_ARG_3
#
undef
HWY_NEON_BUILD_PARAM_1
#
undef
HWY_NEON_BUILD_PARAM_2
#
undef
HWY_NEON_BUILD_PARAM_3
#
undef
HWY_NEON_BUILD_RET_1
#
undef
HWY_NEON_BUILD_RET_2
#
undef
HWY_NEON_BUILD_RET_3
#
undef
HWY_NEON_BUILD_TPL_1
#
undef
HWY_NEON_BUILD_TPL_2
#
undef
HWY_NEON_BUILD_TPL_3
#
undef
HWY_NEON_DEF_FUNCTION
#
undef
HWY_NEON_DEF_FUNCTION_ALL_FLOATS
#
undef
HWY_NEON_DEF_FUNCTION_ALL_TYPES
#
undef
HWY_NEON_DEF_FUNCTION_INT_8
#
undef
HWY_NEON_DEF_FUNCTION_INT_16
#
undef
HWY_NEON_DEF_FUNCTION_INT_32
#
undef
HWY_NEON_DEF_FUNCTION_INT_8_16_32
#
undef
HWY_NEON_DEF_FUNCTION_INTS
#
undef
HWY_NEON_DEF_FUNCTION_INTS_UINTS
#
undef
HWY_NEON_DEF_FUNCTION_TPL
#
undef
HWY_NEON_DEF_FUNCTION_UINT_8
#
undef
HWY_NEON_DEF_FUNCTION_UINT_16
#
undef
HWY_NEON_DEF_FUNCTION_UINT_32
#
undef
HWY_NEON_DEF_FUNCTION_UINT_8_16_32
#
undef
HWY_NEON_DEF_FUNCTION_UINTS
#
undef
HWY_NEON_EVAL
}
}
HWY_AFTER_NAMESPACE
(
)
;
