#
include
<
cstdint
>
#
include
<
limits
>
#
include
<
random
>
#
include
<
thread
>
#
include
<
type_traits
>
#
include
<
vector
>
#
include
"
gtest
/
gtest
.
h
"
#
include
"
absl
/
base
/
attributes
.
h
"
#
include
"
absl
/
base
/
config
.
h
"
#
include
"
absl
/
base
/
internal
/
low_level_scheduling
.
h
"
#
include
"
absl
/
base
/
internal
/
scheduling_mode
.
h
"
#
include
"
absl
/
base
/
internal
/
spinlock
.
h
"
#
include
"
absl
/
base
/
internal
/
sysinfo
.
h
"
#
include
"
absl
/
base
/
macros
.
h
"
#
include
"
absl
/
synchronization
/
blocking_counter
.
h
"
#
include
"
absl
/
synchronization
/
notification
.
h
"
constexpr
int32_t
kNumThreads
=
10
;
constexpr
int32_t
kIters
=
1000
;
namespace
absl
{
ABSL_NAMESPACE_BEGIN
namespace
base_internal
{
struct
SpinLockTest
{
static
uint32_t
EncodeWaitCycles
(
int64_t
wait_start_time
int64_t
wait_end_time
)
{
return
SpinLock
:
:
EncodeWaitCycles
(
wait_start_time
wait_end_time
)
;
}
static
uint64_t
DecodeWaitCycles
(
uint32_t
lock_value
)
{
return
SpinLock
:
:
DecodeWaitCycles
(
lock_value
)
;
}
}
;
namespace
{
static
constexpr
int
kArrayLength
=
10
;
static
uint32_t
values
[
kArrayLength
]
;
ABSL_CONST_INIT
static
SpinLock
static_cooperative_spinlock
(
absl
:
:
kConstInit
base_internal
:
:
SCHEDULE_COOPERATIVE_AND_KERNEL
)
;
ABSL_CONST_INIT
static
SpinLock
static_noncooperative_spinlock
(
absl
:
:
kConstInit
base_internal
:
:
SCHEDULE_KERNEL_ONLY
)
;
static
uint32_t
Hash32
(
uint32_t
a
uint32_t
c
)
{
uint32_t
b
=
0x9e3779b9UL
;
a
-
=
b
;
a
-
=
c
;
a
^
=
(
c
>
>
13
)
;
b
-
=
c
;
b
-
=
a
;
b
^
=
(
a
<
<
8
)
;
c
-
=
a
;
c
-
=
b
;
c
^
=
(
b
>
>
13
)
;
a
-
=
b
;
a
-
=
c
;
a
^
=
(
c
>
>
12
)
;
b
-
=
c
;
b
-
=
a
;
b
^
=
(
a
<
<
16
)
;
c
-
=
a
;
c
-
=
b
;
c
^
=
(
b
>
>
5
)
;
a
-
=
b
;
a
-
=
c
;
a
^
=
(
c
>
>
3
)
;
b
-
=
c
;
b
-
=
a
;
b
^
=
(
a
<
<
10
)
;
c
-
=
a
;
c
-
=
b
;
c
^
=
(
b
>
>
15
)
;
return
c
;
}
static
void
TestFunction
(
int
thread_salt
SpinLock
*
spinlock
)
{
for
(
int
i
=
0
;
i
<
kIters
;
i
+
+
)
{
SpinLockHolder
h
(
spinlock
)
;
for
(
int
j
=
0
;
j
<
kArrayLength
;
j
+
+
)
{
const
int
index
=
(
j
+
thread_salt
)
%
kArrayLength
;
values
[
index
]
=
Hash32
(
values
[
index
]
thread_salt
)
;
std
:
:
this_thread
:
:
yield
(
)
;
}
}
}
static
void
ThreadedTest
(
SpinLock
*
spinlock
)
{
std
:
:
vector
<
std
:
:
thread
>
threads
;
threads
.
reserve
(
kNumThreads
)
;
for
(
int
i
=
0
;
i
<
kNumThreads
;
+
+
i
)
{
threads
.
push_back
(
std
:
:
thread
(
TestFunction
i
spinlock
)
)
;
}
for
(
auto
&
thread
:
threads
)
{
thread
.
join
(
)
;
}
SpinLockHolder
h
(
spinlock
)
;
for
(
int
i
=
1
;
i
<
kArrayLength
;
i
+
+
)
{
EXPECT_EQ
(
values
[
0
]
values
[
i
]
)
;
}
}
#
ifndef
ABSL_HAVE_THREAD_SANITIZER
static_assert
(
std
:
:
is_trivially_destructible
<
SpinLock
>
(
)
"
"
)
;
#
endif
TEST
(
SpinLock
StackNonCooperativeDisablesScheduling
)
{
SpinLock
spinlock
(
base_internal
:
:
SCHEDULE_KERNEL_ONLY
)
;
spinlock
.
Lock
(
)
;
EXPECT_FALSE
(
base_internal
:
:
SchedulingGuard
:
:
ReschedulingIsAllowed
(
)
)
;
spinlock
.
Unlock
(
)
;
}
TEST
(
SpinLock
StaticNonCooperativeDisablesScheduling
)
{
static_noncooperative_spinlock
.
Lock
(
)
;
EXPECT_FALSE
(
base_internal
:
:
SchedulingGuard
:
:
ReschedulingIsAllowed
(
)
)
;
static_noncooperative_spinlock
.
Unlock
(
)
;
}
TEST
(
SpinLock
WaitCyclesEncoding
)
{
const
int
kProfileTimestampShift
=
7
;
const
int
kLockwordReservedShift
=
3
;
const
uint32_t
kSpinLockSleeper
=
8
;
const
int
kMaxCyclesShift
=
32
-
kLockwordReservedShift
+
kProfileTimestampShift
;
const
uint64_t
kMaxCycles
=
(
int64_t
{
1
}
<
<
kMaxCyclesShift
)
-
1
;
const
uint32_t
kLockwordReservedMask
=
(
1
<
<
kLockwordReservedShift
)
-
1
;
const
uint64_t
kProfileTimestampMask
=
(
1
<
<
kProfileTimestampShift
)
-
1
;
std
:
:
default_random_engine
generator
;
std
:
:
uniform_int_distribution
<
uint64_t
>
time_distribution
(
0
std
:
:
numeric_limits
<
uint64_t
>
:
:
max
(
)
>
>
4
)
;
std
:
:
uniform_int_distribution
<
uint64_t
>
cycle_distribution
(
0
kMaxCycles
)
;
for
(
int
i
=
0
;
i
<
100
;
i
+
+
)
{
int64_t
start_time
=
time_distribution
(
generator
)
;
int64_t
cycles
=
cycle_distribution
(
generator
)
;
int64_t
end_time
=
start_time
+
cycles
;
uint32_t
lock_value
=
SpinLockTest
:
:
EncodeWaitCycles
(
start_time
end_time
)
;
EXPECT_EQ
(
0
lock_value
&
kLockwordReservedMask
)
;
uint64_t
decoded
=
SpinLockTest
:
:
DecodeWaitCycles
(
lock_value
)
;
EXPECT_EQ
(
0
decoded
&
kProfileTimestampMask
)
;
EXPECT_EQ
(
cycles
&
~
kProfileTimestampMask
decoded
)
;
}
int64_t
start_time
=
time_distribution
(
generator
)
;
EXPECT_EQ
(
kSpinLockSleeper
SpinLockTest
:
:
EncodeWaitCycles
(
start_time
start_time
)
)
;
EXPECT_EQ
(
0
SpinLockTest
:
:
DecodeWaitCycles
(
0
)
)
;
EXPECT_EQ
(
0
SpinLockTest
:
:
DecodeWaitCycles
(
kLockwordReservedMask
)
)
;
EXPECT_EQ
(
kMaxCycles
&
~
kProfileTimestampMask
SpinLockTest
:
:
DecodeWaitCycles
(
~
kLockwordReservedMask
)
)
;
int64_t
sleeper_cycles
=
kSpinLockSleeper
<
<
(
kProfileTimestampShift
-
kLockwordReservedShift
)
;
uint32_t
sleeper_value
=
SpinLockTest
:
:
EncodeWaitCycles
(
start_time
start_time
+
sleeper_cycles
)
;
EXPECT_NE
(
sleeper_value
kSpinLockSleeper
)
;
uint32_t
max_value
=
SpinLockTest
:
:
EncodeWaitCycles
(
start_time
start_time
+
kMaxCycles
)
;
uint64_t
max_value_decoded
=
SpinLockTest
:
:
DecodeWaitCycles
(
max_value
)
;
uint64_t
expected_max_value_decoded
=
kMaxCycles
&
~
kProfileTimestampMask
;
EXPECT_EQ
(
expected_max_value_decoded
max_value_decoded
)
;
const
int64_t
step
=
(
1
<
<
kProfileTimestampShift
)
;
uint32_t
after_max_value
=
SpinLockTest
:
:
EncodeWaitCycles
(
start_time
start_time
+
kMaxCycles
+
step
)
;
uint64_t
after_max_value_decoded
=
SpinLockTest
:
:
DecodeWaitCycles
(
after_max_value
)
;
EXPECT_EQ
(
expected_max_value_decoded
after_max_value_decoded
)
;
uint32_t
before_max_value
=
SpinLockTest
:
:
EncodeWaitCycles
(
start_time
start_time
+
kMaxCycles
-
step
)
;
uint64_t
before_max_value_decoded
=
SpinLockTest
:
:
DecodeWaitCycles
(
before_max_value
)
;
EXPECT_GT
(
expected_max_value_decoded
before_max_value_decoded
)
;
}
TEST
(
SpinLockWithThreads
StackSpinLock
)
{
SpinLock
spinlock
;
ThreadedTest
(
&
spinlock
)
;
}
TEST
(
SpinLockWithThreads
StackCooperativeSpinLock
)
{
SpinLock
spinlock
(
base_internal
:
:
SCHEDULE_COOPERATIVE_AND_KERNEL
)
;
ThreadedTest
(
&
spinlock
)
;
}
TEST
(
SpinLockWithThreads
StackNonCooperativeSpinLock
)
{
SpinLock
spinlock
(
base_internal
:
:
SCHEDULE_KERNEL_ONLY
)
;
ThreadedTest
(
&
spinlock
)
;
}
TEST
(
SpinLockWithThreads
StaticCooperativeSpinLock
)
{
ThreadedTest
(
&
static_cooperative_spinlock
)
;
}
TEST
(
SpinLockWithThreads
StaticNonCooperativeSpinLock
)
{
ThreadedTest
(
&
static_noncooperative_spinlock
)
;
}
TEST
(
SpinLockWithThreads
DoesNotDeadlock
)
{
struct
Helper
{
static
void
NotifyThenLock
(
Notification
*
locked
SpinLock
*
spinlock
BlockingCounter
*
b
)
{
locked
-
>
WaitForNotification
(
)
;
b
-
>
DecrementCount
(
)
;
SpinLockHolder
l
(
spinlock
)
;
}
static
void
LockThenWait
(
Notification
*
locked
SpinLock
*
spinlock
BlockingCounter
*
b
)
{
SpinLockHolder
l
(
spinlock
)
;
locked
-
>
Notify
(
)
;
b
-
>
Wait
(
)
;
}
static
void
DeadlockTest
(
SpinLock
*
spinlock
int
num_spinners
)
{
Notification
locked
;
BlockingCounter
counter
(
num_spinners
)
;
std
:
:
vector
<
std
:
:
thread
>
threads
;
threads
.
push_back
(
std
:
:
thread
(
Helper
:
:
LockThenWait
&
locked
spinlock
&
counter
)
)
;
for
(
int
i
=
0
;
i
<
num_spinners
;
+
+
i
)
{
threads
.
push_back
(
std
:
:
thread
(
Helper
:
:
NotifyThenLock
&
locked
spinlock
&
counter
)
)
;
}
for
(
auto
&
thread
:
threads
)
{
thread
.
join
(
)
;
}
}
}
;
SpinLock
stack_cooperative_spinlock
(
base_internal
:
:
SCHEDULE_COOPERATIVE_AND_KERNEL
)
;
SpinLock
stack_noncooperative_spinlock
(
base_internal
:
:
SCHEDULE_KERNEL_ONLY
)
;
Helper
:
:
DeadlockTest
(
&
stack_cooperative_spinlock
base_internal
:
:
NumCPUs
(
)
*
2
)
;
Helper
:
:
DeadlockTest
(
&
stack_noncooperative_spinlock
base_internal
:
:
NumCPUs
(
)
*
2
)
;
Helper
:
:
DeadlockTest
(
&
static_cooperative_spinlock
base_internal
:
:
NumCPUs
(
)
*
2
)
;
Helper
:
:
DeadlockTest
(
&
static_noncooperative_spinlock
base_internal
:
:
NumCPUs
(
)
*
2
)
;
}
}
}
ABSL_NAMESPACE_END
}
