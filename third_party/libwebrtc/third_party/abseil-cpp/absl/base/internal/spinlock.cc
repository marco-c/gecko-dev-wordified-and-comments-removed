#
include
"
absl
/
base
/
internal
/
spinlock
.
h
"
#
include
<
algorithm
>
#
include
<
atomic
>
#
include
<
limits
>
#
include
"
absl
/
base
/
attributes
.
h
"
#
include
"
absl
/
base
/
config
.
h
"
#
include
"
absl
/
base
/
internal
/
atomic_hook
.
h
"
#
include
"
absl
/
base
/
internal
/
cycleclock
.
h
"
#
include
"
absl
/
base
/
internal
/
spinlock_wait
.
h
"
#
include
"
absl
/
base
/
internal
/
sysinfo
.
h
"
#
include
"
absl
/
base
/
call_once
.
h
"
namespace
absl
{
ABSL_NAMESPACE_BEGIN
namespace
base_internal
{
ABSL_INTERNAL_ATOMIC_HOOK_ATTRIBUTES
static
base_internal
:
:
AtomicHook
<
void
(
*
)
(
const
void
*
lock
int64_t
wait_cycles
)
>
submit_profile_data
;
void
RegisterSpinLockProfiler
(
void
(
*
fn
)
(
const
void
*
contendedlock
int64_t
wait_cycles
)
)
{
submit_profile_data
.
Store
(
fn
)
;
}
#
ifdef
ABSL_INTERNAL_NEED_REDUNDANT_CONSTEXPR_DECL
constexpr
uint32_t
SpinLock
:
:
kSpinLockHeld
;
constexpr
uint32_t
SpinLock
:
:
kSpinLockCooperative
;
constexpr
uint32_t
SpinLock
:
:
kSpinLockDisabledScheduling
;
constexpr
uint32_t
SpinLock
:
:
kSpinLockSleeper
;
constexpr
uint32_t
SpinLock
:
:
kWaitTimeMask
;
#
endif
SpinLock
:
:
SpinLock
(
base_internal
:
:
SchedulingMode
mode
)
:
lockword_
(
IsCooperative
(
mode
)
?
kSpinLockCooperative
:
0
)
{
ABSL_TSAN_MUTEX_CREATE
(
this
__tsan_mutex_not_static
)
;
}
uint32_t
SpinLock
:
:
SpinLoop
(
)
{
ABSL_CONST_INIT
static
absl
:
:
once_flag
init_adaptive_spin_count
;
ABSL_CONST_INIT
static
int
adaptive_spin_count
=
0
;
base_internal
:
:
LowLevelCallOnce
(
&
init_adaptive_spin_count
[
]
(
)
{
adaptive_spin_count
=
base_internal
:
:
NumCPUs
(
)
>
1
?
1000
:
1
;
}
)
;
int
c
=
adaptive_spin_count
;
uint32_t
lock_value
;
do
{
lock_value
=
lockword_
.
load
(
std
:
:
memory_order_relaxed
)
;
}
while
(
(
lock_value
&
kSpinLockHeld
)
!
=
0
&
&
-
-
c
>
0
)
;
return
lock_value
;
}
void
SpinLock
:
:
SlowLock
(
)
{
uint32_t
lock_value
=
SpinLoop
(
)
;
lock_value
=
TryLockInternal
(
lock_value
0
)
;
if
(
(
lock_value
&
kSpinLockHeld
)
=
=
0
)
{
return
;
}
base_internal
:
:
SchedulingMode
scheduling_mode
;
if
(
(
lock_value
&
kSpinLockCooperative
)
!
=
0
)
{
scheduling_mode
=
base_internal
:
:
SCHEDULE_COOPERATIVE_AND_KERNEL
;
}
else
{
scheduling_mode
=
base_internal
:
:
SCHEDULE_KERNEL_ONLY
;
}
int64_t
wait_start_time
=
CycleClock
:
:
Now
(
)
;
uint32_t
wait_cycles
=
0
;
int
lock_wait_call_count
=
0
;
while
(
(
lock_value
&
kSpinLockHeld
)
!
=
0
)
{
if
(
(
lock_value
&
kWaitTimeMask
)
=
=
0
)
{
if
(
lockword_
.
compare_exchange_strong
(
lock_value
lock_value
|
kSpinLockSleeper
std
:
:
memory_order_relaxed
std
:
:
memory_order_relaxed
)
)
{
lock_value
|
=
kSpinLockSleeper
;
}
else
if
(
(
lock_value
&
kSpinLockHeld
)
=
=
0
)
{
lock_value
=
TryLockInternal
(
lock_value
wait_cycles
)
;
continue
;
}
else
if
(
(
lock_value
&
kWaitTimeMask
)
=
=
0
)
{
continue
;
}
}
ABSL_TSAN_MUTEX_PRE_DIVERT
(
this
0
)
;
base_internal
:
:
SpinLockDelay
(
&
lockword_
lock_value
+
+
lock_wait_call_count
scheduling_mode
)
;
ABSL_TSAN_MUTEX_POST_DIVERT
(
this
0
)
;
lock_value
=
SpinLoop
(
)
;
wait_cycles
=
EncodeWaitCycles
(
wait_start_time
CycleClock
:
:
Now
(
)
)
;
lock_value
=
TryLockInternal
(
lock_value
wait_cycles
)
;
}
}
void
SpinLock
:
:
SlowUnlock
(
uint32_t
lock_value
)
{
base_internal
:
:
SpinLockWake
(
&
lockword_
false
)
;
if
(
(
lock_value
&
kWaitTimeMask
)
!
=
kSpinLockSleeper
)
{
const
int64_t
wait_cycles
=
DecodeWaitCycles
(
lock_value
)
;
ABSL_TSAN_MUTEX_PRE_DIVERT
(
this
0
)
;
submit_profile_data
(
this
wait_cycles
)
;
ABSL_TSAN_MUTEX_POST_DIVERT
(
this
0
)
;
}
}
static
constexpr
int
kProfileTimestampShift
=
7
;
static
constexpr
int
kLockwordReservedShift
=
3
;
uint32_t
SpinLock
:
:
EncodeWaitCycles
(
int64_t
wait_start_time
int64_t
wait_end_time
)
{
static
const
int64_t
kMaxWaitTime
=
std
:
:
numeric_limits
<
uint32_t
>
:
:
max
(
)
>
>
kLockwordReservedShift
;
int64_t
scaled_wait_time
=
(
wait_end_time
-
wait_start_time
)
>
>
kProfileTimestampShift
;
uint32_t
clamped
=
static_cast
<
uint32_t
>
(
std
:
:
min
(
scaled_wait_time
kMaxWaitTime
)
<
<
kLockwordReservedShift
)
;
if
(
clamped
=
=
0
)
{
return
kSpinLockSleeper
;
}
const
uint32_t
kMinWaitTime
=
kSpinLockSleeper
+
(
1
<
<
kLockwordReservedShift
)
;
if
(
clamped
=
=
kSpinLockSleeper
)
{
return
kMinWaitTime
;
}
return
clamped
;
}
int64_t
SpinLock
:
:
DecodeWaitCycles
(
uint32_t
lock_value
)
{
const
int64_t
scaled_wait_time
=
static_cast
<
uint32_t
>
(
lock_value
&
kWaitTimeMask
)
;
return
scaled_wait_time
<
<
(
kProfileTimestampShift
-
kLockwordReservedShift
)
;
}
}
ABSL_NAMESPACE_END
}
