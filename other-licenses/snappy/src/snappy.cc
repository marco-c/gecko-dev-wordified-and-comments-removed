#
include
"
snappy
-
internal
.
h
"
#
include
"
snappy
-
sinksource
.
h
"
#
include
"
snappy
.
h
"
#
if
!
defined
(
SNAPPY_HAVE_BMI2
)
#
if
defined
(
__BMI2__
)
|
|
(
defined
(
_MSC_VER
)
&
&
defined
(
__AVX2__
)
)
#
define
SNAPPY_HAVE_BMI2
1
#
else
#
define
SNAPPY_HAVE_BMI2
0
#
endif
#
endif
#
if
!
defined
(
SNAPPY_HAVE_X86_CRC32
)
#
if
defined
(
__SSE4_2__
)
#
define
SNAPPY_HAVE_X86_CRC32
1
#
else
#
define
SNAPPY_HAVE_X86_CRC32
0
#
endif
#
endif
#
if
!
defined
(
SNAPPY_HAVE_NEON_CRC32
)
#
if
SNAPPY_HAVE_NEON
&
&
defined
(
__ARM_FEATURE_CRC32
)
#
define
SNAPPY_HAVE_NEON_CRC32
1
#
else
#
define
SNAPPY_HAVE_NEON_CRC32
0
#
endif
#
endif
#
if
SNAPPY_HAVE_BMI2
|
|
SNAPPY_HAVE_X86_CRC32
#
include
<
immintrin
.
h
>
#
elif
SNAPPY_HAVE_NEON_CRC32
#
include
<
arm_acle
.
h
>
#
endif
#
include
<
algorithm
>
#
include
<
array
>
#
include
<
cstddef
>
#
include
<
cstdint
>
#
include
<
cstdio
>
#
include
<
cstring
>
#
include
<
memory
>
#
include
<
string
>
#
include
<
utility
>
#
include
<
vector
>
namespace
snappy
{
namespace
{
constexpr
int
kSlopBytes
=
64
;
using
internal
:
:
char_table
;
using
internal
:
:
COPY_1_BYTE_OFFSET
;
using
internal
:
:
COPY_2_BYTE_OFFSET
;
using
internal
:
:
COPY_4_BYTE_OFFSET
;
using
internal
:
:
kMaximumTagLength
;
using
internal
:
:
LITERAL
;
#
if
SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE
using
internal
:
:
V128
;
using
internal
:
:
V128_Load
;
using
internal
:
:
V128_LoadU
;
using
internal
:
:
V128_Shuffle
;
using
internal
:
:
V128_StoreU
;
using
internal
:
:
V128_DupChar
;
#
endif
inline
constexpr
int16_t
MakeEntry
(
int16_t
len
int16_t
offset
)
{
return
len
-
(
offset
<
<
8
)
;
}
inline
constexpr
int16_t
LengthMinusOffset
(
int
data
int
type
)
{
return
type
=
=
3
?
0xFF
:
type
=
=
2
?
MakeEntry
(
data
+
1
0
)
:
type
=
=
1
?
MakeEntry
(
(
data
&
7
)
+
4
data
>
>
3
)
:
data
<
60
?
MakeEntry
(
data
+
1
1
)
:
0xFF
;
}
inline
constexpr
int16_t
LengthMinusOffset
(
uint8_t
tag
)
{
return
LengthMinusOffset
(
tag
>
>
2
tag
&
3
)
;
}
template
<
size_t
.
.
.
Ints
>
struct
index_sequence
{
}
;
template
<
std
:
:
size_t
N
size_t
.
.
.
Is
>
struct
make_index_sequence
:
make_index_sequence
<
N
-
1
N
-
1
Is
.
.
.
>
{
}
;
template
<
size_t
.
.
.
Is
>
struct
make_index_sequence
<
0
Is
.
.
.
>
:
index_sequence
<
Is
.
.
.
>
{
}
;
template
<
size_t
.
.
.
seq
>
constexpr
std
:
:
array
<
int16_t
256
>
MakeTable
(
index_sequence
<
seq
.
.
.
>
)
{
return
std
:
:
array
<
int16_t
256
>
{
LengthMinusOffset
(
seq
)
.
.
.
}
;
}
alignas
(
64
)
const
std
:
:
array
<
int16_t
256
>
kLengthMinusOffset
=
MakeTable
(
make_index_sequence
<
256
>
{
}
)
;
inline
uint16_t
*
TableEntry
(
uint16_t
*
table
uint32_t
bytes
uint32_t
mask
)
{
#
if
SNAPPY_HAVE_NEON_CRC32
const
uint32_t
hash
=
__crc32cw
(
bytes
mask
)
;
#
elif
SNAPPY_HAVE_X86_CRC32
const
uint32_t
hash
=
_mm_crc32_u32
(
bytes
mask
)
;
#
else
constexpr
uint32_t
kMagic
=
0x1e35a7bd
;
const
uint32_t
hash
=
(
kMagic
*
bytes
)
>
>
(
31
-
kMaxHashTableBits
)
;
#
endif
return
reinterpret_cast
<
uint16_t
*
>
(
reinterpret_cast
<
uintptr_t
>
(
table
)
+
(
hash
&
mask
)
)
;
}
inline
uint16_t
*
TableEntry4ByteMatch
(
uint16_t
*
table
uint32_t
bytes
uint32_t
mask
)
{
constexpr
uint32_t
kMagic
=
2654435761U
;
const
uint32_t
hash
=
(
kMagic
*
bytes
)
>
>
(
32
-
kMaxHashTableBits
)
;
return
reinterpret_cast
<
uint16_t
*
>
(
reinterpret_cast
<
uintptr_t
>
(
table
)
+
(
hash
&
mask
)
)
;
}
inline
uint16_t
*
TableEntry8ByteMatch
(
uint16_t
*
table
uint64_t
bytes
uint32_t
mask
)
{
constexpr
uint64_t
kMagic
=
58295818150454627ULL
;
const
uint32_t
hash
=
(
kMagic
*
bytes
)
>
>
(
64
-
kMaxHashTableBits
)
;
return
reinterpret_cast
<
uint16_t
*
>
(
reinterpret_cast
<
uintptr_t
>
(
table
)
+
(
hash
&
mask
)
)
;
}
}
size_t
MaxCompressedLength
(
size_t
source_bytes
)
{
return
32
+
source_bytes
+
source_bytes
/
6
;
}
namespace
{
void
UnalignedCopy64
(
const
void
*
src
void
*
dst
)
{
char
tmp
[
8
]
;
std
:
:
memcpy
(
tmp
src
8
)
;
std
:
:
memcpy
(
dst
tmp
8
)
;
}
void
UnalignedCopy128
(
const
void
*
src
void
*
dst
)
{
char
tmp
[
16
]
;
std
:
:
memcpy
(
tmp
src
16
)
;
std
:
:
memcpy
(
dst
tmp
16
)
;
}
template
<
bool
use_16bytes_chunk
>
inline
void
ConditionalUnalignedCopy128
(
const
char
*
src
char
*
dst
)
{
if
(
use_16bytes_chunk
)
{
UnalignedCopy128
(
src
dst
)
;
}
else
{
UnalignedCopy64
(
src
dst
)
;
UnalignedCopy64
(
src
+
8
dst
+
8
)
;
}
}
inline
char
*
IncrementalCopySlow
(
const
char
*
src
char
*
op
char
*
const
op_limit
)
{
#
ifdef
__clang__
#
pragma
clang
loop
unroll
(
disable
)
#
endif
while
(
op
<
op_limit
)
{
*
op
+
+
=
*
src
+
+
;
}
return
op_limit
;
}
#
if
SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE
template
<
size_t
.
.
.
indexes
>
inline
constexpr
std
:
:
array
<
char
sizeof
.
.
.
(
indexes
)
>
MakePatternMaskBytes
(
int
index_offset
int
pattern_size
index_sequence
<
indexes
.
.
.
>
)
{
return
{
static_cast
<
char
>
(
(
index_offset
+
indexes
)
%
pattern_size
)
.
.
.
}
;
}
template
<
size_t
.
.
.
pattern_sizes_minus_one
>
inline
constexpr
std
:
:
array
<
std
:
:
array
<
char
sizeof
(
V128
)
>
sizeof
.
.
.
(
pattern_sizes_minus_one
)
>
MakePatternMaskBytesTable
(
int
index_offset
index_sequence
<
pattern_sizes_minus_one
.
.
.
>
)
{
return
{
MakePatternMaskBytes
(
index_offset
pattern_sizes_minus_one
+
1
make_index_sequence
<
sizeof
(
V128
)
>
(
)
)
.
.
.
}
;
}
alignas
(
16
)
constexpr
std
:
:
array
<
std
:
:
array
<
char
sizeof
(
V128
)
>
16
>
pattern_generation_masks
=
MakePatternMaskBytesTable
(
0
make_index_sequence
<
16
>
(
)
)
;
alignas
(
16
)
constexpr
std
:
:
array
<
std
:
:
array
<
char
sizeof
(
V128
)
>
16
>
pattern_reshuffle_masks
=
MakePatternMaskBytesTable
(
16
make_index_sequence
<
16
>
(
)
)
;
SNAPPY_ATTRIBUTE_ALWAYS_INLINE
static
inline
V128
LoadPattern
(
const
char
*
src
const
size_t
pattern_size
)
{
V128
generation_mask
=
V128_Load
(
reinterpret_cast
<
const
V128
*
>
(
pattern_generation_masks
[
pattern_size
-
1
]
.
data
(
)
)
)
;
SNAPPY_ANNOTATE_MEMORY_IS_INITIALIZED
(
src
+
pattern_size
16
-
pattern_size
)
;
return
V128_Shuffle
(
V128_LoadU
(
reinterpret_cast
<
const
V128
*
>
(
src
)
)
generation_mask
)
;
}
SNAPPY_ATTRIBUTE_ALWAYS_INLINE
static
inline
std
:
:
pair
<
V128
V128
>
LoadPatternAndReshuffleMask
(
const
char
*
src
const
size_t
pattern_size
)
{
V128
pattern
=
LoadPattern
(
src
pattern_size
)
;
V128
reshuffle_mask
=
V128_Load
(
reinterpret_cast
<
const
V128
*
>
(
pattern_reshuffle_masks
[
pattern_size
-
1
]
.
data
(
)
)
)
;
return
{
pattern
reshuffle_mask
}
;
}
#
endif
SNAPPY_ATTRIBUTE_ALWAYS_INLINE
static
inline
bool
Copy64BytesWithPatternExtension
(
char
*
dst
size_t
offset
)
{
#
if
SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE
if
(
SNAPPY_PREDICT_TRUE
(
offset
<
=
16
)
)
{
switch
(
offset
)
{
case
0
:
return
false
;
case
1
:
{
V128
pattern
=
V128_DupChar
(
dst
[
-
1
]
)
;
for
(
int
i
=
0
;
i
<
4
;
i
+
+
)
{
V128_StoreU
(
reinterpret_cast
<
V128
*
>
(
dst
+
16
*
i
)
pattern
)
;
}
return
true
;
}
case
2
:
case
4
:
case
8
:
case
16
:
{
V128
pattern
=
LoadPattern
(
dst
-
offset
offset
)
;
for
(
int
i
=
0
;
i
<
4
;
i
+
+
)
{
V128_StoreU
(
reinterpret_cast
<
V128
*
>
(
dst
+
16
*
i
)
pattern
)
;
}
return
true
;
}
default
:
{
auto
pattern_and_reshuffle_mask
=
LoadPatternAndReshuffleMask
(
dst
-
offset
offset
)
;
V128
pattern
=
pattern_and_reshuffle_mask
.
first
;
V128
reshuffle_mask
=
pattern_and_reshuffle_mask
.
second
;
for
(
int
i
=
0
;
i
<
4
;
i
+
+
)
{
V128_StoreU
(
reinterpret_cast
<
V128
*
>
(
dst
+
16
*
i
)
pattern
)
;
pattern
=
V128_Shuffle
(
pattern
reshuffle_mask
)
;
}
return
true
;
}
}
}
#
else
if
(
SNAPPY_PREDICT_TRUE
(
offset
<
16
)
)
{
if
(
SNAPPY_PREDICT_FALSE
(
offset
=
=
0
)
)
return
false
;
for
(
int
i
=
0
;
i
<
16
;
i
+
+
)
dst
[
i
]
=
(
dst
-
offset
)
[
i
]
;
static
std
:
:
array
<
uint8_t
16
>
pattern_sizes
=
[
]
(
)
{
std
:
:
array
<
uint8_t
16
>
res
;
for
(
int
i
=
1
;
i
<
16
;
i
+
+
)
res
[
i
]
=
(
16
/
i
+
1
)
*
i
;
return
res
;
}
(
)
;
offset
=
pattern_sizes
[
offset
]
;
for
(
int
i
=
1
;
i
<
4
;
i
+
+
)
{
std
:
:
memcpy
(
dst
+
i
*
16
dst
+
i
*
16
-
offset
16
)
;
}
return
true
;
}
#
endif
for
(
int
i
=
0
;
i
<
4
;
i
+
+
)
{
std
:
:
memcpy
(
dst
+
i
*
16
dst
+
i
*
16
-
offset
16
)
;
}
return
true
;
}
inline
char
*
IncrementalCopy
(
const
char
*
src
char
*
op
char
*
const
op_limit
char
*
const
buf_limit
)
{
#
if
SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE
constexpr
int
big_pattern_size_lower_bound
=
16
;
#
else
constexpr
int
big_pattern_size_lower_bound
=
8
;
#
endif
assert
(
src
<
op
)
;
assert
(
op
<
op_limit
)
;
assert
(
op_limit
<
=
buf_limit
)
;
assert
(
op_limit
-
op
<
=
64
)
;
size_t
pattern_size
=
op
-
src
;
if
(
pattern_size
<
big_pattern_size_lower_bound
)
{
#
if
SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE
if
(
SNAPPY_PREDICT_TRUE
(
op_limit
<
=
buf_limit
-
15
)
)
{
auto
pattern_and_reshuffle_mask
=
LoadPatternAndReshuffleMask
(
src
pattern_size
)
;
V128
pattern
=
pattern_and_reshuffle_mask
.
first
;
V128
reshuffle_mask
=
pattern_and_reshuffle_mask
.
second
;
V128_StoreU
(
reinterpret_cast
<
V128
*
>
(
op
)
pattern
)
;
if
(
op
+
16
<
op_limit
)
{
pattern
=
V128_Shuffle
(
pattern
reshuffle_mask
)
;
V128_StoreU
(
reinterpret_cast
<
V128
*
>
(
op
+
16
)
pattern
)
;
}
if
(
op
+
32
<
op_limit
)
{
pattern
=
V128_Shuffle
(
pattern
reshuffle_mask
)
;
V128_StoreU
(
reinterpret_cast
<
V128
*
>
(
op
+
32
)
pattern
)
;
}
if
(
op
+
48
<
op_limit
)
{
pattern
=
V128_Shuffle
(
pattern
reshuffle_mask
)
;
V128_StoreU
(
reinterpret_cast
<
V128
*
>
(
op
+
48
)
pattern
)
;
}
return
op_limit
;
}
char
*
const
op_end
=
buf_limit
-
15
;
if
(
SNAPPY_PREDICT_TRUE
(
op
<
op_end
)
)
{
auto
pattern_and_reshuffle_mask
=
LoadPatternAndReshuffleMask
(
src
pattern_size
)
;
V128
pattern
=
pattern_and_reshuffle_mask
.
first
;
V128
reshuffle_mask
=
pattern_and_reshuffle_mask
.
second
;
#
ifdef
__clang__
#
pragma
clang
loop
unroll
(
disable
)
#
endif
do
{
V128_StoreU
(
reinterpret_cast
<
V128
*
>
(
op
)
pattern
)
;
pattern
=
V128_Shuffle
(
pattern
reshuffle_mask
)
;
op
+
=
16
;
}
while
(
SNAPPY_PREDICT_TRUE
(
op
<
op_end
)
)
;
}
return
IncrementalCopySlow
(
op
-
pattern_size
op
op_limit
)
;
#
else
if
(
SNAPPY_PREDICT_TRUE
(
op
<
=
buf_limit
-
11
)
)
{
while
(
pattern_size
<
8
)
{
UnalignedCopy64
(
src
op
)
;
op
+
=
pattern_size
;
pattern_size
*
=
2
;
}
if
(
SNAPPY_PREDICT_TRUE
(
op
>
=
op_limit
)
)
return
op_limit
;
}
else
{
return
IncrementalCopySlow
(
src
op
op_limit
)
;
}
#
endif
}
assert
(
pattern_size
>
=
big_pattern_size_lower_bound
)
;
constexpr
bool
use_16bytes_chunk
=
big_pattern_size_lower_bound
=
=
16
;
if
(
SNAPPY_PREDICT_TRUE
(
op_limit
<
=
buf_limit
-
15
)
)
{
ConditionalUnalignedCopy128
<
use_16bytes_chunk
>
(
src
op
)
;
if
(
op
+
16
<
op_limit
)
{
ConditionalUnalignedCopy128
<
use_16bytes_chunk
>
(
src
+
16
op
+
16
)
;
}
if
(
op
+
32
<
op_limit
)
{
ConditionalUnalignedCopy128
<
use_16bytes_chunk
>
(
src
+
32
op
+
32
)
;
}
if
(
op
+
48
<
op_limit
)
{
ConditionalUnalignedCopy128
<
use_16bytes_chunk
>
(
src
+
48
op
+
48
)
;
}
return
op_limit
;
}
#
ifdef
__clang__
#
pragma
clang
loop
unroll
(
disable
)
#
endif
for
(
char
*
op_end
=
buf_limit
-
16
;
op
<
op_end
;
op
+
=
16
src
+
=
16
)
{
ConditionalUnalignedCopy128
<
use_16bytes_chunk
>
(
src
op
)
;
}
if
(
op
>
=
op_limit
)
return
op_limit
;
if
(
SNAPPY_PREDICT_FALSE
(
op
<
=
buf_limit
-
8
)
)
{
UnalignedCopy64
(
src
op
)
;
src
+
=
8
;
op
+
=
8
;
}
return
IncrementalCopySlow
(
src
op
op_limit
)
;
}
}
template
<
bool
allow_fast_path
>
static
inline
char
*
EmitLiteral
(
char
*
op
const
char
*
literal
int
len
)
{
assert
(
len
>
0
)
;
int
n
=
len
-
1
;
if
(
allow_fast_path
&
&
len
<
=
16
)
{
*
op
+
+
=
LITERAL
|
(
n
<
<
2
)
;
UnalignedCopy128
(
literal
op
)
;
return
op
+
len
;
}
if
(
n
<
60
)
{
*
op
+
+
=
LITERAL
|
(
n
<
<
2
)
;
}
else
{
int
count
=
(
Bits
:
:
Log2Floor
(
n
)
>
>
3
)
+
1
;
assert
(
count
>
=
1
)
;
assert
(
count
<
=
4
)
;
*
op
+
+
=
LITERAL
|
(
(
59
+
count
)
<
<
2
)
;
LittleEndian
:
:
Store32
(
op
n
)
;
op
+
=
count
;
}
if
(
allow_fast_path
)
{
char
*
destination
=
op
;
const
char
*
source
=
literal
;
const
char
*
end
=
destination
+
len
;
do
{
std
:
:
memcpy
(
destination
source
16
)
;
destination
+
=
16
;
source
+
=
16
;
}
while
(
destination
<
end
)
;
}
else
{
std
:
:
memcpy
(
op
literal
len
)
;
}
return
op
+
len
;
}
template
<
bool
len_less_than_12
>
static
inline
char
*
EmitCopyAtMost64
(
char
*
op
size_t
offset
size_t
len
)
{
assert
(
len
<
=
64
)
;
assert
(
len
>
=
4
)
;
assert
(
offset
<
65536
)
;
assert
(
len_less_than_12
=
=
(
len
<
12
)
)
;
if
(
len_less_than_12
)
{
uint32_t
u
=
(
len
<
<
2
)
+
(
offset
<
<
8
)
;
uint32_t
copy1
=
COPY_1_BYTE_OFFSET
-
(
4
<
<
2
)
+
(
(
offset
>
>
3
)
&
0xe0
)
;
uint32_t
copy2
=
COPY_2_BYTE_OFFSET
-
(
1
<
<
2
)
;
u
+
=
offset
<
2048
?
copy1
:
copy2
;
LittleEndian
:
:
Store32
(
op
u
)
;
op
+
=
offset
<
2048
?
2
:
3
;
}
else
{
uint32_t
u
=
COPY_2_BYTE_OFFSET
+
(
(
len
-
1
)
<
<
2
)
+
(
offset
<
<
8
)
;
LittleEndian
:
:
Store32
(
op
u
)
;
op
+
=
3
;
}
return
op
;
}
template
<
bool
len_less_than_12
>
static
inline
char
*
EmitCopy
(
char
*
op
size_t
offset
size_t
len
)
{
assert
(
len_less_than_12
=
=
(
len
<
12
)
)
;
if
(
len_less_than_12
)
{
return
EmitCopyAtMost64
<
true
>
(
op
offset
len
)
;
}
else
{
while
(
SNAPPY_PREDICT_FALSE
(
len
>
=
68
)
)
{
op
=
EmitCopyAtMost64
<
false
>
(
op
offset
64
)
;
len
-
=
64
;
}
if
(
len
>
64
)
{
op
=
EmitCopyAtMost64
<
false
>
(
op
offset
60
)
;
len
-
=
60
;
}
if
(
len
<
12
)
{
op
=
EmitCopyAtMost64
<
true
>
(
op
offset
len
)
;
}
else
{
op
=
EmitCopyAtMost64
<
false
>
(
op
offset
len
)
;
}
return
op
;
}
}
bool
GetUncompressedLength
(
const
char
*
start
size_t
n
size_t
*
result
)
{
uint32_t
v
=
0
;
const
char
*
limit
=
start
+
n
;
if
(
Varint
:
:
Parse32WithLimit
(
start
limit
&
v
)
!
=
NULL
)
{
*
result
=
v
;
return
true
;
}
else
{
return
false
;
}
}
namespace
{
uint32_t
CalculateTableSize
(
uint32_t
input_size
)
{
static_assert
(
kMaxHashTableSize
>
=
kMinHashTableSize
"
kMaxHashTableSize
should
be
greater
or
equal
to
kMinHashTableSize
.
"
)
;
if
(
input_size
>
kMaxHashTableSize
)
{
return
kMaxHashTableSize
;
}
if
(
input_size
<
kMinHashTableSize
)
{
return
kMinHashTableSize
;
}
return
2u
<
<
Bits
:
:
Log2Floor
(
input_size
-
1
)
;
}
}
namespace
internal
{
WorkingMemory
:
:
WorkingMemory
(
size_t
input_size
)
{
const
size_t
max_fragment_size
=
std
:
:
min
(
input_size
kBlockSize
)
;
const
size_t
table_size
=
CalculateTableSize
(
max_fragment_size
)
;
size_
=
table_size
*
sizeof
(
*
table_
)
+
max_fragment_size
+
MaxCompressedLength
(
max_fragment_size
)
;
mem_
=
std
:
:
allocator
<
char
>
(
)
.
allocate
(
size_
)
;
table_
=
reinterpret_cast
<
uint16_t
*
>
(
mem_
)
;
input_
=
mem_
+
table_size
*
sizeof
(
*
table_
)
;
output_
=
input_
+
max_fragment_size
;
}
WorkingMemory
:
:
~
WorkingMemory
(
)
{
std
:
:
allocator
<
char
>
(
)
.
deallocate
(
mem_
size_
)
;
}
uint16_t
*
WorkingMemory
:
:
GetHashTable
(
size_t
fragment_size
int
*
table_size
)
const
{
const
size_t
htsize
=
CalculateTableSize
(
fragment_size
)
;
memset
(
table_
0
htsize
*
sizeof
(
*
table_
)
)
;
*
table_size
=
htsize
;
return
table_
;
}
}
namespace
internal
{
char
*
CompressFragment
(
const
char
*
input
size_t
input_size
char
*
op
uint16_t
*
table
const
int
table_size
)
{
const
char
*
ip
=
input
;
assert
(
input_size
<
=
kBlockSize
)
;
assert
(
(
table_size
&
(
table_size
-
1
)
)
=
=
0
)
;
const
uint32_t
mask
=
2
*
(
table_size
-
1
)
;
const
char
*
ip_end
=
input
+
input_size
;
const
char
*
base_ip
=
ip
;
const
size_t
kInputMarginBytes
=
15
;
if
(
SNAPPY_PREDICT_TRUE
(
input_size
>
=
kInputMarginBytes
)
)
{
const
char
*
ip_limit
=
input
+
input_size
-
kInputMarginBytes
;
for
(
uint32_t
preload
=
LittleEndian
:
:
Load32
(
ip
+
1
)
;
;
)
{
const
char
*
next_emit
=
ip
+
+
;
uint64_t
data
=
LittleEndian
:
:
Load64
(
ip
)
;
uint32_t
skip
=
32
;
const
char
*
candidate
;
if
(
ip_limit
-
ip
>
=
16
)
{
auto
delta
=
ip
-
base_ip
;
for
(
int
j
=
0
;
j
<
4
;
+
+
j
)
{
for
(
int
k
=
0
;
k
<
4
;
+
+
k
)
{
int
i
=
4
*
j
+
k
;
uint32_t
dword
=
i
=
=
0
?
preload
:
static_cast
<
uint32_t
>
(
data
)
;
assert
(
dword
=
=
LittleEndian
:
:
Load32
(
ip
+
i
)
)
;
uint16_t
*
table_entry
=
TableEntry
(
table
dword
mask
)
;
candidate
=
base_ip
+
*
table_entry
;
assert
(
candidate
>
=
base_ip
)
;
assert
(
candidate
<
ip
+
i
)
;
*
table_entry
=
delta
+
i
;
if
(
SNAPPY_PREDICT_FALSE
(
LittleEndian
:
:
Load32
(
candidate
)
=
=
dword
)
)
{
*
op
=
LITERAL
|
(
i
<
<
2
)
;
UnalignedCopy128
(
next_emit
op
+
1
)
;
ip
+
=
i
;
op
=
op
+
i
+
2
;
goto
emit_match
;
}
data
>
>
=
8
;
}
data
=
LittleEndian
:
:
Load64
(
ip
+
4
*
j
+
4
)
;
}
ip
+
=
16
;
skip
+
=
16
;
}
while
(
true
)
{
assert
(
static_cast
<
uint32_t
>
(
data
)
=
=
LittleEndian
:
:
Load32
(
ip
)
)
;
uint16_t
*
table_entry
=
TableEntry
(
table
data
mask
)
;
uint32_t
bytes_between_hash_lookups
=
skip
>
>
5
;
skip
+
=
bytes_between_hash_lookups
;
const
char
*
next_ip
=
ip
+
bytes_between_hash_lookups
;
if
(
SNAPPY_PREDICT_FALSE
(
next_ip
>
ip_limit
)
)
{
ip
=
next_emit
;
goto
emit_remainder
;
}
candidate
=
base_ip
+
*
table_entry
;
assert
(
candidate
>
=
base_ip
)
;
assert
(
candidate
<
ip
)
;
*
table_entry
=
ip
-
base_ip
;
if
(
SNAPPY_PREDICT_FALSE
(
static_cast
<
uint32_t
>
(
data
)
=
=
LittleEndian
:
:
Load32
(
candidate
)
)
)
{
break
;
}
data
=
LittleEndian
:
:
Load32
(
next_ip
)
;
ip
=
next_ip
;
}
assert
(
next_emit
+
16
<
=
ip_end
)
;
op
=
EmitLiteral
<
true
>
(
op
next_emit
ip
-
next_emit
)
;
emit_match
:
do
{
const
char
*
base
=
ip
;
std
:
:
pair
<
size_t
bool
>
p
=
FindMatchLength
(
candidate
+
4
ip
+
4
ip_end
&
data
)
;
size_t
matched
=
4
+
p
.
first
;
ip
+
=
matched
;
size_t
offset
=
base
-
candidate
;
assert
(
0
=
=
memcmp
(
base
candidate
matched
)
)
;
if
(
p
.
second
)
{
op
=
EmitCopy
<
true
>
(
op
offset
matched
)
;
}
else
{
op
=
EmitCopy
<
false
>
(
op
offset
matched
)
;
}
if
(
SNAPPY_PREDICT_FALSE
(
ip
>
=
ip_limit
)
)
{
goto
emit_remainder
;
}
assert
(
(
data
&
0xFFFFFFFFFF
)
=
=
(
LittleEndian
:
:
Load64
(
ip
)
&
0xFFFFFFFFFF
)
)
;
*
TableEntry
(
table
LittleEndian
:
:
Load32
(
ip
-
1
)
mask
)
=
ip
-
base_ip
-
1
;
uint16_t
*
table_entry
=
TableEntry
(
table
data
mask
)
;
candidate
=
base_ip
+
*
table_entry
;
*
table_entry
=
ip
-
base_ip
;
}
while
(
static_cast
<
uint32_t
>
(
data
)
=
=
LittleEndian
:
:
Load32
(
candidate
)
)
;
preload
=
data
>
>
8
;
}
}
emit_remainder
:
if
(
ip
<
ip_end
)
{
op
=
EmitLiteral
<
false
>
(
op
ip
ip_end
-
ip
)
;
}
return
op
;
}
char
*
CompressFragmentDoubleHash
(
const
char
*
input
size_t
input_size
char
*
op
uint16_t
*
table
const
int
table_size
uint16_t
*
table2
const
int
table_size2
)
{
(
void
)
table_size2
;
assert
(
table_size
=
=
table_size2
)
;
const
char
*
ip
=
input
;
assert
(
input_size
<
=
kBlockSize
)
;
assert
(
(
table_size
&
(
table_size
-
1
)
)
=
=
0
)
;
const
uint32_t
mask
=
2
*
(
table_size
-
1
)
;
const
char
*
ip_end
=
input
+
input_size
;
const
char
*
base_ip
=
ip
;
const
size_t
kInputMarginBytes
=
15
;
if
(
SNAPPY_PREDICT_TRUE
(
input_size
>
=
kInputMarginBytes
)
)
{
const
char
*
ip_limit
=
input
+
input_size
-
kInputMarginBytes
;
for
(
;
;
)
{
const
char
*
next_emit
=
ip
+
+
;
uint64_t
data
=
LittleEndian
:
:
Load64
(
ip
)
;
uint32_t
skip
=
512
;
const
char
*
candidate
;
uint32_t
candidate_length
;
while
(
true
)
{
assert
(
static_cast
<
uint32_t
>
(
data
)
=
=
LittleEndian
:
:
Load32
(
ip
)
)
;
uint16_t
*
table_entry2
=
TableEntry8ByteMatch
(
table2
data
mask
)
;
uint32_t
bytes_between_hash_lookups
=
skip
>
>
9
;
skip
+
+
;
const
char
*
next_ip
=
ip
+
bytes_between_hash_lookups
;
if
(
SNAPPY_PREDICT_FALSE
(
next_ip
>
ip_limit
)
)
{
ip
=
next_emit
;
goto
emit_remainder
;
}
candidate
=
base_ip
+
*
table_entry2
;
assert
(
candidate
>
=
base_ip
)
;
assert
(
candidate
<
ip
)
;
*
table_entry2
=
ip
-
base_ip
;
if
(
SNAPPY_PREDICT_FALSE
(
static_cast
<
uint32_t
>
(
data
)
=
=
LittleEndian
:
:
Load32
(
candidate
)
)
)
{
candidate_length
=
FindMatchLengthPlain
(
candidate
+
4
ip
+
4
ip_end
)
+
4
;
break
;
}
uint16_t
*
table_entry
=
TableEntry4ByteMatch
(
table
data
mask
)
;
candidate
=
base_ip
+
*
table_entry
;
assert
(
candidate
>
=
base_ip
)
;
assert
(
candidate
<
ip
)
;
*
table_entry
=
ip
-
base_ip
;
if
(
SNAPPY_PREDICT_FALSE
(
static_cast
<
uint32_t
>
(
data
)
=
=
LittleEndian
:
:
Load32
(
candidate
)
)
)
{
candidate_length
=
FindMatchLengthPlain
(
candidate
+
4
ip
+
4
ip_end
)
+
4
;
table_entry2
=
TableEntry8ByteMatch
(
table2
LittleEndian
:
:
Load64
(
ip
+
1
)
mask
)
;
auto
candidate2
=
base_ip
+
*
table_entry2
;
size_t
candidate_length2
=
FindMatchLengthPlain
(
candidate2
ip
+
1
ip_end
)
;
if
(
candidate_length2
>
candidate_length
)
{
*
table_entry2
=
ip
-
base_ip
;
candidate
=
candidate2
;
candidate_length
=
candidate_length2
;
+
+
ip
;
}
break
;
}
data
=
LittleEndian
:
:
Load64
(
next_ip
)
;
ip
=
next_ip
;
}
while
(
ip
>
next_emit
&
&
candidate
>
base_ip
&
&
*
(
ip
-
1
)
=
=
*
(
candidate
-
1
)
)
{
-
-
ip
;
-
-
candidate
;
+
+
candidate_length
;
}
*
TableEntry8ByteMatch
(
table2
LittleEndian
:
:
Load64
(
ip
+
1
)
mask
)
=
ip
-
base_ip
+
1
;
*
TableEntry8ByteMatch
(
table2
LittleEndian
:
:
Load64
(
ip
+
2
)
mask
)
=
ip
-
base_ip
+
2
;
*
TableEntry4ByteMatch
(
table
LittleEndian
:
:
Load32
(
ip
+
1
)
mask
)
=
ip
-
base_ip
+
1
;
assert
(
next_emit
+
16
<
=
ip_end
)
;
if
(
ip
-
next_emit
>
0
)
{
op
=
EmitLiteral
<
true
>
(
op
next_emit
ip
-
next_emit
)
;
}
do
{
const
char
*
base
=
ip
;
ip
+
=
candidate_length
;
size_t
offset
=
base
-
candidate
;
if
(
candidate_length
<
12
)
{
op
=
EmitCopy
<
true
>
(
op
offset
candidate_length
)
;
}
else
{
op
=
EmitCopy
<
false
>
(
op
offset
candidate_length
)
;
}
if
(
SNAPPY_PREDICT_FALSE
(
ip
>
=
ip_limit
)
)
{
goto
emit_remainder
;
}
if
(
ip
-
base_ip
>
7
)
{
*
TableEntry8ByteMatch
(
table2
LittleEndian
:
:
Load64
(
ip
-
7
)
mask
)
=
ip
-
base_ip
-
7
;
*
TableEntry8ByteMatch
(
table2
LittleEndian
:
:
Load64
(
ip
-
4
)
mask
)
=
ip
-
base_ip
-
4
;
}
*
TableEntry8ByteMatch
(
table2
LittleEndian
:
:
Load64
(
ip
-
3
)
mask
)
=
ip
-
base_ip
-
3
;
*
TableEntry8ByteMatch
(
table2
LittleEndian
:
:
Load64
(
ip
-
2
)
mask
)
=
ip
-
base_ip
-
2
;
*
TableEntry4ByteMatch
(
table
LittleEndian
:
:
Load32
(
ip
-
2
)
mask
)
=
ip
-
base_ip
-
2
;
*
TableEntry4ByteMatch
(
table
LittleEndian
:
:
Load32
(
ip
-
1
)
mask
)
=
ip
-
base_ip
-
1
;
uint16_t
*
table_entry
=
TableEntry8ByteMatch
(
table2
LittleEndian
:
:
Load64
(
ip
)
mask
)
;
candidate
=
base_ip
+
*
table_entry
;
*
table_entry
=
ip
-
base_ip
;
if
(
LittleEndian
:
:
Load32
(
ip
)
=
=
LittleEndian
:
:
Load32
(
candidate
)
)
{
candidate_length
=
FindMatchLengthPlain
(
candidate
+
4
ip
+
4
ip_end
)
+
4
;
continue
;
}
table_entry
=
TableEntry4ByteMatch
(
table
LittleEndian
:
:
Load32
(
ip
)
mask
)
;
candidate
=
base_ip
+
*
table_entry
;
*
table_entry
=
ip
-
base_ip
;
if
(
LittleEndian
:
:
Load32
(
ip
)
=
=
LittleEndian
:
:
Load32
(
candidate
)
)
{
candidate_length
=
FindMatchLengthPlain
(
candidate
+
4
ip
+
4
ip_end
)
+
4
;
continue
;
}
break
;
}
while
(
true
)
;
}
}
emit_remainder
:
if
(
ip
<
ip_end
)
{
op
=
EmitLiteral
<
false
>
(
op
ip
ip_end
-
ip
)
;
}
return
op
;
}
}
static
inline
void
Report
(
int
token
const
char
*
algorithm
size_t
compressed_size
size_t
uncompressed_size
)
{
(
void
)
token
;
(
void
)
algorithm
;
(
void
)
compressed_size
;
(
void
)
uncompressed_size
;
}
static
inline
uint32_t
ExtractLowBytes
(
const
uint32_t
&
v
int
n
)
{
assert
(
n
>
=
0
)
;
assert
(
n
<
=
4
)
;
#
if
SNAPPY_HAVE_BMI2
return
_bzhi_u32
(
v
8
*
n
)
;
#
else
uint64_t
mask
=
0xffffffff
;
return
v
&
~
(
mask
<
<
(
8
*
n
)
)
;
#
endif
}
static
inline
bool
LeftShiftOverflows
(
uint8_t
value
uint32_t
shift
)
{
assert
(
shift
<
32
)
;
static
const
uint8_t
masks
[
]
=
{
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x80
0xc0
0xe0
0xf0
0xf8
0xfc
0xfe
}
;
return
(
value
&
masks
[
shift
]
)
!
=
0
;
}
inline
bool
Copy64BytesWithPatternExtension
(
ptrdiff_t
dst
size_t
offset
)
{
(
void
)
dst
;
return
offset
!
=
0
;
}
void
MemCopy64
(
char
*
dst
const
void
*
src
size_t
size
)
{
constexpr
int
kShortMemCopy
=
32
;
assert
(
size
<
=
64
)
;
assert
(
std
:
:
less_equal
<
const
void
*
>
(
)
(
static_cast
<
const
char
*
>
(
src
)
+
size
dst
)
|
|
std
:
:
less_equal
<
const
void
*
>
(
)
(
dst
+
size
src
)
)
;
#
if
defined
(
__x86_64__
)
&
&
defined
(
__AVX__
)
assert
(
kShortMemCopy
<
=
32
)
;
__m256i
data
=
_mm256_lddqu_si256
(
static_cast
<
const
__m256i
*
>
(
src
)
)
;
_mm256_storeu_si256
(
reinterpret_cast
<
__m256i
*
>
(
dst
)
data
)
;
if
(
SNAPPY_PREDICT_FALSE
(
size
>
kShortMemCopy
)
)
{
data
=
_mm256_lddqu_si256
(
static_cast
<
const
__m256i
*
>
(
src
)
+
1
)
;
_mm256_storeu_si256
(
reinterpret_cast
<
__m256i
*
>
(
dst
)
+
1
data
)
;
}
#
else
std
:
:
memmove
(
dst
src
kShortMemCopy
)
;
if
(
SNAPPY_PREDICT_FALSE
(
size
>
kShortMemCopy
)
)
{
std
:
:
memmove
(
dst
+
kShortMemCopy
static_cast
<
const
uint8_t
*
>
(
src
)
+
kShortMemCopy
64
-
kShortMemCopy
)
;
}
#
endif
}
void
MemCopy64
(
ptrdiff_t
dst
const
void
*
src
size_t
size
)
{
(
void
)
dst
;
(
void
)
src
;
(
void
)
size
;
}
void
ClearDeferred
(
const
void
*
*
deferred_src
size_t
*
deferred_length
uint8_t
*
safe_source
)
{
*
deferred_src
=
safe_source
;
*
deferred_length
=
0
;
}
void
DeferMemCopy
(
const
void
*
*
deferred_src
size_t
*
deferred_length
const
void
*
src
size_t
length
)
{
*
deferred_src
=
src
;
*
deferred_length
=
length
;
}
SNAPPY_ATTRIBUTE_ALWAYS_INLINE
inline
size_t
AdvanceToNextTagARMOptimized
(
const
uint8_t
*
*
ip_p
size_t
*
tag
)
{
const
uint8_t
*
&
ip
=
*
ip_p
;
const
size_t
tag_type
=
*
tag
&
3
;
const
bool
is_literal
=
(
tag_type
=
=
0
)
;
if
(
is_literal
)
{
size_t
next_literal_tag
=
(
*
tag
>
>
2
)
+
1
;
*
tag
=
ip
[
next_literal_tag
]
;
ip
+
=
next_literal_tag
+
1
;
}
else
{
*
tag
=
ip
[
tag_type
]
;
ip
+
=
tag_type
+
1
;
}
return
tag_type
;
}
SNAPPY_ATTRIBUTE_ALWAYS_INLINE
inline
size_t
AdvanceToNextTagX86Optimized
(
const
uint8_t
*
*
ip_p
size_t
*
tag
)
{
const
uint8_t
*
&
ip
=
*
ip_p
;
size_t
literal_len
=
*
tag
>
>
2
;
size_t
tag_type
=
*
tag
;
bool
is_literal
;
#
if
defined
(
__GCC_ASM_FLAG_OUTPUTS__
)
&
&
defined
(
__x86_64__
)
asm
(
"
and
3
%
k
[
tag_type
]
\
n
\
t
"
:
[
tag_type
]
"
+
r
"
(
tag_type
)
"
=
ccz
"
(
is_literal
)
:
:
"
cc
"
)
;
#
else
tag_type
&
=
3
;
is_literal
=
(
tag_type
=
=
0
)
;
#
endif
size_t
tag_literal
=
static_cast
<
const
volatile
uint8_t
*
>
(
ip
)
[
1
+
literal_len
]
;
size_t
tag_copy
=
static_cast
<
const
volatile
uint8_t
*
>
(
ip
)
[
tag_type
]
;
*
tag
=
is_literal
?
tag_literal
:
tag_copy
;
const
uint8_t
*
ip_copy
=
ip
+
1
+
tag_type
;
const
uint8_t
*
ip_literal
=
ip
+
2
+
literal_len
;
ip
=
is_literal
?
ip_literal
:
ip_copy
;
#
if
defined
(
__GNUC__
)
&
&
defined
(
__x86_64__
)
asm
(
"
"
:
:
"
r
"
(
tag_copy
)
)
;
#
endif
return
tag_type
;
}
inline
uint32_t
ExtractOffset
(
uint32_t
val
size_t
tag_type
)
{
#
if
defined
(
__x86_64__
)
constexpr
uint64_t
kExtractMasksCombined
=
0x0000FFFF00FF0000ull
;
uint16_t
result
;
memcpy
(
&
result
reinterpret_cast
<
const
char
*
>
(
&
kExtractMasksCombined
)
+
2
*
tag_type
sizeof
(
result
)
)
;
return
val
&
result
;
#
elif
defined
(
__aarch64__
)
constexpr
uint64_t
kExtractMasksCombined
=
0x0000FFFF00FF0000ull
;
return
val
&
static_cast
<
uint32_t
>
(
(
kExtractMasksCombined
>
>
(
tag_type
*
16
)
)
&
0xFFFF
)
;
#
else
static
constexpr
uint32_t
kExtractMasks
[
4
]
=
{
0
0xFF
0xFFFF
0
}
;
return
val
&
kExtractMasks
[
tag_type
]
;
#
endif
}
;
template
<
typename
T
>
std
:
:
pair
<
const
uint8_t
*
ptrdiff_t
>
DecompressBranchless
(
const
uint8_t
*
ip
const
uint8_t
*
ip_limit
ptrdiff_t
op
T
op_base
ptrdiff_t
op_limit_min_slop
)
{
uint8_t
safe_source
[
64
]
;
const
void
*
deferred_src
;
size_t
deferred_length
;
ClearDeferred
(
&
deferred_src
&
deferred_length
safe_source
)
;
op_limit_min_slop
-
=
kSlopBytes
;
if
(
2
*
(
kSlopBytes
+
1
)
<
ip_limit
-
ip
&
&
op
<
op_limit_min_slop
)
{
const
uint8_t
*
const
ip_limit_min_slop
=
ip_limit
-
2
*
kSlopBytes
-
1
;
ip
+
+
;
size_t
tag
=
ip
[
-
1
]
;
#
if
defined
(
__clang__
)
&
&
defined
(
__aarch64__
)
asm
(
"
"
:
:
"
r
"
(
tag
)
)
;
#
endif
do
{
SNAPPY_PREFETCH
(
ip
+
128
)
;
for
(
int
i
=
0
;
i
<
2
;
i
+
+
)
{
const
uint8_t
*
old_ip
=
ip
;
assert
(
tag
=
=
ip
[
-
1
]
)
;
ptrdiff_t
len_minus_offset
=
kLengthMinusOffset
[
tag
]
;
uint32_t
next
;
#
if
defined
(
__aarch64__
)
size_t
tag_type
=
AdvanceToNextTagARMOptimized
(
&
ip
&
tag
)
;
next
=
LittleEndian
:
:
Load16
(
old_ip
)
;
#
else
size_t
tag_type
=
AdvanceToNextTagX86Optimized
(
&
ip
&
tag
)
;
next
=
LittleEndian
:
:
Load32
(
old_ip
)
;
#
endif
size_t
len
=
len_minus_offset
&
0xFF
;
ptrdiff_t
extracted
=
ExtractOffset
(
next
tag_type
)
;
ptrdiff_t
len_min_offset
=
len_minus_offset
-
extracted
;
if
(
SNAPPY_PREDICT_FALSE
(
len_minus_offset
>
extracted
)
)
{
if
(
SNAPPY_PREDICT_FALSE
(
len
&
0x80
)
)
{
break_loop
:
ip
=
old_ip
;
goto
exit
;
}
assert
(
tag_type
=
=
1
|
|
tag_type
=
=
2
)
;
std
:
:
ptrdiff_t
delta
=
(
op
+
deferred_length
)
+
len_min_offset
-
len
;
MemCopy64
(
op_base
+
op
deferred_src
deferred_length
)
;
op
+
=
deferred_length
;
ClearDeferred
(
&
deferred_src
&
deferred_length
safe_source
)
;
if
(
SNAPPY_PREDICT_FALSE
(
delta
<
0
|
|
!
Copy64BytesWithPatternExtension
(
op_base
+
op
len
-
len_min_offset
)
)
)
{
goto
break_loop
;
}
op
+
=
len
;
continue
;
}
std
:
:
ptrdiff_t
delta
=
(
op
+
deferred_length
)
+
len_min_offset
-
len
;
if
(
SNAPPY_PREDICT_FALSE
(
delta
<
0
)
)
{
if
(
tag_type
!
=
0
)
goto
break_loop
;
MemCopy64
(
op_base
+
op
deferred_src
deferred_length
)
;
op
+
=
deferred_length
;
DeferMemCopy
(
&
deferred_src
&
deferred_length
old_ip
len
)
;
continue
;
}
const
void
*
from
=
tag_type
?
reinterpret_cast
<
void
*
>
(
op_base
+
delta
)
:
old_ip
;
MemCopy64
(
op_base
+
op
deferred_src
deferred_length
)
;
op
+
=
deferred_length
;
DeferMemCopy
(
&
deferred_src
&
deferred_length
from
len
)
;
}
}
while
(
ip
<
ip_limit_min_slop
&
&
static_cast
<
ptrdiff_t
>
(
op
+
deferred_length
)
<
op_limit_min_slop
)
;
exit
:
ip
-
-
;
assert
(
ip
<
=
ip_limit
)
;
}
if
(
deferred_length
)
{
MemCopy64
(
op_base
+
op
deferred_src
deferred_length
)
;
op
+
=
deferred_length
;
ClearDeferred
(
&
deferred_src
&
deferred_length
safe_source
)
;
}
return
{
ip
op
}
;
}
class
SnappyDecompressor
{
private
:
Source
*
reader_
;
const
char
*
ip_
;
const
char
*
ip_limit_
;
const
char
*
ip_limit_min_maxtaglen_
;
uint32_t
peeked_
;
bool
eof_
;
char
scratch_
[
kMaximumTagLength
]
;
bool
RefillTag
(
)
;
void
ResetLimit
(
const
char
*
ip
)
{
ip_limit_min_maxtaglen_
=
ip_limit_
-
std
:
:
min
<
ptrdiff_t
>
(
ip_limit_
-
ip
kMaximumTagLength
-
1
)
;
}
public
:
explicit
SnappyDecompressor
(
Source
*
reader
)
:
reader_
(
reader
)
ip_
(
NULL
)
ip_limit_
(
NULL
)
peeked_
(
0
)
eof_
(
false
)
{
}
~
SnappyDecompressor
(
)
{
reader_
-
>
Skip
(
peeked_
)
;
}
bool
eof
(
)
const
{
return
eof_
;
}
bool
ReadUncompressedLength
(
uint32_t
*
result
)
{
assert
(
ip_
=
=
NULL
)
;
*
result
=
0
;
uint32_t
shift
=
0
;
while
(
true
)
{
if
(
shift
>
=
32
)
return
false
;
size_t
n
;
const
char
*
ip
=
reader_
-
>
Peek
(
&
n
)
;
if
(
n
=
=
0
)
return
false
;
const
unsigned
char
c
=
*
(
reinterpret_cast
<
const
unsigned
char
*
>
(
ip
)
)
;
reader_
-
>
Skip
(
1
)
;
uint32_t
val
=
c
&
0x7f
;
if
(
LeftShiftOverflows
(
static_cast
<
uint8_t
>
(
val
)
shift
)
)
return
false
;
*
result
|
=
val
<
<
shift
;
if
(
c
<
128
)
{
break
;
}
shift
+
=
7
;
}
return
true
;
}
template
<
class
Writer
>
#
if
defined
(
__GNUC__
)
&
&
defined
(
__x86_64__
)
__attribute__
(
(
aligned
(
32
)
)
)
#
endif
void
DecompressAllTags
(
Writer
*
writer
)
{
const
char
*
ip
=
ip_
;
ResetLimit
(
ip
)
;
auto
op
=
writer
-
>
GetOutputPtr
(
)
;
#
define
MAYBE_REFILL
(
)
\
if
(
SNAPPY_PREDICT_FALSE
(
ip
>
=
ip_limit_min_maxtaglen_
)
)
{
\
ip_
=
ip
;
\
if
(
SNAPPY_PREDICT_FALSE
(
!
RefillTag
(
)
)
)
goto
exit
;
\
ip
=
ip_
;
\
ResetLimit
(
ip
)
;
\
}
\
preload
=
static_cast
<
uint8_t
>
(
*
ip
)
uint32_t
preload
;
MAYBE_REFILL
(
)
;
for
(
;
;
)
{
{
ptrdiff_t
op_limit_min_slop
;
auto
op_base
=
writer
-
>
GetBase
(
&
op_limit_min_slop
)
;
if
(
op_base
)
{
auto
res
=
DecompressBranchless
(
reinterpret_cast
<
const
uint8_t
*
>
(
ip
)
reinterpret_cast
<
const
uint8_t
*
>
(
ip_limit_
)
op
-
op_base
op_base
op_limit_min_slop
)
;
ip
=
reinterpret_cast
<
const
char
*
>
(
res
.
first
)
;
op
=
op_base
+
res
.
second
;
MAYBE_REFILL
(
)
;
}
}
const
uint8_t
c
=
static_cast
<
uint8_t
>
(
preload
)
;
ip
+
+
;
if
(
SNAPPY_PREDICT_FALSE
(
(
c
&
0x3
)
=
=
LITERAL
)
)
{
size_t
literal_length
=
(
c
>
>
2
)
+
1u
;
if
(
writer
-
>
TryFastAppend
(
ip
ip_limit_
-
ip
literal_length
&
op
)
)
{
assert
(
literal_length
<
61
)
;
ip
+
=
literal_length
;
preload
=
static_cast
<
uint8_t
>
(
*
ip
)
;
continue
;
}
if
(
SNAPPY_PREDICT_FALSE
(
literal_length
>
=
61
)
)
{
const
size_t
literal_length_length
=
literal_length
-
60
;
literal_length
=
ExtractLowBytes
(
LittleEndian
:
:
Load32
(
ip
)
literal_length_length
)
+
1
;
ip
+
=
literal_length_length
;
}
size_t
avail
=
ip_limit_
-
ip
;
while
(
avail
<
literal_length
)
{
if
(
!
writer
-
>
Append
(
ip
avail
&
op
)
)
goto
exit
;
literal_length
-
=
avail
;
reader_
-
>
Skip
(
peeked_
)
;
size_t
n
;
ip
=
reader_
-
>
Peek
(
&
n
)
;
avail
=
n
;
peeked_
=
avail
;
if
(
avail
=
=
0
)
goto
exit
;
ip_limit_
=
ip
+
avail
;
ResetLimit
(
ip
)
;
}
if
(
!
writer
-
>
Append
(
ip
literal_length
&
op
)
)
goto
exit
;
ip
+
=
literal_length
;
MAYBE_REFILL
(
)
;
}
else
{
if
(
SNAPPY_PREDICT_FALSE
(
(
c
&
3
)
=
=
COPY_4_BYTE_OFFSET
)
)
{
const
size_t
copy_offset
=
LittleEndian
:
:
Load32
(
ip
)
;
const
size_t
length
=
(
c
>
>
2
)
+
1
;
ip
+
=
4
;
if
(
!
writer
-
>
AppendFromSelf
(
copy_offset
length
&
op
)
)
goto
exit
;
}
else
{
const
ptrdiff_t
entry
=
kLengthMinusOffset
[
c
]
;
preload
=
LittleEndian
:
:
Load32
(
ip
)
;
const
uint32_t
trailer
=
ExtractLowBytes
(
preload
c
&
3
)
;
const
uint32_t
length
=
entry
&
0xff
;
assert
(
length
>
0
)
;
const
uint32_t
copy_offset
=
trailer
-
entry
+
length
;
if
(
!
writer
-
>
AppendFromSelf
(
copy_offset
length
&
op
)
)
goto
exit
;
ip
+
=
(
c
&
3
)
;
preload
>
>
=
(
c
&
3
)
*
8
;
if
(
ip
<
ip_limit_min_maxtaglen_
)
continue
;
}
MAYBE_REFILL
(
)
;
}
}
#
undef
MAYBE_REFILL
exit
:
writer
-
>
SetOutputPtr
(
op
)
;
}
}
;
constexpr
uint32_t
CalculateNeeded
(
uint8_t
tag
)
{
return
(
(
tag
&
3
)
=
=
0
&
&
tag
>
=
(
60
*
4
)
)
?
(
tag
>
>
2
)
-
58
:
(
0x05030201
>
>
(
(
tag
*
8
)
&
31
)
)
&
0xFF
;
}
#
if
__cplusplus
>
=
201402L
constexpr
bool
VerifyCalculateNeeded
(
)
{
for
(
int
i
=
0
;
i
<
1
;
i
+
+
)
{
if
(
CalculateNeeded
(
i
)
!
=
(
char_table
[
i
]
>
>
11
)
+
1
)
return
false
;
}
return
true
;
}
static_assert
(
VerifyCalculateNeeded
(
)
"
"
)
;
#
endif
bool
SnappyDecompressor
:
:
RefillTag
(
)
{
const
char
*
ip
=
ip_
;
if
(
ip
=
=
ip_limit_
)
{
reader_
-
>
Skip
(
peeked_
)
;
size_t
n
;
ip
=
reader_
-
>
Peek
(
&
n
)
;
peeked_
=
n
;
eof_
=
(
n
=
=
0
)
;
if
(
eof_
)
return
false
;
ip_limit_
=
ip
+
n
;
}
assert
(
ip
<
ip_limit_
)
;
const
unsigned
char
c
=
*
(
reinterpret_cast
<
const
unsigned
char
*
>
(
ip
)
)
;
const
uint32_t
needed
=
CalculateNeeded
(
c
)
;
assert
(
needed
<
=
sizeof
(
scratch_
)
)
;
uint32_t
nbuf
=
ip_limit_
-
ip
;
if
(
nbuf
<
needed
)
{
std
:
:
memmove
(
scratch_
ip
nbuf
)
;
reader_
-
>
Skip
(
peeked_
)
;
peeked_
=
0
;
while
(
nbuf
<
needed
)
{
size_t
length
;
const
char
*
src
=
reader_
-
>
Peek
(
&
length
)
;
if
(
length
=
=
0
)
return
false
;
uint32_t
to_add
=
std
:
:
min
<
uint32_t
>
(
needed
-
nbuf
length
)
;
std
:
:
memcpy
(
scratch_
+
nbuf
src
to_add
)
;
nbuf
+
=
to_add
;
reader_
-
>
Skip
(
to_add
)
;
}
assert
(
nbuf
=
=
needed
)
;
ip_
=
scratch_
;
ip_limit_
=
scratch_
+
needed
;
}
else
if
(
nbuf
<
kMaximumTagLength
)
{
std
:
:
memmove
(
scratch_
ip
nbuf
)
;
reader_
-
>
Skip
(
peeked_
)
;
peeked_
=
0
;
ip_
=
scratch_
;
ip_limit_
=
scratch_
+
nbuf
;
}
else
{
ip_
=
ip
;
}
return
true
;
}
template
<
typename
Writer
>
static
bool
InternalUncompress
(
Source
*
r
Writer
*
writer
)
{
SnappyDecompressor
decompressor
(
r
)
;
uint32_t
uncompressed_len
=
0
;
if
(
!
decompressor
.
ReadUncompressedLength
(
&
uncompressed_len
)
)
return
false
;
return
InternalUncompressAllTags
(
&
decompressor
writer
r
-
>
Available
(
)
uncompressed_len
)
;
}
template
<
typename
Writer
>
static
bool
InternalUncompressAllTags
(
SnappyDecompressor
*
decompressor
Writer
*
writer
uint32_t
compressed_len
uint32_t
uncompressed_len
)
{
int
token
=
0
;
Report
(
token
"
snappy_uncompress
"
compressed_len
uncompressed_len
)
;
writer
-
>
SetExpectedLength
(
uncompressed_len
)
;
decompressor
-
>
DecompressAllTags
(
writer
)
;
writer
-
>
Flush
(
)
;
return
(
decompressor
-
>
eof
(
)
&
&
writer
-
>
CheckLength
(
)
)
;
}
bool
GetUncompressedLength
(
Source
*
source
uint32_t
*
result
)
{
SnappyDecompressor
decompressor
(
source
)
;
return
decompressor
.
ReadUncompressedLength
(
result
)
;
}
size_t
Compress
(
Source
*
reader
Sink
*
writer
CompressionOptions
options
)
{
assert
(
options
.
level
=
=
1
|
|
options
.
level
=
=
2
)
;
int
token
=
0
;
size_t
written
=
0
;
size_t
N
=
reader
-
>
Available
(
)
;
const
size_t
uncompressed_size
=
N
;
char
ulength
[
Varint
:
:
kMax32
]
;
char
*
p
=
Varint
:
:
Encode32
(
ulength
N
)
;
writer
-
>
Append
(
ulength
p
-
ulength
)
;
written
+
=
(
p
-
ulength
)
;
internal
:
:
WorkingMemory
wmem
(
N
)
;
while
(
N
>
0
)
{
size_t
fragment_size
;
const
char
*
fragment
=
reader
-
>
Peek
(
&
fragment_size
)
;
assert
(
fragment_size
!
=
0
)
;
const
size_t
num_to_read
=
std
:
:
min
(
N
kBlockSize
)
;
size_t
bytes_read
=
fragment_size
;
size_t
pending_advance
=
0
;
if
(
bytes_read
>
=
num_to_read
)
{
pending_advance
=
num_to_read
;
fragment_size
=
num_to_read
;
}
else
{
char
*
scratch
=
wmem
.
GetScratchInput
(
)
;
std
:
:
memcpy
(
scratch
fragment
bytes_read
)
;
reader
-
>
Skip
(
bytes_read
)
;
while
(
bytes_read
<
num_to_read
)
{
fragment
=
reader
-
>
Peek
(
&
fragment_size
)
;
size_t
n
=
std
:
:
min
<
size_t
>
(
fragment_size
num_to_read
-
bytes_read
)
;
std
:
:
memcpy
(
scratch
+
bytes_read
fragment
n
)
;
bytes_read
+
=
n
;
reader
-
>
Skip
(
n
)
;
}
assert
(
bytes_read
=
=
num_to_read
)
;
fragment
=
scratch
;
fragment_size
=
num_to_read
;
}
assert
(
fragment_size
=
=
num_to_read
)
;
int
table_size
;
uint16_t
*
table
=
wmem
.
GetHashTable
(
num_to_read
&
table_size
)
;
int
max_output
=
MaxCompressedLength
(
num_to_read
)
;
char
*
dest
=
writer
-
>
GetAppendBuffer
(
max_output
wmem
.
GetScratchOutput
(
)
)
;
char
*
end
=
nullptr
;
if
(
options
.
level
=
=
1
)
{
end
=
internal
:
:
CompressFragment
(
fragment
fragment_size
dest
table
table_size
)
;
}
else
if
(
options
.
level
=
=
2
)
{
end
=
internal
:
:
CompressFragmentDoubleHash
(
fragment
fragment_size
dest
table
table_size
>
>
1
table
+
(
table_size
>
>
1
)
table_size
>
>
1
)
;
}
writer
-
>
Append
(
dest
end
-
dest
)
;
written
+
=
(
end
-
dest
)
;
N
-
=
num_to_read
;
reader
-
>
Skip
(
pending_advance
)
;
}
Report
(
token
"
snappy_compress
"
written
uncompressed_size
)
;
return
written
;
}
class
SnappyIOVecReader
:
public
Source
{
public
:
SnappyIOVecReader
(
const
struct
iovec
*
iov
size_t
total_size
)
:
curr_iov_
(
iov
)
curr_pos_
(
total_size
>
0
?
reinterpret_cast
<
const
char
*
>
(
iov
-
>
iov_base
)
:
nullptr
)
curr_size_remaining_
(
total_size
>
0
?
iov
-
>
iov_len
:
0
)
total_size_remaining_
(
total_size
)
{
if
(
total_size
>
0
&
&
curr_size_remaining_
=
=
0
)
Advance
(
)
;
}
~
SnappyIOVecReader
(
)
override
=
default
;
size_t
Available
(
)
const
override
{
return
total_size_remaining_
;
}
const
char
*
Peek
(
size_t
*
len
)
override
{
*
len
=
curr_size_remaining_
;
return
curr_pos_
;
}
void
Skip
(
size_t
n
)
override
{
while
(
n
>
=
curr_size_remaining_
&
&
n
>
0
)
{
n
-
=
curr_size_remaining_
;
Advance
(
)
;
}
curr_size_remaining_
-
=
n
;
total_size_remaining_
-
=
n
;
curr_pos_
+
=
n
;
}
private
:
void
Advance
(
)
{
do
{
assert
(
total_size_remaining_
>
=
curr_size_remaining_
)
;
total_size_remaining_
-
=
curr_size_remaining_
;
if
(
total_size_remaining_
=
=
0
)
{
curr_pos_
=
nullptr
;
curr_size_remaining_
=
0
;
return
;
}
+
+
curr_iov_
;
curr_pos_
=
reinterpret_cast
<
const
char
*
>
(
curr_iov_
-
>
iov_base
)
;
curr_size_remaining_
=
curr_iov_
-
>
iov_len
;
}
while
(
curr_size_remaining_
=
=
0
)
;
}
const
struct
iovec
*
curr_iov_
;
const
char
*
curr_pos_
;
size_t
curr_size_remaining_
;
size_t
total_size_remaining_
;
}
;
class
SnappyIOVecWriter
{
private
:
const
struct
iovec
*
output_iov_end_
;
#
if
!
defined
(
NDEBUG
)
const
struct
iovec
*
output_iov_
;
#
endif
const
struct
iovec
*
curr_iov_
;
char
*
curr_iov_output_
;
size_t
curr_iov_remaining_
;
size_t
total_written_
;
size_t
output_limit_
;
static
inline
char
*
GetIOVecPointer
(
const
struct
iovec
*
iov
size_t
offset
)
{
return
reinterpret_cast
<
char
*
>
(
iov
-
>
iov_base
)
+
offset
;
}
public
:
inline
SnappyIOVecWriter
(
const
struct
iovec
*
iov
size_t
iov_count
)
:
output_iov_end_
(
iov
+
iov_count
)
#
if
!
defined
(
NDEBUG
)
output_iov_
(
iov
)
#
endif
curr_iov_
(
iov
)
curr_iov_output_
(
iov_count
?
reinterpret_cast
<
char
*
>
(
iov
-
>
iov_base
)
:
nullptr
)
curr_iov_remaining_
(
iov_count
?
iov
-
>
iov_len
:
0
)
total_written_
(
0
)
output_limit_
(
-
1
)
{
}
inline
void
SetExpectedLength
(
size_t
len
)
{
output_limit_
=
len
;
}
inline
bool
CheckLength
(
)
const
{
return
total_written_
=
=
output_limit_
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
char
*
*
)
{
if
(
total_written_
+
len
>
output_limit_
)
{
return
false
;
}
return
AppendNoCheck
(
ip
len
)
;
}
char
*
GetOutputPtr
(
)
{
return
nullptr
;
}
char
*
GetBase
(
ptrdiff_t
*
)
{
return
nullptr
;
}
void
SetOutputPtr
(
char
*
op
)
{
(
void
)
op
;
}
inline
bool
AppendNoCheck
(
const
char
*
ip
size_t
len
)
{
while
(
len
>
0
)
{
if
(
curr_iov_remaining_
=
=
0
)
{
if
(
curr_iov_
+
1
>
=
output_iov_end_
)
{
return
false
;
}
+
+
curr_iov_
;
curr_iov_output_
=
reinterpret_cast
<
char
*
>
(
curr_iov_
-
>
iov_base
)
;
curr_iov_remaining_
=
curr_iov_
-
>
iov_len
;
}
const
size_t
to_write
=
std
:
:
min
(
len
curr_iov_remaining_
)
;
std
:
:
memcpy
(
curr_iov_output_
ip
to_write
)
;
curr_iov_output_
+
=
to_write
;
curr_iov_remaining_
-
=
to_write
;
total_written_
+
=
to_write
;
ip
+
=
to_write
;
len
-
=
to_write
;
}
return
true
;
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
len
char
*
*
)
{
const
size_t
space_left
=
output_limit_
-
total_written_
;
if
(
len
<
=
16
&
&
available
>
=
16
+
kMaximumTagLength
&
&
space_left
>
=
16
&
&
curr_iov_remaining_
>
=
16
)
{
UnalignedCopy128
(
ip
curr_iov_output_
)
;
curr_iov_output_
+
=
len
;
curr_iov_remaining_
-
=
len
;
total_written_
+
=
len
;
return
true
;
}
return
false
;
}
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
char
*
*
)
{
if
(
offset
-
1u
>
=
total_written_
)
{
return
false
;
}
const
size_t
space_left
=
output_limit_
-
total_written_
;
if
(
len
>
space_left
)
{
return
false
;
}
const
iovec
*
from_iov
=
curr_iov_
;
size_t
from_iov_offset
=
curr_iov_
-
>
iov_len
-
curr_iov_remaining_
;
while
(
offset
>
0
)
{
if
(
from_iov_offset
>
=
offset
)
{
from_iov_offset
-
=
offset
;
break
;
}
offset
-
=
from_iov_offset
;
-
-
from_iov
;
#
if
!
defined
(
NDEBUG
)
assert
(
from_iov
>
=
output_iov_
)
;
#
endif
from_iov_offset
=
from_iov
-
>
iov_len
;
}
while
(
len
>
0
)
{
assert
(
from_iov
<
=
curr_iov_
)
;
if
(
from_iov
!
=
curr_iov_
)
{
const
size_t
to_copy
=
std
:
:
min
(
from_iov
-
>
iov_len
-
from_iov_offset
len
)
;
AppendNoCheck
(
GetIOVecPointer
(
from_iov
from_iov_offset
)
to_copy
)
;
len
-
=
to_copy
;
if
(
len
>
0
)
{
+
+
from_iov
;
from_iov_offset
=
0
;
}
}
else
{
size_t
to_copy
=
curr_iov_remaining_
;
if
(
to_copy
=
=
0
)
{
if
(
curr_iov_
+
1
>
=
output_iov_end_
)
{
return
false
;
}
+
+
curr_iov_
;
curr_iov_output_
=
reinterpret_cast
<
char
*
>
(
curr_iov_
-
>
iov_base
)
;
curr_iov_remaining_
=
curr_iov_
-
>
iov_len
;
continue
;
}
if
(
to_copy
>
len
)
{
to_copy
=
len
;
}
assert
(
to_copy
>
0
)
;
IncrementalCopy
(
GetIOVecPointer
(
from_iov
from_iov_offset
)
curr_iov_output_
curr_iov_output_
+
to_copy
curr_iov_output_
+
curr_iov_remaining_
)
;
curr_iov_output_
+
=
to_copy
;
curr_iov_remaining_
-
=
to_copy
;
from_iov_offset
+
=
to_copy
;
total_written_
+
=
to_copy
;
len
-
=
to_copy
;
}
}
return
true
;
}
inline
void
Flush
(
)
{
}
}
;
bool
RawUncompressToIOVec
(
const
char
*
compressed
size_t
compressed_length
const
struct
iovec
*
iov
size_t
iov_cnt
)
{
ByteArraySource
reader
(
compressed
compressed_length
)
;
return
RawUncompressToIOVec
(
&
reader
iov
iov_cnt
)
;
}
bool
RawUncompressToIOVec
(
Source
*
compressed
const
struct
iovec
*
iov
size_t
iov_cnt
)
{
SnappyIOVecWriter
output
(
iov
iov_cnt
)
;
return
InternalUncompress
(
compressed
&
output
)
;
}
class
SnappyArrayWriter
{
private
:
char
*
base_
;
char
*
op_
;
char
*
op_limit_
;
char
*
op_limit_min_slop_
;
public
:
inline
explicit
SnappyArrayWriter
(
char
*
dst
)
:
base_
(
dst
)
op_
(
dst
)
op_limit_
(
dst
)
op_limit_min_slop_
(
dst
)
{
}
inline
void
SetExpectedLength
(
size_t
len
)
{
op_limit_
=
op_
+
len
;
op_limit_min_slop_
=
op_limit_
-
std
:
:
min
<
size_t
>
(
kSlopBytes
-
1
len
)
;
}
inline
bool
CheckLength
(
)
const
{
return
op_
=
=
op_limit_
;
}
char
*
GetOutputPtr
(
)
{
return
op_
;
}
char
*
GetBase
(
ptrdiff_t
*
op_limit_min_slop
)
{
*
op_limit_min_slop
=
op_limit_min_slop_
-
base_
;
return
base_
;
}
void
SetOutputPtr
(
char
*
op
)
{
op_
=
op
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
char
*
*
op_p
)
{
char
*
op
=
*
op_p
;
const
size_t
space_left
=
op_limit_
-
op
;
if
(
space_left
<
len
)
return
false
;
std
:
:
memcpy
(
op
ip
len
)
;
*
op_p
=
op
+
len
;
return
true
;
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
len
char
*
*
op_p
)
{
char
*
op
=
*
op_p
;
const
size_t
space_left
=
op_limit_
-
op
;
if
(
len
<
=
16
&
&
available
>
=
16
+
kMaximumTagLength
&
&
space_left
>
=
16
)
{
UnalignedCopy128
(
ip
op
)
;
*
op_p
=
op
+
len
;
return
true
;
}
else
{
return
false
;
}
}
SNAPPY_ATTRIBUTE_ALWAYS_INLINE
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
char
*
*
op_p
)
{
assert
(
len
>
0
)
;
char
*
const
op
=
*
op_p
;
assert
(
op
>
=
base_
)
;
char
*
const
op_end
=
op
+
len
;
if
(
SNAPPY_PREDICT_FALSE
(
static_cast
<
size_t
>
(
op
-
base_
)
<
offset
)
)
return
false
;
if
(
SNAPPY_PREDICT_FALSE
(
(
kSlopBytes
<
64
&
&
len
>
kSlopBytes
)
|
|
op
>
=
op_limit_min_slop_
|
|
offset
<
len
)
)
{
if
(
op_end
>
op_limit_
|
|
offset
=
=
0
)
return
false
;
*
op_p
=
IncrementalCopy
(
op
-
offset
op
op_end
op_limit_
)
;
return
true
;
}
std
:
:
memmove
(
op
op
-
offset
kSlopBytes
)
;
*
op_p
=
op_end
;
return
true
;
}
inline
size_t
Produced
(
)
const
{
assert
(
op_
>
=
base_
)
;
return
op_
-
base_
;
}
inline
void
Flush
(
)
{
}
}
;
bool
RawUncompress
(
const
char
*
compressed
size_t
compressed_length
char
*
uncompressed
)
{
ByteArraySource
reader
(
compressed
compressed_length
)
;
return
RawUncompress
(
&
reader
uncompressed
)
;
}
bool
RawUncompress
(
Source
*
compressed
char
*
uncompressed
)
{
SnappyArrayWriter
output
(
uncompressed
)
;
return
InternalUncompress
(
compressed
&
output
)
;
}
bool
Uncompress
(
const
char
*
compressed
size_t
compressed_length
std
:
:
string
*
uncompressed
)
{
size_t
ulength
;
if
(
!
GetUncompressedLength
(
compressed
compressed_length
&
ulength
)
)
{
return
false
;
}
if
(
ulength
>
uncompressed
-
>
max_size
(
)
)
{
return
false
;
}
STLStringResizeUninitialized
(
uncompressed
ulength
)
;
return
RawUncompress
(
compressed
compressed_length
string_as_array
(
uncompressed
)
)
;
}
class
SnappyDecompressionValidator
{
private
:
size_t
expected_
;
size_t
produced_
;
public
:
inline
SnappyDecompressionValidator
(
)
:
expected_
(
0
)
produced_
(
0
)
{
}
inline
void
SetExpectedLength
(
size_t
len
)
{
expected_
=
len
;
}
size_t
GetOutputPtr
(
)
{
return
produced_
;
}
size_t
GetBase
(
ptrdiff_t
*
op_limit_min_slop
)
{
*
op_limit_min_slop
=
std
:
:
numeric_limits
<
ptrdiff_t
>
:
:
max
(
)
-
kSlopBytes
+
1
;
return
1
;
}
void
SetOutputPtr
(
size_t
op
)
{
produced_
=
op
;
}
inline
bool
CheckLength
(
)
const
{
return
expected_
=
=
produced_
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
size_t
*
produced
)
{
(
void
)
ip
;
*
produced
+
=
len
;
return
*
produced
<
=
expected_
;
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
length
size_t
*
produced
)
{
(
void
)
ip
;
(
void
)
available
;
(
void
)
length
;
(
void
)
produced
;
return
false
;
}
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
size_t
*
produced
)
{
if
(
*
produced
<
=
offset
-
1u
)
return
false
;
*
produced
+
=
len
;
return
*
produced
<
=
expected_
;
}
inline
void
Flush
(
)
{
}
}
;
bool
IsValidCompressedBuffer
(
const
char
*
compressed
size_t
compressed_length
)
{
ByteArraySource
reader
(
compressed
compressed_length
)
;
SnappyDecompressionValidator
writer
;
return
InternalUncompress
(
&
reader
&
writer
)
;
}
bool
IsValidCompressed
(
Source
*
compressed
)
{
SnappyDecompressionValidator
writer
;
return
InternalUncompress
(
compressed
&
writer
)
;
}
void
RawCompress
(
const
char
*
input
size_t
input_length
char
*
compressed
size_t
*
compressed_length
CompressionOptions
options
)
{
ByteArraySource
reader
(
input
input_length
)
;
UncheckedByteArraySink
writer
(
compressed
)
;
Compress
(
&
reader
&
writer
options
)
;
*
compressed_length
=
(
writer
.
CurrentDestination
(
)
-
compressed
)
;
}
void
RawCompressFromIOVec
(
const
struct
iovec
*
iov
size_t
uncompressed_length
char
*
compressed
size_t
*
compressed_length
CompressionOptions
options
)
{
SnappyIOVecReader
reader
(
iov
uncompressed_length
)
;
UncheckedByteArraySink
writer
(
compressed
)
;
Compress
(
&
reader
&
writer
options
)
;
*
compressed_length
=
writer
.
CurrentDestination
(
)
-
compressed
;
}
size_t
Compress
(
const
char
*
input
size_t
input_length
std
:
:
string
*
compressed
CompressionOptions
options
)
{
STLStringResizeUninitialized
(
compressed
MaxCompressedLength
(
input_length
)
)
;
size_t
compressed_length
;
RawCompress
(
input
input_length
string_as_array
(
compressed
)
&
compressed_length
options
)
;
compressed
-
>
erase
(
compressed_length
)
;
return
compressed_length
;
}
size_t
CompressFromIOVec
(
const
struct
iovec
*
iov
size_t
iov_cnt
std
:
:
string
*
compressed
CompressionOptions
options
)
{
size_t
uncompressed_length
=
0
;
for
(
size_t
i
=
0
;
i
<
iov_cnt
;
+
+
i
)
{
uncompressed_length
+
=
iov
[
i
]
.
iov_len
;
}
STLStringResizeUninitialized
(
compressed
MaxCompressedLength
(
uncompressed_length
)
)
;
size_t
compressed_length
;
RawCompressFromIOVec
(
iov
uncompressed_length
string_as_array
(
compressed
)
&
compressed_length
options
)
;
compressed
-
>
erase
(
compressed_length
)
;
return
compressed_length
;
}
template
<
typename
Allocator
>
class
SnappyScatteredWriter
{
Allocator
allocator_
;
std
:
:
vector
<
char
*
>
blocks_
;
size_t
expected_
;
size_t
full_size_
;
char
*
op_base_
;
char
*
op_ptr_
;
char
*
op_limit_
;
char
*
op_limit_min_slop_
;
inline
size_t
Size
(
)
const
{
return
full_size_
+
(
op_ptr_
-
op_base_
)
;
}
bool
SlowAppend
(
const
char
*
ip
size_t
len
)
;
bool
SlowAppendFromSelf
(
size_t
offset
size_t
len
)
;
public
:
inline
explicit
SnappyScatteredWriter
(
const
Allocator
&
allocator
)
:
allocator_
(
allocator
)
full_size_
(
0
)
op_base_
(
NULL
)
op_ptr_
(
NULL
)
op_limit_
(
NULL
)
op_limit_min_slop_
(
NULL
)
{
}
char
*
GetOutputPtr
(
)
{
return
op_ptr_
;
}
char
*
GetBase
(
ptrdiff_t
*
op_limit_min_slop
)
{
*
op_limit_min_slop
=
op_limit_min_slop_
-
op_base_
;
return
op_base_
;
}
void
SetOutputPtr
(
char
*
op
)
{
op_ptr_
=
op
;
}
inline
void
SetExpectedLength
(
size_t
len
)
{
assert
(
blocks_
.
empty
(
)
)
;
expected_
=
len
;
}
inline
bool
CheckLength
(
)
const
{
return
Size
(
)
=
=
expected_
;
}
inline
size_t
Produced
(
)
const
{
return
Size
(
)
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
char
*
*
op_p
)
{
char
*
op
=
*
op_p
;
size_t
avail
=
op_limit_
-
op
;
if
(
len
<
=
avail
)
{
std
:
:
memcpy
(
op
ip
len
)
;
*
op_p
=
op
+
len
;
return
true
;
}
else
{
op_ptr_
=
op
;
bool
res
=
SlowAppend
(
ip
len
)
;
*
op_p
=
op_ptr_
;
return
res
;
}
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
length
char
*
*
op_p
)
{
char
*
op
=
*
op_p
;
const
int
space_left
=
op_limit_
-
op
;
if
(
length
<
=
16
&
&
available
>
=
16
+
kMaximumTagLength
&
&
space_left
>
=
16
)
{
UnalignedCopy128
(
ip
op
)
;
*
op_p
=
op
+
length
;
return
true
;
}
else
{
return
false
;
}
}
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
char
*
*
op_p
)
{
char
*
op
=
*
op_p
;
assert
(
op
>
=
op_base_
)
;
if
(
SNAPPY_PREDICT_FALSE
(
(
kSlopBytes
<
64
&
&
len
>
kSlopBytes
)
|
|
static_cast
<
size_t
>
(
op
-
op_base_
)
<
offset
|
|
op
>
=
op_limit_min_slop_
|
|
offset
<
len
)
)
{
if
(
offset
=
=
0
)
return
false
;
if
(
SNAPPY_PREDICT_FALSE
(
static_cast
<
size_t
>
(
op
-
op_base_
)
<
offset
|
|
op
+
len
>
op_limit_
)
)
{
op_ptr_
=
op
;
bool
res
=
SlowAppendFromSelf
(
offset
len
)
;
*
op_p
=
op_ptr_
;
return
res
;
}
*
op_p
=
IncrementalCopy
(
op
-
offset
op
op
+
len
op_limit_
)
;
return
true
;
}
char
*
const
op_end
=
op
+
len
;
std
:
:
memmove
(
op
op
-
offset
kSlopBytes
)
;
*
op_p
=
op_end
;
return
true
;
}
inline
void
Flush
(
)
{
allocator_
.
Flush
(
Produced
(
)
)
;
}
}
;
template
<
typename
Allocator
>
bool
SnappyScatteredWriter
<
Allocator
>
:
:
SlowAppend
(
const
char
*
ip
size_t
len
)
{
size_t
avail
=
op_limit_
-
op_ptr_
;
while
(
len
>
avail
)
{
std
:
:
memcpy
(
op_ptr_
ip
avail
)
;
op_ptr_
+
=
avail
;
assert
(
op_limit_
-
op_ptr_
=
=
0
)
;
full_size_
+
=
(
op_ptr_
-
op_base_
)
;
len
-
=
avail
;
ip
+
=
avail
;
if
(
full_size_
+
len
>
expected_
)
return
false
;
size_t
bsize
=
std
:
:
min
<
size_t
>
(
kBlockSize
expected_
-
full_size_
)
;
op_base_
=
allocator_
.
Allocate
(
bsize
)
;
op_ptr_
=
op_base_
;
op_limit_
=
op_base_
+
bsize
;
op_limit_min_slop_
=
op_limit_
-
std
:
:
min
<
size_t
>
(
kSlopBytes
-
1
bsize
)
;
blocks_
.
push_back
(
op_base_
)
;
avail
=
bsize
;
}
std
:
:
memcpy
(
op_ptr_
ip
len
)
;
op_ptr_
+
=
len
;
return
true
;
}
template
<
typename
Allocator
>
bool
SnappyScatteredWriter
<
Allocator
>
:
:
SlowAppendFromSelf
(
size_t
offset
size_t
len
)
{
const
size_t
cur
=
Size
(
)
;
if
(
offset
-
1u
>
=
cur
)
return
false
;
if
(
expected_
-
cur
<
len
)
return
false
;
size_t
src
=
cur
-
offset
;
char
*
op
=
op_ptr_
;
while
(
len
-
-
>
0
)
{
char
c
=
blocks_
[
src
>
>
kBlockLog
]
[
src
&
(
kBlockSize
-
1
)
]
;
if
(
!
Append
(
&
c
1
&
op
)
)
{
op_ptr_
=
op
;
return
false
;
}
src
+
+
;
}
op_ptr_
=
op
;
return
true
;
}
class
SnappySinkAllocator
{
public
:
explicit
SnappySinkAllocator
(
Sink
*
dest
)
:
dest_
(
dest
)
{
}
char
*
Allocate
(
int
size
)
{
Datablock
block
(
new
char
[
size
]
size
)
;
blocks_
.
push_back
(
block
)
;
return
block
.
data
;
}
void
Flush
(
size_t
size
)
{
size_t
size_written
=
0
;
for
(
Datablock
&
block
:
blocks_
)
{
size_t
block_size
=
std
:
:
min
<
size_t
>
(
block
.
size
size
-
size_written
)
;
dest_
-
>
AppendAndTakeOwnership
(
block
.
data
block_size
&
SnappySinkAllocator
:
:
Deleter
NULL
)
;
size_written
+
=
block_size
;
}
blocks_
.
clear
(
)
;
}
private
:
struct
Datablock
{
char
*
data
;
size_t
size
;
Datablock
(
char
*
p
size_t
s
)
:
data
(
p
)
size
(
s
)
{
}
}
;
static
void
Deleter
(
void
*
arg
const
char
*
bytes
size_t
size
)
{
(
void
)
arg
;
(
void
)
size
;
delete
[
]
bytes
;
}
Sink
*
dest_
;
std
:
:
vector
<
Datablock
>
blocks_
;
}
;
size_t
UncompressAsMuchAsPossible
(
Source
*
compressed
Sink
*
uncompressed
)
{
SnappySinkAllocator
allocator
(
uncompressed
)
;
SnappyScatteredWriter
<
SnappySinkAllocator
>
writer
(
allocator
)
;
InternalUncompress
(
compressed
&
writer
)
;
return
writer
.
Produced
(
)
;
}
bool
Uncompress
(
Source
*
compressed
Sink
*
uncompressed
)
{
SnappyDecompressor
decompressor
(
compressed
)
;
uint32_t
uncompressed_len
=
0
;
if
(
!
decompressor
.
ReadUncompressedLength
(
&
uncompressed_len
)
)
{
return
false
;
}
char
c
;
size_t
allocated_size
;
char
*
buf
=
uncompressed
-
>
GetAppendBufferVariable
(
1
uncompressed_len
&
c
1
&
allocated_size
)
;
const
size_t
compressed_len
=
compressed
-
>
Available
(
)
;
if
(
allocated_size
>
=
uncompressed_len
)
{
SnappyArrayWriter
writer
(
buf
)
;
bool
result
=
InternalUncompressAllTags
(
&
decompressor
&
writer
compressed_len
uncompressed_len
)
;
uncompressed
-
>
Append
(
buf
writer
.
Produced
(
)
)
;
return
result
;
}
else
{
SnappySinkAllocator
allocator
(
uncompressed
)
;
SnappyScatteredWriter
<
SnappySinkAllocator
>
writer
(
allocator
)
;
return
InternalUncompressAllTags
(
&
decompressor
&
writer
compressed_len
uncompressed_len
)
;
}
}
}
