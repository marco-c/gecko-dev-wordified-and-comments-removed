#
include
"
snappy
.
h
"
#
include
"
snappy
-
internal
.
h
"
#
include
"
snappy
-
sinksource
.
h
"
#
if
!
defined
(
SNAPPY_HAVE_SSSE3
)
#
if
defined
(
__SSSE3__
)
|
|
defined
(
__AVX__
)
#
define
SNAPPY_HAVE_SSSE3
1
#
else
#
define
SNAPPY_HAVE_SSSE3
0
#
endif
#
endif
#
if
!
defined
(
SNAPPY_HAVE_BMI2
)
#
if
defined
(
__BMI2__
)
|
|
(
defined
(
_MSC_VER
)
&
&
defined
(
__AVX2__
)
)
#
define
SNAPPY_HAVE_BMI2
1
#
else
#
define
SNAPPY_HAVE_BMI2
0
#
endif
#
endif
#
if
SNAPPY_HAVE_SSSE3
#
include
<
tmmintrin
.
h
>
#
endif
#
if
SNAPPY_HAVE_BMI2
#
include
<
immintrin
.
h
>
#
endif
#
include
<
stdio
.
h
>
#
include
<
algorithm
>
#
include
<
string
>
#
include
<
vector
>
namespace
snappy
{
using
internal
:
:
COPY_1_BYTE_OFFSET
;
using
internal
:
:
COPY_2_BYTE_OFFSET
;
using
internal
:
:
LITERAL
;
using
internal
:
:
char_table
;
using
internal
:
:
kMaximumTagLength
;
static
inline
uint32
HashBytes
(
uint32
bytes
int
shift
)
{
uint32
kMul
=
0x1e35a7bd
;
return
(
bytes
*
kMul
)
>
>
shift
;
}
static
inline
uint32
Hash
(
const
char
*
p
int
shift
)
{
return
HashBytes
(
UNALIGNED_LOAD32
(
p
)
shift
)
;
}
size_t
MaxCompressedLength
(
size_t
source_len
)
{
return
32
+
source_len
+
source_len
/
6
;
}
namespace
{
void
UnalignedCopy64
(
const
void
*
src
void
*
dst
)
{
char
tmp
[
8
]
;
memcpy
(
tmp
src
8
)
;
memcpy
(
dst
tmp
8
)
;
}
void
UnalignedCopy128
(
const
void
*
src
void
*
dst
)
{
char
tmp
[
16
]
;
memcpy
(
tmp
src
16
)
;
memcpy
(
dst
tmp
16
)
;
}
inline
char
*
IncrementalCopySlow
(
const
char
*
src
char
*
op
char
*
const
op_limit
)
{
#
ifdef
__clang__
#
pragma
clang
loop
unroll
(
disable
)
#
endif
while
(
op
<
op_limit
)
{
*
op
+
+
=
*
src
+
+
;
}
return
op_limit
;
}
#
if
SNAPPY_HAVE_SSSE3
alignas
(
16
)
const
char
pshufb_fill_patterns
[
7
]
[
16
]
=
{
{
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
}
{
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
}
{
0
1
2
0
1
2
0
1
2
0
1
2
0
1
2
0
}
{
0
1
2
3
0
1
2
3
0
1
2
3
0
1
2
3
}
{
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
0
}
{
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
}
{
0
1
2
3
4
5
6
0
1
2
3
4
5
6
0
1
}
}
;
#
endif
inline
char
*
IncrementalCopy
(
const
char
*
src
char
*
op
char
*
const
op_limit
char
*
const
buf_limit
)
{
assert
(
src
<
op
)
;
assert
(
op
<
=
op_limit
)
;
assert
(
op_limit
<
=
buf_limit
)
;
size_t
pattern_size
=
op
-
src
;
if
(
SNAPPY_PREDICT_FALSE
(
pattern_size
<
8
)
)
{
#
if
SNAPPY_HAVE_SSSE3
if
(
SNAPPY_PREDICT_TRUE
(
op
<
=
buf_limit
-
16
)
)
{
const
__m128i
shuffle_mask
=
_mm_load_si128
(
reinterpret_cast
<
const
__m128i
*
>
(
pshufb_fill_patterns
)
+
pattern_size
-
1
)
;
const
__m128i
pattern
=
_mm_shuffle_epi8
(
_mm_loadl_epi64
(
reinterpret_cast
<
const
__m128i
*
>
(
src
)
)
shuffle_mask
)
;
SNAPPY_ANNOTATE_MEMORY_IS_INITIALIZED
(
&
pattern
sizeof
(
pattern
)
)
;
pattern_size
*
=
16
/
pattern_size
;
char
*
op_end
=
std
:
:
min
(
op_limit
buf_limit
-
15
)
;
while
(
op
<
op_end
)
{
_mm_storeu_si128
(
reinterpret_cast
<
__m128i
*
>
(
op
)
pattern
)
;
op
+
=
pattern_size
;
}
if
(
SNAPPY_PREDICT_TRUE
(
op
>
=
op_limit
)
)
return
op_limit
;
}
return
IncrementalCopySlow
(
src
op
op_limit
)
;
#
else
if
(
SNAPPY_PREDICT_TRUE
(
op
<
=
buf_limit
-
11
)
)
{
while
(
pattern_size
<
8
)
{
UnalignedCopy64
(
src
op
)
;
op
+
=
pattern_size
;
pattern_size
*
=
2
;
}
if
(
SNAPPY_PREDICT_TRUE
(
op
>
=
op_limit
)
)
return
op_limit
;
}
else
{
return
IncrementalCopySlow
(
src
op
op_limit
)
;
}
#
endif
}
assert
(
pattern_size
>
=
8
)
;
if
(
SNAPPY_PREDICT_TRUE
(
op_limit
<
=
buf_limit
-
16
)
)
{
UnalignedCopy64
(
src
op
)
;
UnalignedCopy64
(
src
+
8
op
+
8
)
;
if
(
op
+
16
<
op_limit
)
{
UnalignedCopy64
(
src
+
16
op
+
16
)
;
UnalignedCopy64
(
src
+
24
op
+
24
)
;
}
if
(
op
+
32
<
op_limit
)
{
UnalignedCopy64
(
src
+
32
op
+
32
)
;
UnalignedCopy64
(
src
+
40
op
+
40
)
;
}
if
(
op
+
48
<
op_limit
)
{
UnalignedCopy64
(
src
+
48
op
+
48
)
;
UnalignedCopy64
(
src
+
56
op
+
56
)
;
}
return
op_limit
;
}
#
ifdef
__clang__
#
pragma
clang
loop
unroll
(
disable
)
#
endif
for
(
char
*
op_end
=
buf_limit
-
16
;
op
<
op_end
;
op
+
=
16
src
+
=
16
)
{
UnalignedCopy64
(
src
op
)
;
UnalignedCopy64
(
src
+
8
op
+
8
)
;
}
if
(
op
>
=
op_limit
)
return
op_limit
;
if
(
SNAPPY_PREDICT_FALSE
(
op
<
=
buf_limit
-
8
)
)
{
UnalignedCopy64
(
src
op
)
;
src
+
=
8
;
op
+
=
8
;
}
return
IncrementalCopySlow
(
src
op
op_limit
)
;
}
}
template
<
bool
allow_fast_path
>
static
inline
char
*
EmitLiteral
(
char
*
op
const
char
*
literal
int
len
)
{
assert
(
len
>
0
)
;
int
n
=
len
-
1
;
if
(
allow_fast_path
&
&
len
<
=
16
)
{
*
op
+
+
=
LITERAL
|
(
n
<
<
2
)
;
UnalignedCopy128
(
literal
op
)
;
return
op
+
len
;
}
if
(
n
<
60
)
{
*
op
+
+
=
LITERAL
|
(
n
<
<
2
)
;
}
else
{
int
count
=
(
Bits
:
:
Log2Floor
(
n
)
>
>
3
)
+
1
;
assert
(
count
>
=
1
)
;
assert
(
count
<
=
4
)
;
*
op
+
+
=
LITERAL
|
(
(
59
+
count
)
<
<
2
)
;
LittleEndian
:
:
Store32
(
op
n
)
;
op
+
=
count
;
}
memcpy
(
op
literal
len
)
;
return
op
+
len
;
}
template
<
bool
len_less_than_12
>
static
inline
char
*
EmitCopyAtMost64
(
char
*
op
size_t
offset
size_t
len
)
{
assert
(
len
<
=
64
)
;
assert
(
len
>
=
4
)
;
assert
(
offset
<
65536
)
;
assert
(
len_less_than_12
=
=
(
len
<
12
)
)
;
if
(
len_less_than_12
&
&
SNAPPY_PREDICT_TRUE
(
offset
<
2048
)
)
{
*
op
+
+
=
COPY_1_BYTE_OFFSET
+
(
(
len
-
4
)
<
<
2
)
+
(
(
offset
>
>
3
)
&
0xe0
)
;
*
op
+
+
=
offset
&
0xff
;
}
else
{
uint32
u
=
COPY_2_BYTE_OFFSET
+
(
(
len
-
1
)
<
<
2
)
+
(
offset
<
<
8
)
;
LittleEndian
:
:
Store32
(
op
u
)
;
op
+
=
3
;
}
return
op
;
}
template
<
bool
len_less_than_12
>
static
inline
char
*
EmitCopy
(
char
*
op
size_t
offset
size_t
len
)
{
assert
(
len_less_than_12
=
=
(
len
<
12
)
)
;
if
(
len_less_than_12
)
{
return
EmitCopyAtMost64
<
true
>
(
op
offset
len
)
;
}
else
{
while
(
SNAPPY_PREDICT_FALSE
(
len
>
=
68
)
)
{
op
=
EmitCopyAtMost64
<
false
>
(
op
offset
64
)
;
len
-
=
64
;
}
if
(
len
>
64
)
{
op
=
EmitCopyAtMost64
<
false
>
(
op
offset
60
)
;
len
-
=
60
;
}
if
(
len
<
12
)
{
op
=
EmitCopyAtMost64
<
true
>
(
op
offset
len
)
;
}
else
{
op
=
EmitCopyAtMost64
<
false
>
(
op
offset
len
)
;
}
return
op
;
}
}
bool
GetUncompressedLength
(
const
char
*
start
size_t
n
size_t
*
result
)
{
uint32
v
=
0
;
const
char
*
limit
=
start
+
n
;
if
(
Varint
:
:
Parse32WithLimit
(
start
limit
&
v
)
!
=
NULL
)
{
*
result
=
v
;
return
true
;
}
else
{
return
false
;
}
}
namespace
{
uint32
CalculateTableSize
(
uint32
input_size
)
{
static_assert
(
kMaxHashTableSize
>
=
kMinHashTableSize
"
kMaxHashTableSize
should
be
greater
or
equal
to
kMinHashTableSize
.
"
)
;
if
(
input_size
>
kMaxHashTableSize
)
{
return
kMaxHashTableSize
;
}
if
(
input_size
<
kMinHashTableSize
)
{
return
kMinHashTableSize
;
}
return
2u
<
<
Bits
:
:
Log2Floor
(
input_size
-
1
)
;
}
}
namespace
internal
{
WorkingMemory
:
:
WorkingMemory
(
size_t
input_size
)
{
const
size_t
max_fragment_size
=
std
:
:
min
(
input_size
kBlockSize
)
;
const
size_t
table_size
=
CalculateTableSize
(
max_fragment_size
)
;
size_
=
table_size
*
sizeof
(
*
table_
)
+
max_fragment_size
+
MaxCompressedLength
(
max_fragment_size
)
;
mem_
=
std
:
:
allocator
<
char
>
(
)
.
allocate
(
size_
)
;
table_
=
reinterpret_cast
<
uint16
*
>
(
mem_
)
;
input_
=
mem_
+
table_size
*
sizeof
(
*
table_
)
;
output_
=
input_
+
max_fragment_size
;
}
WorkingMemory
:
:
~
WorkingMemory
(
)
{
std
:
:
allocator
<
char
>
(
)
.
deallocate
(
mem_
size_
)
;
}
uint16
*
WorkingMemory
:
:
GetHashTable
(
size_t
fragment_size
int
*
table_size
)
const
{
const
size_t
htsize
=
CalculateTableSize
(
fragment_size
)
;
memset
(
table_
0
htsize
*
sizeof
(
*
table_
)
)
;
*
table_size
=
htsize
;
return
table_
;
}
}
#
ifdef
ARCH_K8
typedef
uint64
EightBytesReference
;
static
inline
EightBytesReference
GetEightBytesAt
(
const
char
*
ptr
)
{
return
UNALIGNED_LOAD64
(
ptr
)
;
}
static
inline
uint32
GetUint32AtOffset
(
uint64
v
int
offset
)
{
assert
(
offset
>
=
0
)
;
assert
(
offset
<
=
4
)
;
return
v
>
>
(
LittleEndian
:
:
IsLittleEndian
(
)
?
8
*
offset
:
32
-
8
*
offset
)
;
}
#
else
typedef
const
char
*
EightBytesReference
;
static
inline
EightBytesReference
GetEightBytesAt
(
const
char
*
ptr
)
{
return
ptr
;
}
static
inline
uint32
GetUint32AtOffset
(
const
char
*
v
int
offset
)
{
assert
(
offset
>
=
0
)
;
assert
(
offset
<
=
4
)
;
return
UNALIGNED_LOAD32
(
v
+
offset
)
;
}
#
endif
namespace
internal
{
char
*
CompressFragment
(
const
char
*
input
size_t
input_size
char
*
op
uint16
*
table
const
int
table_size
)
{
const
char
*
ip
=
input
;
assert
(
input_size
<
=
kBlockSize
)
;
assert
(
(
table_size
&
(
table_size
-
1
)
)
=
=
0
)
;
const
int
shift
=
32
-
Bits
:
:
Log2Floor
(
table_size
)
;
assert
(
static_cast
<
int
>
(
kuint32max
>
>
shift
)
=
=
table_size
-
1
)
;
const
char
*
ip_end
=
input
+
input_size
;
const
char
*
base_ip
=
ip
;
const
char
*
next_emit
=
ip
;
const
size_t
kInputMarginBytes
=
15
;
if
(
SNAPPY_PREDICT_TRUE
(
input_size
>
=
kInputMarginBytes
)
)
{
const
char
*
ip_limit
=
input
+
input_size
-
kInputMarginBytes
;
for
(
uint32
next_hash
=
Hash
(
+
+
ip
shift
)
;
;
)
{
assert
(
next_emit
<
ip
)
;
uint32
skip
=
32
;
const
char
*
next_ip
=
ip
;
const
char
*
candidate
;
do
{
ip
=
next_ip
;
uint32
hash
=
next_hash
;
assert
(
hash
=
=
Hash
(
ip
shift
)
)
;
uint32
bytes_between_hash_lookups
=
skip
>
>
5
;
skip
+
=
bytes_between_hash_lookups
;
next_ip
=
ip
+
bytes_between_hash_lookups
;
if
(
SNAPPY_PREDICT_FALSE
(
next_ip
>
ip_limit
)
)
{
goto
emit_remainder
;
}
next_hash
=
Hash
(
next_ip
shift
)
;
candidate
=
base_ip
+
table
[
hash
]
;
assert
(
candidate
>
=
base_ip
)
;
assert
(
candidate
<
ip
)
;
table
[
hash
]
=
ip
-
base_ip
;
}
while
(
SNAPPY_PREDICT_TRUE
(
UNALIGNED_LOAD32
(
ip
)
!
=
UNALIGNED_LOAD32
(
candidate
)
)
)
;
assert
(
next_emit
+
16
<
=
ip_end
)
;
op
=
EmitLiteral
<
true
>
(
op
next_emit
ip
-
next_emit
)
;
EightBytesReference
input_bytes
;
uint32
candidate_bytes
=
0
;
do
{
const
char
*
base
=
ip
;
std
:
:
pair
<
size_t
bool
>
p
=
FindMatchLength
(
candidate
+
4
ip
+
4
ip_end
)
;
size_t
matched
=
4
+
p
.
first
;
ip
+
=
matched
;
size_t
offset
=
base
-
candidate
;
assert
(
0
=
=
memcmp
(
base
candidate
matched
)
)
;
if
(
p
.
second
)
{
op
=
EmitCopy
<
true
>
(
op
offset
matched
)
;
}
else
{
op
=
EmitCopy
<
false
>
(
op
offset
matched
)
;
}
next_emit
=
ip
;
if
(
SNAPPY_PREDICT_FALSE
(
ip
>
=
ip_limit
)
)
{
goto
emit_remainder
;
}
input_bytes
=
GetEightBytesAt
(
ip
-
1
)
;
uint32
prev_hash
=
HashBytes
(
GetUint32AtOffset
(
input_bytes
0
)
shift
)
;
table
[
prev_hash
]
=
ip
-
base_ip
-
1
;
uint32
cur_hash
=
HashBytes
(
GetUint32AtOffset
(
input_bytes
1
)
shift
)
;
candidate
=
base_ip
+
table
[
cur_hash
]
;
candidate_bytes
=
UNALIGNED_LOAD32
(
candidate
)
;
table
[
cur_hash
]
=
ip
-
base_ip
;
}
while
(
GetUint32AtOffset
(
input_bytes
1
)
=
=
candidate_bytes
)
;
next_hash
=
HashBytes
(
GetUint32AtOffset
(
input_bytes
2
)
shift
)
;
+
+
ip
;
}
}
emit_remainder
:
if
(
next_emit
<
ip_end
)
{
op
=
EmitLiteral
<
false
>
(
op
next_emit
ip_end
-
next_emit
)
;
}
return
op
;
}
}
static
inline
void
Report
(
const
char
*
algorithm
size_t
compressed_size
size_t
uncompressed_size
)
{
}
static
inline
uint32
ExtractLowBytes
(
uint32
v
int
n
)
{
assert
(
n
>
=
0
)
;
assert
(
n
<
=
4
)
;
#
if
SNAPPY_HAVE_BMI2
return
_bzhi_u32
(
v
8
*
n
)
;
#
else
uint64
mask
=
0xffffffff
;
return
v
&
~
(
mask
<
<
(
8
*
n
)
)
;
#
endif
}
static
inline
bool
LeftShiftOverflows
(
uint8
value
uint32
shift
)
{
assert
(
shift
<
32
)
;
static
const
uint8
masks
[
]
=
{
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x80
0xc0
0xe0
0xf0
0xf8
0xfc
0xfe
}
;
return
(
value
&
masks
[
shift
]
)
!
=
0
;
}
class
SnappyDecompressor
{
private
:
Source
*
reader_
;
const
char
*
ip_
;
const
char
*
ip_limit_
;
uint32
peeked_
;
bool
eof_
;
char
scratch_
[
kMaximumTagLength
]
;
bool
RefillTag
(
)
;
public
:
explicit
SnappyDecompressor
(
Source
*
reader
)
:
reader_
(
reader
)
ip_
(
NULL
)
ip_limit_
(
NULL
)
peeked_
(
0
)
eof_
(
false
)
{
}
~
SnappyDecompressor
(
)
{
reader_
-
>
Skip
(
peeked_
)
;
}
bool
eof
(
)
const
{
return
eof_
;
}
bool
ReadUncompressedLength
(
uint32
*
result
)
{
assert
(
ip_
=
=
NULL
)
;
*
result
=
0
;
uint32
shift
=
0
;
while
(
true
)
{
if
(
shift
>
=
32
)
return
false
;
size_t
n
;
const
char
*
ip
=
reader_
-
>
Peek
(
&
n
)
;
if
(
n
=
=
0
)
return
false
;
const
unsigned
char
c
=
*
(
reinterpret_cast
<
const
unsigned
char
*
>
(
ip
)
)
;
reader_
-
>
Skip
(
1
)
;
uint32
val
=
c
&
0x7f
;
if
(
LeftShiftOverflows
(
static_cast
<
uint8
>
(
val
)
shift
)
)
return
false
;
*
result
|
=
val
<
<
shift
;
if
(
c
<
128
)
{
break
;
}
shift
+
=
7
;
}
return
true
;
}
template
<
class
Writer
>
#
if
defined
(
__GNUC__
)
&
&
defined
(
__x86_64__
)
__attribute__
(
(
aligned
(
32
)
)
)
#
endif
void
DecompressAllTags
(
Writer
*
writer
)
{
#
if
defined
(
__GNUC__
)
&
&
defined
(
__x86_64__
)
asm
(
"
.
byte
0x0f
0x1f
0x84
0x00
0x00
0x00
0x00
0x00
"
)
;
asm
(
"
.
byte
0x0f
0x1f
0x84
0x00
0x00
0x00
0x00
0x00
"
)
;
#
endif
const
char
*
ip
=
ip_
;
#
define
MAYBE_REFILL
(
)
\
if
(
ip_limit_
-
ip
<
kMaximumTagLength
)
{
\
ip_
=
ip
;
\
if
(
!
RefillTag
(
)
)
return
;
\
ip
=
ip_
;
\
}
MAYBE_REFILL
(
)
;
for
(
;
;
)
{
const
unsigned
char
c
=
*
(
reinterpret_cast
<
const
unsigned
char
*
>
(
ip
+
+
)
)
;
if
(
SNAPPY_PREDICT_FALSE
(
(
c
&
0x3
)
=
=
LITERAL
)
)
{
size_t
literal_length
=
(
c
>
>
2
)
+
1u
;
if
(
writer
-
>
TryFastAppend
(
ip
ip_limit_
-
ip
literal_length
)
)
{
assert
(
literal_length
<
61
)
;
ip
+
=
literal_length
;
continue
;
}
if
(
SNAPPY_PREDICT_FALSE
(
literal_length
>
=
61
)
)
{
const
size_t
literal_length_length
=
literal_length
-
60
;
literal_length
=
ExtractLowBytes
(
LittleEndian
:
:
Load32
(
ip
)
literal_length_length
)
+
1
;
ip
+
=
literal_length_length
;
}
size_t
avail
=
ip_limit_
-
ip
;
while
(
avail
<
literal_length
)
{
if
(
!
writer
-
>
Append
(
ip
avail
)
)
return
;
literal_length
-
=
avail
;
reader_
-
>
Skip
(
peeked_
)
;
size_t
n
;
ip
=
reader_
-
>
Peek
(
&
n
)
;
avail
=
n
;
peeked_
=
avail
;
if
(
avail
=
=
0
)
return
;
ip_limit_
=
ip
+
avail
;
}
if
(
!
writer
-
>
Append
(
ip
literal_length
)
)
{
return
;
}
ip
+
=
literal_length
;
MAYBE_REFILL
(
)
;
}
else
{
const
size_t
entry
=
char_table
[
c
]
;
const
size_t
trailer
=
ExtractLowBytes
(
LittleEndian
:
:
Load32
(
ip
)
entry
>
>
11
)
;
const
size_t
length
=
entry
&
0xff
;
ip
+
=
entry
>
>
11
;
const
size_t
copy_offset
=
entry
&
0x700
;
if
(
!
writer
-
>
AppendFromSelf
(
copy_offset
+
trailer
length
)
)
{
return
;
}
MAYBE_REFILL
(
)
;
}
}
#
undef
MAYBE_REFILL
}
}
;
bool
SnappyDecompressor
:
:
RefillTag
(
)
{
const
char
*
ip
=
ip_
;
if
(
ip
=
=
ip_limit_
)
{
reader_
-
>
Skip
(
peeked_
)
;
size_t
n
;
ip
=
reader_
-
>
Peek
(
&
n
)
;
peeked_
=
n
;
eof_
=
(
n
=
=
0
)
;
if
(
eof_
)
return
false
;
ip_limit_
=
ip
+
n
;
}
assert
(
ip
<
ip_limit_
)
;
const
unsigned
char
c
=
*
(
reinterpret_cast
<
const
unsigned
char
*
>
(
ip
)
)
;
const
uint32
entry
=
char_table
[
c
]
;
const
uint32
needed
=
(
entry
>
>
11
)
+
1
;
assert
(
needed
<
=
sizeof
(
scratch_
)
)
;
uint32
nbuf
=
ip_limit_
-
ip
;
if
(
nbuf
<
needed
)
{
memmove
(
scratch_
ip
nbuf
)
;
reader_
-
>
Skip
(
peeked_
)
;
peeked_
=
0
;
while
(
nbuf
<
needed
)
{
size_t
length
;
const
char
*
src
=
reader_
-
>
Peek
(
&
length
)
;
if
(
length
=
=
0
)
return
false
;
uint32
to_add
=
std
:
:
min
<
uint32
>
(
needed
-
nbuf
length
)
;
memcpy
(
scratch_
+
nbuf
src
to_add
)
;
nbuf
+
=
to_add
;
reader_
-
>
Skip
(
to_add
)
;
}
assert
(
nbuf
=
=
needed
)
;
ip_
=
scratch_
;
ip_limit_
=
scratch_
+
needed
;
}
else
if
(
nbuf
<
kMaximumTagLength
)
{
memmove
(
scratch_
ip
nbuf
)
;
reader_
-
>
Skip
(
peeked_
)
;
peeked_
=
0
;
ip_
=
scratch_
;
ip_limit_
=
scratch_
+
nbuf
;
}
else
{
ip_
=
ip
;
}
return
true
;
}
template
<
typename
Writer
>
static
bool
InternalUncompress
(
Source
*
r
Writer
*
writer
)
{
SnappyDecompressor
decompressor
(
r
)
;
uint32
uncompressed_len
=
0
;
if
(
!
decompressor
.
ReadUncompressedLength
(
&
uncompressed_len
)
)
return
false
;
return
InternalUncompressAllTags
(
&
decompressor
writer
r
-
>
Available
(
)
uncompressed_len
)
;
}
template
<
typename
Writer
>
static
bool
InternalUncompressAllTags
(
SnappyDecompressor
*
decompressor
Writer
*
writer
uint32
compressed_len
uint32
uncompressed_len
)
{
Report
(
"
snappy_uncompress
"
compressed_len
uncompressed_len
)
;
writer
-
>
SetExpectedLength
(
uncompressed_len
)
;
decompressor
-
>
DecompressAllTags
(
writer
)
;
writer
-
>
Flush
(
)
;
return
(
decompressor
-
>
eof
(
)
&
&
writer
-
>
CheckLength
(
)
)
;
}
bool
GetUncompressedLength
(
Source
*
source
uint32
*
result
)
{
SnappyDecompressor
decompressor
(
source
)
;
return
decompressor
.
ReadUncompressedLength
(
result
)
;
}
size_t
Compress
(
Source
*
reader
Sink
*
writer
)
{
size_t
written
=
0
;
size_t
N
=
reader
-
>
Available
(
)
;
const
size_t
uncompressed_size
=
N
;
char
ulength
[
Varint
:
:
kMax32
]
;
char
*
p
=
Varint
:
:
Encode32
(
ulength
N
)
;
writer
-
>
Append
(
ulength
p
-
ulength
)
;
written
+
=
(
p
-
ulength
)
;
internal
:
:
WorkingMemory
wmem
(
N
)
;
while
(
N
>
0
)
{
size_t
fragment_size
;
const
char
*
fragment
=
reader
-
>
Peek
(
&
fragment_size
)
;
assert
(
fragment_size
!
=
0
)
;
const
size_t
num_to_read
=
std
:
:
min
(
N
kBlockSize
)
;
size_t
bytes_read
=
fragment_size
;
size_t
pending_advance
=
0
;
if
(
bytes_read
>
=
num_to_read
)
{
pending_advance
=
num_to_read
;
fragment_size
=
num_to_read
;
}
else
{
char
*
scratch
=
wmem
.
GetScratchInput
(
)
;
memcpy
(
scratch
fragment
bytes_read
)
;
reader
-
>
Skip
(
bytes_read
)
;
while
(
bytes_read
<
num_to_read
)
{
fragment
=
reader
-
>
Peek
(
&
fragment_size
)
;
size_t
n
=
std
:
:
min
<
size_t
>
(
fragment_size
num_to_read
-
bytes_read
)
;
memcpy
(
scratch
+
bytes_read
fragment
n
)
;
bytes_read
+
=
n
;
reader
-
>
Skip
(
n
)
;
}
assert
(
bytes_read
=
=
num_to_read
)
;
fragment
=
scratch
;
fragment_size
=
num_to_read
;
}
assert
(
fragment_size
=
=
num_to_read
)
;
int
table_size
;
uint16
*
table
=
wmem
.
GetHashTable
(
num_to_read
&
table_size
)
;
const
int
max_output
=
MaxCompressedLength
(
num_to_read
)
;
char
*
dest
=
writer
-
>
GetAppendBuffer
(
max_output
wmem
.
GetScratchOutput
(
)
)
;
char
*
end
=
internal
:
:
CompressFragment
(
fragment
fragment_size
dest
table
table_size
)
;
writer
-
>
Append
(
dest
end
-
dest
)
;
written
+
=
(
end
-
dest
)
;
N
-
=
num_to_read
;
reader
-
>
Skip
(
pending_advance
)
;
}
Report
(
"
snappy_compress
"
written
uncompressed_size
)
;
return
written
;
}
class
SnappyIOVecWriter
{
private
:
const
struct
iovec
*
output_iov_end_
;
#
if
!
defined
(
NDEBUG
)
const
struct
iovec
*
output_iov_
;
#
endif
const
struct
iovec
*
curr_iov_
;
char
*
curr_iov_output_
;
size_t
curr_iov_remaining_
;
size_t
total_written_
;
size_t
output_limit_
;
static
inline
char
*
GetIOVecPointer
(
const
struct
iovec
*
iov
size_t
offset
)
{
return
reinterpret_cast
<
char
*
>
(
iov
-
>
iov_base
)
+
offset
;
}
public
:
inline
SnappyIOVecWriter
(
const
struct
iovec
*
iov
size_t
iov_count
)
:
output_iov_end_
(
iov
+
iov_count
)
#
if
!
defined
(
NDEBUG
)
output_iov_
(
iov
)
#
endif
curr_iov_
(
iov
)
curr_iov_output_
(
iov_count
?
reinterpret_cast
<
char
*
>
(
iov
-
>
iov_base
)
:
nullptr
)
curr_iov_remaining_
(
iov_count
?
iov
-
>
iov_len
:
0
)
total_written_
(
0
)
output_limit_
(
-
1
)
{
}
inline
void
SetExpectedLength
(
size_t
len
)
{
output_limit_
=
len
;
}
inline
bool
CheckLength
(
)
const
{
return
total_written_
=
=
output_limit_
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
)
{
if
(
total_written_
+
len
>
output_limit_
)
{
return
false
;
}
return
AppendNoCheck
(
ip
len
)
;
}
inline
bool
AppendNoCheck
(
const
char
*
ip
size_t
len
)
{
while
(
len
>
0
)
{
if
(
curr_iov_remaining_
=
=
0
)
{
if
(
curr_iov_
+
1
>
=
output_iov_end_
)
{
return
false
;
}
+
+
curr_iov_
;
curr_iov_output_
=
reinterpret_cast
<
char
*
>
(
curr_iov_
-
>
iov_base
)
;
curr_iov_remaining_
=
curr_iov_
-
>
iov_len
;
}
const
size_t
to_write
=
std
:
:
min
(
len
curr_iov_remaining_
)
;
memcpy
(
curr_iov_output_
ip
to_write
)
;
curr_iov_output_
+
=
to_write
;
curr_iov_remaining_
-
=
to_write
;
total_written_
+
=
to_write
;
ip
+
=
to_write
;
len
-
=
to_write
;
}
return
true
;
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
len
)
{
const
size_t
space_left
=
output_limit_
-
total_written_
;
if
(
len
<
=
16
&
&
available
>
=
16
+
kMaximumTagLength
&
&
space_left
>
=
16
&
&
curr_iov_remaining_
>
=
16
)
{
UnalignedCopy128
(
ip
curr_iov_output_
)
;
curr_iov_output_
+
=
len
;
curr_iov_remaining_
-
=
len
;
total_written_
+
=
len
;
return
true
;
}
return
false
;
}
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
)
{
if
(
offset
-
1u
>
=
total_written_
)
{
return
false
;
}
const
size_t
space_left
=
output_limit_
-
total_written_
;
if
(
len
>
space_left
)
{
return
false
;
}
const
iovec
*
from_iov
=
curr_iov_
;
size_t
from_iov_offset
=
curr_iov_
-
>
iov_len
-
curr_iov_remaining_
;
while
(
offset
>
0
)
{
if
(
from_iov_offset
>
=
offset
)
{
from_iov_offset
-
=
offset
;
break
;
}
offset
-
=
from_iov_offset
;
-
-
from_iov
;
#
if
!
defined
(
NDEBUG
)
assert
(
from_iov
>
=
output_iov_
)
;
#
endif
from_iov_offset
=
from_iov
-
>
iov_len
;
}
while
(
len
>
0
)
{
assert
(
from_iov
<
=
curr_iov_
)
;
if
(
from_iov
!
=
curr_iov_
)
{
const
size_t
to_copy
=
std
:
:
min
(
from_iov
-
>
iov_len
-
from_iov_offset
len
)
;
AppendNoCheck
(
GetIOVecPointer
(
from_iov
from_iov_offset
)
to_copy
)
;
len
-
=
to_copy
;
if
(
len
>
0
)
{
+
+
from_iov
;
from_iov_offset
=
0
;
}
}
else
{
size_t
to_copy
=
curr_iov_remaining_
;
if
(
to_copy
=
=
0
)
{
if
(
curr_iov_
+
1
>
=
output_iov_end_
)
{
return
false
;
}
+
+
curr_iov_
;
curr_iov_output_
=
reinterpret_cast
<
char
*
>
(
curr_iov_
-
>
iov_base
)
;
curr_iov_remaining_
=
curr_iov_
-
>
iov_len
;
continue
;
}
if
(
to_copy
>
len
)
{
to_copy
=
len
;
}
IncrementalCopy
(
GetIOVecPointer
(
from_iov
from_iov_offset
)
curr_iov_output_
curr_iov_output_
+
to_copy
curr_iov_output_
+
curr_iov_remaining_
)
;
curr_iov_output_
+
=
to_copy
;
curr_iov_remaining_
-
=
to_copy
;
from_iov_offset
+
=
to_copy
;
total_written_
+
=
to_copy
;
len
-
=
to_copy
;
}
}
return
true
;
}
inline
void
Flush
(
)
{
}
}
;
bool
RawUncompressToIOVec
(
const
char
*
compressed
size_t
compressed_length
const
struct
iovec
*
iov
size_t
iov_cnt
)
{
ByteArraySource
reader
(
compressed
compressed_length
)
;
return
RawUncompressToIOVec
(
&
reader
iov
iov_cnt
)
;
}
bool
RawUncompressToIOVec
(
Source
*
compressed
const
struct
iovec
*
iov
size_t
iov_cnt
)
{
SnappyIOVecWriter
output
(
iov
iov_cnt
)
;
return
InternalUncompress
(
compressed
&
output
)
;
}
class
SnappyArrayWriter
{
private
:
char
*
base_
;
char
*
op_
;
char
*
op_limit_
;
public
:
inline
explicit
SnappyArrayWriter
(
char
*
dst
)
:
base_
(
dst
)
op_
(
dst
)
op_limit_
(
dst
)
{
}
inline
void
SetExpectedLength
(
size_t
len
)
{
op_limit_
=
op_
+
len
;
}
inline
bool
CheckLength
(
)
const
{
return
op_
=
=
op_limit_
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
)
{
char
*
op
=
op_
;
const
size_t
space_left
=
op_limit_
-
op
;
if
(
space_left
<
len
)
{
return
false
;
}
memcpy
(
op
ip
len
)
;
op_
=
op
+
len
;
return
true
;
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
len
)
{
char
*
op
=
op_
;
const
size_t
space_left
=
op_limit_
-
op
;
if
(
len
<
=
16
&
&
available
>
=
16
+
kMaximumTagLength
&
&
space_left
>
=
16
)
{
UnalignedCopy128
(
ip
op
)
;
op_
=
op
+
len
;
return
true
;
}
else
{
return
false
;
}
}
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
)
{
char
*
const
op_end
=
op_
+
len
;
if
(
Produced
(
)
<
=
offset
-
1u
|
|
op_end
>
op_limit_
)
return
false
;
op_
=
IncrementalCopy
(
op_
-
offset
op_
op_end
op_limit_
)
;
return
true
;
}
inline
size_t
Produced
(
)
const
{
assert
(
op_
>
=
base_
)
;
return
op_
-
base_
;
}
inline
void
Flush
(
)
{
}
}
;
bool
RawUncompress
(
const
char
*
compressed
size_t
n
char
*
uncompressed
)
{
ByteArraySource
reader
(
compressed
n
)
;
return
RawUncompress
(
&
reader
uncompressed
)
;
}
bool
RawUncompress
(
Source
*
compressed
char
*
uncompressed
)
{
SnappyArrayWriter
output
(
uncompressed
)
;
return
InternalUncompress
(
compressed
&
output
)
;
}
bool
Uncompress
(
const
char
*
compressed
size_t
n
std
:
:
string
*
uncompressed
)
{
size_t
ulength
;
if
(
!
GetUncompressedLength
(
compressed
n
&
ulength
)
)
{
return
false
;
}
if
(
ulength
>
uncompressed
-
>
max_size
(
)
)
{
return
false
;
}
STLStringResizeUninitialized
(
uncompressed
ulength
)
;
return
RawUncompress
(
compressed
n
string_as_array
(
uncompressed
)
)
;
}
class
SnappyDecompressionValidator
{
private
:
size_t
expected_
;
size_t
produced_
;
public
:
inline
SnappyDecompressionValidator
(
)
:
expected_
(
0
)
produced_
(
0
)
{
}
inline
void
SetExpectedLength
(
size_t
len
)
{
expected_
=
len
;
}
inline
bool
CheckLength
(
)
const
{
return
expected_
=
=
produced_
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
)
{
produced_
+
=
len
;
return
produced_
<
=
expected_
;
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
length
)
{
return
false
;
}
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
)
{
if
(
produced_
<
=
offset
-
1u
)
return
false
;
produced_
+
=
len
;
return
produced_
<
=
expected_
;
}
inline
void
Flush
(
)
{
}
}
;
bool
IsValidCompressedBuffer
(
const
char
*
compressed
size_t
n
)
{
ByteArraySource
reader
(
compressed
n
)
;
SnappyDecompressionValidator
writer
;
return
InternalUncompress
(
&
reader
&
writer
)
;
}
bool
IsValidCompressed
(
Source
*
compressed
)
{
SnappyDecompressionValidator
writer
;
return
InternalUncompress
(
compressed
&
writer
)
;
}
void
RawCompress
(
const
char
*
input
size_t
input_length
char
*
compressed
size_t
*
compressed_length
)
{
ByteArraySource
reader
(
input
input_length
)
;
UncheckedByteArraySink
writer
(
compressed
)
;
Compress
(
&
reader
&
writer
)
;
*
compressed_length
=
(
writer
.
CurrentDestination
(
)
-
compressed
)
;
}
size_t
Compress
(
const
char
*
input
size_t
input_length
std
:
:
string
*
compressed
)
{
STLStringResizeUninitialized
(
compressed
MaxCompressedLength
(
input_length
)
)
;
size_t
compressed_length
;
RawCompress
(
input
input_length
string_as_array
(
compressed
)
&
compressed_length
)
;
compressed
-
>
resize
(
compressed_length
)
;
return
compressed_length
;
}
template
<
typename
Allocator
>
class
SnappyScatteredWriter
{
Allocator
allocator_
;
std
:
:
vector
<
char
*
>
blocks_
;
size_t
expected_
;
size_t
full_size_
;
char
*
op_base_
;
char
*
op_ptr_
;
char
*
op_limit_
;
inline
size_t
Size
(
)
const
{
return
full_size_
+
(
op_ptr_
-
op_base_
)
;
}
bool
SlowAppend
(
const
char
*
ip
size_t
len
)
;
bool
SlowAppendFromSelf
(
size_t
offset
size_t
len
)
;
public
:
inline
explicit
SnappyScatteredWriter
(
const
Allocator
&
allocator
)
:
allocator_
(
allocator
)
full_size_
(
0
)
op_base_
(
NULL
)
op_ptr_
(
NULL
)
op_limit_
(
NULL
)
{
}
inline
void
SetExpectedLength
(
size_t
len
)
{
assert
(
blocks_
.
empty
(
)
)
;
expected_
=
len
;
}
inline
bool
CheckLength
(
)
const
{
return
Size
(
)
=
=
expected_
;
}
inline
size_t
Produced
(
)
const
{
return
Size
(
)
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
)
{
size_t
avail
=
op_limit_
-
op_ptr_
;
if
(
len
<
=
avail
)
{
memcpy
(
op_ptr_
ip
len
)
;
op_ptr_
+
=
len
;
return
true
;
}
else
{
return
SlowAppend
(
ip
len
)
;
}
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
length
)
{
char
*
op
=
op_ptr_
;
const
int
space_left
=
op_limit_
-
op
;
if
(
length
<
=
16
&
&
available
>
=
16
+
kMaximumTagLength
&
&
space_left
>
=
16
)
{
UnalignedCopy128
(
ip
op
)
;
op_ptr_
=
op
+
length
;
return
true
;
}
else
{
return
false
;
}
}
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
)
{
char
*
const
op_end
=
op_ptr_
+
len
;
if
(
SNAPPY_PREDICT_TRUE
(
offset
-
1u
<
op_ptr_
-
op_base_
&
&
op_end
<
=
op_limit_
)
)
{
op_ptr_
=
IncrementalCopy
(
op_ptr_
-
offset
op_ptr_
op_end
op_limit_
)
;
return
true
;
}
return
SlowAppendFromSelf
(
offset
len
)
;
}
inline
void
Flush
(
)
{
allocator_
.
Flush
(
Produced
(
)
)
;
}
}
;
template
<
typename
Allocator
>
bool
SnappyScatteredWriter
<
Allocator
>
:
:
SlowAppend
(
const
char
*
ip
size_t
len
)
{
size_t
avail
=
op_limit_
-
op_ptr_
;
while
(
len
>
avail
)
{
memcpy
(
op_ptr_
ip
avail
)
;
op_ptr_
+
=
avail
;
assert
(
op_limit_
-
op_ptr_
=
=
0
)
;
full_size_
+
=
(
op_ptr_
-
op_base_
)
;
len
-
=
avail
;
ip
+
=
avail
;
if
(
full_size_
+
len
>
expected_
)
{
return
false
;
}
size_t
bsize
=
std
:
:
min
<
size_t
>
(
kBlockSize
expected_
-
full_size_
)
;
op_base_
=
allocator_
.
Allocate
(
bsize
)
;
op_ptr_
=
op_base_
;
op_limit_
=
op_base_
+
bsize
;
blocks_
.
push_back
(
op_base_
)
;
avail
=
bsize
;
}
memcpy
(
op_ptr_
ip
len
)
;
op_ptr_
+
=
len
;
return
true
;
}
template
<
typename
Allocator
>
bool
SnappyScatteredWriter
<
Allocator
>
:
:
SlowAppendFromSelf
(
size_t
offset
size_t
len
)
{
const
size_t
cur
=
Size
(
)
;
if
(
offset
-
1u
>
=
cur
)
return
false
;
if
(
expected_
-
cur
<
len
)
return
false
;
size_t
src
=
cur
-
offset
;
while
(
len
-
-
>
0
)
{
char
c
=
blocks_
[
src
>
>
kBlockLog
]
[
src
&
(
kBlockSize
-
1
)
]
;
Append
(
&
c
1
)
;
src
+
+
;
}
return
true
;
}
class
SnappySinkAllocator
{
public
:
explicit
SnappySinkAllocator
(
Sink
*
dest
)
:
dest_
(
dest
)
{
}
~
SnappySinkAllocator
(
)
{
}
char
*
Allocate
(
int
size
)
{
Datablock
block
(
new
char
[
size
]
size
)
;
blocks_
.
push_back
(
block
)
;
return
block
.
data
;
}
void
Flush
(
size_t
size
)
{
size_t
size_written
=
0
;
size_t
block_size
;
for
(
int
i
=
0
;
i
<
blocks_
.
size
(
)
;
+
+
i
)
{
block_size
=
std
:
:
min
<
size_t
>
(
blocks_
[
i
]
.
size
size
-
size_written
)
;
dest_
-
>
AppendAndTakeOwnership
(
blocks_
[
i
]
.
data
block_size
&
SnappySinkAllocator
:
:
Deleter
NULL
)
;
size_written
+
=
block_size
;
}
blocks_
.
clear
(
)
;
}
private
:
struct
Datablock
{
char
*
data
;
size_t
size
;
Datablock
(
char
*
p
size_t
s
)
:
data
(
p
)
size
(
s
)
{
}
}
;
static
void
Deleter
(
void
*
arg
const
char
*
bytes
size_t
size
)
{
delete
[
]
bytes
;
}
Sink
*
dest_
;
std
:
:
vector
<
Datablock
>
blocks_
;
}
;
size_t
UncompressAsMuchAsPossible
(
Source
*
compressed
Sink
*
uncompressed
)
{
SnappySinkAllocator
allocator
(
uncompressed
)
;
SnappyScatteredWriter
<
SnappySinkAllocator
>
writer
(
allocator
)
;
InternalUncompress
(
compressed
&
writer
)
;
return
writer
.
Produced
(
)
;
}
bool
Uncompress
(
Source
*
compressed
Sink
*
uncompressed
)
{
SnappyDecompressor
decompressor
(
compressed
)
;
uint32
uncompressed_len
=
0
;
if
(
!
decompressor
.
ReadUncompressedLength
(
&
uncompressed_len
)
)
{
return
false
;
}
char
c
;
size_t
allocated_size
;
char
*
buf
=
uncompressed
-
>
GetAppendBufferVariable
(
1
uncompressed_len
&
c
1
&
allocated_size
)
;
const
size_t
compressed_len
=
compressed
-
>
Available
(
)
;
if
(
allocated_size
>
=
uncompressed_len
)
{
SnappyArrayWriter
writer
(
buf
)
;
bool
result
=
InternalUncompressAllTags
(
&
decompressor
&
writer
compressed_len
uncompressed_len
)
;
uncompressed
-
>
Append
(
buf
writer
.
Produced
(
)
)
;
return
result
;
}
else
{
SnappySinkAllocator
allocator
(
uncompressed
)
;
SnappyScatteredWriter
<
SnappySinkAllocator
>
writer
(
allocator
)
;
return
InternalUncompressAllTags
(
&
decompressor
&
writer
compressed_len
uncompressed_len
)
;
}
}
}
