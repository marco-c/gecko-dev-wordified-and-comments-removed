#
include
"
snappy
.
h
"
#
include
"
snappy
-
internal
.
h
"
#
include
"
snappy
-
sinksource
.
h
"
#
include
<
stdio
.
h
>
#
include
<
algorithm
>
#
include
<
string
>
#
include
<
vector
>
namespace
snappy
{
using
internal
:
:
COPY_1_BYTE_OFFSET
;
using
internal
:
:
COPY_2_BYTE_OFFSET
;
using
internal
:
:
COPY_4_BYTE_OFFSET
;
using
internal
:
:
LITERAL
;
using
internal
:
:
char_table
;
using
internal
:
:
kMaximumTagLength
;
using
internal
:
:
wordmask
;
static
inline
uint32
HashBytes
(
uint32
bytes
int
shift
)
{
uint32
kMul
=
0x1e35a7bd
;
return
(
bytes
*
kMul
)
>
>
shift
;
}
static
inline
uint32
Hash
(
const
char
*
p
int
shift
)
{
return
HashBytes
(
UNALIGNED_LOAD32
(
p
)
shift
)
;
}
size_t
MaxCompressedLength
(
size_t
source_len
)
{
return
32
+
source_len
+
source_len
/
6
;
}
static
inline
void
IncrementalCopy
(
const
char
*
src
char
*
op
ssize_t
len
)
{
assert
(
len
>
0
)
;
do
{
*
op
+
+
=
*
src
+
+
;
}
while
(
-
-
len
>
0
)
;
}
namespace
{
const
int
kMaxIncrementCopyOverflow
=
10
;
inline
void
IncrementalCopyFastPath
(
const
char
*
src
char
*
op
ssize_t
len
)
{
while
(
PREDICT_FALSE
(
op
-
src
<
8
)
)
{
UnalignedCopy64
(
src
op
)
;
len
-
=
op
-
src
;
op
+
=
op
-
src
;
}
while
(
len
>
0
)
{
UnalignedCopy64
(
src
op
)
;
src
+
=
8
;
op
+
=
8
;
len
-
=
8
;
}
}
}
static
inline
char
*
EmitLiteral
(
char
*
op
const
char
*
literal
int
len
bool
allow_fast_path
)
{
int
n
=
len
-
1
;
if
(
n
<
60
)
{
*
op
+
+
=
LITERAL
|
(
n
<
<
2
)
;
if
(
allow_fast_path
&
&
len
<
=
16
)
{
UnalignedCopy64
(
literal
op
)
;
UnalignedCopy64
(
literal
+
8
op
+
8
)
;
return
op
+
len
;
}
}
else
{
char
*
base
=
op
;
int
count
=
0
;
op
+
+
;
while
(
n
>
0
)
{
*
op
+
+
=
n
&
0xff
;
n
>
>
=
8
;
count
+
+
;
}
assert
(
count
>
=
1
)
;
assert
(
count
<
=
4
)
;
*
base
=
LITERAL
|
(
(
59
+
count
)
<
<
2
)
;
}
memcpy
(
op
literal
len
)
;
return
op
+
len
;
}
static
inline
char
*
EmitCopyLessThan64
(
char
*
op
size_t
offset
int
len
)
{
assert
(
len
<
=
64
)
;
assert
(
len
>
=
4
)
;
assert
(
offset
<
65536
)
;
if
(
(
len
<
12
)
&
&
(
offset
<
2048
)
)
{
size_t
len_minus_4
=
len
-
4
;
assert
(
len_minus_4
<
8
)
;
*
op
+
+
=
COPY_1_BYTE_OFFSET
+
(
(
len_minus_4
)
<
<
2
)
+
(
(
offset
>
>
8
)
<
<
5
)
;
*
op
+
+
=
offset
&
0xff
;
}
else
{
*
op
+
+
=
COPY_2_BYTE_OFFSET
+
(
(
len
-
1
)
<
<
2
)
;
LittleEndian
:
:
Store16
(
op
offset
)
;
op
+
=
2
;
}
return
op
;
}
static
inline
char
*
EmitCopy
(
char
*
op
size_t
offset
int
len
)
{
while
(
PREDICT_FALSE
(
len
>
=
68
)
)
{
op
=
EmitCopyLessThan64
(
op
offset
64
)
;
len
-
=
64
;
}
if
(
len
>
64
)
{
op
=
EmitCopyLessThan64
(
op
offset
60
)
;
len
-
=
60
;
}
op
=
EmitCopyLessThan64
(
op
offset
len
)
;
return
op
;
}
bool
GetUncompressedLength
(
const
char
*
start
size_t
n
size_t
*
result
)
{
uint32
v
=
0
;
const
char
*
limit
=
start
+
n
;
if
(
Varint
:
:
Parse32WithLimit
(
start
limit
&
v
)
!
=
NULL
)
{
*
result
=
v
;
return
true
;
}
else
{
return
false
;
}
}
namespace
internal
{
uint16
*
WorkingMemory
:
:
GetHashTable
(
size_t
input_size
int
*
table_size
)
{
assert
(
kMaxHashTableSize
>
=
256
)
;
size_t
htsize
=
256
;
while
(
htsize
<
kMaxHashTableSize
&
&
htsize
<
input_size
)
{
htsize
<
<
=
1
;
}
uint16
*
table
;
if
(
htsize
<
=
ARRAYSIZE
(
small_table_
)
)
{
table
=
small_table_
;
}
else
{
if
(
large_table_
=
=
NULL
)
{
large_table_
=
new
uint16
[
kMaxHashTableSize
]
;
}
table
=
large_table_
;
}
*
table_size
=
htsize
;
memset
(
table
0
htsize
*
sizeof
(
*
table
)
)
;
return
table
;
}
}
#
ifdef
ARCH_K8
typedef
uint64
EightBytesReference
;
static
inline
EightBytesReference
GetEightBytesAt
(
const
char
*
ptr
)
{
return
UNALIGNED_LOAD64
(
ptr
)
;
}
static
inline
uint32
GetUint32AtOffset
(
uint64
v
int
offset
)
{
assert
(
offset
>
=
0
)
;
assert
(
offset
<
=
4
)
;
return
v
>
>
(
LittleEndian
:
:
IsLittleEndian
(
)
?
8
*
offset
:
32
-
8
*
offset
)
;
}
#
else
typedef
const
char
*
EightBytesReference
;
static
inline
EightBytesReference
GetEightBytesAt
(
const
char
*
ptr
)
{
return
ptr
;
}
static
inline
uint32
GetUint32AtOffset
(
const
char
*
v
int
offset
)
{
assert
(
offset
>
=
0
)
;
assert
(
offset
<
=
4
)
;
return
UNALIGNED_LOAD32
(
v
+
offset
)
;
}
#
endif
namespace
internal
{
char
*
CompressFragment
(
const
char
*
input
size_t
input_size
char
*
op
uint16
*
table
const
int
table_size
)
{
const
char
*
ip
=
input
;
assert
(
input_size
<
=
kBlockSize
)
;
assert
(
(
table_size
&
(
table_size
-
1
)
)
=
=
0
)
;
const
int
shift
=
32
-
Bits
:
:
Log2Floor
(
table_size
)
;
assert
(
static_cast
<
int
>
(
kuint32max
>
>
shift
)
=
=
table_size
-
1
)
;
const
char
*
ip_end
=
input
+
input_size
;
const
char
*
base_ip
=
ip
;
const
char
*
next_emit
=
ip
;
const
size_t
kInputMarginBytes
=
15
;
if
(
PREDICT_TRUE
(
input_size
>
=
kInputMarginBytes
)
)
{
const
char
*
ip_limit
=
input
+
input_size
-
kInputMarginBytes
;
for
(
uint32
next_hash
=
Hash
(
+
+
ip
shift
)
;
;
)
{
assert
(
next_emit
<
ip
)
;
uint32
skip
=
32
;
const
char
*
next_ip
=
ip
;
const
char
*
candidate
;
do
{
ip
=
next_ip
;
uint32
hash
=
next_hash
;
assert
(
hash
=
=
Hash
(
ip
shift
)
)
;
uint32
bytes_between_hash_lookups
=
skip
>
>
5
;
skip
+
=
bytes_between_hash_lookups
;
next_ip
=
ip
+
bytes_between_hash_lookups
;
if
(
PREDICT_FALSE
(
next_ip
>
ip_limit
)
)
{
goto
emit_remainder
;
}
next_hash
=
Hash
(
next_ip
shift
)
;
candidate
=
base_ip
+
table
[
hash
]
;
assert
(
candidate
>
=
base_ip
)
;
assert
(
candidate
<
ip
)
;
table
[
hash
]
=
ip
-
base_ip
;
}
while
(
PREDICT_TRUE
(
UNALIGNED_LOAD32
(
ip
)
!
=
UNALIGNED_LOAD32
(
candidate
)
)
)
;
assert
(
next_emit
+
16
<
=
ip_end
)
;
op
=
EmitLiteral
(
op
next_emit
ip
-
next_emit
true
)
;
EightBytesReference
input_bytes
;
uint32
candidate_bytes
=
0
;
do
{
const
char
*
base
=
ip
;
int
matched
=
4
+
FindMatchLength
(
candidate
+
4
ip
+
4
ip_end
)
;
ip
+
=
matched
;
size_t
offset
=
base
-
candidate
;
assert
(
0
=
=
memcmp
(
base
candidate
matched
)
)
;
op
=
EmitCopy
(
op
offset
matched
)
;
const
char
*
insert_tail
=
ip
-
1
;
next_emit
=
ip
;
if
(
PREDICT_FALSE
(
ip
>
=
ip_limit
)
)
{
goto
emit_remainder
;
}
input_bytes
=
GetEightBytesAt
(
insert_tail
)
;
uint32
prev_hash
=
HashBytes
(
GetUint32AtOffset
(
input_bytes
0
)
shift
)
;
table
[
prev_hash
]
=
ip
-
base_ip
-
1
;
uint32
cur_hash
=
HashBytes
(
GetUint32AtOffset
(
input_bytes
1
)
shift
)
;
candidate
=
base_ip
+
table
[
cur_hash
]
;
candidate_bytes
=
UNALIGNED_LOAD32
(
candidate
)
;
table
[
cur_hash
]
=
ip
-
base_ip
;
}
while
(
GetUint32AtOffset
(
input_bytes
1
)
=
=
candidate_bytes
)
;
next_hash
=
HashBytes
(
GetUint32AtOffset
(
input_bytes
2
)
shift
)
;
+
+
ip
;
}
}
emit_remainder
:
if
(
next_emit
<
ip_end
)
{
op
=
EmitLiteral
(
op
next_emit
ip_end
-
next_emit
false
)
;
}
return
op
;
}
}
class
SnappyDecompressor
{
private
:
Source
*
reader_
;
const
char
*
ip_
;
const
char
*
ip_limit_
;
uint32
peeked_
;
bool
eof_
;
char
scratch_
[
kMaximumTagLength
]
;
bool
RefillTag
(
)
;
public
:
explicit
SnappyDecompressor
(
Source
*
reader
)
:
reader_
(
reader
)
ip_
(
NULL
)
ip_limit_
(
NULL
)
peeked_
(
0
)
eof_
(
false
)
{
}
~
SnappyDecompressor
(
)
{
reader_
-
>
Skip
(
peeked_
)
;
}
bool
eof
(
)
const
{
return
eof_
;
}
bool
ReadUncompressedLength
(
uint32
*
result
)
{
assert
(
ip_
=
=
NULL
)
;
*
result
=
0
;
uint32
shift
=
0
;
while
(
true
)
{
if
(
shift
>
=
32
)
return
false
;
size_t
n
;
const
char
*
ip
=
reader_
-
>
Peek
(
&
n
)
;
if
(
n
=
=
0
)
return
false
;
const
unsigned
char
c
=
*
(
reinterpret_cast
<
const
unsigned
char
*
>
(
ip
)
)
;
reader_
-
>
Skip
(
1
)
;
uint32
val
=
c
&
0x7f
;
if
(
(
(
val
<
<
shift
)
>
>
shift
)
!
=
val
)
return
false
;
*
result
|
=
val
<
<
shift
;
if
(
c
<
128
)
{
break
;
}
shift
+
=
7
;
}
return
true
;
}
template
<
class
Writer
>
void
DecompressAllTags
(
Writer
*
writer
)
{
const
char
*
ip
=
ip_
;
#
define
MAYBE_REFILL
(
)
\
if
(
ip_limit_
-
ip
<
kMaximumTagLength
)
{
\
ip_
=
ip
;
\
if
(
!
RefillTag
(
)
)
return
;
\
ip
=
ip_
;
\
}
MAYBE_REFILL
(
)
;
for
(
;
;
)
{
const
unsigned
char
c
=
*
(
reinterpret_cast
<
const
unsigned
char
*
>
(
ip
+
+
)
)
;
if
(
(
c
&
0x3
)
=
=
LITERAL
)
{
size_t
literal_length
=
(
c
>
>
2
)
+
1u
;
if
(
writer
-
>
TryFastAppend
(
ip
ip_limit_
-
ip
literal_length
)
)
{
assert
(
literal_length
<
61
)
;
ip
+
=
literal_length
;
continue
;
}
if
(
PREDICT_FALSE
(
literal_length
>
=
61
)
)
{
const
size_t
literal_length_length
=
literal_length
-
60
;
literal_length
=
(
LittleEndian
:
:
Load32
(
ip
)
&
wordmask
[
literal_length_length
]
)
+
1
;
ip
+
=
literal_length_length
;
}
size_t
avail
=
ip_limit_
-
ip
;
while
(
avail
<
literal_length
)
{
if
(
!
writer
-
>
Append
(
ip
avail
)
)
return
;
literal_length
-
=
avail
;
reader_
-
>
Skip
(
peeked_
)
;
size_t
n
;
ip
=
reader_
-
>
Peek
(
&
n
)
;
avail
=
n
;
peeked_
=
avail
;
if
(
avail
=
=
0
)
return
;
ip_limit_
=
ip
+
avail
;
}
if
(
!
writer
-
>
Append
(
ip
literal_length
)
)
{
return
;
}
ip
+
=
literal_length
;
MAYBE_REFILL
(
)
;
}
else
{
const
uint32
entry
=
char_table
[
c
]
;
const
uint32
trailer
=
LittleEndian
:
:
Load32
(
ip
)
&
wordmask
[
entry
>
>
11
]
;
const
uint32
length
=
entry
&
0xff
;
ip
+
=
entry
>
>
11
;
const
uint32
copy_offset
=
entry
&
0x700
;
if
(
!
writer
-
>
AppendFromSelf
(
copy_offset
+
trailer
length
)
)
{
return
;
}
MAYBE_REFILL
(
)
;
}
}
#
undef
MAYBE_REFILL
}
}
;
bool
SnappyDecompressor
:
:
RefillTag
(
)
{
const
char
*
ip
=
ip_
;
if
(
ip
=
=
ip_limit_
)
{
reader_
-
>
Skip
(
peeked_
)
;
size_t
n
;
ip
=
reader_
-
>
Peek
(
&
n
)
;
peeked_
=
n
;
if
(
n
=
=
0
)
{
eof_
=
true
;
return
false
;
}
ip_limit_
=
ip
+
n
;
}
assert
(
ip
<
ip_limit_
)
;
const
unsigned
char
c
=
*
(
reinterpret_cast
<
const
unsigned
char
*
>
(
ip
)
)
;
const
uint32
entry
=
char_table
[
c
]
;
const
uint32
needed
=
(
entry
>
>
11
)
+
1
;
assert
(
needed
<
=
sizeof
(
scratch_
)
)
;
uint32
nbuf
=
ip_limit_
-
ip
;
if
(
nbuf
<
needed
)
{
memmove
(
scratch_
ip
nbuf
)
;
reader_
-
>
Skip
(
peeked_
)
;
peeked_
=
0
;
while
(
nbuf
<
needed
)
{
size_t
length
;
const
char
*
src
=
reader_
-
>
Peek
(
&
length
)
;
if
(
length
=
=
0
)
return
false
;
uint32
to_add
=
min
<
uint32
>
(
needed
-
nbuf
length
)
;
memcpy
(
scratch_
+
nbuf
src
to_add
)
;
nbuf
+
=
to_add
;
reader_
-
>
Skip
(
to_add
)
;
}
assert
(
nbuf
=
=
needed
)
;
ip_
=
scratch_
;
ip_limit_
=
scratch_
+
needed
;
}
else
if
(
nbuf
<
kMaximumTagLength
)
{
memmove
(
scratch_
ip
nbuf
)
;
reader_
-
>
Skip
(
peeked_
)
;
peeked_
=
0
;
ip_
=
scratch_
;
ip_limit_
=
scratch_
+
nbuf
;
}
else
{
ip_
=
ip
;
}
return
true
;
}
template
<
typename
Writer
>
static
bool
InternalUncompress
(
Source
*
r
Writer
*
writer
)
{
SnappyDecompressor
decompressor
(
r
)
;
uint32
uncompressed_len
=
0
;
if
(
!
decompressor
.
ReadUncompressedLength
(
&
uncompressed_len
)
)
return
false
;
return
InternalUncompressAllTags
(
&
decompressor
writer
uncompressed_len
)
;
}
template
<
typename
Writer
>
static
bool
InternalUncompressAllTags
(
SnappyDecompressor
*
decompressor
Writer
*
writer
uint32
uncompressed_len
)
{
writer
-
>
SetExpectedLength
(
uncompressed_len
)
;
decompressor
-
>
DecompressAllTags
(
writer
)
;
writer
-
>
Flush
(
)
;
return
(
decompressor
-
>
eof
(
)
&
&
writer
-
>
CheckLength
(
)
)
;
}
bool
GetUncompressedLength
(
Source
*
source
uint32
*
result
)
{
SnappyDecompressor
decompressor
(
source
)
;
return
decompressor
.
ReadUncompressedLength
(
result
)
;
}
size_t
Compress
(
Source
*
reader
Sink
*
writer
)
{
size_t
written
=
0
;
size_t
N
=
reader
-
>
Available
(
)
;
char
ulength
[
Varint
:
:
kMax32
]
;
char
*
p
=
Varint
:
:
Encode32
(
ulength
N
)
;
writer
-
>
Append
(
ulength
p
-
ulength
)
;
written
+
=
(
p
-
ulength
)
;
internal
:
:
WorkingMemory
wmem
;
char
*
scratch
=
NULL
;
char
*
scratch_output
=
NULL
;
while
(
N
>
0
)
{
size_t
fragment_size
;
const
char
*
fragment
=
reader
-
>
Peek
(
&
fragment_size
)
;
assert
(
fragment_size
!
=
0
)
;
const
size_t
num_to_read
=
min
(
N
kBlockSize
)
;
size_t
bytes_read
=
fragment_size
;
size_t
pending_advance
=
0
;
if
(
bytes_read
>
=
num_to_read
)
{
pending_advance
=
num_to_read
;
fragment_size
=
num_to_read
;
}
else
{
if
(
scratch
=
=
NULL
)
{
scratch
=
new
char
[
num_to_read
]
;
}
memcpy
(
scratch
fragment
bytes_read
)
;
reader
-
>
Skip
(
bytes_read
)
;
while
(
bytes_read
<
num_to_read
)
{
fragment
=
reader
-
>
Peek
(
&
fragment_size
)
;
size_t
n
=
min
<
size_t
>
(
fragment_size
num_to_read
-
bytes_read
)
;
memcpy
(
scratch
+
bytes_read
fragment
n
)
;
bytes_read
+
=
n
;
reader
-
>
Skip
(
n
)
;
}
assert
(
bytes_read
=
=
num_to_read
)
;
fragment
=
scratch
;
fragment_size
=
num_to_read
;
}
assert
(
fragment_size
=
=
num_to_read
)
;
int
table_size
;
uint16
*
table
=
wmem
.
GetHashTable
(
num_to_read
&
table_size
)
;
const
int
max_output
=
MaxCompressedLength
(
num_to_read
)
;
if
(
scratch_output
=
=
NULL
)
{
scratch_output
=
new
char
[
max_output
]
;
}
else
{
}
char
*
dest
=
writer
-
>
GetAppendBuffer
(
max_output
scratch_output
)
;
char
*
end
=
internal
:
:
CompressFragment
(
fragment
fragment_size
dest
table
table_size
)
;
writer
-
>
Append
(
dest
end
-
dest
)
;
written
+
=
(
end
-
dest
)
;
N
-
=
num_to_read
;
reader
-
>
Skip
(
pending_advance
)
;
}
delete
[
]
scratch
;
delete
[
]
scratch_output
;
return
written
;
}
class
SnappyIOVecWriter
{
private
:
const
struct
iovec
*
output_iov_
;
const
size_t
output_iov_count_
;
size_t
curr_iov_index_
;
size_t
curr_iov_written_
;
size_t
total_written_
;
size_t
output_limit_
;
inline
char
*
GetIOVecPointer
(
size_t
index
size_t
offset
)
{
return
reinterpret_cast
<
char
*
>
(
output_iov_
[
index
]
.
iov_base
)
+
offset
;
}
public
:
inline
SnappyIOVecWriter
(
const
struct
iovec
*
iov
size_t
iov_count
)
:
output_iov_
(
iov
)
output_iov_count_
(
iov_count
)
curr_iov_index_
(
0
)
curr_iov_written_
(
0
)
total_written_
(
0
)
output_limit_
(
-
1
)
{
}
inline
void
SetExpectedLength
(
size_t
len
)
{
output_limit_
=
len
;
}
inline
bool
CheckLength
(
)
const
{
return
total_written_
=
=
output_limit_
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
)
{
if
(
total_written_
+
len
>
output_limit_
)
{
return
false
;
}
while
(
len
>
0
)
{
assert
(
curr_iov_written_
<
=
output_iov_
[
curr_iov_index_
]
.
iov_len
)
;
if
(
curr_iov_written_
>
=
output_iov_
[
curr_iov_index_
]
.
iov_len
)
{
if
(
curr_iov_index_
+
1
>
=
output_iov_count_
)
{
return
false
;
}
curr_iov_written_
=
0
;
+
+
curr_iov_index_
;
}
const
size_t
to_write
=
std
:
:
min
(
len
output_iov_
[
curr_iov_index_
]
.
iov_len
-
curr_iov_written_
)
;
memcpy
(
GetIOVecPointer
(
curr_iov_index_
curr_iov_written_
)
ip
to_write
)
;
curr_iov_written_
+
=
to_write
;
total_written_
+
=
to_write
;
ip
+
=
to_write
;
len
-
=
to_write
;
}
return
true
;
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
len
)
{
const
size_t
space_left
=
output_limit_
-
total_written_
;
if
(
len
<
=
16
&
&
available
>
=
16
+
kMaximumTagLength
&
&
space_left
>
=
16
&
&
output_iov_
[
curr_iov_index_
]
.
iov_len
-
curr_iov_written_
>
=
16
)
{
char
*
ptr
=
GetIOVecPointer
(
curr_iov_index_
curr_iov_written_
)
;
UnalignedCopy64
(
ip
ptr
)
;
UnalignedCopy64
(
ip
+
8
ptr
+
8
)
;
curr_iov_written_
+
=
len
;
total_written_
+
=
len
;
return
true
;
}
return
false
;
}
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
)
{
if
(
offset
>
total_written_
|
|
offset
=
=
0
)
{
return
false
;
}
const
size_t
space_left
=
output_limit_
-
total_written_
;
if
(
len
>
space_left
)
{
return
false
;
}
size_t
from_iov_index
=
curr_iov_index_
;
size_t
from_iov_offset
=
curr_iov_written_
;
while
(
offset
>
0
)
{
if
(
from_iov_offset
>
=
offset
)
{
from_iov_offset
-
=
offset
;
break
;
}
offset
-
=
from_iov_offset
;
assert
(
from_iov_index
>
0
)
;
-
-
from_iov_index
;
from_iov_offset
=
output_iov_
[
from_iov_index
]
.
iov_len
;
}
while
(
len
>
0
)
{
assert
(
from_iov_index
<
=
curr_iov_index_
)
;
if
(
from_iov_index
!
=
curr_iov_index_
)
{
const
size_t
to_copy
=
std
:
:
min
(
output_iov_
[
from_iov_index
]
.
iov_len
-
from_iov_offset
len
)
;
Append
(
GetIOVecPointer
(
from_iov_index
from_iov_offset
)
to_copy
)
;
len
-
=
to_copy
;
if
(
len
>
0
)
{
+
+
from_iov_index
;
from_iov_offset
=
0
;
}
}
else
{
assert
(
curr_iov_written_
<
=
output_iov_
[
curr_iov_index_
]
.
iov_len
)
;
size_t
to_copy
=
std
:
:
min
(
output_iov_
[
curr_iov_index_
]
.
iov_len
-
curr_iov_written_
len
)
;
if
(
to_copy
=
=
0
)
{
if
(
curr_iov_index_
+
1
>
=
output_iov_count_
)
{
return
false
;
}
+
+
curr_iov_index_
;
curr_iov_written_
=
0
;
continue
;
}
if
(
to_copy
>
len
)
{
to_copy
=
len
;
}
IncrementalCopy
(
GetIOVecPointer
(
from_iov_index
from_iov_offset
)
GetIOVecPointer
(
curr_iov_index_
curr_iov_written_
)
to_copy
)
;
curr_iov_written_
+
=
to_copy
;
from_iov_offset
+
=
to_copy
;
total_written_
+
=
to_copy
;
len
-
=
to_copy
;
}
}
return
true
;
}
inline
void
Flush
(
)
{
}
}
;
bool
RawUncompressToIOVec
(
const
char
*
compressed
size_t
compressed_length
const
struct
iovec
*
iov
size_t
iov_cnt
)
{
ByteArraySource
reader
(
compressed
compressed_length
)
;
return
RawUncompressToIOVec
(
&
reader
iov
iov_cnt
)
;
}
bool
RawUncompressToIOVec
(
Source
*
compressed
const
struct
iovec
*
iov
size_t
iov_cnt
)
{
SnappyIOVecWriter
output
(
iov
iov_cnt
)
;
return
InternalUncompress
(
compressed
&
output
)
;
}
class
SnappyArrayWriter
{
private
:
char
*
base_
;
char
*
op_
;
char
*
op_limit_
;
public
:
inline
explicit
SnappyArrayWriter
(
char
*
dst
)
:
base_
(
dst
)
op_
(
dst
)
op_limit_
(
dst
)
{
}
inline
void
SetExpectedLength
(
size_t
len
)
{
op_limit_
=
op_
+
len
;
}
inline
bool
CheckLength
(
)
const
{
return
op_
=
=
op_limit_
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
)
{
char
*
op
=
op_
;
const
size_t
space_left
=
op_limit_
-
op
;
if
(
space_left
<
len
)
{
return
false
;
}
memcpy
(
op
ip
len
)
;
op_
=
op
+
len
;
return
true
;
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
len
)
{
char
*
op
=
op_
;
const
size_t
space_left
=
op_limit_
-
op
;
if
(
len
<
=
16
&
&
available
>
=
16
+
kMaximumTagLength
&
&
space_left
>
=
16
)
{
UnalignedCopy64
(
ip
op
)
;
UnalignedCopy64
(
ip
+
8
op
+
8
)
;
op_
=
op
+
len
;
return
true
;
}
else
{
return
false
;
}
}
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
)
{
char
*
op
=
op_
;
const
size_t
space_left
=
op_limit_
-
op
;
assert
(
op
>
=
base_
)
;
size_t
produced
=
op
-
base_
;
if
(
produced
<
=
offset
-
1u
)
{
return
false
;
}
if
(
len
<
=
16
&
&
offset
>
=
8
&
&
space_left
>
=
16
)
{
UnalignedCopy64
(
op
-
offset
op
)
;
UnalignedCopy64
(
op
-
offset
+
8
op
+
8
)
;
}
else
{
if
(
space_left
>
=
len
+
kMaxIncrementCopyOverflow
)
{
IncrementalCopyFastPath
(
op
-
offset
op
len
)
;
}
else
{
if
(
space_left
<
len
)
{
return
false
;
}
IncrementalCopy
(
op
-
offset
op
len
)
;
}
}
op_
=
op
+
len
;
return
true
;
}
inline
size_t
Produced
(
)
const
{
return
op_
-
base_
;
}
inline
void
Flush
(
)
{
}
}
;
bool
RawUncompress
(
const
char
*
compressed
size_t
n
char
*
uncompressed
)
{
ByteArraySource
reader
(
compressed
n
)
;
return
RawUncompress
(
&
reader
uncompressed
)
;
}
bool
RawUncompress
(
Source
*
compressed
char
*
uncompressed
)
{
SnappyArrayWriter
output
(
uncompressed
)
;
return
InternalUncompress
(
compressed
&
output
)
;
}
bool
Uncompress
(
const
char
*
compressed
size_t
n
string
*
uncompressed
)
{
size_t
ulength
;
if
(
!
GetUncompressedLength
(
compressed
n
&
ulength
)
)
{
return
false
;
}
if
(
ulength
>
uncompressed
-
>
max_size
(
)
)
{
return
false
;
}
STLStringResizeUninitialized
(
uncompressed
ulength
)
;
return
RawUncompress
(
compressed
n
string_as_array
(
uncompressed
)
)
;
}
class
SnappyDecompressionValidator
{
private
:
size_t
expected_
;
size_t
produced_
;
public
:
inline
SnappyDecompressionValidator
(
)
:
expected_
(
0
)
produced_
(
0
)
{
}
inline
void
SetExpectedLength
(
size_t
len
)
{
expected_
=
len
;
}
inline
bool
CheckLength
(
)
const
{
return
expected_
=
=
produced_
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
)
{
produced_
+
=
len
;
return
produced_
<
=
expected_
;
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
length
)
{
return
false
;
}
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
)
{
if
(
produced_
<
=
offset
-
1u
)
return
false
;
produced_
+
=
len
;
return
produced_
<
=
expected_
;
}
inline
void
Flush
(
)
{
}
}
;
bool
IsValidCompressedBuffer
(
const
char
*
compressed
size_t
n
)
{
ByteArraySource
reader
(
compressed
n
)
;
SnappyDecompressionValidator
writer
;
return
InternalUncompress
(
&
reader
&
writer
)
;
}
bool
IsValidCompressed
(
Source
*
compressed
)
{
SnappyDecompressionValidator
writer
;
return
InternalUncompress
(
compressed
&
writer
)
;
}
void
RawCompress
(
const
char
*
input
size_t
input_length
char
*
compressed
size_t
*
compressed_length
)
{
ByteArraySource
reader
(
input
input_length
)
;
UncheckedByteArraySink
writer
(
compressed
)
;
Compress
(
&
reader
&
writer
)
;
*
compressed_length
=
(
writer
.
CurrentDestination
(
)
-
compressed
)
;
}
size_t
Compress
(
const
char
*
input
size_t
input_length
string
*
compressed
)
{
compressed
-
>
resize
(
MaxCompressedLength
(
input_length
)
)
;
size_t
compressed_length
;
RawCompress
(
input
input_length
string_as_array
(
compressed
)
&
compressed_length
)
;
compressed
-
>
resize
(
compressed_length
)
;
return
compressed_length
;
}
template
<
typename
Allocator
>
class
SnappyScatteredWriter
{
Allocator
allocator_
;
vector
<
char
*
>
blocks_
;
size_t
expected_
;
size_t
full_size_
;
char
*
op_base_
;
char
*
op_ptr_
;
char
*
op_limit_
;
inline
size_t
Size
(
)
const
{
return
full_size_
+
(
op_ptr_
-
op_base_
)
;
}
bool
SlowAppend
(
const
char
*
ip
size_t
len
)
;
bool
SlowAppendFromSelf
(
size_t
offset
size_t
len
)
;
public
:
inline
explicit
SnappyScatteredWriter
(
const
Allocator
&
allocator
)
:
allocator_
(
allocator
)
full_size_
(
0
)
op_base_
(
NULL
)
op_ptr_
(
NULL
)
op_limit_
(
NULL
)
{
}
inline
void
SetExpectedLength
(
size_t
len
)
{
assert
(
blocks_
.
empty
(
)
)
;
expected_
=
len
;
}
inline
bool
CheckLength
(
)
const
{
return
Size
(
)
=
=
expected_
;
}
inline
size_t
Produced
(
)
const
{
return
Size
(
)
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
)
{
size_t
avail
=
op_limit_
-
op_ptr_
;
if
(
len
<
=
avail
)
{
memcpy
(
op_ptr_
ip
len
)
;
op_ptr_
+
=
len
;
return
true
;
}
else
{
return
SlowAppend
(
ip
len
)
;
}
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
length
)
{
char
*
op
=
op_ptr_
;
const
int
space_left
=
op_limit_
-
op
;
if
(
length
<
=
16
&
&
available
>
=
16
+
kMaximumTagLength
&
&
space_left
>
=
16
)
{
UNALIGNED_STORE64
(
op
UNALIGNED_LOAD64
(
ip
)
)
;
UNALIGNED_STORE64
(
op
+
8
UNALIGNED_LOAD64
(
ip
+
8
)
)
;
op_ptr_
=
op
+
length
;
return
true
;
}
else
{
return
false
;
}
}
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
)
{
if
(
offset
-
1u
<
op_ptr_
-
op_base_
)
{
const
size_t
space_left
=
op_limit_
-
op_ptr_
;
if
(
space_left
>
=
len
+
kMaxIncrementCopyOverflow
)
{
IncrementalCopyFastPath
(
op_ptr_
-
offset
op_ptr_
len
)
;
op_ptr_
+
=
len
;
return
true
;
}
}
return
SlowAppendFromSelf
(
offset
len
)
;
}
inline
void
Flush
(
)
{
allocator_
.
Flush
(
Produced
(
)
)
;
}
}
;
template
<
typename
Allocator
>
bool
SnappyScatteredWriter
<
Allocator
>
:
:
SlowAppend
(
const
char
*
ip
size_t
len
)
{
size_t
avail
=
op_limit_
-
op_ptr_
;
while
(
len
>
avail
)
{
memcpy
(
op_ptr_
ip
avail
)
;
op_ptr_
+
=
avail
;
assert
(
op_limit_
-
op_ptr_
=
=
0
)
;
full_size_
+
=
(
op_ptr_
-
op_base_
)
;
len
-
=
avail
;
ip
+
=
avail
;
if
(
full_size_
+
len
>
expected_
)
{
return
false
;
}
size_t
bsize
=
min
<
size_t
>
(
kBlockSize
expected_
-
full_size_
)
;
op_base_
=
allocator_
.
Allocate
(
bsize
)
;
op_ptr_
=
op_base_
;
op_limit_
=
op_base_
+
bsize
;
blocks_
.
push_back
(
op_base_
)
;
avail
=
bsize
;
}
memcpy
(
op_ptr_
ip
len
)
;
op_ptr_
+
=
len
;
return
true
;
}
template
<
typename
Allocator
>
bool
SnappyScatteredWriter
<
Allocator
>
:
:
SlowAppendFromSelf
(
size_t
offset
size_t
len
)
{
const
size_t
cur
=
Size
(
)
;
if
(
offset
-
1u
>
=
cur
)
return
false
;
if
(
expected_
-
cur
<
len
)
return
false
;
size_t
src
=
cur
-
offset
;
while
(
len
-
-
>
0
)
{
char
c
=
blocks_
[
src
>
>
kBlockLog
]
[
src
&
(
kBlockSize
-
1
)
]
;
Append
(
&
c
1
)
;
src
+
+
;
}
return
true
;
}
class
SnappySinkAllocator
{
public
:
explicit
SnappySinkAllocator
(
Sink
*
dest
)
:
dest_
(
dest
)
{
}
~
SnappySinkAllocator
(
)
{
}
char
*
Allocate
(
int
size
)
{
Datablock
block
(
new
char
[
size
]
size
)
;
blocks_
.
push_back
(
block
)
;
return
block
.
data
;
}
void
Flush
(
size_t
size
)
{
size_t
size_written
=
0
;
size_t
block_size
;
for
(
int
i
=
0
;
i
<
blocks_
.
size
(
)
;
+
+
i
)
{
block_size
=
min
<
size_t
>
(
blocks_
[
i
]
.
size
size
-
size_written
)
;
dest_
-
>
AppendAndTakeOwnership
(
blocks_
[
i
]
.
data
block_size
&
SnappySinkAllocator
:
:
Deleter
NULL
)
;
size_written
+
=
block_size
;
}
blocks_
.
clear
(
)
;
}
private
:
struct
Datablock
{
char
*
data
;
size_t
size
;
Datablock
(
char
*
p
size_t
s
)
:
data
(
p
)
size
(
s
)
{
}
}
;
static
void
Deleter
(
void
*
arg
const
char
*
bytes
size_t
size
)
{
delete
[
]
bytes
;
}
Sink
*
dest_
;
vector
<
Datablock
>
blocks_
;
}
;
size_t
UncompressAsMuchAsPossible
(
Source
*
compressed
Sink
*
uncompressed
)
{
SnappySinkAllocator
allocator
(
uncompressed
)
;
SnappyScatteredWriter
<
SnappySinkAllocator
>
writer
(
allocator
)
;
InternalUncompress
(
compressed
&
writer
)
;
return
writer
.
Produced
(
)
;
}
bool
Uncompress
(
Source
*
compressed
Sink
*
uncompressed
)
{
SnappyDecompressor
decompressor
(
compressed
)
;
uint32
uncompressed_len
=
0
;
if
(
!
decompressor
.
ReadUncompressedLength
(
&
uncompressed_len
)
)
{
return
false
;
}
char
c
;
size_t
allocated_size
;
char
*
buf
=
uncompressed
-
>
GetAppendBufferVariable
(
1
uncompressed_len
&
c
1
&
allocated_size
)
;
if
(
allocated_size
>
=
uncompressed_len
)
{
SnappyArrayWriter
writer
(
buf
)
;
bool
result
=
InternalUncompressAllTags
(
&
decompressor
&
writer
uncompressed_len
)
;
uncompressed
-
>
Append
(
buf
writer
.
Produced
(
)
)
;
return
result
;
}
else
{
SnappySinkAllocator
allocator
(
uncompressed
)
;
SnappyScatteredWriter
<
SnappySinkAllocator
>
writer
(
allocator
)
;
return
InternalUncompressAllTags
(
&
decompressor
&
writer
uncompressed_len
)
;
}
}
}
