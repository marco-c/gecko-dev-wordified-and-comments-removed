#
include
"
snappy
-
internal
.
h
"
#
include
"
snappy
-
sinksource
.
h
"
#
include
"
snappy
.
h
"
#
if
!
defined
(
SNAPPY_HAVE_SSSE3
)
#
if
defined
(
__SSSE3__
)
|
|
defined
(
__AVX__
)
#
define
SNAPPY_HAVE_SSSE3
1
#
else
#
define
SNAPPY_HAVE_SSSE3
0
#
endif
#
endif
#
if
!
defined
(
SNAPPY_HAVE_BMI2
)
#
if
defined
(
__BMI2__
)
|
|
(
defined
(
_MSC_VER
)
&
&
defined
(
__AVX2__
)
)
#
define
SNAPPY_HAVE_BMI2
1
#
else
#
define
SNAPPY_HAVE_BMI2
0
#
endif
#
endif
#
if
SNAPPY_HAVE_SSSE3
#
include
<
tmmintrin
.
h
>
#
endif
#
if
SNAPPY_HAVE_BMI2
#
include
<
immintrin
.
h
>
#
endif
#
include
<
algorithm
>
#
include
<
array
>
#
include
<
cstddef
>
#
include
<
cstdint
>
#
include
<
cstdio
>
#
include
<
cstring
>
#
include
<
string
>
#
include
<
utility
>
#
include
<
vector
>
namespace
snappy
{
namespace
{
constexpr
int
kSlopBytes
=
64
;
using
internal
:
:
char_table
;
using
internal
:
:
COPY_1_BYTE_OFFSET
;
using
internal
:
:
COPY_2_BYTE_OFFSET
;
using
internal
:
:
COPY_4_BYTE_OFFSET
;
using
internal
:
:
kMaximumTagLength
;
using
internal
:
:
LITERAL
;
inline
constexpr
int16_t
MakeEntry
(
int16_t
len
int16_t
offset
)
{
return
len
-
(
offset
<
<
8
)
;
}
inline
constexpr
int16_t
LengthMinusOffset
(
int
data
int
type
)
{
return
type
=
=
3
?
0xFF
:
type
=
=
2
?
MakeEntry
(
data
+
1
0
)
:
type
=
=
1
?
MakeEntry
(
(
data
&
7
)
+
4
data
>
>
3
)
:
data
<
60
?
MakeEntry
(
data
+
1
1
)
:
0xFF
;
}
inline
constexpr
int16_t
LengthMinusOffset
(
uint8_t
tag
)
{
return
LengthMinusOffset
(
tag
>
>
2
tag
&
3
)
;
}
template
<
size_t
.
.
.
Ints
>
struct
index_sequence
{
}
;
template
<
std
:
:
size_t
N
size_t
.
.
.
Is
>
struct
make_index_sequence
:
make_index_sequence
<
N
-
1
N
-
1
Is
.
.
.
>
{
}
;
template
<
size_t
.
.
.
Is
>
struct
make_index_sequence
<
0
Is
.
.
.
>
:
index_sequence
<
Is
.
.
.
>
{
}
;
template
<
size_t
.
.
.
seq
>
constexpr
std
:
:
array
<
int16_t
256
>
MakeTable
(
index_sequence
<
seq
.
.
.
>
)
{
return
std
:
:
array
<
int16_t
256
>
{
LengthMinusOffset
(
seq
)
.
.
.
}
;
}
struct
{
alignas
(
64
)
const
std
:
:
array
<
int16_t
256
>
length_minus_offset
;
uint32_t
extract_masks
[
4
]
;
}
table
=
{
MakeTable
(
make_index_sequence
<
256
>
{
}
)
{
0
0xFF
0xFFFF
0
}
}
;
inline
uint32_t
HashBytes
(
uint32_t
bytes
uint32_t
mask
)
{
constexpr
uint32_t
kMagic
=
0x1e35a7bd
;
return
(
(
kMagic
*
bytes
)
>
>
(
32
-
kMaxHashTableBits
)
)
&
mask
;
}
}
size_t
MaxCompressedLength
(
size_t
source_bytes
)
{
return
32
+
source_bytes
+
source_bytes
/
6
;
}
namespace
{
void
UnalignedCopy64
(
const
void
*
src
void
*
dst
)
{
char
tmp
[
8
]
;
std
:
:
memcpy
(
tmp
src
8
)
;
std
:
:
memcpy
(
dst
tmp
8
)
;
}
void
UnalignedCopy128
(
const
void
*
src
void
*
dst
)
{
char
tmp
[
16
]
;
std
:
:
memcpy
(
tmp
src
16
)
;
std
:
:
memcpy
(
dst
tmp
16
)
;
}
template
<
bool
use_16bytes_chunk
>
inline
void
ConditionalUnalignedCopy128
(
const
char
*
src
char
*
dst
)
{
if
(
use_16bytes_chunk
)
{
UnalignedCopy128
(
src
dst
)
;
}
else
{
UnalignedCopy64
(
src
dst
)
;
UnalignedCopy64
(
src
+
8
dst
+
8
)
;
}
}
inline
char
*
IncrementalCopySlow
(
const
char
*
src
char
*
op
char
*
const
op_limit
)
{
#
ifdef
__clang__
#
pragma
clang
loop
unroll
(
disable
)
#
endif
while
(
op
<
op_limit
)
{
*
op
+
+
=
*
src
+
+
;
}
return
op_limit
;
}
#
if
SNAPPY_HAVE_SSSE3
template
<
size_t
.
.
.
indexes
>
inline
constexpr
std
:
:
array
<
char
sizeof
.
.
.
(
indexes
)
>
MakePatternMaskBytes
(
int
index_offset
int
pattern_size
index_sequence
<
indexes
.
.
.
>
)
{
return
{
static_cast
<
char
>
(
(
index_offset
+
indexes
)
%
pattern_size
)
.
.
.
}
;
}
template
<
size_t
.
.
.
pattern_sizes_minus_one
>
inline
constexpr
std
:
:
array
<
std
:
:
array
<
char
sizeof
(
__m128i
)
>
sizeof
.
.
.
(
pattern_sizes_minus_one
)
>
MakePatternMaskBytesTable
(
int
index_offset
index_sequence
<
pattern_sizes_minus_one
.
.
.
>
)
{
return
{
MakePatternMaskBytes
(
index_offset
pattern_sizes_minus_one
+
1
make_index_sequence
<
sizeof
(
__m128i
)
>
(
)
)
.
.
.
}
;
}
alignas
(
16
)
constexpr
std
:
:
array
<
std
:
:
array
<
char
sizeof
(
__m128i
)
>
16
>
pattern_generation_masks
=
MakePatternMaskBytesTable
(
0
make_index_sequence
<
16
>
(
)
)
;
alignas
(
16
)
constexpr
std
:
:
array
<
std
:
:
array
<
char
sizeof
(
__m128i
)
>
16
>
pattern_reshuffle_masks
=
MakePatternMaskBytesTable
(
16
make_index_sequence
<
16
>
(
)
)
;
SNAPPY_ATTRIBUTE_ALWAYS_INLINE
static
inline
__m128i
LoadPattern
(
const
char
*
src
const
size_t
pattern_size
)
{
__m128i
generation_mask
=
_mm_load_si128
(
reinterpret_cast
<
const
__m128i
*
>
(
pattern_generation_masks
[
pattern_size
-
1
]
.
data
(
)
)
)
;
SNAPPY_ANNOTATE_MEMORY_IS_INITIALIZED
(
src
+
pattern_size
16
-
pattern_size
)
;
return
_mm_shuffle_epi8
(
_mm_loadu_si128
(
reinterpret_cast
<
const
__m128i
*
>
(
src
)
)
generation_mask
)
;
}
SNAPPY_ATTRIBUTE_ALWAYS_INLINE
static
inline
std
:
:
pair
<
__m128i
__m128i
>
LoadPatternAndReshuffleMask
(
const
char
*
src
const
size_t
pattern_size
)
{
__m128i
pattern
=
LoadPattern
(
src
pattern_size
)
;
__m128i
reshuffle_mask
=
_mm_load_si128
(
reinterpret_cast
<
const
__m128i
*
>
(
pattern_reshuffle_masks
[
pattern_size
-
1
]
.
data
(
)
)
)
;
return
{
pattern
reshuffle_mask
}
;
}
#
endif
SNAPPY_ATTRIBUTE_ALWAYS_INLINE
static
inline
bool
Copy64BytesWithPatternExtension
(
char
*
dst
size_t
offset
)
{
#
if
SNAPPY_HAVE_SSSE3
if
(
SNAPPY_PREDICT_TRUE
(
offset
<
=
16
)
)
{
switch
(
offset
)
{
case
0
:
return
false
;
case
1
:
{
std
:
:
memset
(
dst
dst
[
-
1
]
64
)
;
return
true
;
}
case
2
:
case
4
:
case
8
:
case
16
:
{
__m128i
pattern
=
LoadPattern
(
dst
-
offset
offset
)
;
for
(
int
i
=
0
;
i
<
4
;
i
+
+
)
{
_mm_storeu_si128
(
reinterpret_cast
<
__m128i
*
>
(
dst
+
16
*
i
)
pattern
)
;
}
return
true
;
}
default
:
{
auto
pattern_and_reshuffle_mask
=
LoadPatternAndReshuffleMask
(
dst
-
offset
offset
)
;
__m128i
pattern
=
pattern_and_reshuffle_mask
.
first
;
__m128i
reshuffle_mask
=
pattern_and_reshuffle_mask
.
second
;
for
(
int
i
=
0
;
i
<
4
;
i
+
+
)
{
_mm_storeu_si128
(
reinterpret_cast
<
__m128i
*
>
(
dst
+
16
*
i
)
pattern
)
;
pattern
=
_mm_shuffle_epi8
(
pattern
reshuffle_mask
)
;
}
return
true
;
}
}
}
#
else
if
(
SNAPPY_PREDICT_TRUE
(
offset
<
16
)
)
{
if
(
SNAPPY_PREDICT_FALSE
(
offset
=
=
0
)
)
return
false
;
for
(
int
i
=
0
;
i
<
16
;
i
+
+
)
dst
[
i
]
=
(
dst
-
offset
)
[
i
]
;
static
std
:
:
array
<
uint8_t
16
>
pattern_sizes
=
[
]
(
)
{
std
:
:
array
<
uint8_t
16
>
res
;
for
(
int
i
=
1
;
i
<
16
;
i
+
+
)
res
[
i
]
=
(
16
/
i
+
1
)
*
i
;
return
res
;
}
(
)
;
offset
=
pattern_sizes
[
offset
]
;
for
(
int
i
=
1
;
i
<
4
;
i
+
+
)
{
std
:
:
memcpy
(
dst
+
i
*
16
dst
+
i
*
16
-
offset
16
)
;
}
return
true
;
}
#
endif
for
(
int
i
=
0
;
i
<
4
;
i
+
+
)
{
std
:
:
memcpy
(
dst
+
i
*
16
dst
+
i
*
16
-
offset
16
)
;
}
return
true
;
}
inline
char
*
IncrementalCopy
(
const
char
*
src
char
*
op
char
*
const
op_limit
char
*
const
buf_limit
)
{
#
if
SNAPPY_HAVE_SSSE3
constexpr
int
big_pattern_size_lower_bound
=
16
;
#
else
constexpr
int
big_pattern_size_lower_bound
=
8
;
#
endif
assert
(
src
<
op
)
;
assert
(
op
<
op_limit
)
;
assert
(
op_limit
<
=
buf_limit
)
;
assert
(
op_limit
-
op
<
=
64
)
;
size_t
pattern_size
=
op
-
src
;
if
(
pattern_size
<
big_pattern_size_lower_bound
)
{
#
if
SNAPPY_HAVE_SSSE3
if
(
SNAPPY_PREDICT_TRUE
(
op_limit
<
=
buf_limit
-
15
)
)
{
auto
pattern_and_reshuffle_mask
=
LoadPatternAndReshuffleMask
(
src
pattern_size
)
;
__m128i
pattern
=
pattern_and_reshuffle_mask
.
first
;
__m128i
reshuffle_mask
=
pattern_and_reshuffle_mask
.
second
;
_mm_storeu_si128
(
reinterpret_cast
<
__m128i
*
>
(
op
)
pattern
)
;
if
(
op
+
16
<
op_limit
)
{
pattern
=
_mm_shuffle_epi8
(
pattern
reshuffle_mask
)
;
_mm_storeu_si128
(
reinterpret_cast
<
__m128i
*
>
(
op
+
16
)
pattern
)
;
}
if
(
op
+
32
<
op_limit
)
{
pattern
=
_mm_shuffle_epi8
(
pattern
reshuffle_mask
)
;
_mm_storeu_si128
(
reinterpret_cast
<
__m128i
*
>
(
op
+
32
)
pattern
)
;
}
if
(
op
+
48
<
op_limit
)
{
pattern
=
_mm_shuffle_epi8
(
pattern
reshuffle_mask
)
;
_mm_storeu_si128
(
reinterpret_cast
<
__m128i
*
>
(
op
+
48
)
pattern
)
;
}
return
op_limit
;
}
char
*
const
op_end
=
buf_limit
-
15
;
if
(
SNAPPY_PREDICT_TRUE
(
op
<
op_end
)
)
{
auto
pattern_and_reshuffle_mask
=
LoadPatternAndReshuffleMask
(
src
pattern_size
)
;
__m128i
pattern
=
pattern_and_reshuffle_mask
.
first
;
__m128i
reshuffle_mask
=
pattern_and_reshuffle_mask
.
second
;
#
ifdef
__clang__
#
pragma
clang
loop
unroll
(
disable
)
#
endif
do
{
_mm_storeu_si128
(
reinterpret_cast
<
__m128i
*
>
(
op
)
pattern
)
;
pattern
=
_mm_shuffle_epi8
(
pattern
reshuffle_mask
)
;
op
+
=
16
;
}
while
(
SNAPPY_PREDICT_TRUE
(
op
<
op_end
)
)
;
}
return
IncrementalCopySlow
(
op
-
pattern_size
op
op_limit
)
;
#
else
if
(
SNAPPY_PREDICT_TRUE
(
op
<
=
buf_limit
-
11
)
)
{
while
(
pattern_size
<
8
)
{
UnalignedCopy64
(
src
op
)
;
op
+
=
pattern_size
;
pattern_size
*
=
2
;
}
if
(
SNAPPY_PREDICT_TRUE
(
op
>
=
op_limit
)
)
return
op_limit
;
}
else
{
return
IncrementalCopySlow
(
src
op
op_limit
)
;
}
#
endif
}
assert
(
pattern_size
>
=
big_pattern_size_lower_bound
)
;
constexpr
bool
use_16bytes_chunk
=
big_pattern_size_lower_bound
=
=
16
;
if
(
SNAPPY_PREDICT_TRUE
(
op_limit
<
=
buf_limit
-
15
)
)
{
ConditionalUnalignedCopy128
<
use_16bytes_chunk
>
(
src
op
)
;
if
(
op
+
16
<
op_limit
)
{
ConditionalUnalignedCopy128
<
use_16bytes_chunk
>
(
src
+
16
op
+
16
)
;
}
if
(
op
+
32
<
op_limit
)
{
ConditionalUnalignedCopy128
<
use_16bytes_chunk
>
(
src
+
32
op
+
32
)
;
}
if
(
op
+
48
<
op_limit
)
{
ConditionalUnalignedCopy128
<
use_16bytes_chunk
>
(
src
+
48
op
+
48
)
;
}
return
op_limit
;
}
#
ifdef
__clang__
#
pragma
clang
loop
unroll
(
disable
)
#
endif
for
(
char
*
op_end
=
buf_limit
-
16
;
op
<
op_end
;
op
+
=
16
src
+
=
16
)
{
ConditionalUnalignedCopy128
<
use_16bytes_chunk
>
(
src
op
)
;
}
if
(
op
>
=
op_limit
)
return
op_limit
;
if
(
SNAPPY_PREDICT_FALSE
(
op
<
=
buf_limit
-
8
)
)
{
UnalignedCopy64
(
src
op
)
;
src
+
=
8
;
op
+
=
8
;
}
return
IncrementalCopySlow
(
src
op
op_limit
)
;
}
}
template
<
bool
allow_fast_path
>
static
inline
char
*
EmitLiteral
(
char
*
op
const
char
*
literal
int
len
)
{
assert
(
len
>
0
)
;
int
n
=
len
-
1
;
if
(
allow_fast_path
&
&
len
<
=
16
)
{
*
op
+
+
=
LITERAL
|
(
n
<
<
2
)
;
UnalignedCopy128
(
literal
op
)
;
return
op
+
len
;
}
if
(
n
<
60
)
{
*
op
+
+
=
LITERAL
|
(
n
<
<
2
)
;
}
else
{
int
count
=
(
Bits
:
:
Log2Floor
(
n
)
>
>
3
)
+
1
;
assert
(
count
>
=
1
)
;
assert
(
count
<
=
4
)
;
*
op
+
+
=
LITERAL
|
(
(
59
+
count
)
<
<
2
)
;
LittleEndian
:
:
Store32
(
op
n
)
;
op
+
=
count
;
}
std
:
:
memcpy
(
op
literal
len
)
;
return
op
+
len
;
}
template
<
bool
len_less_than_12
>
static
inline
char
*
EmitCopyAtMost64
(
char
*
op
size_t
offset
size_t
len
)
{
assert
(
len
<
=
64
)
;
assert
(
len
>
=
4
)
;
assert
(
offset
<
65536
)
;
assert
(
len_less_than_12
=
=
(
len
<
12
)
)
;
if
(
len_less_than_12
)
{
uint32_t
u
=
(
len
<
<
2
)
+
(
offset
<
<
8
)
;
uint32_t
copy1
=
COPY_1_BYTE_OFFSET
-
(
4
<
<
2
)
+
(
(
offset
>
>
3
)
&
0xe0
)
;
uint32_t
copy2
=
COPY_2_BYTE_OFFSET
-
(
1
<
<
2
)
;
u
+
=
offset
<
2048
?
copy1
:
copy2
;
LittleEndian
:
:
Store32
(
op
u
)
;
op
+
=
offset
<
2048
?
2
:
3
;
}
else
{
uint32_t
u
=
COPY_2_BYTE_OFFSET
+
(
(
len
-
1
)
<
<
2
)
+
(
offset
<
<
8
)
;
LittleEndian
:
:
Store32
(
op
u
)
;
op
+
=
3
;
}
return
op
;
}
template
<
bool
len_less_than_12
>
static
inline
char
*
EmitCopy
(
char
*
op
size_t
offset
size_t
len
)
{
assert
(
len_less_than_12
=
=
(
len
<
12
)
)
;
if
(
len_less_than_12
)
{
return
EmitCopyAtMost64
<
true
>
(
op
offset
len
)
;
}
else
{
while
(
SNAPPY_PREDICT_FALSE
(
len
>
=
68
)
)
{
op
=
EmitCopyAtMost64
<
false
>
(
op
offset
64
)
;
len
-
=
64
;
}
if
(
len
>
64
)
{
op
=
EmitCopyAtMost64
<
false
>
(
op
offset
60
)
;
len
-
=
60
;
}
if
(
len
<
12
)
{
op
=
EmitCopyAtMost64
<
true
>
(
op
offset
len
)
;
}
else
{
op
=
EmitCopyAtMost64
<
false
>
(
op
offset
len
)
;
}
return
op
;
}
}
bool
GetUncompressedLength
(
const
char
*
start
size_t
n
size_t
*
result
)
{
uint32_t
v
=
0
;
const
char
*
limit
=
start
+
n
;
if
(
Varint
:
:
Parse32WithLimit
(
start
limit
&
v
)
!
=
NULL
)
{
*
result
=
v
;
return
true
;
}
else
{
return
false
;
}
}
namespace
{
uint32_t
CalculateTableSize
(
uint32_t
input_size
)
{
static_assert
(
kMaxHashTableSize
>
=
kMinHashTableSize
"
kMaxHashTableSize
should
be
greater
or
equal
to
kMinHashTableSize
.
"
)
;
if
(
input_size
>
kMaxHashTableSize
)
{
return
kMaxHashTableSize
;
}
if
(
input_size
<
kMinHashTableSize
)
{
return
kMinHashTableSize
;
}
return
2u
<
<
Bits
:
:
Log2Floor
(
input_size
-
1
)
;
}
}
namespace
internal
{
WorkingMemory
:
:
WorkingMemory
(
size_t
input_size
)
{
const
size_t
max_fragment_size
=
std
:
:
min
(
input_size
kBlockSize
)
;
const
size_t
table_size
=
CalculateTableSize
(
max_fragment_size
)
;
size_
=
table_size
*
sizeof
(
*
table_
)
+
max_fragment_size
+
MaxCompressedLength
(
max_fragment_size
)
;
mem_
=
std
:
:
allocator
<
char
>
(
)
.
allocate
(
size_
)
;
table_
=
reinterpret_cast
<
uint16_t
*
>
(
mem_
)
;
input_
=
mem_
+
table_size
*
sizeof
(
*
table_
)
;
output_
=
input_
+
max_fragment_size
;
}
WorkingMemory
:
:
~
WorkingMemory
(
)
{
std
:
:
allocator
<
char
>
(
)
.
deallocate
(
mem_
size_
)
;
}
uint16_t
*
WorkingMemory
:
:
GetHashTable
(
size_t
fragment_size
int
*
table_size
)
const
{
const
size_t
htsize
=
CalculateTableSize
(
fragment_size
)
;
memset
(
table_
0
htsize
*
sizeof
(
*
table_
)
)
;
*
table_size
=
htsize
;
return
table_
;
}
}
namespace
internal
{
char
*
CompressFragment
(
const
char
*
input
size_t
input_size
char
*
op
uint16_t
*
table
const
int
table_size
)
{
const
char
*
ip
=
input
;
assert
(
input_size
<
=
kBlockSize
)
;
assert
(
(
table_size
&
(
table_size
-
1
)
)
=
=
0
)
;
const
uint32_t
mask
=
table_size
-
1
;
const
char
*
ip_end
=
input
+
input_size
;
const
char
*
base_ip
=
ip
;
const
size_t
kInputMarginBytes
=
15
;
if
(
SNAPPY_PREDICT_TRUE
(
input_size
>
=
kInputMarginBytes
)
)
{
const
char
*
ip_limit
=
input
+
input_size
-
kInputMarginBytes
;
for
(
uint32_t
preload
=
LittleEndian
:
:
Load32
(
ip
+
1
)
;
;
)
{
const
char
*
next_emit
=
ip
+
+
;
uint64_t
data
=
LittleEndian
:
:
Load64
(
ip
)
;
uint32_t
skip
=
32
;
const
char
*
candidate
;
if
(
ip_limit
-
ip
>
=
16
)
{
auto
delta
=
ip
-
base_ip
;
for
(
int
j
=
0
;
j
<
4
;
+
+
j
)
{
for
(
int
k
=
0
;
k
<
4
;
+
+
k
)
{
int
i
=
4
*
j
+
k
;
uint32_t
dword
=
i
=
=
0
?
preload
:
static_cast
<
uint32_t
>
(
data
)
;
assert
(
dword
=
=
LittleEndian
:
:
Load32
(
ip
+
i
)
)
;
uint32_t
hash
=
HashBytes
(
dword
mask
)
;
candidate
=
base_ip
+
table
[
hash
]
;
assert
(
candidate
>
=
base_ip
)
;
assert
(
candidate
<
ip
+
i
)
;
table
[
hash
]
=
delta
+
i
;
if
(
SNAPPY_PREDICT_FALSE
(
LittleEndian
:
:
Load32
(
candidate
)
=
=
dword
)
)
{
*
op
=
LITERAL
|
(
i
<
<
2
)
;
UnalignedCopy128
(
next_emit
op
+
1
)
;
ip
+
=
i
;
op
=
op
+
i
+
2
;
goto
emit_match
;
}
data
>
>
=
8
;
}
data
=
LittleEndian
:
:
Load64
(
ip
+
4
*
j
+
4
)
;
}
ip
+
=
16
;
skip
+
=
16
;
}
while
(
true
)
{
assert
(
static_cast
<
uint32_t
>
(
data
)
=
=
LittleEndian
:
:
Load32
(
ip
)
)
;
uint32_t
hash
=
HashBytes
(
data
mask
)
;
uint32_t
bytes_between_hash_lookups
=
skip
>
>
5
;
skip
+
=
bytes_between_hash_lookups
;
const
char
*
next_ip
=
ip
+
bytes_between_hash_lookups
;
if
(
SNAPPY_PREDICT_FALSE
(
next_ip
>
ip_limit
)
)
{
ip
=
next_emit
;
goto
emit_remainder
;
}
candidate
=
base_ip
+
table
[
hash
]
;
assert
(
candidate
>
=
base_ip
)
;
assert
(
candidate
<
ip
)
;
table
[
hash
]
=
ip
-
base_ip
;
if
(
SNAPPY_PREDICT_FALSE
(
static_cast
<
uint32_t
>
(
data
)
=
=
LittleEndian
:
:
Load32
(
candidate
)
)
)
{
break
;
}
data
=
LittleEndian
:
:
Load32
(
next_ip
)
;
ip
=
next_ip
;
}
assert
(
next_emit
+
16
<
=
ip_end
)
;
op
=
EmitLiteral
<
true
>
(
op
next_emit
ip
-
next_emit
)
;
emit_match
:
do
{
const
char
*
base
=
ip
;
std
:
:
pair
<
size_t
bool
>
p
=
FindMatchLength
(
candidate
+
4
ip
+
4
ip_end
&
data
)
;
size_t
matched
=
4
+
p
.
first
;
ip
+
=
matched
;
size_t
offset
=
base
-
candidate
;
assert
(
0
=
=
memcmp
(
base
candidate
matched
)
)
;
if
(
p
.
second
)
{
op
=
EmitCopy
<
true
>
(
op
offset
matched
)
;
}
else
{
op
=
EmitCopy
<
false
>
(
op
offset
matched
)
;
}
if
(
SNAPPY_PREDICT_FALSE
(
ip
>
=
ip_limit
)
)
{
goto
emit_remainder
;
}
assert
(
(
data
&
0xFFFFFFFFFF
)
=
=
(
LittleEndian
:
:
Load64
(
ip
)
&
0xFFFFFFFFFF
)
)
;
table
[
HashBytes
(
LittleEndian
:
:
Load32
(
ip
-
1
)
mask
)
]
=
ip
-
base_ip
-
1
;
uint32_t
hash
=
HashBytes
(
data
mask
)
;
candidate
=
base_ip
+
table
[
hash
]
;
table
[
hash
]
=
ip
-
base_ip
;
}
while
(
static_cast
<
uint32_t
>
(
data
)
=
=
LittleEndian
:
:
Load32
(
candidate
)
)
;
preload
=
data
>
>
8
;
}
}
emit_remainder
:
if
(
ip
<
ip_end
)
{
op
=
EmitLiteral
<
false
>
(
op
ip
ip_end
-
ip
)
;
}
return
op
;
}
}
static
inline
void
Report
(
const
char
*
algorithm
size_t
compressed_size
size_t
uncompressed_size
)
{
(
void
)
algorithm
;
(
void
)
compressed_size
;
(
void
)
uncompressed_size
;
}
static
inline
uint32_t
ExtractLowBytes
(
uint32_t
v
int
n
)
{
assert
(
n
>
=
0
)
;
assert
(
n
<
=
4
)
;
#
if
SNAPPY_HAVE_BMI2
return
_bzhi_u32
(
v
8
*
n
)
;
#
else
uint64_t
mask
=
0xffffffff
;
return
v
&
~
(
mask
<
<
(
8
*
n
)
)
;
#
endif
}
static
inline
bool
LeftShiftOverflows
(
uint8_t
value
uint32_t
shift
)
{
assert
(
shift
<
32
)
;
static
const
uint8_t
masks
[
]
=
{
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x00
0x80
0xc0
0xe0
0xf0
0xf8
0xfc
0xfe
}
;
return
(
value
&
masks
[
shift
]
)
!
=
0
;
}
inline
bool
Copy64BytesWithPatternExtension
(
ptrdiff_t
dst
size_t
offset
)
{
(
void
)
dst
;
return
offset
!
=
0
;
}
void
MemCopy
(
char
*
dst
const
uint8_t
*
src
size_t
size
)
{
std
:
:
memcpy
(
dst
src
size
)
;
}
void
MemCopy
(
ptrdiff_t
dst
const
uint8_t
*
src
size_t
size
)
{
(
void
)
dst
;
(
void
)
src
;
(
void
)
size
;
}
void
MemMove
(
char
*
dst
const
void
*
src
size_t
size
)
{
std
:
:
memmove
(
dst
src
size
)
;
}
void
MemMove
(
ptrdiff_t
dst
const
void
*
src
size_t
size
)
{
(
void
)
dst
;
(
void
)
src
;
(
void
)
size
;
}
SNAPPY_ATTRIBUTE_ALWAYS_INLINE
size_t
AdvanceToNextTag
(
const
uint8_t
*
*
ip_p
size_t
*
tag
)
{
const
uint8_t
*
&
ip
=
*
ip_p
;
size_t
literal_len
=
*
tag
>
>
2
;
size_t
tag_type
=
*
tag
;
bool
is_literal
;
#
if
defined
(
__GNUC__
)
&
&
defined
(
__x86_64__
)
&
&
defined
(
__GCC_ASM_FLAG_OUTPUTS__
)
asm
(
"
and
3
%
k
[
tag_type
]
\
n
\
t
"
:
[
tag_type
]
"
+
r
"
(
tag_type
)
"
=
ccz
"
(
is_literal
)
)
;
#
else
tag_type
&
=
3
;
is_literal
=
(
tag_type
=
=
0
)
;
#
endif
size_t
tag_literal
=
static_cast
<
const
volatile
uint8_t
*
>
(
ip
)
[
1
+
literal_len
]
;
size_t
tag_copy
=
static_cast
<
const
volatile
uint8_t
*
>
(
ip
)
[
tag_type
]
;
*
tag
=
is_literal
?
tag_literal
:
tag_copy
;
const
uint8_t
*
ip_copy
=
ip
+
1
+
tag_type
;
const
uint8_t
*
ip_literal
=
ip
+
2
+
literal_len
;
ip
=
is_literal
?
ip_literal
:
ip_copy
;
#
if
defined
(
__GNUC__
)
&
&
defined
(
__x86_64__
)
asm
(
"
"
:
:
"
r
"
(
tag_copy
)
)
;
#
endif
return
tag_type
;
}
inline
uint32_t
ExtractOffset
(
uint32_t
val
size_t
tag_type
)
{
return
val
&
table
.
extract_masks
[
tag_type
]
;
}
;
template
<
typename
T
>
std
:
:
pair
<
const
uint8_t
*
ptrdiff_t
>
DecompressBranchless
(
const
uint8_t
*
ip
const
uint8_t
*
ip_limit
ptrdiff_t
op
T
op_base
ptrdiff_t
op_limit_min_slop
)
{
op_limit_min_slop
-
=
kSlopBytes
;
if
(
2
*
(
kSlopBytes
+
1
)
<
ip_limit
-
ip
&
&
op
<
op_limit_min_slop
)
{
const
uint8_t
*
const
ip_limit_min_slop
=
ip_limit
-
2
*
kSlopBytes
-
1
;
ip
+
+
;
size_t
tag
=
ip
[
-
1
]
;
do
{
for
(
int
i
=
0
;
i
<
2
;
i
+
+
)
{
const
uint8_t
*
old_ip
=
ip
;
assert
(
tag
=
=
ip
[
-
1
]
)
;
ptrdiff_t
len_min_offset
=
table
.
length_minus_offset
[
tag
]
;
size_t
tag_type
=
AdvanceToNextTag
(
&
ip
&
tag
)
;
uint32_t
next
=
LittleEndian
:
:
Load32
(
old_ip
)
;
size_t
len
=
len_min_offset
&
0xFF
;
len_min_offset
-
=
ExtractOffset
(
next
tag_type
)
;
if
(
SNAPPY_PREDICT_FALSE
(
len_min_offset
>
0
)
)
{
if
(
SNAPPY_PREDICT_FALSE
(
len
&
0x80
)
)
{
break_loop
:
ip
=
old_ip
;
goto
exit
;
}
assert
(
tag_type
=
=
1
|
|
tag_type
=
=
2
)
;
std
:
:
ptrdiff_t
delta
=
op
+
len_min_offset
-
len
;
if
(
SNAPPY_PREDICT_FALSE
(
delta
<
0
|
|
!
Copy64BytesWithPatternExtension
(
op_base
+
op
len
-
len_min_offset
)
)
)
{
goto
break_loop
;
}
op
+
=
len
;
continue
;
}
std
:
:
ptrdiff_t
delta
=
op
+
len_min_offset
-
len
;
if
(
SNAPPY_PREDICT_FALSE
(
delta
<
0
)
)
{
#
if
defined
(
__GNUC__
)
&
&
defined
(
__x86_64__
)
asm
(
"
"
)
;
#
endif
if
(
tag_type
!
=
0
)
goto
break_loop
;
MemCopy
(
op_base
+
op
old_ip
64
)
;
op
+
=
len
;
continue
;
}
const
void
*
from
=
tag_type
?
reinterpret_cast
<
void
*
>
(
op_base
+
delta
)
:
old_ip
;
MemMove
(
op_base
+
op
from
64
)
;
op
+
=
len
;
}
}
while
(
ip
<
ip_limit_min_slop
&
&
op
<
op_limit_min_slop
)
;
exit
:
ip
-
-
;
assert
(
ip
<
=
ip_limit
)
;
}
return
{
ip
op
}
;
}
class
SnappyDecompressor
{
private
:
Source
*
reader_
;
const
char
*
ip_
;
const
char
*
ip_limit_
;
const
char
*
ip_limit_min_maxtaglen_
;
uint32_t
peeked_
;
bool
eof_
;
char
scratch_
[
kMaximumTagLength
]
;
bool
RefillTag
(
)
;
void
ResetLimit
(
const
char
*
ip
)
{
ip_limit_min_maxtaglen_
=
ip_limit_
-
std
:
:
min
<
ptrdiff_t
>
(
ip_limit_
-
ip
kMaximumTagLength
-
1
)
;
}
public
:
explicit
SnappyDecompressor
(
Source
*
reader
)
:
reader_
(
reader
)
ip_
(
NULL
)
ip_limit_
(
NULL
)
peeked_
(
0
)
eof_
(
false
)
{
}
~
SnappyDecompressor
(
)
{
reader_
-
>
Skip
(
peeked_
)
;
}
bool
eof
(
)
const
{
return
eof_
;
}
bool
ReadUncompressedLength
(
uint32_t
*
result
)
{
assert
(
ip_
=
=
NULL
)
;
*
result
=
0
;
uint32_t
shift
=
0
;
while
(
true
)
{
if
(
shift
>
=
32
)
return
false
;
size_t
n
;
const
char
*
ip
=
reader_
-
>
Peek
(
&
n
)
;
if
(
n
=
=
0
)
return
false
;
const
unsigned
char
c
=
*
(
reinterpret_cast
<
const
unsigned
char
*
>
(
ip
)
)
;
reader_
-
>
Skip
(
1
)
;
uint32_t
val
=
c
&
0x7f
;
if
(
LeftShiftOverflows
(
static_cast
<
uint8_t
>
(
val
)
shift
)
)
return
false
;
*
result
|
=
val
<
<
shift
;
if
(
c
<
128
)
{
break
;
}
shift
+
=
7
;
}
return
true
;
}
template
<
class
Writer
>
#
if
defined
(
__GNUC__
)
&
&
defined
(
__x86_64__
)
__attribute__
(
(
aligned
(
32
)
)
)
#
endif
void
DecompressAllTags
(
Writer
*
writer
)
{
const
char
*
ip
=
ip_
;
ResetLimit
(
ip
)
;
auto
op
=
writer
-
>
GetOutputPtr
(
)
;
#
define
MAYBE_REFILL
(
)
\
if
(
SNAPPY_PREDICT_FALSE
(
ip
>
=
ip_limit_min_maxtaglen_
)
)
{
\
ip_
=
ip
;
\
if
(
SNAPPY_PREDICT_FALSE
(
!
RefillTag
(
)
)
)
goto
exit
;
\
ip
=
ip_
;
\
ResetLimit
(
ip
)
;
\
}
\
preload
=
static_cast
<
uint8_t
>
(
*
ip
)
uint32_t
preload
;
MAYBE_REFILL
(
)
;
for
(
;
;
)
{
{
ptrdiff_t
op_limit_min_slop
;
auto
op_base
=
writer
-
>
GetBase
(
&
op_limit_min_slop
)
;
if
(
op_base
)
{
auto
res
=
DecompressBranchless
(
reinterpret_cast
<
const
uint8_t
*
>
(
ip
)
reinterpret_cast
<
const
uint8_t
*
>
(
ip_limit_
)
op
-
op_base
op_base
op_limit_min_slop
)
;
ip
=
reinterpret_cast
<
const
char
*
>
(
res
.
first
)
;
op
=
op_base
+
res
.
second
;
MAYBE_REFILL
(
)
;
}
}
const
uint8_t
c
=
static_cast
<
uint8_t
>
(
preload
)
;
ip
+
+
;
if
(
SNAPPY_PREDICT_FALSE
(
(
c
&
0x3
)
=
=
LITERAL
)
)
{
size_t
literal_length
=
(
c
>
>
2
)
+
1u
;
if
(
writer
-
>
TryFastAppend
(
ip
ip_limit_
-
ip
literal_length
&
op
)
)
{
assert
(
literal_length
<
61
)
;
ip
+
=
literal_length
;
preload
=
static_cast
<
uint8_t
>
(
*
ip
)
;
continue
;
}
if
(
SNAPPY_PREDICT_FALSE
(
literal_length
>
=
61
)
)
{
const
size_t
literal_length_length
=
literal_length
-
60
;
literal_length
=
ExtractLowBytes
(
LittleEndian
:
:
Load32
(
ip
)
literal_length_length
)
+
1
;
ip
+
=
literal_length_length
;
}
size_t
avail
=
ip_limit_
-
ip
;
while
(
avail
<
literal_length
)
{
if
(
!
writer
-
>
Append
(
ip
avail
&
op
)
)
goto
exit
;
literal_length
-
=
avail
;
reader_
-
>
Skip
(
peeked_
)
;
size_t
n
;
ip
=
reader_
-
>
Peek
(
&
n
)
;
avail
=
n
;
peeked_
=
avail
;
if
(
avail
=
=
0
)
goto
exit
;
ip_limit_
=
ip
+
avail
;
ResetLimit
(
ip
)
;
}
if
(
!
writer
-
>
Append
(
ip
literal_length
&
op
)
)
goto
exit
;
ip
+
=
literal_length
;
MAYBE_REFILL
(
)
;
}
else
{
if
(
SNAPPY_PREDICT_FALSE
(
(
c
&
3
)
=
=
COPY_4_BYTE_OFFSET
)
)
{
const
size_t
copy_offset
=
LittleEndian
:
:
Load32
(
ip
)
;
const
size_t
length
=
(
c
>
>
2
)
+
1
;
ip
+
=
4
;
if
(
!
writer
-
>
AppendFromSelf
(
copy_offset
length
&
op
)
)
goto
exit
;
}
else
{
const
ptrdiff_t
entry
=
table
.
length_minus_offset
[
c
]
;
preload
=
LittleEndian
:
:
Load32
(
ip
)
;
const
uint32_t
trailer
=
ExtractLowBytes
(
preload
c
&
3
)
;
const
uint32_t
length
=
entry
&
0xff
;
assert
(
length
>
0
)
;
const
uint32_t
copy_offset
=
trailer
-
entry
+
length
;
if
(
!
writer
-
>
AppendFromSelf
(
copy_offset
length
&
op
)
)
goto
exit
;
ip
+
=
(
c
&
3
)
;
preload
>
>
=
(
c
&
3
)
*
8
;
if
(
ip
<
ip_limit_min_maxtaglen_
)
continue
;
}
MAYBE_REFILL
(
)
;
}
}
#
undef
MAYBE_REFILL
exit
:
writer
-
>
SetOutputPtr
(
op
)
;
}
}
;
constexpr
uint32_t
CalculateNeeded
(
uint8_t
tag
)
{
return
(
(
tag
&
3
)
=
=
0
&
&
tag
>
=
(
60
*
4
)
)
?
(
tag
>
>
2
)
-
58
:
(
0x05030201
>
>
(
(
tag
*
8
)
&
31
)
)
&
0xFF
;
}
#
if
__cplusplus
>
=
201402L
constexpr
bool
VerifyCalculateNeeded
(
)
{
for
(
int
i
=
0
;
i
<
1
;
i
+
+
)
{
if
(
CalculateNeeded
(
i
)
!
=
(
char_table
[
i
]
>
>
11
)
+
1
)
return
false
;
}
return
true
;
}
static_assert
(
VerifyCalculateNeeded
(
)
"
"
)
;
#
endif
bool
SnappyDecompressor
:
:
RefillTag
(
)
{
const
char
*
ip
=
ip_
;
if
(
ip
=
=
ip_limit_
)
{
reader_
-
>
Skip
(
peeked_
)
;
size_t
n
;
ip
=
reader_
-
>
Peek
(
&
n
)
;
peeked_
=
n
;
eof_
=
(
n
=
=
0
)
;
if
(
eof_
)
return
false
;
ip_limit_
=
ip
+
n
;
}
assert
(
ip
<
ip_limit_
)
;
const
unsigned
char
c
=
*
(
reinterpret_cast
<
const
unsigned
char
*
>
(
ip
)
)
;
const
uint32_t
needed
=
CalculateNeeded
(
c
)
;
assert
(
needed
<
=
sizeof
(
scratch_
)
)
;
uint32_t
nbuf
=
ip_limit_
-
ip
;
if
(
nbuf
<
needed
)
{
std
:
:
memmove
(
scratch_
ip
nbuf
)
;
reader_
-
>
Skip
(
peeked_
)
;
peeked_
=
0
;
while
(
nbuf
<
needed
)
{
size_t
length
;
const
char
*
src
=
reader_
-
>
Peek
(
&
length
)
;
if
(
length
=
=
0
)
return
false
;
uint32_t
to_add
=
std
:
:
min
<
uint32_t
>
(
needed
-
nbuf
length
)
;
std
:
:
memcpy
(
scratch_
+
nbuf
src
to_add
)
;
nbuf
+
=
to_add
;
reader_
-
>
Skip
(
to_add
)
;
}
assert
(
nbuf
=
=
needed
)
;
ip_
=
scratch_
;
ip_limit_
=
scratch_
+
needed
;
}
else
if
(
nbuf
<
kMaximumTagLength
)
{
std
:
:
memmove
(
scratch_
ip
nbuf
)
;
reader_
-
>
Skip
(
peeked_
)
;
peeked_
=
0
;
ip_
=
scratch_
;
ip_limit_
=
scratch_
+
nbuf
;
}
else
{
ip_
=
ip
;
}
return
true
;
}
template
<
typename
Writer
>
static
bool
InternalUncompress
(
Source
*
r
Writer
*
writer
)
{
SnappyDecompressor
decompressor
(
r
)
;
uint32_t
uncompressed_len
=
0
;
if
(
!
decompressor
.
ReadUncompressedLength
(
&
uncompressed_len
)
)
return
false
;
return
InternalUncompressAllTags
(
&
decompressor
writer
r
-
>
Available
(
)
uncompressed_len
)
;
}
template
<
typename
Writer
>
static
bool
InternalUncompressAllTags
(
SnappyDecompressor
*
decompressor
Writer
*
writer
uint32_t
compressed_len
uint32_t
uncompressed_len
)
{
Report
(
"
snappy_uncompress
"
compressed_len
uncompressed_len
)
;
writer
-
>
SetExpectedLength
(
uncompressed_len
)
;
decompressor
-
>
DecompressAllTags
(
writer
)
;
writer
-
>
Flush
(
)
;
return
(
decompressor
-
>
eof
(
)
&
&
writer
-
>
CheckLength
(
)
)
;
}
bool
GetUncompressedLength
(
Source
*
source
uint32_t
*
result
)
{
SnappyDecompressor
decompressor
(
source
)
;
return
decompressor
.
ReadUncompressedLength
(
result
)
;
}
size_t
Compress
(
Source
*
reader
Sink
*
writer
)
{
size_t
written
=
0
;
size_t
N
=
reader
-
>
Available
(
)
;
const
size_t
uncompressed_size
=
N
;
char
ulength
[
Varint
:
:
kMax32
]
;
char
*
p
=
Varint
:
:
Encode32
(
ulength
N
)
;
writer
-
>
Append
(
ulength
p
-
ulength
)
;
written
+
=
(
p
-
ulength
)
;
internal
:
:
WorkingMemory
wmem
(
N
)
;
while
(
N
>
0
)
{
size_t
fragment_size
;
const
char
*
fragment
=
reader
-
>
Peek
(
&
fragment_size
)
;
assert
(
fragment_size
!
=
0
)
;
const
size_t
num_to_read
=
std
:
:
min
(
N
kBlockSize
)
;
size_t
bytes_read
=
fragment_size
;
size_t
pending_advance
=
0
;
if
(
bytes_read
>
=
num_to_read
)
{
pending_advance
=
num_to_read
;
fragment_size
=
num_to_read
;
}
else
{
char
*
scratch
=
wmem
.
GetScratchInput
(
)
;
std
:
:
memcpy
(
scratch
fragment
bytes_read
)
;
reader
-
>
Skip
(
bytes_read
)
;
while
(
bytes_read
<
num_to_read
)
{
fragment
=
reader
-
>
Peek
(
&
fragment_size
)
;
size_t
n
=
std
:
:
min
<
size_t
>
(
fragment_size
num_to_read
-
bytes_read
)
;
std
:
:
memcpy
(
scratch
+
bytes_read
fragment
n
)
;
bytes_read
+
=
n
;
reader
-
>
Skip
(
n
)
;
}
assert
(
bytes_read
=
=
num_to_read
)
;
fragment
=
scratch
;
fragment_size
=
num_to_read
;
}
assert
(
fragment_size
=
=
num_to_read
)
;
int
table_size
;
uint16_t
*
table
=
wmem
.
GetHashTable
(
num_to_read
&
table_size
)
;
const
int
max_output
=
MaxCompressedLength
(
num_to_read
)
;
char
*
dest
=
writer
-
>
GetAppendBuffer
(
max_output
wmem
.
GetScratchOutput
(
)
)
;
char
*
end
=
internal
:
:
CompressFragment
(
fragment
fragment_size
dest
table
table_size
)
;
writer
-
>
Append
(
dest
end
-
dest
)
;
written
+
=
(
end
-
dest
)
;
N
-
=
num_to_read
;
reader
-
>
Skip
(
pending_advance
)
;
}
Report
(
"
snappy_compress
"
written
uncompressed_size
)
;
return
written
;
}
class
SnappyIOVecWriter
{
private
:
const
struct
iovec
*
output_iov_end_
;
#
if
!
defined
(
NDEBUG
)
const
struct
iovec
*
output_iov_
;
#
endif
const
struct
iovec
*
curr_iov_
;
char
*
curr_iov_output_
;
size_t
curr_iov_remaining_
;
size_t
total_written_
;
size_t
output_limit_
;
static
inline
char
*
GetIOVecPointer
(
const
struct
iovec
*
iov
size_t
offset
)
{
return
reinterpret_cast
<
char
*
>
(
iov
-
>
iov_base
)
+
offset
;
}
public
:
inline
SnappyIOVecWriter
(
const
struct
iovec
*
iov
size_t
iov_count
)
:
output_iov_end_
(
iov
+
iov_count
)
#
if
!
defined
(
NDEBUG
)
output_iov_
(
iov
)
#
endif
curr_iov_
(
iov
)
curr_iov_output_
(
iov_count
?
reinterpret_cast
<
char
*
>
(
iov
-
>
iov_base
)
:
nullptr
)
curr_iov_remaining_
(
iov_count
?
iov
-
>
iov_len
:
0
)
total_written_
(
0
)
output_limit_
(
-
1
)
{
}
inline
void
SetExpectedLength
(
size_t
len
)
{
output_limit_
=
len
;
}
inline
bool
CheckLength
(
)
const
{
return
total_written_
=
=
output_limit_
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
char
*
*
)
{
if
(
total_written_
+
len
>
output_limit_
)
{
return
false
;
}
return
AppendNoCheck
(
ip
len
)
;
}
char
*
GetOutputPtr
(
)
{
return
nullptr
;
}
char
*
GetBase
(
ptrdiff_t
*
)
{
return
nullptr
;
}
void
SetOutputPtr
(
char
*
op
)
{
(
void
)
op
;
}
inline
bool
AppendNoCheck
(
const
char
*
ip
size_t
len
)
{
while
(
len
>
0
)
{
if
(
curr_iov_remaining_
=
=
0
)
{
if
(
curr_iov_
+
1
>
=
output_iov_end_
)
{
return
false
;
}
+
+
curr_iov_
;
curr_iov_output_
=
reinterpret_cast
<
char
*
>
(
curr_iov_
-
>
iov_base
)
;
curr_iov_remaining_
=
curr_iov_
-
>
iov_len
;
}
const
size_t
to_write
=
std
:
:
min
(
len
curr_iov_remaining_
)
;
std
:
:
memcpy
(
curr_iov_output_
ip
to_write
)
;
curr_iov_output_
+
=
to_write
;
curr_iov_remaining_
-
=
to_write
;
total_written_
+
=
to_write
;
ip
+
=
to_write
;
len
-
=
to_write
;
}
return
true
;
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
len
char
*
*
)
{
const
size_t
space_left
=
output_limit_
-
total_written_
;
if
(
len
<
=
16
&
&
available
>
=
16
+
kMaximumTagLength
&
&
space_left
>
=
16
&
&
curr_iov_remaining_
>
=
16
)
{
UnalignedCopy128
(
ip
curr_iov_output_
)
;
curr_iov_output_
+
=
len
;
curr_iov_remaining_
-
=
len
;
total_written_
+
=
len
;
return
true
;
}
return
false
;
}
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
char
*
*
)
{
if
(
offset
-
1u
>
=
total_written_
)
{
return
false
;
}
const
size_t
space_left
=
output_limit_
-
total_written_
;
if
(
len
>
space_left
)
{
return
false
;
}
const
iovec
*
from_iov
=
curr_iov_
;
size_t
from_iov_offset
=
curr_iov_
-
>
iov_len
-
curr_iov_remaining_
;
while
(
offset
>
0
)
{
if
(
from_iov_offset
>
=
offset
)
{
from_iov_offset
-
=
offset
;
break
;
}
offset
-
=
from_iov_offset
;
-
-
from_iov
;
#
if
!
defined
(
NDEBUG
)
assert
(
from_iov
>
=
output_iov_
)
;
#
endif
from_iov_offset
=
from_iov
-
>
iov_len
;
}
while
(
len
>
0
)
{
assert
(
from_iov
<
=
curr_iov_
)
;
if
(
from_iov
!
=
curr_iov_
)
{
const
size_t
to_copy
=
std
:
:
min
(
from_iov
-
>
iov_len
-
from_iov_offset
len
)
;
AppendNoCheck
(
GetIOVecPointer
(
from_iov
from_iov_offset
)
to_copy
)
;
len
-
=
to_copy
;
if
(
len
>
0
)
{
+
+
from_iov
;
from_iov_offset
=
0
;
}
}
else
{
size_t
to_copy
=
curr_iov_remaining_
;
if
(
to_copy
=
=
0
)
{
if
(
curr_iov_
+
1
>
=
output_iov_end_
)
{
return
false
;
}
+
+
curr_iov_
;
curr_iov_output_
=
reinterpret_cast
<
char
*
>
(
curr_iov_
-
>
iov_base
)
;
curr_iov_remaining_
=
curr_iov_
-
>
iov_len
;
continue
;
}
if
(
to_copy
>
len
)
{
to_copy
=
len
;
}
assert
(
to_copy
>
0
)
;
IncrementalCopy
(
GetIOVecPointer
(
from_iov
from_iov_offset
)
curr_iov_output_
curr_iov_output_
+
to_copy
curr_iov_output_
+
curr_iov_remaining_
)
;
curr_iov_output_
+
=
to_copy
;
curr_iov_remaining_
-
=
to_copy
;
from_iov_offset
+
=
to_copy
;
total_written_
+
=
to_copy
;
len
-
=
to_copy
;
}
}
return
true
;
}
inline
void
Flush
(
)
{
}
}
;
bool
RawUncompressToIOVec
(
const
char
*
compressed
size_t
compressed_length
const
struct
iovec
*
iov
size_t
iov_cnt
)
{
ByteArraySource
reader
(
compressed
compressed_length
)
;
return
RawUncompressToIOVec
(
&
reader
iov
iov_cnt
)
;
}
bool
RawUncompressToIOVec
(
Source
*
compressed
const
struct
iovec
*
iov
size_t
iov_cnt
)
{
SnappyIOVecWriter
output
(
iov
iov_cnt
)
;
return
InternalUncompress
(
compressed
&
output
)
;
}
class
SnappyArrayWriter
{
private
:
char
*
base_
;
char
*
op_
;
char
*
op_limit_
;
char
*
op_limit_min_slop_
;
public
:
inline
explicit
SnappyArrayWriter
(
char
*
dst
)
:
base_
(
dst
)
op_
(
dst
)
op_limit_
(
dst
)
op_limit_min_slop_
(
dst
)
{
}
inline
void
SetExpectedLength
(
size_t
len
)
{
op_limit_
=
op_
+
len
;
op_limit_min_slop_
=
op_limit_
-
std
:
:
min
<
size_t
>
(
kSlopBytes
-
1
len
)
;
}
inline
bool
CheckLength
(
)
const
{
return
op_
=
=
op_limit_
;
}
char
*
GetOutputPtr
(
)
{
return
op_
;
}
char
*
GetBase
(
ptrdiff_t
*
op_limit_min_slop
)
{
*
op_limit_min_slop
=
op_limit_min_slop_
-
base_
;
return
base_
;
}
void
SetOutputPtr
(
char
*
op
)
{
op_
=
op
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
char
*
*
op_p
)
{
char
*
op
=
*
op_p
;
const
size_t
space_left
=
op_limit_
-
op
;
if
(
space_left
<
len
)
return
false
;
std
:
:
memcpy
(
op
ip
len
)
;
*
op_p
=
op
+
len
;
return
true
;
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
len
char
*
*
op_p
)
{
char
*
op
=
*
op_p
;
const
size_t
space_left
=
op_limit_
-
op
;
if
(
len
<
=
16
&
&
available
>
=
16
+
kMaximumTagLength
&
&
space_left
>
=
16
)
{
UnalignedCopy128
(
ip
op
)
;
*
op_p
=
op
+
len
;
return
true
;
}
else
{
return
false
;
}
}
SNAPPY_ATTRIBUTE_ALWAYS_INLINE
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
char
*
*
op_p
)
{
assert
(
len
>
0
)
;
char
*
const
op
=
*
op_p
;
assert
(
op
>
=
base_
)
;
char
*
const
op_end
=
op
+
len
;
if
(
SNAPPY_PREDICT_FALSE
(
static_cast
<
size_t
>
(
op
-
base_
)
<
offset
)
)
return
false
;
if
(
SNAPPY_PREDICT_FALSE
(
(
kSlopBytes
<
64
&
&
len
>
kSlopBytes
)
|
|
op
>
=
op_limit_min_slop_
|
|
offset
<
len
)
)
{
if
(
op_end
>
op_limit_
|
|
offset
=
=
0
)
return
false
;
*
op_p
=
IncrementalCopy
(
op
-
offset
op
op_end
op_limit_
)
;
return
true
;
}
std
:
:
memmove
(
op
op
-
offset
kSlopBytes
)
;
*
op_p
=
op_end
;
return
true
;
}
inline
size_t
Produced
(
)
const
{
assert
(
op_
>
=
base_
)
;
return
op_
-
base_
;
}
inline
void
Flush
(
)
{
}
}
;
bool
RawUncompress
(
const
char
*
compressed
size_t
compressed_length
char
*
uncompressed
)
{
ByteArraySource
reader
(
compressed
compressed_length
)
;
return
RawUncompress
(
&
reader
uncompressed
)
;
}
bool
RawUncompress
(
Source
*
compressed
char
*
uncompressed
)
{
SnappyArrayWriter
output
(
uncompressed
)
;
return
InternalUncompress
(
compressed
&
output
)
;
}
bool
Uncompress
(
const
char
*
compressed
size_t
compressed_length
std
:
:
string
*
uncompressed
)
{
size_t
ulength
;
if
(
!
GetUncompressedLength
(
compressed
compressed_length
&
ulength
)
)
{
return
false
;
}
if
(
ulength
>
uncompressed
-
>
max_size
(
)
)
{
return
false
;
}
STLStringResizeUninitialized
(
uncompressed
ulength
)
;
return
RawUncompress
(
compressed
compressed_length
string_as_array
(
uncompressed
)
)
;
}
class
SnappyDecompressionValidator
{
private
:
size_t
expected_
;
size_t
produced_
;
public
:
inline
SnappyDecompressionValidator
(
)
:
expected_
(
0
)
produced_
(
0
)
{
}
inline
void
SetExpectedLength
(
size_t
len
)
{
expected_
=
len
;
}
size_t
GetOutputPtr
(
)
{
return
produced_
;
}
size_t
GetBase
(
ptrdiff_t
*
op_limit_min_slop
)
{
*
op_limit_min_slop
=
std
:
:
numeric_limits
<
ptrdiff_t
>
:
:
max
(
)
-
kSlopBytes
+
1
;
return
1
;
}
void
SetOutputPtr
(
size_t
op
)
{
produced_
=
op
;
}
inline
bool
CheckLength
(
)
const
{
return
expected_
=
=
produced_
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
size_t
*
produced
)
{
(
void
)
ip
;
*
produced
+
=
len
;
return
*
produced
<
=
expected_
;
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
length
size_t
*
produced
)
{
(
void
)
ip
;
(
void
)
available
;
(
void
)
length
;
(
void
)
produced
;
return
false
;
}
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
size_t
*
produced
)
{
if
(
*
produced
<
=
offset
-
1u
)
return
false
;
*
produced
+
=
len
;
return
*
produced
<
=
expected_
;
}
inline
void
Flush
(
)
{
}
}
;
bool
IsValidCompressedBuffer
(
const
char
*
compressed
size_t
compressed_length
)
{
ByteArraySource
reader
(
compressed
compressed_length
)
;
SnappyDecompressionValidator
writer
;
return
InternalUncompress
(
&
reader
&
writer
)
;
}
bool
IsValidCompressed
(
Source
*
compressed
)
{
SnappyDecompressionValidator
writer
;
return
InternalUncompress
(
compressed
&
writer
)
;
}
void
RawCompress
(
const
char
*
input
size_t
input_length
char
*
compressed
size_t
*
compressed_length
)
{
ByteArraySource
reader
(
input
input_length
)
;
UncheckedByteArraySink
writer
(
compressed
)
;
Compress
(
&
reader
&
writer
)
;
*
compressed_length
=
(
writer
.
CurrentDestination
(
)
-
compressed
)
;
}
size_t
Compress
(
const
char
*
input
size_t
input_length
std
:
:
string
*
compressed
)
{
STLStringResizeUninitialized
(
compressed
MaxCompressedLength
(
input_length
)
)
;
size_t
compressed_length
;
RawCompress
(
input
input_length
string_as_array
(
compressed
)
&
compressed_length
)
;
compressed
-
>
resize
(
compressed_length
)
;
return
compressed_length
;
}
template
<
typename
Allocator
>
class
SnappyScatteredWriter
{
Allocator
allocator_
;
std
:
:
vector
<
char
*
>
blocks_
;
size_t
expected_
;
size_t
full_size_
;
char
*
op_base_
;
char
*
op_ptr_
;
char
*
op_limit_
;
char
*
op_limit_min_slop_
;
inline
size_t
Size
(
)
const
{
return
full_size_
+
(
op_ptr_
-
op_base_
)
;
}
bool
SlowAppend
(
const
char
*
ip
size_t
len
)
;
bool
SlowAppendFromSelf
(
size_t
offset
size_t
len
)
;
public
:
inline
explicit
SnappyScatteredWriter
(
const
Allocator
&
allocator
)
:
allocator_
(
allocator
)
full_size_
(
0
)
op_base_
(
NULL
)
op_ptr_
(
NULL
)
op_limit_
(
NULL
)
op_limit_min_slop_
(
NULL
)
{
}
char
*
GetOutputPtr
(
)
{
return
op_ptr_
;
}
char
*
GetBase
(
ptrdiff_t
*
op_limit_min_slop
)
{
*
op_limit_min_slop
=
op_limit_min_slop_
-
op_base_
;
return
op_base_
;
}
void
SetOutputPtr
(
char
*
op
)
{
op_ptr_
=
op
;
}
inline
void
SetExpectedLength
(
size_t
len
)
{
assert
(
blocks_
.
empty
(
)
)
;
expected_
=
len
;
}
inline
bool
CheckLength
(
)
const
{
return
Size
(
)
=
=
expected_
;
}
inline
size_t
Produced
(
)
const
{
return
Size
(
)
;
}
inline
bool
Append
(
const
char
*
ip
size_t
len
char
*
*
op_p
)
{
char
*
op
=
*
op_p
;
size_t
avail
=
op_limit_
-
op
;
if
(
len
<
=
avail
)
{
std
:
:
memcpy
(
op
ip
len
)
;
*
op_p
=
op
+
len
;
return
true
;
}
else
{
op_ptr_
=
op
;
bool
res
=
SlowAppend
(
ip
len
)
;
*
op_p
=
op_ptr_
;
return
res
;
}
}
inline
bool
TryFastAppend
(
const
char
*
ip
size_t
available
size_t
length
char
*
*
op_p
)
{
char
*
op
=
*
op_p
;
const
int
space_left
=
op_limit_
-
op
;
if
(
length
<
=
16
&
&
available
>
=
16
+
kMaximumTagLength
&
&
space_left
>
=
16
)
{
UnalignedCopy128
(
ip
op
)
;
*
op_p
=
op
+
length
;
return
true
;
}
else
{
return
false
;
}
}
inline
bool
AppendFromSelf
(
size_t
offset
size_t
len
char
*
*
op_p
)
{
char
*
op
=
*
op_p
;
assert
(
op
>
=
op_base_
)
;
if
(
SNAPPY_PREDICT_FALSE
(
(
kSlopBytes
<
64
&
&
len
>
kSlopBytes
)
|
|
static_cast
<
size_t
>
(
op
-
op_base_
)
<
offset
|
|
op
>
=
op_limit_min_slop_
|
|
offset
<
len
)
)
{
if
(
offset
=
=
0
)
return
false
;
if
(
SNAPPY_PREDICT_FALSE
(
static_cast
<
size_t
>
(
op
-
op_base_
)
<
offset
|
|
op
+
len
>
op_limit_
)
)
{
op_ptr_
=
op
;
bool
res
=
SlowAppendFromSelf
(
offset
len
)
;
*
op_p
=
op_ptr_
;
return
res
;
}
*
op_p
=
IncrementalCopy
(
op
-
offset
op
op
+
len
op_limit_
)
;
return
true
;
}
char
*
const
op_end
=
op
+
len
;
std
:
:
memmove
(
op
op
-
offset
kSlopBytes
)
;
*
op_p
=
op_end
;
return
true
;
}
inline
void
Flush
(
)
{
allocator_
.
Flush
(
Produced
(
)
)
;
}
}
;
template
<
typename
Allocator
>
bool
SnappyScatteredWriter
<
Allocator
>
:
:
SlowAppend
(
const
char
*
ip
size_t
len
)
{
size_t
avail
=
op_limit_
-
op_ptr_
;
while
(
len
>
avail
)
{
std
:
:
memcpy
(
op_ptr_
ip
avail
)
;
op_ptr_
+
=
avail
;
assert
(
op_limit_
-
op_ptr_
=
=
0
)
;
full_size_
+
=
(
op_ptr_
-
op_base_
)
;
len
-
=
avail
;
ip
+
=
avail
;
if
(
full_size_
+
len
>
expected_
)
return
false
;
size_t
bsize
=
std
:
:
min
<
size_t
>
(
kBlockSize
expected_
-
full_size_
)
;
op_base_
=
allocator_
.
Allocate
(
bsize
)
;
op_ptr_
=
op_base_
;
op_limit_
=
op_base_
+
bsize
;
op_limit_min_slop_
=
op_limit_
-
std
:
:
min
<
size_t
>
(
kSlopBytes
-
1
bsize
)
;
blocks_
.
push_back
(
op_base_
)
;
avail
=
bsize
;
}
std
:
:
memcpy
(
op_ptr_
ip
len
)
;
op_ptr_
+
=
len
;
return
true
;
}
template
<
typename
Allocator
>
bool
SnappyScatteredWriter
<
Allocator
>
:
:
SlowAppendFromSelf
(
size_t
offset
size_t
len
)
{
const
size_t
cur
=
Size
(
)
;
if
(
offset
-
1u
>
=
cur
)
return
false
;
if
(
expected_
-
cur
<
len
)
return
false
;
size_t
src
=
cur
-
offset
;
char
*
op
=
op_ptr_
;
while
(
len
-
-
>
0
)
{
char
c
=
blocks_
[
src
>
>
kBlockLog
]
[
src
&
(
kBlockSize
-
1
)
]
;
if
(
!
Append
(
&
c
1
&
op
)
)
{
op_ptr_
=
op
;
return
false
;
}
src
+
+
;
}
op_ptr_
=
op
;
return
true
;
}
class
SnappySinkAllocator
{
public
:
explicit
SnappySinkAllocator
(
Sink
*
dest
)
:
dest_
(
dest
)
{
}
~
SnappySinkAllocator
(
)
{
}
char
*
Allocate
(
int
size
)
{
Datablock
block
(
new
char
[
size
]
size
)
;
blocks_
.
push_back
(
block
)
;
return
block
.
data
;
}
void
Flush
(
size_t
size
)
{
size_t
size_written
=
0
;
for
(
Datablock
&
block
:
blocks_
)
{
size_t
block_size
=
std
:
:
min
<
size_t
>
(
block
.
size
size
-
size_written
)
;
dest_
-
>
AppendAndTakeOwnership
(
block
.
data
block_size
&
SnappySinkAllocator
:
:
Deleter
NULL
)
;
size_written
+
=
block_size
;
}
blocks_
.
clear
(
)
;
}
private
:
struct
Datablock
{
char
*
data
;
size_t
size
;
Datablock
(
char
*
p
size_t
s
)
:
data
(
p
)
size
(
s
)
{
}
}
;
static
void
Deleter
(
void
*
arg
const
char
*
bytes
size_t
size
)
{
(
void
)
arg
;
(
void
)
size
;
delete
[
]
bytes
;
}
Sink
*
dest_
;
std
:
:
vector
<
Datablock
>
blocks_
;
}
;
size_t
UncompressAsMuchAsPossible
(
Source
*
compressed
Sink
*
uncompressed
)
{
SnappySinkAllocator
allocator
(
uncompressed
)
;
SnappyScatteredWriter
<
SnappySinkAllocator
>
writer
(
allocator
)
;
InternalUncompress
(
compressed
&
writer
)
;
return
writer
.
Produced
(
)
;
}
bool
Uncompress
(
Source
*
compressed
Sink
*
uncompressed
)
{
SnappyDecompressor
decompressor
(
compressed
)
;
uint32_t
uncompressed_len
=
0
;
if
(
!
decompressor
.
ReadUncompressedLength
(
&
uncompressed_len
)
)
{
return
false
;
}
char
c
;
size_t
allocated_size
;
char
*
buf
=
uncompressed
-
>
GetAppendBufferVariable
(
1
uncompressed_len
&
c
1
&
allocated_size
)
;
const
size_t
compressed_len
=
compressed
-
>
Available
(
)
;
if
(
allocated_size
>
=
uncompressed_len
)
{
SnappyArrayWriter
writer
(
buf
)
;
bool
result
=
InternalUncompressAllTags
(
&
decompressor
&
writer
compressed_len
uncompressed_len
)
;
uncompressed
-
>
Append
(
buf
writer
.
Produced
(
)
)
;
return
result
;
}
else
{
SnappySinkAllocator
allocator
(
uncompressed
)
;
SnappyScatteredWriter
<
SnappySinkAllocator
>
writer
(
allocator
)
;
return
InternalUncompressAllTags
(
&
decompressor
&
writer
compressed_len
uncompressed_len
)
;
}
}
}
