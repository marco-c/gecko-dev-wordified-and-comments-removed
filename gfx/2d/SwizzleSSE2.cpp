#
include
"
Swizzle
.
h
"
#
include
<
emmintrin
.
h
>
namespace
mozilla
{
namespace
gfx
{
static
MOZ_ALWAYS_INLINE
__m128i
LoadRemainder_SSE2
(
const
uint8_t
*
aSrc
size_t
aLength
)
{
__m128i
px
;
if
(
aLength
>
=
2
)
{
px
=
_mm_loadl_epi64
(
reinterpret_cast
<
const
__m128i
*
>
(
aSrc
)
)
;
if
(
aLength
>
=
3
)
{
px
=
_mm_unpacklo_epi64
(
px
_mm_cvtsi32_si128
(
*
reinterpret_cast
<
const
uint32_t
*
>
(
aSrc
+
2
*
4
)
)
)
;
}
}
else
{
px
=
_mm_cvtsi32_si128
(
*
reinterpret_cast
<
const
uint32_t
*
>
(
aSrc
)
)
;
}
return
px
;
}
static
MOZ_ALWAYS_INLINE
void
StoreRemainder_SSE2
(
uint8_t
*
aDst
size_t
aLength
const
__m128i
&
aSrc
)
{
if
(
aLength
>
=
2
)
{
_mm_storel_epi64
(
reinterpret_cast
<
__m128i
*
>
(
aDst
)
aSrc
)
;
if
(
aLength
>
=
3
)
{
*
reinterpret_cast
<
uint32_t
*
>
(
aDst
+
2
*
4
)
=
_mm_cvtsi128_si32
(
_mm_srli_si128
(
aSrc
2
*
4
)
)
;
}
}
else
{
*
reinterpret_cast
<
uint32_t
*
>
(
aDst
)
=
_mm_cvtsi128_si32
(
aSrc
)
;
}
}
template
<
bool
aSwapRB
bool
aOpaqueAlpha
>
static
MOZ_ALWAYS_INLINE
__m128i
PremultiplyVector_SSE2
(
const
__m128i
&
aSrc
)
{
const
__m128i
mask
=
_mm_set1_epi32
(
0x00FF00FF
)
;
__m128i
rb
=
_mm_and_si128
(
mask
aSrc
)
;
if
(
aSwapRB
)
{
rb
=
_mm_shufflelo_epi16
(
rb
_MM_SHUFFLE
(
2
3
0
1
)
)
;
rb
=
_mm_shufflehi_epi16
(
rb
_MM_SHUFFLE
(
2
3
0
1
)
)
;
}
__m128i
ga
=
_mm_srli_epi16
(
aSrc
8
)
;
__m128i
alphas
=
_mm_shufflelo_epi16
(
ga
_MM_SHUFFLE
(
3
3
1
1
)
)
;
alphas
=
_mm_shufflehi_epi16
(
alphas
_MM_SHUFFLE
(
3
3
1
1
)
)
;
rb
=
_mm_add_epi16
(
_mm_mullo_epi16
(
rb
alphas
)
mask
)
;
rb
=
_mm_add_epi16
(
rb
_mm_srli_epi16
(
rb
8
)
)
;
if
(
!
aOpaqueAlpha
)
{
ga
=
_mm_or_si128
(
ga
_mm_set1_epi32
(
0x00FF0000
)
)
;
}
ga
=
_mm_add_epi16
(
_mm_mullo_epi16
(
ga
alphas
)
mask
)
;
ga
=
_mm_add_epi16
(
ga
_mm_srli_epi16
(
ga
8
)
)
;
if
(
aOpaqueAlpha
)
{
ga
=
_mm_or_si128
(
ga
_mm_set1_epi32
(
0xFF000000
)
)
;
}
rb
=
_mm_srli_epi16
(
rb
8
)
;
ga
=
_mm_andnot_si128
(
mask
ga
)
;
return
_mm_or_si128
(
rb
ga
)
;
}
template
<
bool
aSwapRB
bool
aOpaqueAlpha
>
static
MOZ_ALWAYS_INLINE
void
PremultiplyChunk_SSE2
(
const
uint8_t
*
&
aSrc
uint8_t
*
&
aDst
int32_t
aAlignedRow
int32_t
aRemainder
)
{
for
(
const
uint8_t
*
end
=
aSrc
+
aAlignedRow
;
aSrc
<
end
;
)
{
__m128i
px
=
_mm_loadu_si128
(
reinterpret_cast
<
const
__m128i
*
>
(
aSrc
)
)
;
px
=
PremultiplyVector_SSE2
<
aSwapRB
aOpaqueAlpha
>
(
px
)
;
_mm_storeu_si128
(
reinterpret_cast
<
__m128i
*
>
(
aDst
)
px
)
;
aSrc
+
=
4
*
4
;
aDst
+
=
4
*
4
;
}
if
(
aRemainder
)
{
__m128i
px
=
LoadRemainder_SSE2
(
aSrc
aRemainder
)
;
px
=
PremultiplyVector_SSE2
<
aSwapRB
aOpaqueAlpha
>
(
px
)
;
StoreRemainder_SSE2
(
aDst
aRemainder
px
)
;
}
}
template
<
bool
aSwapRB
bool
aOpaqueAlpha
>
void
PremultiplyRow_SSE2
(
const
uint8_t
*
aSrc
uint8_t
*
aDst
int32_t
aLength
)
{
int32_t
alignedRow
=
4
*
(
aLength
&
~
3
)
;
int32_t
remainder
=
aLength
&
3
;
PremultiplyChunk_SSE2
<
aSwapRB
aOpaqueAlpha
>
(
aSrc
aDst
alignedRow
remainder
)
;
}
template
<
bool
aSwapRB
bool
aOpaqueAlpha
>
void
Premultiply_SSE2
(
const
uint8_t
*
aSrc
int32_t
aSrcGap
uint8_t
*
aDst
int32_t
aDstGap
IntSize
aSize
)
{
int32_t
alignedRow
=
4
*
(
aSize
.
width
&
~
3
)
;
int32_t
remainder
=
aSize
.
width
&
3
;
aSrcGap
+
=
4
*
remainder
;
aDstGap
+
=
4
*
remainder
;
for
(
int32_t
height
=
aSize
.
height
;
height
>
0
;
height
-
-
)
{
PremultiplyChunk_SSE2
<
aSwapRB
aOpaqueAlpha
>
(
aSrc
aDst
alignedRow
remainder
)
;
aSrc
+
=
aSrcGap
;
aDst
+
=
aDstGap
;
}
}
template
void
PremultiplyRow_SSE2
<
false
false
>
(
const
uint8_t
*
uint8_t
*
int32_t
)
;
template
void
PremultiplyRow_SSE2
<
false
true
>
(
const
uint8_t
*
uint8_t
*
int32_t
)
;
template
void
PremultiplyRow_SSE2
<
true
false
>
(
const
uint8_t
*
uint8_t
*
int32_t
)
;
template
void
PremultiplyRow_SSE2
<
true
true
>
(
const
uint8_t
*
uint8_t
*
int32_t
)
;
template
void
Premultiply_SSE2
<
false
false
>
(
const
uint8_t
*
int32_t
uint8_t
*
int32_t
IntSize
)
;
template
void
Premultiply_SSE2
<
false
true
>
(
const
uint8_t
*
int32_t
uint8_t
*
int32_t
IntSize
)
;
template
void
Premultiply_SSE2
<
true
false
>
(
const
uint8_t
*
int32_t
uint8_t
*
int32_t
IntSize
)
;
template
void
Premultiply_SSE2
<
true
true
>
(
const
uint8_t
*
int32_t
uint8_t
*
int32_t
IntSize
)
;
#
define
UNPREMULQ_SSE2
(
x
)
\
(
0x10001U
*
(
0xFF0220U
/
(
(
x
)
*
(
(
x
)
<
0x20
?
0x100
:
8
)
)
)
)
#
define
UNPREMULQ_SSE2_2
(
x
)
UNPREMULQ_SSE2
(
x
)
UNPREMULQ_SSE2
(
(
x
)
+
1
)
#
define
UNPREMULQ_SSE2_4
(
x
)
UNPREMULQ_SSE2_2
(
x
)
UNPREMULQ_SSE2_2
(
(
x
)
+
2
)
#
define
UNPREMULQ_SSE2_8
(
x
)
UNPREMULQ_SSE2_4
(
x
)
UNPREMULQ_SSE2_4
(
(
x
)
+
4
)
#
define
UNPREMULQ_SSE2_16
(
x
)
UNPREMULQ_SSE2_8
(
x
)
UNPREMULQ_SSE2_8
(
(
x
)
+
8
)
#
define
UNPREMULQ_SSE2_32
(
x
)
UNPREMULQ_SSE2_16
(
x
)
UNPREMULQ_SSE2_16
(
(
x
)
+
16
)
static
const
uint32_t
sUnpremultiplyTable_SSE2
[
256
]
=
{
0
UNPREMULQ_SSE2
(
1
)
UNPREMULQ_SSE2_2
(
2
)
UNPREMULQ_SSE2_4
(
4
)
UNPREMULQ_SSE2_8
(
8
)
UNPREMULQ_SSE2_16
(
16
)
UNPREMULQ_SSE2_32
(
32
)
UNPREMULQ_SSE2_32
(
64
)
UNPREMULQ_SSE2_32
(
96
)
UNPREMULQ_SSE2_32
(
128
)
UNPREMULQ_SSE2_32
(
160
)
UNPREMULQ_SSE2_32
(
192
)
UNPREMULQ_SSE2_32
(
224
)
}
;
template
<
bool
aSwapRB
>
static
MOZ_ALWAYS_INLINE
__m128i
UnpremultiplyVector_SSE2
(
const
__m128i
&
aSrc
)
{
__m128i
rb
=
_mm_and_si128
(
aSrc
_mm_set1_epi32
(
0x00FF00FF
)
)
;
if
(
aSwapRB
)
{
rb
=
_mm_shufflelo_epi16
(
rb
_MM_SHUFFLE
(
2
3
0
1
)
)
;
rb
=
_mm_shufflehi_epi16
(
rb
_MM_SHUFFLE
(
2
3
0
1
)
)
;
}
__m128i
ga
=
_mm_srli_epi16
(
aSrc
8
)
;
int
a1
=
_mm_extract_epi16
(
ga
1
)
;
int
a2
=
_mm_extract_epi16
(
ga
3
)
;
int
a3
=
_mm_extract_epi16
(
ga
5
)
;
int
a4
=
_mm_extract_epi16
(
ga
7
)
;
__m128i
q12
=
_mm_unpacklo_epi32
(
_mm_cvtsi32_si128
(
sUnpremultiplyTable_SSE2
[
a1
]
)
_mm_cvtsi32_si128
(
sUnpremultiplyTable_SSE2
[
a2
]
)
)
;
__m128i
q34
=
_mm_unpacklo_epi32
(
_mm_cvtsi32_si128
(
sUnpremultiplyTable_SSE2
[
a3
]
)
_mm_cvtsi32_si128
(
sUnpremultiplyTable_SSE2
[
a4
]
)
)
;
__m128i
q1234
=
_mm_unpacklo_epi64
(
q12
q34
)
;
__m128i
scale
=
_mm_cmplt_epi32
(
ga
_mm_set1_epi32
(
0x00200000
)
)
;
scale
=
_mm_xor_si128
(
scale
_mm_set1_epi16
(
8
)
)
;
scale
=
_mm_and_si128
(
scale
_mm_set1_epi16
(
0x108
)
)
;
ga
=
_mm_and_si128
(
ga
_mm_set1_epi32
(
0x000000FF
)
)
;
rb
=
_mm_mullo_epi16
(
rb
scale
)
;
ga
=
_mm_mullo_epi16
(
ga
scale
)
;
rb
=
_mm_mulhi_epu16
(
rb
q1234
)
;
ga
=
_mm_mulhi_epu16
(
ga
q1234
)
;
ga
=
_mm_slli_si128
(
ga
1
)
;
ga
=
_mm_or_si128
(
ga
_mm_and_si128
(
aSrc
_mm_set1_epi32
(
0xFF000000
)
)
)
;
return
_mm_or_si128
(
rb
ga
)
;
}
template
<
bool
aSwapRB
>
void
Unpremultiply_SSE2
(
const
uint8_t
*
aSrc
int32_t
aSrcGap
uint8_t
*
aDst
int32_t
aDstGap
IntSize
aSize
)
{
int32_t
alignedRow
=
4
*
(
aSize
.
width
&
~
3
)
;
int32_t
remainder
=
aSize
.
width
&
3
;
aSrcGap
+
=
4
*
remainder
;
aDstGap
+
=
4
*
remainder
;
for
(
int32_t
height
=
aSize
.
height
;
height
>
0
;
height
-
-
)
{
for
(
const
uint8_t
*
end
=
aSrc
+
alignedRow
;
aSrc
<
end
;
)
{
__m128i
px
=
_mm_loadu_si128
(
reinterpret_cast
<
const
__m128i
*
>
(
aSrc
)
)
;
px
=
UnpremultiplyVector_SSE2
<
aSwapRB
>
(
px
)
;
_mm_storeu_si128
(
reinterpret_cast
<
__m128i
*
>
(
aDst
)
px
)
;
aSrc
+
=
4
*
4
;
aDst
+
=
4
*
4
;
}
if
(
remainder
)
{
__m128i
px
=
LoadRemainder_SSE2
(
aSrc
remainder
)
;
px
=
UnpremultiplyVector_SSE2
<
aSwapRB
>
(
px
)
;
StoreRemainder_SSE2
(
aDst
remainder
px
)
;
}
aSrc
+
=
aSrcGap
;
aDst
+
=
aDstGap
;
}
}
template
void
Unpremultiply_SSE2
<
false
>
(
const
uint8_t
*
int32_t
uint8_t
*
int32_t
IntSize
)
;
template
void
Unpremultiply_SSE2
<
true
>
(
const
uint8_t
*
int32_t
uint8_t
*
int32_t
IntSize
)
;
template
<
bool
aSwapRB
bool
aOpaqueAlpha
>
static
MOZ_ALWAYS_INLINE
__m128i
SwizzleVector_SSE2
(
const
__m128i
&
aSrc
)
{
__m128i
rb
=
_mm_and_si128
(
aSrc
_mm_set1_epi32
(
0x00FF00FF
)
)
;
rb
=
_mm_shufflelo_epi16
(
rb
_MM_SHUFFLE
(
2
3
0
1
)
)
;
rb
=
_mm_shufflehi_epi16
(
rb
_MM_SHUFFLE
(
2
3
0
1
)
)
;
__m128i
ga
=
_mm_and_si128
(
aSrc
_mm_set1_epi32
(
0xFF00FF00
)
)
;
if
(
aOpaqueAlpha
)
{
ga
=
_mm_or_si128
(
ga
_mm_set1_epi32
(
0xFF000000
)
)
;
}
return
_mm_or_si128
(
rb
ga
)
;
}
#
if
0
template
<
>
MOZ_ALWAYS_INLINE
__m128i
SwizzleVector_SSE2
<
false
true
>
(
const
__m128i
&
aSrc
)
{
return
_mm_or_si128
(
aSrc
_mm_set1_epi32
(
0xFF000000
)
)
;
}
template
<
>
MOZ_ALWAYS_INLINE
__m128i
SwizzleVector_SSE2
<
false
false
>
(
const
__m128i
&
aSrc
)
{
return
aSrc
;
}
#
endif
template
<
bool
aSwapRB
bool
aOpaqueAlpha
>
static
MOZ_ALWAYS_INLINE
void
SwizzleChunk_SSE2
(
const
uint8_t
*
&
aSrc
uint8_t
*
&
aDst
int32_t
aAlignedRow
int32_t
aRemainder
)
{
for
(
const
uint8_t
*
end
=
aSrc
+
aAlignedRow
;
aSrc
<
end
;
)
{
__m128i
px
=
_mm_loadu_si128
(
reinterpret_cast
<
const
__m128i
*
>
(
aSrc
)
)
;
px
=
SwizzleVector_SSE2
<
aSwapRB
aOpaqueAlpha
>
(
px
)
;
_mm_storeu_si128
(
reinterpret_cast
<
__m128i
*
>
(
aDst
)
px
)
;
aSrc
+
=
4
*
4
;
aDst
+
=
4
*
4
;
}
if
(
aRemainder
)
{
__m128i
px
=
LoadRemainder_SSE2
(
aSrc
aRemainder
)
;
px
=
SwizzleVector_SSE2
<
aSwapRB
aOpaqueAlpha
>
(
px
)
;
StoreRemainder_SSE2
(
aDst
aRemainder
px
)
;
}
}
template
<
bool
aSwapRB
bool
aOpaqueAlpha
>
void
SwizzleRow_SSE2
(
const
uint8_t
*
aSrc
uint8_t
*
aDst
int32_t
aLength
)
{
int32_t
alignedRow
=
4
*
(
aLength
&
~
3
)
;
int32_t
remainder
=
aLength
&
3
;
SwizzleChunk_SSE2
<
aSwapRB
aOpaqueAlpha
>
(
aSrc
aDst
alignedRow
remainder
)
;
}
template
<
bool
aSwapRB
bool
aOpaqueAlpha
>
void
Swizzle_SSE2
(
const
uint8_t
*
aSrc
int32_t
aSrcGap
uint8_t
*
aDst
int32_t
aDstGap
IntSize
aSize
)
{
int32_t
alignedRow
=
4
*
(
aSize
.
width
&
~
3
)
;
int32_t
remainder
=
aSize
.
width
&
3
;
aSrcGap
+
=
4
*
remainder
;
aDstGap
+
=
4
*
remainder
;
for
(
int32_t
height
=
aSize
.
height
;
height
>
0
;
height
-
-
)
{
SwizzleChunk_SSE2
<
aSwapRB
aOpaqueAlpha
>
(
aSrc
aDst
alignedRow
remainder
)
;
aSrc
+
=
aSrcGap
;
aDst
+
=
aDstGap
;
}
}
template
void
SwizzleRow_SSE2
<
true
false
>
(
const
uint8_t
*
uint8_t
*
int32_t
)
;
template
void
SwizzleRow_SSE2
<
true
true
>
(
const
uint8_t
*
uint8_t
*
int32_t
)
;
template
void
Swizzle_SSE2
<
true
false
>
(
const
uint8_t
*
int32_t
uint8_t
*
int32_t
IntSize
)
;
template
void
Swizzle_SSE2
<
true
true
>
(
const
uint8_t
*
int32_t
uint8_t
*
int32_t
IntSize
)
;
}
}
