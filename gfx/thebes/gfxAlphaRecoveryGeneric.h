#
ifndef
_GFXALPHARECOVERY_GENERIC_H_
#
define
_GFXALPHARECOVERY_GENERIC_H_
#
include
"
gfxAlphaRecovery
.
h
"
#
include
"
gfxImageSurface
.
h
"
#
include
"
nsDebug
.
h
"
#
include
<
xsimd
/
xsimd
.
hpp
>
template
<
typename
Arch
>
bool
gfxAlphaRecovery
:
:
RecoverAlphaGeneric
(
gfxImageSurface
*
blackSurf
const
gfxImageSurface
*
whiteSurf
)
{
using
batch_type
=
xsimd
:
:
batch
<
uint8_t
Arch
>
;
constexpr
size_t
batch_size
=
batch_type
:
:
size
;
static
const
batch_type
greenMask
=
{
0x00
0x00
0xff
0x00
0x00
0x00
0xff
0x00
0x00
0x00
0xff
0x00
0x00
0x00
0xff
0x00
}
;
static
const
batch_type
alphaMask
=
{
0xff
0x00
0x00
0x00
0xff
0x00
0x00
0x00
0xff
0x00
0x00
0x00
0xff
0x00
0x00
0x00
}
;
mozilla
:
:
gfx
:
:
IntSize
size
=
blackSurf
-
>
GetSize
(
)
;
if
(
size
!
=
whiteSurf
-
>
GetSize
(
)
|
|
(
blackSurf
-
>
Format
(
)
!
=
mozilla
:
:
gfx
:
:
SurfaceFormat
:
:
A8R8G8B8_UINT32
&
&
blackSurf
-
>
Format
(
)
!
=
mozilla
:
:
gfx
:
:
SurfaceFormat
:
:
X8R8G8B8_UINT32
)
|
|
(
whiteSurf
-
>
Format
(
)
!
=
mozilla
:
:
gfx
:
:
SurfaceFormat
:
:
A8R8G8B8_UINT32
&
&
whiteSurf
-
>
Format
(
)
!
=
mozilla
:
:
gfx
:
:
SurfaceFormat
:
:
X8R8G8B8_UINT32
)
)
return
false
;
blackSurf
-
>
Flush
(
)
;
whiteSurf
-
>
Flush
(
)
;
unsigned
char
*
blackData
=
blackSurf
-
>
Data
(
)
;
unsigned
char
*
whiteData
=
whiteSurf
-
>
Data
(
)
;
if
(
(
NS_PTR_TO_UINT32
(
blackData
)
&
0xf
)
!
=
(
NS_PTR_TO_UINT32
(
whiteData
)
&
0xf
)
|
|
(
blackSurf
-
>
Stride
(
)
-
whiteSurf
-
>
Stride
(
)
)
&
0xf
)
{
return
false
;
}
for
(
int32_t
i
=
0
;
i
<
size
.
height
;
+
+
i
)
{
int32_t
j
=
0
;
while
(
NS_PTR_TO_UINT32
(
blackData
)
&
0xf
&
&
j
<
size
.
width
)
{
*
(
(
uint32_t
*
)
blackData
)
=
RecoverPixel
(
*
reinterpret_cast
<
uint32_t
*
>
(
blackData
)
*
reinterpret_cast
<
uint32_t
*
>
(
whiteData
)
)
;
blackData
+
=
4
;
whiteData
+
=
4
;
j
+
+
;
}
for
(
;
j
<
size
.
width
-
8
;
j
+
=
8
)
{
auto
black1
=
batch_type
:
:
load_aligned
(
blackData
)
;
auto
white1
=
batch_type
:
:
load_aligned
(
whiteData
)
;
auto
black2
=
batch_type
:
:
load_aligned
(
blackData
+
batch_size
)
;
auto
white2
=
batch_type
:
:
load_aligned
(
whiteData
+
batch_size
)
;
white1
=
xsimd
:
:
ssub
(
white1
black1
)
;
white2
=
xsimd
:
:
ssub
(
white2
black2
)
;
white1
=
xsimd
:
:
ssub
(
greenMask
white1
)
;
white2
=
xsimd
:
:
ssub
(
greenMask
white2
)
;
black1
=
xsimd
:
:
bitwise_andnot
(
alphaMask
black1
)
;
black2
=
xsimd
:
:
bitwise_andnot
(
alphaMask
black2
)
;
white1
=
xsimd
:
:
slide_left
<
2
>
(
white1
)
;
white2
=
xsimd
:
:
slide_left
<
2
>
(
white2
)
;
white1
=
alphaMask
&
white1
;
white2
=
alphaMask
&
white2
;
black1
=
white1
|
black1
;
black2
=
white2
|
black2
;
black1
.
store_aligned
(
blackData
)
;
black2
.
store_aligned
(
blackData
+
batch_size
)
;
blackData
+
=
2
*
batch_size
;
whiteData
+
=
2
*
batch_size
;
}
for
(
;
j
<
size
.
width
-
4
;
j
+
=
4
)
{
auto
black
=
batch_type
:
:
load_aligned
(
blackData
)
;
auto
white
=
batch_type
:
:
load_aligned
(
whiteData
)
;
white
=
xsimd
:
:
ssub
(
white
black
)
;
white
=
xsimd
:
:
ssub
(
greenMask
white
)
;
black
=
xsimd
:
:
bitwise_andnot
(
alphaMask
black
)
;
white
=
xsimd
:
:
slide_left
<
2
>
(
white
)
;
white
=
alphaMask
&
white
;
black
=
white
|
black
;
black
.
store_aligned
(
blackData
)
;
blackData
+
=
batch_size
;
whiteData
+
=
batch_size
;
}
while
(
j
<
size
.
width
)
{
*
(
(
uint32_t
*
)
blackData
)
=
RecoverPixel
(
*
reinterpret_cast
<
uint32_t
*
>
(
blackData
)
*
reinterpret_cast
<
uint32_t
*
>
(
whiteData
)
)
;
blackData
+
=
4
;
whiteData
+
=
4
;
j
+
+
;
}
blackData
+
=
blackSurf
-
>
Stride
(
)
-
j
*
4
;
whiteData
+
=
whiteSurf
-
>
Stride
(
)
-
j
*
4
;
}
blackSurf
-
>
MarkDirty
(
)
;
return
true
;
}
#
endif
