template
<
typename
P
>
static
inline
void
scale_row
(
P
*
dst
int
dstWidth
const
P
*
src
int
srcWidth
int
span
int
frac
)
{
for
(
P
*
end
=
dst
+
span
;
dst
<
end
;
dst
+
+
)
{
*
dst
=
*
src
;
for
(
frac
+
=
srcWidth
;
frac
>
=
dstWidth
;
frac
-
=
dstWidth
)
{
src
+
+
;
}
}
}
static
NO_INLINE
void
scale_blit
(
Texture
&
srctex
const
IntRect
&
srcReq
int
srcZ
Texture
&
dsttex
const
IntRect
&
dstReq
int
dstZ
bool
invertY
const
IntRect
&
clipRect
)
{
int
srcWidth
=
srcReq
.
width
(
)
;
int
srcHeight
=
srcReq
.
height
(
)
;
int
dstWidth
=
dstReq
.
width
(
)
;
int
dstHeight
=
dstReq
.
height
(
)
;
IntRect
dstBounds
=
dsttex
.
sample_bounds
(
dstReq
)
;
IntRect
srcBounds
=
srctex
.
sample_bounds
(
srcReq
invertY
)
.
scale
(
srcWidth
srcHeight
dstWidth
dstHeight
true
)
;
dstBounds
.
intersect
(
srcBounds
)
;
IntRect
clippedDest
=
dstBounds
.
intersection
(
clipRect
)
-
dstBounds
.
origin
(
)
;
if
(
clippedDest
.
is_empty
(
)
)
{
return
;
}
srcBounds
=
IntRect
(
dstBounds
)
.
scale
(
dstWidth
dstHeight
srcWidth
srcHeight
)
;
int
bpp
=
srctex
.
bpp
(
)
;
int
srcStride
=
srctex
.
stride
(
)
;
int
destStride
=
dsttex
.
stride
(
)
;
char
*
dest
=
dsttex
.
sample_ptr
(
dstReq
dstBounds
dstZ
)
;
char
*
src
=
srctex
.
sample_ptr
(
srcReq
srcBounds
srcZ
invertY
)
;
if
(
invertY
)
{
srcStride
=
-
srcStride
;
}
int
span
=
clippedDest
.
width
(
)
;
int
fracX
=
srcWidth
*
clippedDest
.
x0
;
int
fracY
=
srcHeight
*
clippedDest
.
y0
;
dest
+
=
destStride
*
clippedDest
.
y0
;
dest
+
=
bpp
*
clippedDest
.
x0
;
src
+
=
srcStride
*
(
fracY
/
dstHeight
)
;
src
+
=
bpp
*
(
fracX
/
dstWidth
)
;
fracY
%
=
dstHeight
;
fracX
%
=
dstWidth
;
for
(
int
rows
=
clippedDest
.
height
(
)
;
rows
>
0
;
rows
-
-
)
{
if
(
srcWidth
=
=
dstWidth
)
{
memcpy
(
dest
src
span
*
bpp
)
;
}
else
{
switch
(
bpp
)
{
case
1
:
scale_row
(
(
uint8_t
*
)
dest
dstWidth
(
uint8_t
*
)
src
srcWidth
span
fracX
)
;
break
;
case
2
:
scale_row
(
(
uint16_t
*
)
dest
dstWidth
(
uint16_t
*
)
src
srcWidth
span
fracX
)
;
break
;
case
4
:
scale_row
(
(
uint32_t
*
)
dest
dstWidth
(
uint32_t
*
)
src
srcWidth
span
fracX
)
;
break
;
default
:
assert
(
false
)
;
break
;
}
}
dest
+
=
destStride
;
for
(
fracY
+
=
srcHeight
;
fracY
>
=
dstHeight
;
fracY
-
=
dstHeight
)
{
src
+
=
srcStride
;
}
}
}
static
void
linear_row_blit
(
uint32_t
*
dest
int
span
const
vec2_scalar
&
srcUV
float
srcDU
int
srcZOffset
sampler2DArray
sampler
)
{
vec2
uv
=
init_interp
(
srcUV
vec2_scalar
(
srcDU
0
.
0f
)
)
;
for
(
;
span
>
=
4
;
span
-
=
4
)
{
auto
srcpx
=
textureLinearPackedRGBA8
(
sampler
ivec2
(
uv
)
srcZOffset
)
;
unaligned_store
(
dest
srcpx
)
;
dest
+
=
4
;
uv
.
x
+
=
4
*
srcDU
;
}
if
(
span
>
0
)
{
auto
srcpx
=
textureLinearPackedRGBA8
(
sampler
ivec2
(
uv
)
srcZOffset
)
;
partial_store_span
(
dest
srcpx
span
)
;
}
}
static
void
linear_row_blit
(
uint8_t
*
dest
int
span
const
vec2_scalar
&
srcUV
float
srcDU
int
srcZOffset
sampler2DArray
sampler
)
{
vec2
uv
=
init_interp
(
srcUV
vec2_scalar
(
srcDU
0
.
0f
)
)
;
for
(
;
span
>
=
4
;
span
-
=
4
)
{
auto
srcpx
=
textureLinearPackedR8
(
sampler
ivec2
(
uv
)
srcZOffset
)
;
unaligned_store
(
dest
srcpx
)
;
dest
+
=
4
;
uv
.
x
+
=
4
*
srcDU
;
}
if
(
span
>
0
)
{
auto
srcpx
=
textureLinearPackedR8
(
sampler
ivec2
(
uv
)
srcZOffset
)
;
partial_store_span
(
dest
srcpx
span
)
;
}
}
static
void
linear_row_blit
(
uint16_t
*
dest
int
span
const
vec2_scalar
&
srcUV
float
srcDU
int
srcZOffset
sampler2DArray
sampler
)
{
vec2
uv
=
init_interp
(
srcUV
vec2_scalar
(
srcDU
0
.
0f
)
)
;
for
(
;
span
>
=
4
;
span
-
=
4
)
{
auto
srcpx
=
textureLinearPackedRG8
(
sampler
ivec2
(
uv
)
srcZOffset
)
;
unaligned_store
(
dest
srcpx
)
;
dest
+
=
4
;
uv
.
x
+
=
4
*
srcDU
;
}
if
(
span
>
0
)
{
auto
srcpx
=
textureLinearPackedRG8
(
sampler
ivec2
(
uv
)
srcZOffset
)
;
partial_store_span
(
dest
srcpx
span
)
;
}
}
static
NO_INLINE
void
linear_blit
(
Texture
&
srctex
const
IntRect
&
srcReq
int
srcZ
Texture
&
dsttex
const
IntRect
&
dstReq
int
dstZ
bool
invertY
const
IntRect
&
clipRect
)
{
assert
(
srctex
.
internal_format
=
=
GL_RGBA8
|
|
srctex
.
internal_format
=
=
GL_R8
|
|
srctex
.
internal_format
=
=
GL_RG8
)
;
IntRect
dstBounds
=
dsttex
.
sample_bounds
(
dstReq
)
;
dstBounds
.
intersect
(
clipRect
)
;
if
(
dstBounds
.
is_empty
(
)
)
{
return
;
}
sampler2DArray_impl
sampler
;
init_sampler
(
&
sampler
srctex
)
;
init_depth
(
&
sampler
srctex
)
;
sampler
.
filter
=
TextureFilter
:
:
LINEAR
;
int
srcZOffset
=
srcZ
*
sampler
.
height_stride
;
vec2_scalar
srcUV
(
srcReq
.
x0
srcReq
.
y0
)
;
vec2_scalar
srcDUV
(
float
(
srcReq
.
width
(
)
)
/
dstReq
.
width
(
)
float
(
srcReq
.
height
(
)
)
/
dstReq
.
height
(
)
)
;
if
(
invertY
)
{
srcUV
.
y
+
=
srcReq
.
height
(
)
;
srcDUV
.
y
=
-
srcDUV
.
y
;
}
srcUV
+
=
srcDUV
*
(
vec2_scalar
(
dstBounds
.
x0
dstBounds
.
y0
)
+
0
.
5f
)
;
srcUV
=
linearQuantize
(
srcUV
128
)
;
srcDUV
*
=
128
.
0f
;
int
bpp
=
dsttex
.
bpp
(
)
;
int
destStride
=
dsttex
.
stride
(
)
;
char
*
dest
=
dsttex
.
sample_ptr
(
dstReq
dstBounds
dstZ
)
;
int
span
=
dstBounds
.
width
(
)
;
for
(
int
rows
=
dstBounds
.
height
(
)
;
rows
>
0
;
rows
-
-
)
{
switch
(
bpp
)
{
case
1
:
linear_row_blit
(
(
uint8_t
*
)
dest
span
srcUV
srcDUV
.
x
srcZOffset
&
sampler
)
;
break
;
case
2
:
linear_row_blit
(
(
uint16_t
*
)
dest
span
srcUV
srcDUV
.
x
srcZOffset
&
sampler
)
;
break
;
case
4
:
linear_row_blit
(
(
uint32_t
*
)
dest
span
srcUV
srcDUV
.
x
srcZOffset
&
sampler
)
;
break
;
default
:
assert
(
false
)
;
break
;
}
dest
+
=
destStride
;
srcUV
.
y
+
=
srcDUV
.
y
;
}
}
static
void
linear_row_composite
(
uint32_t
*
dest
int
span
const
vec2_scalar
&
srcUV
float
srcDU
sampler2D
sampler
)
{
vec2
uv
=
init_interp
(
srcUV
vec2_scalar
(
srcDU
0
.
0f
)
)
;
for
(
;
span
>
=
4
;
span
-
=
4
)
{
WideRGBA8
srcpx
=
textureLinearUnpackedRGBA8
(
sampler
ivec2
(
uv
)
0
)
;
WideRGBA8
dstpx
=
unpack
(
unaligned_load
<
PackedRGBA8
>
(
dest
)
)
;
PackedRGBA8
r
=
pack
(
srcpx
+
dstpx
-
muldiv255
(
dstpx
alphas
(
srcpx
)
)
)
;
unaligned_store
(
dest
r
)
;
dest
+
=
4
;
uv
.
x
+
=
4
*
srcDU
;
}
if
(
span
>
0
)
{
WideRGBA8
srcpx
=
textureLinearUnpackedRGBA8
(
sampler
ivec2
(
uv
)
0
)
;
WideRGBA8
dstpx
=
unpack
(
partial_load_span
<
PackedRGBA8
>
(
dest
span
)
)
;
PackedRGBA8
r
=
pack
(
srcpx
+
dstpx
-
muldiv255
(
dstpx
alphas
(
srcpx
)
)
)
;
partial_store_span
(
dest
r
span
)
;
}
}
static
NO_INLINE
void
linear_composite
(
Texture
&
srctex
const
IntRect
&
srcReq
Texture
&
dsttex
const
IntRect
&
dstReq
bool
invertY
const
IntRect
&
clipRect
)
{
assert
(
srctex
.
bpp
(
)
=
=
4
)
;
assert
(
dsttex
.
bpp
(
)
=
=
4
)
;
IntRect
dstBounds
=
dsttex
.
sample_bounds
(
dstReq
)
;
dstBounds
.
intersect
(
clipRect
)
;
if
(
dstBounds
.
is_empty
(
)
)
{
return
;
}
sampler2D_impl
sampler
;
init_sampler
(
&
sampler
srctex
)
;
sampler
.
filter
=
TextureFilter
:
:
LINEAR
;
vec2_scalar
srcUV
(
srcReq
.
x0
srcReq
.
y0
)
;
vec2_scalar
srcDUV
(
float
(
srcReq
.
width
(
)
)
/
dstReq
.
width
(
)
float
(
srcReq
.
height
(
)
)
/
dstReq
.
height
(
)
)
;
if
(
invertY
)
{
srcUV
.
y
+
=
srcReq
.
height
(
)
;
srcDUV
.
y
=
-
srcDUV
.
y
;
}
srcUV
+
=
srcDUV
*
(
vec2_scalar
(
dstBounds
.
x0
dstBounds
.
y0
)
+
0
.
5f
)
;
srcUV
=
linearQuantize
(
srcUV
128
)
;
srcDUV
*
=
128
.
0f
;
int
destStride
=
dsttex
.
stride
(
)
;
char
*
dest
=
dsttex
.
sample_ptr
(
dstReq
dstBounds
0
)
;
int
span
=
dstBounds
.
width
(
)
;
for
(
int
rows
=
dstBounds
.
height
(
)
;
rows
>
0
;
rows
-
-
)
{
linear_row_composite
(
(
uint32_t
*
)
dest
span
srcUV
srcDUV
.
x
&
sampler
)
;
dest
+
=
destStride
;
srcUV
.
y
+
=
srcDUV
.
y
;
}
}
extern
"
C
"
{
void
BlitFramebuffer
(
GLint
srcX0
GLint
srcY0
GLint
srcX1
GLint
srcY1
GLint
dstX0
GLint
dstY0
GLint
dstX1
GLint
dstY1
GLbitfield
mask
GLenum
filter
)
{
assert
(
mask
=
=
GL_COLOR_BUFFER_BIT
)
;
Framebuffer
*
srcfb
=
get_framebuffer
(
GL_READ_FRAMEBUFFER
)
;
if
(
!
srcfb
|
|
srcfb
-
>
layer
<
0
)
return
;
Framebuffer
*
dstfb
=
get_framebuffer
(
GL_DRAW_FRAMEBUFFER
)
;
if
(
!
dstfb
|
|
dstfb
-
>
layer
<
0
)
return
;
Texture
&
srctex
=
ctx
-
>
textures
[
srcfb
-
>
color_attachment
]
;
if
(
!
srctex
.
buf
|
|
srcfb
-
>
layer
>
=
max
(
srctex
.
depth
1
)
)
return
;
Texture
&
dsttex
=
ctx
-
>
textures
[
dstfb
-
>
color_attachment
]
;
if
(
!
dsttex
.
buf
|
|
dstfb
-
>
layer
>
=
max
(
dsttex
.
depth
1
)
)
return
;
assert
(
!
dsttex
.
locked
)
;
if
(
srctex
.
internal_format
!
=
dsttex
.
internal_format
)
{
assert
(
false
)
;
return
;
}
if
(
srcY1
<
srcY0
)
{
swap
(
srcY0
srcY1
)
;
swap
(
dstY0
dstY1
)
;
}
bool
invertY
=
dstY1
<
dstY0
;
if
(
invertY
)
{
swap
(
dstY0
dstY1
)
;
}
IntRect
srcReq
=
IntRect
{
srcX0
srcY0
srcX1
srcY1
}
-
srctex
.
offset
;
IntRect
dstReq
=
IntRect
{
dstX0
dstY0
dstX1
dstY1
}
-
dsttex
.
offset
;
if
(
srcReq
.
is_empty
(
)
|
|
dstReq
.
is_empty
(
)
)
{
return
;
}
IntRect
clipRect
=
{
0
0
dstReq
.
width
(
)
dstReq
.
height
(
)
}
;
prepare_texture
(
srctex
)
;
prepare_texture
(
dsttex
&
dstReq
)
;
if
(
!
srcReq
.
same_size
(
dstReq
)
&
&
srctex
.
width
>
=
2
&
&
filter
=
=
GL_LINEAR
&
&
(
srctex
.
internal_format
=
=
GL_RGBA8
|
|
srctex
.
internal_format
=
=
GL_R8
|
|
srctex
.
internal_format
=
=
GL_RG8
)
)
{
linear_blit
(
srctex
srcReq
srcfb
-
>
layer
dsttex
dstReq
dstfb
-
>
layer
invertY
dstReq
)
;
}
else
{
scale_blit
(
srctex
srcReq
srcfb
-
>
layer
dsttex
dstReq
dstfb
-
>
layer
invertY
clipRect
)
;
}
}
typedef
Texture
LockedTexture
;
LockedTexture
*
LockTexture
(
GLuint
texId
)
{
Texture
&
tex
=
ctx
-
>
textures
[
texId
]
;
if
(
!
tex
.
buf
)
{
assert
(
tex
.
buf
!
=
nullptr
)
;
return
nullptr
;
}
if
(
__sync_fetch_and_add
(
&
tex
.
locked
1
)
=
=
0
)
{
prepare_texture
(
tex
)
;
}
return
(
LockedTexture
*
)
&
tex
;
}
LockedTexture
*
LockFramebuffer
(
GLuint
fboId
)
{
Framebuffer
&
fb
=
ctx
-
>
framebuffers
[
fboId
]
;
if
(
!
fb
.
color_attachment
|
|
fb
.
layer
>
0
)
{
assert
(
fb
.
color_attachment
!
=
0
)
;
assert
(
fb
.
layer
=
=
0
)
;
return
nullptr
;
}
return
LockTexture
(
fb
.
color_attachment
)
;
}
void
LockResource
(
LockedTexture
*
resource
)
{
if
(
!
resource
)
{
return
;
}
__sync_fetch_and_add
(
&
resource
-
>
locked
1
)
;
}
void
UnlockResource
(
LockedTexture
*
resource
)
{
if
(
!
resource
)
{
return
;
}
if
(
__sync_fetch_and_add
(
&
resource
-
>
locked
-
1
)
<
=
0
)
{
assert
(
0
)
;
}
}
void
*
GetResourceBuffer
(
LockedTexture
*
resource
int32_t
*
width
int32_t
*
height
int32_t
*
stride
)
{
*
width
=
resource
-
>
width
;
*
height
=
resource
-
>
height
;
*
stride
=
resource
-
>
stride
(
)
;
return
resource
-
>
buf
;
}
static
void
unscaled_row_composite
(
uint32_t
*
dest
const
uint32_t
*
src
int
span
)
{
const
uint32_t
*
end
=
src
+
span
;
while
(
src
+
4
<
=
end
)
{
WideRGBA8
srcpx
=
unpack
(
unaligned_load
<
PackedRGBA8
>
(
src
)
)
;
WideRGBA8
dstpx
=
unpack
(
unaligned_load
<
PackedRGBA8
>
(
dest
)
)
;
PackedRGBA8
r
=
pack
(
srcpx
+
dstpx
-
muldiv255
(
dstpx
alphas
(
srcpx
)
)
)
;
unaligned_store
(
dest
r
)
;
src
+
=
4
;
dest
+
=
4
;
}
if
(
src
<
end
)
{
WideRGBA8
srcpx
=
unpack
(
partial_load_span
<
PackedRGBA8
>
(
src
end
-
src
)
)
;
WideRGBA8
dstpx
=
unpack
(
partial_load_span
<
PackedRGBA8
>
(
dest
end
-
src
)
)
;
auto
r
=
pack
(
srcpx
+
dstpx
-
muldiv255
(
dstpx
alphas
(
srcpx
)
)
)
;
partial_store_span
(
dest
r
end
-
src
)
;
}
}
static
NO_INLINE
void
unscaled_composite
(
Texture
&
srctex
const
IntRect
&
srcReq
Texture
&
dsttex
const
IntRect
&
dstReq
bool
invertY
const
IntRect
&
clipRect
)
{
IntRect
bounds
=
dsttex
.
sample_bounds
(
dstReq
)
;
bounds
.
intersect
(
clipRect
)
;
bounds
.
intersect
(
srctex
.
sample_bounds
(
srcReq
invertY
)
)
;
char
*
dest
=
dsttex
.
sample_ptr
(
dstReq
bounds
0
)
;
char
*
src
=
srctex
.
sample_ptr
(
srcReq
bounds
0
invertY
)
;
int
srcStride
=
srctex
.
stride
(
)
;
int
destStride
=
dsttex
.
stride
(
)
;
if
(
invertY
)
{
srcStride
=
-
srcStride
;
}
for
(
int
rows
=
bounds
.
height
(
)
;
rows
>
0
;
rows
-
-
)
{
unscaled_row_composite
(
(
uint32_t
*
)
dest
(
const
uint32_t
*
)
src
bounds
.
width
(
)
)
;
dest
+
=
destStride
;
src
+
=
srcStride
;
}
}
void
Composite
(
LockedTexture
*
lockedDst
LockedTexture
*
lockedSrc
GLint
srcX
GLint
srcY
GLsizei
srcWidth
GLsizei
srcHeight
GLint
dstX
GLint
dstY
GLsizei
dstWidth
GLsizei
dstHeight
GLboolean
opaque
GLboolean
flip
GLenum
filter
GLint
clipX
GLint
clipY
GLsizei
clipWidth
GLsizei
clipHeight
)
{
if
(
!
lockedDst
|
|
!
lockedSrc
)
{
return
;
}
Texture
&
srctex
=
*
lockedSrc
;
Texture
&
dsttex
=
*
lockedDst
;
assert
(
srctex
.
bpp
(
)
=
=
4
)
;
assert
(
dsttex
.
bpp
(
)
=
=
4
)
;
IntRect
srcReq
=
IntRect
{
srcX
srcY
srcX
+
srcWidth
srcY
+
srcHeight
}
-
srctex
.
offset
;
IntRect
dstReq
=
IntRect
{
dstX
dstY
dstX
+
dstWidth
dstY
+
dstHeight
}
-
dsttex
.
offset
;
IntRect
clipRect
=
{
clipX
-
dstX
clipY
-
dstY
clipX
-
dstX
+
clipWidth
clipY
-
dstY
+
clipHeight
}
;
if
(
opaque
)
{
if
(
!
srcReq
.
same_size
(
dstReq
)
&
&
srctex
.
width
>
=
2
&
&
filter
=
=
GL_LINEAR
)
{
linear_blit
(
srctex
srcReq
0
dsttex
dstReq
0
flip
clipRect
)
;
}
else
{
scale_blit
(
srctex
srcReq
0
dsttex
dstReq
0
flip
clipRect
)
;
}
}
else
{
if
(
!
srcReq
.
same_size
(
dstReq
)
&
&
srctex
.
width
>
=
2
)
{
linear_composite
(
srctex
srcReq
dsttex
dstReq
flip
clipRect
)
;
}
else
{
unscaled_composite
(
srctex
srcReq
dsttex
dstReq
flip
clipRect
)
;
}
}
}
}
static
inline
V8
<
int16_t
>
addsat
(
V8
<
int16_t
>
x
V8
<
int16_t
>
y
)
{
#
if
USE_SSE2
return
_mm_adds_epi16
(
x
y
)
;
#
elif
USE_NEON
return
vqaddq_s16
(
x
y
)
;
#
else
auto
r
=
x
+
y
;
auto
overflow
=
(
~
(
x
^
y
)
&
(
r
^
x
)
)
>
>
15
;
auto
limit
=
(
x
>
>
15
)
^
0x7FFF
;
return
(
~
overflow
&
r
)
|
(
overflow
&
limit
)
;
#
endif
}
static
inline
PackedRGBA8
packYUV
(
V8
<
int16_t
>
gg
V8
<
int16_t
>
br
)
{
return
pack
(
bit_cast
<
WideRGBA8
>
(
zip
(
br
gg
)
)
)
|
PackedRGBA8
{
0
0
0
255
0
0
0
255
0
0
0
255
0
0
0
255
}
;
}
struct
YUVMatrix
{
V8
<
int16_t
>
rbCoeffs
;
V8
<
int16_t
>
gCoeffs
;
V8
<
uint16_t
>
yScale
;
V8
<
int16_t
>
yBias
;
V8
<
int16_t
>
uvBias
;
V8
<
int16_t
>
brMask
;
YUVMatrix
(
)
:
rbCoeffs
(
1
<
<
6
)
gCoeffs
(
0
)
yScale
(
1
<
<
(
6
+
1
)
)
yBias
(
0
)
uvBias
(
0
)
brMask
(
0
)
{
}
YUVMatrix
(
double
rv
double
gu
double
gv
double
bu
)
:
rbCoeffs
(
zip
(
I16
(
int16_t
(
bu
*
64
.
0
+
0
.
5
)
)
I16
(
int16_t
(
rv
*
64
.
0
+
0
.
5
)
)
)
)
gCoeffs
(
zip
(
I16
(
-
int16_t
(
gu
*
-
64
.
0
+
0
.
5
)
)
I16
(
-
int16_t
(
gv
*
-
64
.
0
+
0
.
5
)
)
)
)
yScale
(
2
*
74
+
1
)
yBias
(
int16_t
(
-
16
*
74
.
5
)
+
(
1
<
<
5
)
)
uvBias
(
-
128
)
brMask
(
-
1
)
{
}
ALWAYS_INLINE
PackedRGBA8
convert
(
V8
<
int16_t
>
yy
V8
<
int16_t
>
uv
)
const
{
yy
=
bit_cast
<
V8
<
int16_t
>
>
(
(
bit_cast
<
V8
<
uint16_t
>
>
(
yy
)
*
yScale
)
>
>
1
)
+
yBias
;
uv
+
=
uvBias
;
auto
br
=
rbCoeffs
*
uv
;
br
=
addsat
(
yy
&
brMask
br
)
;
br
>
>
=
6
;
auto
gg
=
gCoeffs
*
uv
;
gg
=
addsat
(
yy
addsat
(
gg
bit_cast
<
V8
<
int16_t
>
>
(
bit_cast
<
V4
<
uint32_t
>
>
(
gg
)
>
>
16
)
)
)
;
gg
>
>
=
6
;
return
packYUV
(
gg
br
)
;
}
}
;
enum
YUVColorSpace
{
REC_601
=
0
REC_709
REC_2020
IDENTITY
}
;
static
const
YUVMatrix
yuvMatrix
[
IDENTITY
+
1
]
=
{
{
1
.
5960267857142858
-
0
.
3917622900949137
-
0
.
8129676472377708
2
.
017232142857143
}
{
1
.
7927410714285714
-
0
.
21324861427372963
-
0
.
532909328559444
2
.
1124017857142854
}
{
1
.
678674107142860
-
0
.
187326104219343
-
0
.
650424318505057
2
.
14177232142857
}
{
}
}
;
template
<
typename
S
>
static
ALWAYS_INLINE
V8
<
int16_t
>
linearRowTapsR8
(
S
sampler
I32
ix
int32_t
offsety
int32_t
stridey
int16_t
fracy
)
{
uint8_t
*
buf
=
(
uint8_t
*
)
sampler
-
>
buf
+
offsety
;
auto
a0
=
unaligned_load
<
V2
<
uint8_t
>
>
(
&
buf
[
ix
.
x
]
)
;
auto
b0
=
unaligned_load
<
V2
<
uint8_t
>
>
(
&
buf
[
ix
.
y
]
)
;
auto
c0
=
unaligned_load
<
V2
<
uint8_t
>
>
(
&
buf
[
ix
.
z
]
)
;
auto
d0
=
unaligned_load
<
V2
<
uint8_t
>
>
(
&
buf
[
ix
.
w
]
)
;
auto
abcd0
=
CONVERT
(
combine
(
a0
b0
c0
d0
)
V8
<
int16_t
>
)
;
buf
+
=
stridey
;
auto
a1
=
unaligned_load
<
V2
<
uint8_t
>
>
(
&
buf
[
ix
.
x
]
)
;
auto
b1
=
unaligned_load
<
V2
<
uint8_t
>
>
(
&
buf
[
ix
.
y
]
)
;
auto
c1
=
unaligned_load
<
V2
<
uint8_t
>
>
(
&
buf
[
ix
.
z
]
)
;
auto
d1
=
unaligned_load
<
V2
<
uint8_t
>
>
(
&
buf
[
ix
.
w
]
)
;
auto
abcd1
=
CONVERT
(
combine
(
a1
b1
c1
d1
)
V8
<
int16_t
>
)
;
abcd0
+
=
(
(
abcd1
-
abcd0
)
*
fracy
)
>
>
7
;
return
abcd0
;
}
template
<
typename
S
>
static
inline
V8
<
int16_t
>
textureLinearRowR8
(
S
sampler
I32
ix
int32_t
offsety
int32_t
stridey
int16_t
fracy
)
{
assert
(
sampler
-
>
format
=
=
TextureFormat
:
:
R8
)
;
I32
fracx
=
ix
;
ix
>
>
=
7
;
fracx
=
(
(
fracx
&
(
ix
>
=
0
)
)
|
(
ix
>
int32_t
(
sampler
-
>
width
)
-
2
)
)
&
0x7F
;
ix
=
clampCoord
(
ix
sampler
-
>
width
-
1
)
;
auto
abcd
=
linearRowTapsR8
(
sampler
ix
offsety
stridey
fracy
)
;
auto
abcdl
=
SHUFFLE
(
abcd
abcd
0
0
2
2
4
4
6
6
)
;
auto
abcdh
=
SHUFFLE
(
abcd
abcd
1
1
3
3
5
5
7
7
)
;
abcdl
+
=
(
(
abcdh
-
abcdl
)
*
CONVERT
(
fracx
I16
)
.
xxyyzzww
)
>
>
7
;
return
abcdl
;
}
template
<
typename
S
>
static
inline
V8
<
int16_t
>
textureLinearRowPairedR8
(
S
sampler
S
sampler2
I32
ix
int32_t
offsety
int32_t
stridey
int16_t
fracy
)
{
assert
(
sampler
-
>
format
=
=
TextureFormat
:
:
R8
&
&
sampler2
-
>
format
=
=
TextureFormat
:
:
R8
)
;
assert
(
sampler
-
>
width
=
=
sampler2
-
>
width
&
&
sampler
-
>
height
=
=
sampler2
-
>
height
)
;
assert
(
sampler
-
>
stride
=
=
sampler2
-
>
stride
)
;
I32
fracx
=
ix
;
ix
>
>
=
7
;
fracx
=
(
(
fracx
&
(
ix
>
=
0
)
)
|
(
ix
>
int32_t
(
sampler
-
>
width
)
-
2
)
)
&
0x7F
;
ix
=
clampCoord
(
ix
sampler
-
>
width
-
1
)
;
auto
abcd
=
linearRowTapsR8
(
sampler
ix
offsety
stridey
fracy
)
;
auto
xyzw
=
linearRowTapsR8
(
sampler2
ix
offsety
stridey
fracy
)
;
auto
abcdxyzwl
=
SHUFFLE
(
abcd
xyzw
0
8
2
10
4
12
6
14
)
;
auto
abcdxyzwh
=
SHUFFLE
(
abcd
xyzw
1
9
3
11
5
13
7
15
)
;
abcdxyzwl
+
=
(
(
abcdxyzwh
-
abcdxyzwl
)
*
CONVERT
(
fracx
I16
)
.
xxyyzzww
)
>
>
7
;
return
abcdxyzwl
;
}
const
int
STEP_BITS
=
8
;
static
inline
void
upscaleYUV42R8
(
uint32_t
*
dest
int
span
sampler2D_impl
sampler
[
3
]
I32
yU
int32_t
yDU
int32_t
yOffsetV
int32_t
yStrideV
int16_t
yFracV
I32
cU
int32_t
cDU
int32_t
cOffsetV
int32_t
cStrideV
int16_t
cFracV
const
YUVMatrix
&
colorSpace
)
{
cU
=
(
cU
.
xzxz
+
cU
.
ywyw
)
>
>
1
;
auto
ycFracX
=
CONVERT
(
combine
(
yU
cU
)
V8
<
uint16_t
>
)
<
<
(
16
-
(
STEP_BITS
+
7
)
)
;
auto
ycFracDX
=
combine
(
I16
(
yDU
)
I16
(
cDU
)
)
<
<
(
16
-
(
STEP_BITS
+
7
)
)
;
auto
ycFracV
=
combine
(
I16
(
yFracV
)
I16
(
cFracV
)
)
;
I32
yI
=
yU
>
>
(
STEP_BITS
+
7
)
;
I32
cI
=
cU
>
>
(
STEP_BITS
+
7
)
;
uint8_t
*
yRow
=
(
uint8_t
*
)
sampler
[
0
]
.
buf
+
yOffsetV
;
uint8_t
*
cRow1
=
(
uint8_t
*
)
sampler
[
1
]
.
buf
+
cOffsetV
;
uint8_t
*
cRow2
=
(
uint8_t
*
)
sampler
[
2
]
.
buf
+
cOffsetV
;
auto
ycSrc0
=
CONVERT
(
combine
(
unaligned_load
<
V4
<
uint8_t
>
>
(
&
yRow
[
yI
.
x
]
)
combine
(
unaligned_load
<
V2
<
uint8_t
>
>
(
&
cRow1
[
cI
.
x
]
)
unaligned_load
<
V2
<
uint8_t
>
>
(
&
cRow2
[
cI
.
x
]
)
)
)
V8
<
int16_t
>
)
;
auto
ycSrc1
=
CONVERT
(
combine
(
unaligned_load
<
V4
<
uint8_t
>
>
(
&
yRow
[
yI
.
x
+
yStrideV
]
)
combine
(
unaligned_load
<
V2
<
uint8_t
>
>
(
&
cRow1
[
cI
.
x
+
cStrideV
]
)
unaligned_load
<
V2
<
uint8_t
>
>
(
&
cRow2
[
cI
.
x
+
cStrideV
]
)
)
)
V8
<
int16_t
>
)
;
auto
ycSrc
=
ycSrc0
+
(
(
(
ycSrc1
-
ycSrc0
)
*
ycFracV
)
>
>
7
)
;
for
(
uint32_t
*
end
=
dest
+
span
;
dest
<
end
;
dest
+
=
4
)
{
yU
+
=
yDU
;
I32
yIn
=
yU
>
>
(
STEP_BITS
+
7
)
;
cU
+
=
cDU
;
I32
cIn
=
cU
>
>
(
STEP_BITS
+
7
)
;
auto
ycSrc0n
=
CONVERT
(
combine
(
unaligned_load
<
V4
<
uint8_t
>
>
(
&
yRow
[
yIn
.
x
]
)
combine
(
unaligned_load
<
V2
<
uint8_t
>
>
(
&
cRow1
[
cIn
.
x
]
)
unaligned_load
<
V2
<
uint8_t
>
>
(
&
cRow2
[
cIn
.
x
]
)
)
)
V8
<
int16_t
>
)
;
auto
ycSrc1n
=
CONVERT
(
combine
(
unaligned_load
<
V4
<
uint8_t
>
>
(
&
yRow
[
yIn
.
x
+
yStrideV
]
)
combine
(
unaligned_load
<
V2
<
uint8_t
>
>
(
&
cRow1
[
cIn
.
x
+
cStrideV
]
)
unaligned_load
<
V2
<
uint8_t
>
>
(
&
cRow2
[
cIn
.
x
+
cStrideV
]
)
)
)
V8
<
int16_t
>
)
;
auto
ycSrcn
=
ycSrc0n
+
(
(
(
ycSrc1n
-
ycSrc0n
)
*
ycFracV
)
>
>
7
)
;
auto
yshuf
=
lowHalf
(
ycSrc
)
;
auto
yshufn
=
SHUFFLE
(
yshuf
yIn
.
x
=
=
yI
.
w
?
lowHalf
(
ycSrcn
)
.
yyyy
:
lowHalf
(
ycSrcn
)
1
2
3
4
)
;
if
(
yI
.
y
=
=
yI
.
x
)
{
yshuf
=
yshuf
.
xxyz
;
yshufn
=
yshufn
.
xxyz
;
}
if
(
yI
.
z
=
=
yI
.
y
)
{
yshuf
=
yshuf
.
xyyz
;
yshufn
=
yshufn
.
xyyz
;
}
if
(
yI
.
w
=
=
yI
.
z
)
{
yshuf
=
yshuf
.
xyzz
;
yshufn
=
yshufn
.
xyzz
;
}
auto
cshuf
=
highHalf
(
ycSrc
)
;
auto
cshufn
=
SHUFFLE
(
cshuf
cIn
.
x
=
=
cI
.
y
?
highHalf
(
ycSrcn
)
.
yyww
:
highHalf
(
ycSrcn
)
1
4
3
6
)
;
if
(
cI
.
y
=
=
cI
.
x
)
{
cshuf
=
cshuf
.
xxzz
;
cshufn
=
cshufn
.
xxzz
;
}
auto
yuvPx
=
combine
(
yshuf
cshuf
)
;
yuvPx
+
=
(
(
combine
(
yshufn
cshufn
)
-
yuvPx
)
*
bit_cast
<
V8
<
int16_t
>
>
(
ycFracX
>
>
(
16
-
7
)
)
)
>
>
7
;
ycSrc
=
ycSrcn
;
ycFracX
+
=
ycFracDX
;
yI
=
yIn
;
cI
=
cIn
;
auto
yPx
=
SHUFFLE
(
yuvPx
yuvPx
0
0
1
1
2
2
3
3
)
;
auto
uvPx
=
SHUFFLE
(
yuvPx
yuvPx
4
6
4
6
5
7
5
7
)
+
(
(
SHUFFLE
(
yuvPx
yuvPx
4
6
5
7
4
6
5
7
)
-
SHUFFLE
(
yuvPx
yuvPx
5
7
4
6
5
7
4
6
)
)
>
>
2
)
;
unaligned_store
(
dest
colorSpace
.
convert
(
yPx
uvPx
)
)
;
}
}
static
void
linear_row_yuv
(
uint32_t
*
dest
int
span
const
vec2_scalar
&
srcUV
float
srcDU
const
vec2_scalar
&
chromaUV
float
chromaDU
sampler2D_impl
sampler
[
3
]
int
colorDepth
const
YUVMatrix
&
colorSpace
)
{
I32
yU
=
cast
(
init_interp
(
srcUV
.
x
srcDU
)
*
(
1
<
<
STEP_BITS
)
)
;
int32_t
yV
=
int32_t
(
srcUV
.
y
)
;
I32
cU
=
cast
(
init_interp
(
chromaUV
.
x
chromaDU
)
*
(
1
<
<
STEP_BITS
)
)
;
int32_t
cV
=
int32_t
(
chromaUV
.
y
)
;
int32_t
yDU
=
int32_t
(
(
4
<
<
STEP_BITS
)
*
srcDU
)
;
int32_t
cDU
=
int32_t
(
(
4
<
<
STEP_BITS
)
*
chromaDU
)
;
if
(
sampler
[
0
]
.
width
<
2
|
|
sampler
[
1
]
.
width
<
2
)
{
I16
yuv
=
CONVERT
(
round_pixel
(
(
Float
)
{
texelFetch
(
&
sampler
[
0
]
ivec2
(
srcUV
)
0
)
.
x
.
x
texelFetch
(
&
sampler
[
1
]
ivec2
(
chromaUV
)
0
)
.
x
.
x
texelFetch
(
&
sampler
[
2
]
ivec2
(
chromaUV
)
0
)
.
x
.
x
1
.
0f
}
)
I16
)
;
auto
rgb
=
colorSpace
.
convert
(
zip
(
I16
(
yuv
.
x
)
I16
(
yuv
.
x
)
)
zip
(
I16
(
yuv
.
y
)
I16
(
yuv
.
z
)
)
)
;
for
(
;
span
>
=
4
;
span
-
=
4
)
{
unaligned_store
(
dest
rgb
)
;
dest
+
=
4
;
}
if
(
span
>
0
)
{
partial_store_span
(
dest
rgb
span
)
;
}
}
else
if
(
sampler
[
0
]
.
format
=
=
TextureFormat
:
:
R16
)
{
assert
(
colorDepth
>
8
)
;
int
rescaleBits
=
(
colorDepth
-
1
)
-
8
;
for
(
;
span
>
=
4
;
span
-
=
4
)
{
auto
yPx
=
textureLinearUnpackedR16
(
&
sampler
[
0
]
ivec2
(
yU
>
>
STEP_BITS
yV
)
)
>
>
rescaleBits
;
auto
uPx
=
textureLinearUnpackedR16
(
&
sampler
[
1
]
ivec2
(
cU
>
>
STEP_BITS
cV
)
)
>
>
rescaleBits
;
auto
vPx
=
textureLinearUnpackedR16
(
&
sampler
[
2
]
ivec2
(
cU
>
>
STEP_BITS
cV
)
)
>
>
rescaleBits
;
unaligned_store
(
dest
colorSpace
.
convert
(
zip
(
yPx
yPx
)
zip
(
uPx
vPx
)
)
)
;
dest
+
=
4
;
yU
+
=
yDU
;
cU
+
=
cDU
;
}
if
(
span
>
0
)
{
auto
yPx
=
textureLinearUnpackedR16
(
&
sampler
[
0
]
ivec2
(
yU
>
>
STEP_BITS
yV
)
)
>
>
rescaleBits
;
auto
uPx
=
textureLinearUnpackedR16
(
&
sampler
[
1
]
ivec2
(
cU
>
>
STEP_BITS
cV
)
)
>
>
rescaleBits
;
auto
vPx
=
textureLinearUnpackedR16
(
&
sampler
[
2
]
ivec2
(
cU
>
>
STEP_BITS
cV
)
)
>
>
rescaleBits
;
partial_store_span
(
dest
colorSpace
.
convert
(
zip
(
yPx
yPx
)
zip
(
uPx
vPx
)
)
span
)
;
}
}
else
{
assert
(
sampler
[
0
]
.
format
=
=
TextureFormat
:
:
R8
)
;
assert
(
colorDepth
=
=
8
)
;
int16_t
yFracV
=
yV
&
0x7F
;
yV
>
>
=
7
;
int32_t
yOffsetV
=
clampCoord
(
yV
sampler
[
0
]
.
height
)
*
sampler
[
0
]
.
stride
;
int32_t
yStrideV
=
yV
>
=
0
&
&
yV
<
int32_t
(
sampler
[
0
]
.
height
)
-
1
?
sampler
[
0
]
.
stride
:
0
;
int16_t
cFracV
=
cV
&
0x7F
;
cV
>
>
=
7
;
int32_t
cOffsetV
=
clampCoord
(
cV
sampler
[
1
]
.
height
)
*
sampler
[
1
]
.
stride
;
int32_t
cStrideV
=
cV
>
=
0
&
&
cV
<
int32_t
(
sampler
[
1
]
.
height
)
-
1
?
sampler
[
1
]
.
stride
:
0
;
if
(
yDU
>
=
cDU
&
&
yDU
<
=
(
4
<
<
(
STEP_BITS
+
7
)
)
&
&
cDU
<
=
(
2
<
<
(
STEP_BITS
+
7
)
)
)
{
for
(
;
(
yU
.
x
<
0
|
|
cU
.
x
<
0
)
&
&
span
>
=
4
;
span
-
=
4
)
{
auto
yPx
=
textureLinearRowR8
(
&
sampler
[
0
]
yU
>
>
STEP_BITS
yOffsetV
yStrideV
yFracV
)
;
auto
uvPx
=
textureLinearRowPairedR8
(
&
sampler
[
1
]
&
sampler
[
2
]
cU
>
>
STEP_BITS
cOffsetV
cStrideV
cFracV
)
;
unaligned_store
(
dest
colorSpace
.
convert
(
yPx
uvPx
)
)
;
dest
+
=
4
;
yU
+
=
yDU
;
cU
+
=
cDU
;
}
int
inside
=
min
(
min
(
(
(
(
int
(
sampler
[
0
]
.
width
)
-
4
)
<
<
(
STEP_BITS
+
7
)
)
-
yU
.
x
)
/
yDU
(
(
(
int
(
sampler
[
1
]
.
width
)
-
4
)
<
<
(
STEP_BITS
+
7
)
)
-
cU
.
x
)
/
cDU
)
*
4
span
&
~
3
)
;
if
(
inside
>
0
)
{
upscaleYUV42R8
(
dest
inside
sampler
yU
yDU
yOffsetV
yStrideV
yFracV
cU
cDU
cOffsetV
cStrideV
cFracV
colorSpace
)
;
span
-
=
inside
;
dest
+
=
inside
;
yU
+
=
(
inside
/
4
)
*
yDU
;
cU
+
=
(
inside
/
4
)
*
cDU
;
}
}
for
(
;
span
>
=
4
;
span
-
=
4
)
{
auto
yPx
=
textureLinearRowR8
(
&
sampler
[
0
]
yU
>
>
STEP_BITS
yOffsetV
yStrideV
yFracV
)
;
auto
uvPx
=
textureLinearRowPairedR8
(
&
sampler
[
1
]
&
sampler
[
2
]
cU
>
>
STEP_BITS
cOffsetV
cStrideV
cFracV
)
;
unaligned_store
(
dest
colorSpace
.
convert
(
yPx
uvPx
)
)
;
dest
+
=
4
;
yU
+
=
yDU
;
cU
+
=
cDU
;
}
if
(
span
>
0
)
{
auto
yPx
=
textureLinearRowR8
(
&
sampler
[
0
]
yU
>
>
STEP_BITS
yOffsetV
yStrideV
yFracV
)
;
auto
uvPx
=
textureLinearRowPairedR8
(
&
sampler
[
1
]
&
sampler
[
2
]
cU
>
>
STEP_BITS
cOffsetV
cStrideV
cFracV
)
;
partial_store_span
(
dest
colorSpace
.
convert
(
yPx
uvPx
)
span
)
;
}
}
}
static
void
linear_convert_yuv
(
Texture
&
ytex
Texture
&
utex
Texture
&
vtex
YUVColorSpace
colorSpace
int
colorDepth
const
IntRect
&
srcReq
Texture
&
dsttex
const
IntRect
&
dstReq
bool
invertY
const
IntRect
&
clipRect
)
{
IntRect
dstBounds
=
dsttex
.
sample_bounds
(
dstReq
invertY
)
;
dstBounds
.
intersect
(
clipRect
)
;
if
(
dstBounds
.
is_empty
(
)
)
{
return
;
}
sampler2D_impl
sampler
[
3
]
;
init_sampler
(
&
sampler
[
0
]
ytex
)
;
init_sampler
(
&
sampler
[
1
]
utex
)
;
init_sampler
(
&
sampler
[
2
]
vtex
)
;
vec2_scalar
srcUV
(
srcReq
.
x0
srcReq
.
y0
)
;
vec2_scalar
srcDUV
(
float
(
srcReq
.
width
(
)
)
/
dstReq
.
width
(
)
float
(
srcReq
.
height
(
)
)
/
dstReq
.
height
(
)
)
;
if
(
invertY
)
{
srcUV
.
y
+
=
srcReq
.
height
(
)
;
srcDUV
.
y
=
-
srcDUV
.
y
;
}
srcUV
+
=
srcDUV
*
(
vec2_scalar
(
dstBounds
.
x0
dstBounds
.
y0
)
+
0
.
5f
)
;
vec2_scalar
chromaScale
(
float
(
utex
.
width
)
/
ytex
.
width
float
(
utex
.
height
)
/
ytex
.
height
)
;
vec2_scalar
chromaUV
=
srcUV
*
chromaScale
;
vec2_scalar
chromaDUV
=
srcDUV
*
chromaScale
;
if
(
ytex
.
width
>
=
2
&
&
utex
.
width
>
=
2
)
{
srcUV
=
linearQuantize
(
srcUV
128
)
;
srcDUV
*
=
128
.
0f
;
chromaUV
=
linearQuantize
(
chromaUV
128
)
;
chromaDUV
*
=
128
.
0f
;
}
int
destStride
=
dsttex
.
stride
(
)
;
char
*
dest
=
dsttex
.
sample_ptr
(
dstReq
dstBounds
0
)
;
int
span
=
dstBounds
.
width
(
)
;
for
(
int
rows
=
dstBounds
.
height
(
)
;
rows
>
0
;
rows
-
-
)
{
linear_row_yuv
(
(
uint32_t
*
)
dest
span
srcUV
srcDUV
.
x
chromaUV
chromaDUV
.
x
sampler
colorDepth
yuvMatrix
[
colorSpace
]
)
;
dest
+
=
destStride
;
srcUV
.
y
+
=
srcDUV
.
y
;
chromaUV
.
y
+
=
chromaDUV
.
y
;
}
}
extern
"
C
"
{
void
CompositeYUV
(
LockedTexture
*
lockedDst
LockedTexture
*
lockedY
LockedTexture
*
lockedU
LockedTexture
*
lockedV
YUVColorSpace
colorSpace
GLuint
colorDepth
GLint
srcX
GLint
srcY
GLsizei
srcWidth
GLsizei
srcHeight
GLint
dstX
GLint
dstY
GLsizei
dstWidth
GLsizei
dstHeight
GLboolean
flip
GLint
clipX
GLint
clipY
GLsizei
clipWidth
GLsizei
clipHeight
)
{
if
(
!
lockedDst
|
|
!
lockedY
|
|
!
lockedU
|
|
!
lockedV
)
{
return
;
}
if
(
colorSpace
>
IDENTITY
)
{
assert
(
false
)
;
return
;
}
Texture
&
ytex
=
*
lockedY
;
Texture
&
utex
=
*
lockedU
;
Texture
&
vtex
=
*
lockedV
;
Texture
&
dsttex
=
*
lockedDst
;
assert
(
ytex
.
bpp
(
)
=
=
utex
.
bpp
(
)
&
&
ytex
.
bpp
(
)
=
=
vtex
.
bpp
(
)
)
;
assert
(
(
ytex
.
bpp
(
)
=
=
1
&
&
colorDepth
=
=
8
)
|
|
(
ytex
.
bpp
(
)
=
=
2
&
&
colorDepth
>
8
)
)
;
assert
(
utex
.
width
=
=
vtex
.
width
&
&
utex
.
height
=
=
vtex
.
height
)
;
assert
(
ytex
.
offset
=
=
utex
.
offset
&
&
ytex
.
offset
=
=
vtex
.
offset
)
;
assert
(
dsttex
.
bpp
(
)
=
=
4
)
;
IntRect
srcReq
=
IntRect
{
srcX
srcY
srcX
+
srcWidth
srcY
+
srcHeight
}
-
ytex
.
offset
;
IntRect
dstReq
=
IntRect
{
dstX
dstY
dstX
+
dstWidth
dstY
+
dstHeight
}
-
dsttex
.
offset
;
IntRect
clipRect
=
{
clipX
-
dstX
clipY
-
dstY
clipX
-
dstX
+
clipWidth
clipY
-
dstY
+
clipHeight
}
;
linear_convert_yuv
(
ytex
utex
vtex
colorSpace
colorDepth
srcReq
dsttex
dstReq
flip
clipRect
)
;
}
}
