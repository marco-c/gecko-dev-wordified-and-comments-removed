#
include
"
SkOnce
.
h
"
#
include
"
SkRunnable
.
h
"
#
include
"
SkSemaphore
.
h
"
#
include
"
SkSpinlock
.
h
"
#
include
"
SkTDArray
.
h
"
#
include
"
SkTaskGroup
.
h
"
#
include
"
SkThreadUtils
.
h
"
#
if
defined
(
SK_BUILD_FOR_WIN32
)
static
void
query_num_cores
(
int
*
num_cores
)
{
SYSTEM_INFO
sysinfo
;
GetNativeSystemInfo
(
&
sysinfo
)
;
*
num_cores
=
sysinfo
.
dwNumberOfProcessors
;
}
#
else
#
include
<
unistd
.
h
>
static
void
query_num_cores
(
int
*
num_cores
)
{
*
num_cores
=
(
int
)
sysconf
(
_SC_NPROCESSORS_ONLN
)
;
}
#
endif
SK_DECLARE_STATIC_ONCE
(
g_query_num_cores_once
)
;
int
sk_num_cores
(
)
{
static
int
num_cores
=
0
;
SkOnce
(
&
g_query_num_cores_once
query_num_cores
&
num_cores
)
;
SkASSERT
(
num_cores
>
0
)
;
return
num_cores
;
}
namespace
{
class
ThreadPool
:
SkNoncopyable
{
public
:
static
void
Add
(
SkRunnable
*
task
SkAtomic
<
int32_t
>
*
pending
)
{
if
(
!
gGlobal
)
{
return
task
-
>
run
(
)
;
}
gGlobal
-
>
add
(
&
CallRunnable
task
pending
)
;
}
static
void
Add
(
void
(
*
fn
)
(
void
*
)
void
*
arg
SkAtomic
<
int32_t
>
*
pending
)
{
if
(
!
gGlobal
)
{
return
fn
(
arg
)
;
}
gGlobal
-
>
add
(
fn
arg
pending
)
;
}
static
void
Batch
(
void
(
*
fn
)
(
void
*
)
void
*
args
int
N
size_t
stride
SkAtomic
<
int32_t
>
*
pending
)
{
if
(
!
gGlobal
)
{
for
(
int
i
=
0
;
i
<
N
;
i
+
+
)
{
fn
(
(
char
*
)
args
+
i
*
stride
)
;
}
return
;
}
gGlobal
-
>
batch
(
fn
args
N
stride
pending
)
;
}
static
void
Wait
(
SkAtomic
<
int32_t
>
*
pending
)
{
if
(
!
gGlobal
)
{
SkASSERT
(
pending
-
>
load
(
sk_memory_order_relaxed
)
=
=
0
)
;
return
;
}
while
(
pending
-
>
load
(
sk_memory_order_acquire
)
>
0
)
{
Work
work
;
{
AutoLock
lock
(
&
gGlobal
-
>
fWorkLock
)
;
if
(
gGlobal
-
>
fWork
.
isEmpty
(
)
)
{
continue
;
}
gGlobal
-
>
fWork
.
pop
(
&
work
)
;
}
work
.
fn
(
work
.
arg
)
;
work
.
pending
-
>
fetch_add
(
-
1
sk_memory_order_release
)
;
}
}
private
:
struct
AutoLock
{
AutoLock
(
SkSpinlock
*
lock
)
:
fLock
(
lock
)
{
fLock
-
>
acquire
(
)
;
}
~
AutoLock
(
)
{
fLock
-
>
release
(
)
;
}
private
:
SkSpinlock
*
fLock
;
}
;
static
void
CallRunnable
(
void
*
arg
)
{
static_cast
<
SkRunnable
*
>
(
arg
)
-
>
run
(
)
;
}
struct
Work
{
void
(
*
fn
)
(
void
*
)
;
void
*
arg
;
SkAtomic
<
int32_t
>
*
pending
;
}
;
explicit
ThreadPool
(
int
threads
)
{
if
(
threads
=
=
-
1
)
{
threads
=
sk_num_cores
(
)
;
}
for
(
int
i
=
0
;
i
<
threads
;
i
+
+
)
{
fThreads
.
push
(
new
SkThread
(
&
ThreadPool
:
:
Loop
this
)
)
;
fThreads
.
top
(
)
-
>
start
(
)
;
}
}
~
ThreadPool
(
)
{
SkASSERT
(
fWork
.
isEmpty
(
)
)
;
SkAtomic
<
int
>
dummy
(
0
)
;
for
(
int
i
=
0
;
i
<
fThreads
.
count
(
)
;
i
+
+
)
{
this
-
>
add
(
nullptr
nullptr
&
dummy
)
;
}
for
(
int
i
=
0
;
i
<
fThreads
.
count
(
)
;
i
+
+
)
{
fThreads
[
i
]
-
>
join
(
)
;
}
SkASSERT
(
fWork
.
isEmpty
(
)
)
;
fThreads
.
deleteAll
(
)
;
}
void
add
(
void
(
*
fn
)
(
void
*
)
void
*
arg
SkAtomic
<
int32_t
>
*
pending
)
{
Work
work
=
{
fn
arg
pending
}
;
pending
-
>
fetch_add
(
+
1
sk_memory_order_relaxed
)
;
{
AutoLock
lock
(
&
fWorkLock
)
;
fWork
.
push
(
work
)
;
}
fWorkAvailable
.
signal
(
1
)
;
}
void
batch
(
void
(
*
fn
)
(
void
*
)
void
*
arg
int
N
size_t
stride
SkAtomic
<
int32_t
>
*
pending
)
{
pending
-
>
fetch_add
(
+
N
sk_memory_order_relaxed
)
;
{
AutoLock
lock
(
&
fWorkLock
)
;
Work
*
batch
=
fWork
.
append
(
N
)
;
for
(
int
i
=
0
;
i
<
N
;
i
+
+
)
{
Work
work
=
{
fn
(
char
*
)
arg
+
i
*
stride
pending
}
;
batch
[
i
]
=
work
;
}
}
fWorkAvailable
.
signal
(
N
)
;
}
static
void
Loop
(
void
*
arg
)
{
ThreadPool
*
pool
=
(
ThreadPool
*
)
arg
;
Work
work
;
while
(
true
)
{
pool
-
>
fWorkAvailable
.
wait
(
)
;
{
AutoLock
lock
(
&
pool
-
>
fWorkLock
)
;
if
(
pool
-
>
fWork
.
isEmpty
(
)
)
{
continue
;
}
pool
-
>
fWork
.
pop
(
&
work
)
;
}
if
(
!
work
.
fn
)
{
return
;
}
work
.
fn
(
work
.
arg
)
;
work
.
pending
-
>
fetch_add
(
-
1
sk_memory_order_release
)
;
}
}
SkSpinlock
fWorkLock
;
SkTDArray
<
Work
>
fWork
;
SkSemaphore
fWorkAvailable
;
SkTDArray
<
SkThread
*
>
fThreads
;
static
ThreadPool
*
gGlobal
;
friend
struct
SkTaskGroup
:
:
Enabler
;
friend
int
:
:
sk_parallel_for_thread_count
(
)
;
}
;
ThreadPool
*
ThreadPool
:
:
gGlobal
=
nullptr
;
}
SkTaskGroup
:
:
Enabler
:
:
Enabler
(
int
threads
)
{
SkASSERT
(
ThreadPool
:
:
gGlobal
=
=
nullptr
)
;
if
(
threads
!
=
0
)
{
ThreadPool
:
:
gGlobal
=
new
ThreadPool
(
threads
)
;
}
}
SkTaskGroup
:
:
Enabler
:
:
~
Enabler
(
)
{
delete
ThreadPool
:
:
gGlobal
;
}
SkTaskGroup
:
:
SkTaskGroup
(
)
:
fPending
(
0
)
{
}
void
SkTaskGroup
:
:
wait
(
)
{
ThreadPool
:
:
Wait
(
&
fPending
)
;
}
void
SkTaskGroup
:
:
add
(
SkRunnable
*
task
)
{
ThreadPool
:
:
Add
(
task
&
fPending
)
;
}
void
SkTaskGroup
:
:
add
(
void
(
*
fn
)
(
void
*
)
void
*
arg
)
{
ThreadPool
:
:
Add
(
fn
arg
&
fPending
)
;
}
void
SkTaskGroup
:
:
batch
(
void
(
*
fn
)
(
void
*
)
void
*
args
int
N
size_t
stride
)
{
ThreadPool
:
:
Batch
(
fn
args
N
stride
&
fPending
)
;
}
int
sk_parallel_for_thread_count
(
)
{
if
(
ThreadPool
:
:
gGlobal
!
=
nullptr
)
{
return
ThreadPool
:
:
gGlobal
-
>
fThreads
.
count
(
)
;
}
return
0
;
}
