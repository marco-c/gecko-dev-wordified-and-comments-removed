#
include
<
stddef
.
h
>
#
include
<
stdlib
.
h
>
#
include
<
string
.
h
>
#
include
<
float
.
h
>
#
include
<
stdio
.
h
>
#
include
"
common
/
cl
/
assert_cl
.
h
"
#
include
"
context
.
h
"
#
include
"
handle
.
h
"
#
include
"
grid
.
h
"
#
include
"
path
.
h
"
#
include
"
path_builder
.
h
"
#
include
"
config_cl
.
h
"
#
include
"
export_cl_12
.
h
"
#
include
"
runtime_cl_12
.
h
"
#
include
"
path_builder_cl_12
.
h
"
struct
skc_subbuffer_blocks
{
cl_mem
device
;
void
*
host
;
}
;
struct
skc_subbuffer_cmds
{
cl_mem
device
;
void
*
host
;
cl_event
map
;
}
;
typedef
skc_uint
skc_ringdex_t
;
union
skc_ringdex_expand
{
div_t
qr
;
struct
{
#
ifndef
SKC_DIV_REM_BEFORE_QUOT
skc_uint
subbuf
;
skc_uint
block
;
#
else
skc_uint
block
;
skc_uint
subbuf
;
#
endif
}
;
}
;
struct
skc_release_record
{
struct
skc_path_builder_impl
*
impl
;
skc_grid_t
grid
;
skc_uint
from
;
skc_uint
to
;
}
;
struct
skc_path_builder_impl
{
struct
skc_path_builder
*
path_builder
;
struct
skc_runtime
*
runtime
;
cl_command_queue
cq
;
struct
{
cl_kernel
alloc
;
cl_kernel
copy
;
}
kernels
;
struct
{
skc_uint
subbufs
;
struct
{
skc_uint
buffer
;
skc_uint
subbuf
;
}
blocks_per
;
}
ring
;
struct
{
cl_mem
buffer
;
struct
skc_subbuffer_blocks
*
subbufs
;
}
blocks
;
struct
{
cl_mem
buffer
;
struct
skc_subbuffer_cmds
*
subbufs
;
}
cmds
;
struct
{
struct
skc_release_record
*
records
;
skc_path_t
*
paths
;
}
release
;
cl_mem
reads
;
struct
{
skc_uint
rolling
;
skc_ringdex_t
from
;
skc_ringdex_t
to
;
}
prev
;
struct
{
skc_ringdex_t
from
;
skc_ringdex_t
to
;
}
curr
;
struct
{
struct
skc_path_head
*
head
;
struct
skc_path_node
*
node
;
struct
{
skc_uint
rolling
;
union
skc_tagged_block_id
*
next
;
skc_uint
rem
;
}
ids
;
struct
{
skc_uint
rem
;
skc_uint
rolling
;
float
*
next
;
skc_uint
idx
;
}
subblocks
;
struct
{
skc_uint
one
;
skc_uint
next
;
}
rolling
;
skc_ringdex_t
to
;
}
wip
;
}
;
static
union
skc_ringdex_expand
skc_ringdex_expand
(
struct
skc_path_builder_impl
*
const
impl
skc_ringdex_t
const
ringdex
)
{
return
(
union
skc_ringdex_expand
)
{
.
qr
=
div
(
ringdex
impl
-
>
ring
.
blocks_per
.
subbuf
)
}
;
}
static
void
skc_ringdex_wip_to_block_inc
(
struct
skc_path_builder_impl
*
const
impl
)
{
#
if
1
impl
-
>
wip
.
to
=
(
impl
-
>
wip
.
to
+
1
)
%
impl
-
>
ring
.
blocks_per
.
buffer
;
#
else
impl
-
>
wip
.
to
-
=
(
impl
-
>
wip
.
to
<
impl
-
>
ring
.
blocks_per
.
buffer
)
?
-
1
:
impl
-
>
wip
.
to
;
#
endif
assert
(
impl
-
>
wip
.
to
!
=
impl
-
>
curr
.
from
)
;
}
static
skc_ringdex_t
skc_ringdex_span
(
struct
skc_path_builder_impl
*
const
impl
skc_ringdex_t
const
from
skc_ringdex_t
const
to
)
{
return
(
to
-
from
)
%
impl
-
>
ring
.
blocks_per
.
buffer
;
}
static
void
skc_ringdex_wip_to_subbuf_inc
(
struct
skc_path_builder_impl
*
const
impl
)
{
union
skc_ringdex_expand
const
to
=
skc_ringdex_expand
(
impl
impl
-
>
wip
.
to
)
;
if
(
to
.
block
=
=
0
)
return
;
skc_uint
const
new_subbuf
=
(
to
.
subbuf
+
1
)
%
impl
-
>
ring
.
subbufs
;
impl
-
>
wip
.
to
=
new_subbuf
*
impl
-
>
ring
.
blocks_per
.
subbuf
;
}
static
skc_bool
skc_ringdex_curr_is_equal
(
struct
skc_path_builder_impl
*
const
impl
)
{
return
impl
-
>
curr
.
from
=
=
impl
-
>
curr
.
to
;
}
static
skc_bool
skc_ringdex_prev_is_equal
(
struct
skc_path_builder_impl
*
const
impl
)
{
return
impl
-
>
prev
.
from
=
=
impl
-
>
prev
.
to
;
}
static
skc_uint
skc_ringdex_dont_map_last
(
struct
skc_path_builder_impl
*
const
impl
skc_uint
const
to_block
)
{
return
!
(
(
impl
-
>
wip
.
to
=
=
impl
-
>
curr
.
to
)
|
|
(
to_block
=
=
0
)
)
;
}
static
struct
skc_release_record
*
skc_release_curr
(
struct
skc_path_builder_impl
*
const
impl
)
{
union
skc_ringdex_expand
curr_from
=
skc_ringdex_expand
(
impl
impl
-
>
curr
.
from
)
;
return
impl
-
>
release
.
records
+
curr_from
.
subbuf
;
}
static
void
skc_path_builder_pfn_begin
(
struct
skc_path_builder_impl
*
const
impl
)
{
impl
-
>
wip
.
head
-
>
header
=
(
union
skc_path_header
)
{
.
handle
=
0
.
blocks
=
0
.
nodes
=
0
.
prims
=
0
}
;
impl
-
>
wip
.
head
-
>
bounds
=
(
union
skc_path_bounds
)
{
+
FLT_MIN
+
FLT_MIN
-
FLT_MIN
-
FLT_MIN
}
;
impl
-
>
wip
.
ids
.
next
=
impl
-
>
wip
.
head
-
>
tag_ids
;
impl
-
>
wip
.
ids
.
rem
=
impl
-
>
runtime
-
>
config
-
>
block
.
words
-
SKC_PATH_HEAD_WORDS
;
impl
-
>
wip
.
subblocks
.
rem
=
0
;
}
static
void
skc_path_builder_impl_finalize_node
(
struct
skc_path_builder_impl
*
const
impl
)
{
#
if
1
while
(
impl
-
>
wip
.
ids
.
rem
>
0
)
{
impl
-
>
wip
.
ids
.
rem
-
=
1
;
impl
-
>
wip
.
ids
.
next
-
>
u32
=
SKC_TAGGED_BLOCK_ID_INVALID
;
impl
-
>
wip
.
ids
.
next
+
=
1
;
}
#
else
memset
(
&
impl
-
>
wip
.
ids
.
next
-
>
u32
SKC_TAGGED_BLOCK_ID_INVALID
sizeof
(
impl
-
>
wip
.
ids
.
next
-
>
u32
)
*
impl
-
>
wip
.
ids
.
rem
)
;
impl
-
>
wip
.
ids
.
next
+
=
impl
-
>
wip
.
ids
.
rem
;
impl
-
>
wip
.
ids
.
rem
=
0
;
#
endif
}
static
void
skc_zero_float
(
skc_float
*
p
skc_uint
rem
)
{
memset
(
p
0
sizeof
(
*
p
)
*
rem
)
;
}
static
void
skc_path_builder_finalize_subblocks
(
struct
skc_path_builder
*
const
path_builder
)
{
#
if
0
while
(
path_builder
-
>
line
.
rem
>
0
)
{
-
-
path_builder
-
>
line
.
rem
;
*
path_builder
-
>
line
.
coords
[
0
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
1
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
2
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
3
]
+
+
=
0
.
0f
;
}
while
(
path_builder
-
>
quad
.
rem
>
0
)
{
-
-
path_builder
-
>
quad
.
rem
;
*
path_builder
-
>
line
.
coords
[
0
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
1
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
2
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
3
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
4
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
5
]
+
+
=
0
.
0f
;
}
while
(
path_builder
-
>
cubic
.
rem
>
0
)
{
-
-
path_builder
-
>
cubic
.
rem
;
*
path_builder
-
>
line
.
coords
[
0
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
1
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
2
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
3
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
4
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
5
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
6
]
+
+
=
0
.
0f
;
*
path_builder
-
>
line
.
coords
[
7
]
+
+
=
0
.
0f
;
}
#
else
if
(
path_builder
-
>
line
.
rem
>
0
)
{
skc_zero_float
(
path_builder
-
>
line
.
coords
[
0
]
path_builder
-
>
line
.
rem
)
;
skc_zero_float
(
path_builder
-
>
line
.
coords
[
1
]
path_builder
-
>
line
.
rem
)
;
skc_zero_float
(
path_builder
-
>
line
.
coords
[
2
]
path_builder
-
>
line
.
rem
)
;
skc_zero_float
(
path_builder
-
>
line
.
coords
[
3
]
path_builder
-
>
line
.
rem
)
;
path_builder
-
>
line
.
rem
=
0
;
}
if
(
path_builder
-
>
quad
.
rem
>
0
)
{
skc_zero_float
(
path_builder
-
>
quad
.
coords
[
0
]
path_builder
-
>
quad
.
rem
)
;
skc_zero_float
(
path_builder
-
>
quad
.
coords
[
1
]
path_builder
-
>
quad
.
rem
)
;
skc_zero_float
(
path_builder
-
>
quad
.
coords
[
2
]
path_builder
-
>
quad
.
rem
)
;
skc_zero_float
(
path_builder
-
>
quad
.
coords
[
3
]
path_builder
-
>
quad
.
rem
)
;
skc_zero_float
(
path_builder
-
>
quad
.
coords
[
4
]
path_builder
-
>
quad
.
rem
)
;
skc_zero_float
(
path_builder
-
>
quad
.
coords
[
5
]
path_builder
-
>
quad
.
rem
)
;
path_builder
-
>
quad
.
rem
=
0
;
}
if
(
path_builder
-
>
cubic
.
rem
>
0
)
{
skc_zero_float
(
path_builder
-
>
cubic
.
coords
[
0
]
path_builder
-
>
cubic
.
rem
)
;
skc_zero_float
(
path_builder
-
>
cubic
.
coords
[
1
]
path_builder
-
>
cubic
.
rem
)
;
skc_zero_float
(
path_builder
-
>
cubic
.
coords
[
2
]
path_builder
-
>
cubic
.
rem
)
;
skc_zero_float
(
path_builder
-
>
cubic
.
coords
[
3
]
path_builder
-
>
cubic
.
rem
)
;
skc_zero_float
(
path_builder
-
>
cubic
.
coords
[
4
]
path_builder
-
>
cubic
.
rem
)
;
skc_zero_float
(
path_builder
-
>
cubic
.
coords
[
5
]
path_builder
-
>
cubic
.
rem
)
;
skc_zero_float
(
path_builder
-
>
cubic
.
coords
[
6
]
path_builder
-
>
cubic
.
rem
)
;
skc_zero_float
(
path_builder
-
>
cubic
.
coords
[
7
]
path_builder
-
>
cubic
.
rem
)
;
path_builder
-
>
cubic
.
rem
=
0
;
}
#
endif
}
static
void
skc_path_builder_impl_unmap
(
struct
skc_path_builder_impl
*
const
impl
skc_uint
from
skc_uint
to
)
{
to
=
to
%
impl
-
>
ring
.
subbufs
;
#
if
0
fprintf
(
stderr
"
unmap
:
[
%
2u
%
2u
)
\
n
"
from
to
)
;
#
endif
while
(
from
!
=
to
)
{
from
=
from
%
impl
-
>
ring
.
subbufs
;
struct
skc_subbuffer_blocks
*
const
blocks
=
impl
-
>
blocks
.
subbufs
+
from
;
struct
skc_subbuffer_cmds
*
const
cmds
=
impl
-
>
cmds
.
subbufs
+
from
;
cl
(
EnqueueUnmapMemObject
(
impl
-
>
cq
blocks
-
>
device
blocks
-
>
host
0
NULL
NULL
)
)
;
cl
(
EnqueueUnmapMemObject
(
impl
-
>
cq
cmds
-
>
device
cmds
-
>
host
0
NULL
NULL
)
)
;
from
=
+
+
from
%
impl
-
>
ring
.
subbufs
;
}
}
static
void
skc_path_builder_impl_map
(
struct
skc_path_builder_impl
*
const
impl
skc_uint
from
skc_uint
to
)
{
to
=
to
%
impl
-
>
ring
.
subbufs
;
#
if
0
fprintf
(
stderr
"
map
:
[
%
2u
%
2u
)
\
n
"
from
to
)
;
#
endif
while
(
from
!
=
to
)
{
cl_int
cl_err
;
struct
skc_subbuffer_blocks
*
const
blocks
=
impl
-
>
blocks
.
subbufs
+
from
;
struct
skc_subbuffer_cmds
*
const
cmds
=
impl
-
>
cmds
.
subbufs
+
from
;
blocks
-
>
host
=
clEnqueueMapBuffer
(
impl
-
>
cq
blocks
-
>
device
CL_FALSE
CL_MAP_WRITE_INVALIDATE_REGION
0
impl
-
>
runtime
-
>
config
-
>
paths_copy
.
block
.
subbuf
0
NULL
NULL
&
cl_err
)
;
cl_ok
(
cl_err
)
;
cl
(
ReleaseEvent
(
cmds
-
>
map
)
)
;
cmds
-
>
host
=
clEnqueueMapBuffer
(
impl
-
>
cq
cmds
-
>
device
CL_FALSE
CL_MAP_WRITE_INVALIDATE_REGION
0
impl
-
>
runtime
-
>
config
-
>
paths_copy
.
command
.
subbuf
0
NULL
&
cmds
-
>
map
&
cl_err
)
;
cl_ok
(
cl_err
)
;
from
=
+
+
from
%
impl
-
>
ring
.
subbufs
;
}
}
static
void
skc_path_builder_release_dispose
(
struct
skc_release_record
*
const
release
struct
skc_path_builder_impl
*
const
impl
)
{
struct
skc_runtime
*
runtime
=
impl
-
>
runtime
;
if
(
release
-
>
from
<
=
release
-
>
to
)
{
skc_path_t
const
*
paths
=
impl
-
>
release
.
paths
+
release
-
>
from
;
skc_uint
count
=
release
-
>
to
-
release
-
>
from
;
skc_grid_deps_unmap
(
runtime
-
>
deps
paths
count
)
;
skc_runtime_path_device_release
(
runtime
paths
count
)
;
}
else
{
skc_path_t
const
*
paths_lo
=
impl
-
>
release
.
paths
+
release
-
>
from
;
skc_uint
count_lo
=
impl
-
>
ring
.
blocks_per
.
buffer
-
release
-
>
from
;
skc_grid_deps_unmap
(
runtime
-
>
deps
paths_lo
count_lo
)
;
skc_runtime_path_device_release
(
runtime
paths_lo
count_lo
)
;
skc_grid_deps_unmap
(
runtime
-
>
deps
impl
-
>
release
.
paths
release
-
>
to
)
;
skc_runtime_path_device_release
(
runtime
impl
-
>
release
.
paths
release
-
>
to
)
;
}
release
-
>
to
=
release
-
>
from
;
}
static
void
skc_path_builder_grid_pfn_dispose
(
skc_grid_t
const
grid
)
{
struct
skc_release_record
*
const
release
=
skc_grid_get_data
(
grid
)
;
struct
skc_path_builder_impl
*
const
impl
=
release
-
>
impl
;
skc_path_builder_release_dispose
(
release
impl
)
;
}
static
void
skc_path_builder_complete
(
skc_grid_t
grid
)
{
skc_grid_complete
(
grid
)
;
}
static
void
skc_path_builder_paths_copy_cb
(
cl_event
event
cl_int
status
skc_grid_t
grid
)
{
SKC_CL_CB
(
status
)
;
struct
skc_release_record
*
const
release
=
skc_grid_get_data
(
grid
)
;
SKC_SCHEDULER_SCHEDULE
(
release
-
>
impl
-
>
runtime
-
>
scheduler
skc_path_builder_complete
grid
)
;
}
static
void
skc_path_builder_grid_pfn_waiting
(
skc_grid_t
const
grid
)
{
struct
skc_release_record
*
const
release
=
skc_grid_get_data
(
grid
)
;
struct
skc_path_builder_impl
*
const
impl
=
release
-
>
impl
;
skc_path_builder_finalize_subblocks
(
impl
-
>
path_builder
)
;
union
skc_ringdex_expand
curr_from
=
skc_ringdex_expand
(
impl
impl
-
>
curr
.
from
)
;
union
skc_ringdex_expand
curr_to
=
skc_ringdex_expand
(
impl
impl
-
>
curr
.
to
)
;
skc_uint
const
is_partial
=
curr_to
.
block
>
0
;
skc_uint
const
unmap_to
=
curr_to
.
subbuf
+
is_partial
;
skc_path_builder_impl_unmap
(
impl
curr_from
.
subbuf
unmap_to
)
;
skc_uint
const
pb_prev_span
=
skc_ringdex_span
(
impl
impl
-
>
prev
.
from
impl
-
>
prev
.
to
)
;
skc_uint
const
pb_curr_span
=
skc_ringdex_span
(
impl
impl
-
>
curr
.
from
impl
-
>
curr
.
to
)
;
skc_uint
const
pb_cmds
=
pb_prev_span
+
pb_curr_span
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
alloc
0
SKC_CL_ARG
(
impl
-
>
runtime
-
>
block_pool
.
atomics
.
drw
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
alloc
1
SKC_CL_ARG
(
impl
-
>
reads
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
alloc
2
SKC_CL_ARG
(
curr_from
.
subbuf
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
alloc
3
SKC_CL_ARG
(
pb_cmds
)
)
)
;
skc_device_enqueue_kernel
(
impl
-
>
runtime
-
>
device
SKC_DEVICE_KERNEL_ID_PATHS_ALLOC
impl
-
>
cq
impl
-
>
kernels
.
alloc
1
0
NULL
NULL
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
copy
0
SKC_CL_ARG
(
impl
-
>
runtime
-
>
handle_pool
.
map
.
drw
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
copy
1
SKC_CL_ARG
(
impl
-
>
runtime
-
>
block_pool
.
ids
.
drw
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
copy
2
SKC_CL_ARG
(
impl
-
>
runtime
-
>
block_pool
.
blocks
.
drw
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
copy
3
SKC_CL_ARG
(
impl
-
>
runtime
-
>
block_pool
.
size
-
>
ring_mask
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
copy
4
SKC_CL_ARG
(
impl
-
>
reads
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
copy
5
SKC_CL_ARG
(
curr_from
.
subbuf
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
copy
6
SKC_CL_ARG
(
impl
-
>
cmds
.
buffer
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
copy
7
SKC_CL_ARG
(
impl
-
>
blocks
.
buffer
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
copy
8
SKC_CL_ARG
(
impl
-
>
ring
.
blocks_per
.
buffer
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
copy
9
SKC_CL_ARG
(
impl
-
>
prev
.
rolling
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
copy
10
SKC_CL_ARG
(
impl
-
>
prev
.
from
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
copy
11
SKC_CL_ARG
(
pb_prev_span
)
)
)
;
cl
(
SetKernelArg
(
impl
-
>
kernels
.
copy
12
SKC_CL_ARG
(
impl
-
>
curr
.
from
)
)
)
;
cl_event
complete
;
skc_device_enqueue_kernel
(
impl
-
>
runtime
-
>
device
SKC_DEVICE_KERNEL_ID_PATHS_COPY
impl
-
>
cq
impl
-
>
kernels
.
copy
pb_cmds
0
NULL
&
complete
)
;
cl
(
SetEventCallback
(
complete
CL_COMPLETE
skc_path_builder_paths_copy_cb
grid
)
)
;
cl
(
ReleaseEvent
(
complete
)
)
;
union
skc_ringdex_expand
const
prev_from
=
skc_ringdex_expand
(
impl
impl
-
>
prev
.
from
)
;
skc_uint
const
no_wip
=
impl
-
>
curr
.
to
=
=
impl
-
>
wip
.
to
;
skc_uint
map_to
=
curr_to
.
subbuf
+
(
is_partial
&
&
no_wip
)
;
skc_path_builder_impl_map
(
impl
prev_from
.
subbuf
map_to
)
;
cl
(
Flush
(
impl
-
>
cq
)
)
;
impl
-
>
prev
.
rolling
=
impl
-
>
wip
.
rolling
.
next
;
if
(
no_wip
)
{
skc_ringdex_wip_to_subbuf_inc
(
impl
)
;
impl
-
>
prev
.
from
=
impl
-
>
prev
.
to
=
impl
-
>
wip
.
to
;
impl
-
>
curr
.
from
=
impl
-
>
curr
.
to
=
impl
-
>
wip
.
to
;
}
else
{
impl
-
>
prev
.
from
=
impl
-
>
curr
.
to
;
impl
-
>
prev
.
to
=
impl
-
>
wip
.
to
;
skc_ringdex_wip_to_subbuf_inc
(
impl
)
;
impl
-
>
curr
.
from
=
impl
-
>
wip
.
to
;
impl
-
>
curr
.
to
=
impl
-
>
wip
.
to
;
}
}
static
void
skc_path_builder_impl_acquire_subbuffer
(
struct
skc_path_builder_impl
*
const
impl
skc_uint
const
subbuf
)
{
struct
skc_subbuffer_cmds
*
const
sc
=
impl
-
>
cmds
.
subbufs
+
subbuf
;
struct
skc_release_record
*
const
release
=
impl
-
>
release
.
records
+
subbuf
;
struct
skc_scheduler
*
const
scheduler
=
impl
-
>
runtime
-
>
scheduler
;
SKC_SCHEDULER_WAIT_WHILE
(
scheduler
release
-
>
from
!
=
release
-
>
to
)
;
skc_scheduler_yield
(
scheduler
)
;
cl
(
WaitForEvents
(
1
&
sc
-
>
map
)
)
;
}
static
union
skc_ringdex_expand
skc_path_builder_impl_acquire_block
(
struct
skc_path_builder_impl
*
const
impl
)
{
union
skc_ringdex_expand
const
to
=
skc_ringdex_expand
(
impl
impl
-
>
wip
.
to
)
;
if
(
to
.
block
=
=
0
)
{
skc_path_builder_impl_acquire_subbuffer
(
impl
to
.
subbuf
)
;
}
skc_ringdex_wip_to_block_inc
(
impl
)
;
return
to
;
}
static
skc_uint
skc_rolling_block
(
skc_uint
const
rolling
skc_uint
const
tag
)
{
return
rolling
|
tag
;
}
static
skc_uint
skc_rolling_subblock
(
skc_uint
const
rolling
skc_uint
const
subblock
skc_uint
const
tag
)
{
return
rolling
|
(
subblock
<
<
SKC_TAGGED_BLOCK_ID_BITS_TAG
)
|
tag
;
}
static
void
skc_rolling_inc
(
struct
skc_path_builder_impl
*
const
impl
)
{
impl
-
>
wip
.
rolling
.
next
+
=
impl
-
>
wip
.
rolling
.
one
;
}
static
void
*
skc_path_builder_impl_new_command
(
struct
skc_path_builder_impl
*
const
impl
skc_uint
const
rolling
skc_cmd_paths_copy_tag
const
tag
)
{
impl
-
>
wip
.
head
-
>
header
.
blocks
+
=
1
;
union
skc_ringdex_expand
const
to
=
skc_path_builder_impl_acquire_block
(
impl
)
;
union
skc_tagged_block_id
*
const
cmds_subbuf
=
impl
-
>
cmds
.
subbufs
[
to
.
subbuf
]
.
host
;
cmds_subbuf
[
to
.
block
]
.
u32
=
skc_rolling_block
(
rolling
tag
)
;
#
if
0
cmds_subbuf
[
to
.
block
]
.
u32
=
skc_rolling_block
(
impl
-
>
wip
.
rolling
.
next
tag
)
;
skc_rolling_inc
(
impl
)
;
#
endif
float
*
const
blocks_subbuf
=
impl
-
>
blocks
.
subbufs
[
to
.
subbuf
]
.
host
;
return
blocks_subbuf
+
(
to
.
block
*
impl
-
>
runtime
-
>
config
-
>
block
.
words
)
;
}
static
void
skc_path_builder_impl_flush_node
(
struct
skc_path_builder_impl
*
const
impl
)
{
void
*
const
block
=
skc_path_builder_impl_new_command
(
impl
impl
-
>
wip
.
ids
.
rolling
SKC_CMD_PATHS_COPY_TAG_NODE
)
;
memcpy
(
block
impl
-
>
wip
.
node
impl
-
>
runtime
-
>
config
-
>
block
.
bytes
)
;
}
static
void
skc_path_builder_impl_flush_head
(
struct
skc_path_builder_impl
*
const
impl
)
{
void
*
const
block
=
skc_path_builder_impl_new_command
(
impl
impl
-
>
wip
.
rolling
.
next
SKC_CMD_PATHS_COPY_TAG_HEAD
)
;
memcpy
(
block
impl
-
>
wip
.
head
impl
-
>
runtime
-
>
config
-
>
block
.
bytes
)
;
skc_rolling_inc
(
impl
)
;
impl
-
>
curr
.
to
=
impl
-
>
wip
.
to
;
}
static
void
skc_path_builder_impl_new_node_block
(
struct
skc_path_builder_impl
*
const
impl
)
{
impl
-
>
wip
.
ids
.
next
-
>
u32
=
skc_rolling_block
(
impl
-
>
wip
.
rolling
.
next
SKC_BLOCK_ID_TAG_PATH_NEXT
)
;
if
(
impl
-
>
wip
.
head
-
>
header
.
nodes
>
0
)
skc_path_builder_impl_flush_node
(
impl
)
;
impl
-
>
wip
.
head
-
>
header
.
nodes
+
=
1
;
impl
-
>
wip
.
ids
.
rolling
=
impl
-
>
wip
.
rolling
.
next
;
skc_rolling_inc
(
impl
)
;
impl
-
>
wip
.
ids
.
next
=
impl
-
>
wip
.
node
-
>
tag_ids
;
impl
-
>
wip
.
ids
.
rem
=
impl
-
>
runtime
-
>
config
-
>
block
.
words
;
}
static
void
skc_path_builder_impl_new_segs_block
(
struct
skc_path_builder_impl
*
const
impl
)
{
impl
-
>
wip
.
subblocks
.
rem
=
impl
-
>
runtime
-
>
config
-
>
block
.
subblocks
;
impl
-
>
wip
.
subblocks
.
rolling
=
impl
-
>
wip
.
rolling
.
next
;
impl
-
>
wip
.
subblocks
.
next
=
skc_path_builder_impl_new_command
(
impl
impl
-
>
wip
.
rolling
.
next
SKC_CMD_PATHS_COPY_TAG_SEGS
)
;
impl
-
>
wip
.
subblocks
.
idx
=
0
;
skc_rolling_inc
(
impl
)
;
}
static
void
skc_path_builder_impl_acquire_subblocks
(
struct
skc_path_builder_impl
*
const
impl
skc_block_id_tag
tag
skc_uint
vertices
float
*
*
subblocks
)
{
while
(
true
)
{
if
(
impl
-
>
wip
.
ids
.
rem
=
=
1
)
skc_path_builder_impl_new_node_block
(
impl
)
;
if
(
impl
-
>
wip
.
subblocks
.
rem
=
=
0
)
skc_path_builder_impl_new_segs_block
(
impl
)
;
impl
-
>
wip
.
ids
.
next
-
>
u32
=
skc_rolling_subblock
(
impl
-
>
wip
.
subblocks
.
rolling
impl
-
>
wip
.
subblocks
.
idx
tag
)
;
impl
-
>
wip
.
ids
.
next
+
=
1
;
impl
-
>
wip
.
ids
.
rem
-
=
1
;
skc_uint
rem
=
min
(
vertices
impl
-
>
wip
.
subblocks
.
rem
)
;
vertices
-
=
rem
;
impl
-
>
wip
.
subblocks
.
rem
-
=
rem
;
impl
-
>
wip
.
subblocks
.
idx
+
=
rem
;
do
{
*
subblocks
+
+
=
impl
-
>
wip
.
subblocks
.
next
;
impl
-
>
wip
.
subblocks
.
next
+
=
impl
-
>
runtime
-
>
config
-
>
subblock
.
words
;
}
while
(
-
-
rem
>
0
)
;
if
(
vertices
=
=
0
)
break
;
tag
=
SKC_BLOCK_ID_TAG_PATH_NEXT
;
}
}
static
void
skc_path_builder_pfn_end
(
struct
skc_path_builder_impl
*
const
impl
skc_path_t
*
const
path
)
{
skc_path_builder_finalize_subblocks
(
impl
-
>
path_builder
)
;
skc_path_builder_impl_finalize_node
(
impl
)
;
if
(
impl
-
>
wip
.
head
-
>
header
.
nodes
>
=
1
)
skc_path_builder_impl_flush_node
(
impl
)
;
*
path
=
skc_runtime_handle_device_acquire
(
impl
-
>
runtime
)
;
impl
-
>
wip
.
head
-
>
header
.
handle
=
*
path
;
skc_path_builder_impl_flush_head
(
impl
)
;
struct
skc_release_record
*
const
release
=
skc_release_curr
(
impl
)
;
if
(
release
-
>
grid
=
=
NULL
)
{
release
-
>
grid
=
SKC_GRID_DEPS_ATTACH
(
impl
-
>
runtime
-
>
deps
&
release
-
>
grid
release
skc_path_builder_grid_pfn_waiting
NULL
skc_path_builder_grid_pfn_dispose
)
;
}
skc_grid_map
(
release
-
>
grid
*
path
)
;
impl
-
>
release
.
paths
[
release
-
>
to
]
=
*
path
;
release
-
>
to
=
(
release
-
>
to
+
1
)
%
impl
-
>
ring
.
blocks_per
.
buffer
;
*
path
|
=
SKC_TYPED_HANDLE_TYPE_IS_PATH
;
#
if
1
{
union
skc_ringdex_expand
const
curr_from
=
skc_ringdex_expand
(
impl
impl
-
>
curr
.
from
)
;
union
skc_ringdex_expand
const
curr_to
=
skc_ringdex_expand
(
impl
impl
-
>
curr
.
to
)
;
if
(
curr_from
.
subbuf
!
=
curr_to
.
subbuf
)
{
skc_grid_start
(
release
-
>
grid
)
;
}
}
#
endif
}
static
void
skc_path_builder_pfn_new_line
(
struct
skc_path_builder_impl
*
const
impl
)
{
skc_path_builder_impl_acquire_subblocks
(
impl
SKC_BLOCK_ID_TAG_PATH_LINE
4
impl
-
>
path_builder
-
>
line
.
coords
)
;
impl
-
>
wip
.
head
-
>
header
.
prims
+
=
1
;
impl
-
>
path_builder
-
>
line
.
rem
=
impl
-
>
runtime
-
>
config
-
>
subblock
.
words
;
}
static
void
skc_path_builder_pfn_new_quad
(
struct
skc_path_builder_impl
*
const
impl
)
{
skc_path_builder_impl_acquire_subblocks
(
impl
SKC_BLOCK_ID_TAG_PATH_QUAD
6
impl
-
>
path_builder
-
>
quad
.
coords
)
;
impl
-
>
wip
.
head
-
>
header
.
prims
+
=
1
;
impl
-
>
path_builder
-
>
quad
.
rem
=
impl
-
>
runtime
-
>
config
-
>
subblock
.
words
;
}
static
void
skc_path_builder_pfn_new_cubic
(
struct
skc_path_builder_impl
*
const
impl
)
{
skc_path_builder_impl_acquire_subblocks
(
impl
SKC_BLOCK_ID_TAG_PATH_CUBIC
8
impl
-
>
path_builder
-
>
cubic
.
coords
)
;
impl
-
>
wip
.
head
-
>
header
.
prims
+
=
1
;
impl
-
>
path_builder
-
>
cubic
.
rem
=
impl
-
>
runtime
-
>
config
-
>
subblock
.
words
;
}
static
void
skc_path_builder_pfn_release
(
struct
skc_path_builder_impl
*
const
impl
)
{
if
(
-
-
impl
-
>
path_builder
-
>
refcount
!
=
0
)
return
;
struct
skc_runtime
*
const
runtime
=
impl
-
>
runtime
;
skc_runtime_host_perm_free
(
impl
-
>
runtime
impl
-
>
path_builder
)
;
skc_runtime_release_cq_in_order
(
runtime
impl
-
>
cq
)
;
cl
(
ReleaseKernel
(
impl
-
>
kernels
.
alloc
)
)
;
cl
(
ReleaseKernel
(
impl
-
>
kernels
.
copy
)
)
;
cl
(
ReleaseMemObject
(
impl
-
>
blocks
.
buffer
)
)
;
skc_runtime_host_perm_free
(
runtime
impl
-
>
blocks
.
subbufs
)
;
cl
(
ReleaseMemObject
(
impl
-
>
cmds
.
buffer
)
)
;
skc_runtime_host_perm_free
(
runtime
impl
-
>
cmds
.
subbufs
)
;
skc_runtime_host_perm_free
(
runtime
impl
-
>
release
.
records
)
;
skc_runtime_host_perm_free
(
runtime
impl
-
>
release
.
paths
)
;
skc_runtime_host_perm_free
(
runtime
impl
-
>
wip
.
head
)
;
skc_runtime_host_perm_free
(
runtime
impl
-
>
wip
.
node
)
;
cl
(
ReleaseMemObject
(
impl
-
>
reads
)
)
;
skc_runtime_host_perm_free
(
impl
-
>
runtime
impl
)
;
}
skc_err
skc_path_builder_cl_12_create
(
struct
skc_context
*
const
context
struct
skc_path_builder
*
*
const
path_builder
)
{
struct
skc_runtime
*
const
runtime
=
context
-
>
runtime
;
(
*
path_builder
)
=
skc_runtime_host_perm_alloc
(
runtime
SKC_MEM_FLAGS_READ_WRITE
sizeof
(
*
*
path_builder
)
)
;
SKC_ASSERT_STATE_INIT
(
(
*
path_builder
)
SKC_PATH_BUILDER_STATE_READY
)
;
(
*
path_builder
)
-
>
context
=
context
;
(
*
path_builder
)
-
>
begin
=
skc_path_builder_pfn_begin
;
(
*
path_builder
)
-
>
end
=
skc_path_builder_pfn_end
;
(
*
path_builder
)
-
>
new_line
=
skc_path_builder_pfn_new_line
;
(
*
path_builder
)
-
>
new_quad
=
skc_path_builder_pfn_new_quad
;
(
*
path_builder
)
-
>
new_cubic
=
skc_path_builder_pfn_new_cubic
;
(
*
path_builder
)
-
>
release
=
skc_path_builder_pfn_release
;
(
*
path_builder
)
-
>
line
.
rem
=
0
;
(
*
path_builder
)
-
>
quad
.
rem
=
0
;
(
*
path_builder
)
-
>
cubic
.
rem
=
0
;
(
*
path_builder
)
-
>
refcount
=
1
;
struct
skc_path_builder_impl
*
const
impl
=
skc_runtime_host_perm_alloc
(
runtime
SKC_MEM_FLAGS_READ_WRITE
sizeof
(
*
impl
)
)
;
(
*
path_builder
)
-
>
impl
=
impl
;
impl
-
>
path_builder
=
*
path_builder
;
impl
-
>
runtime
=
runtime
;
impl
-
>
cq
=
skc_runtime_acquire_cq_in_order
(
runtime
)
;
impl
-
>
kernels
.
alloc
=
skc_device_acquire_kernel
(
runtime
-
>
device
SKC_DEVICE_KERNEL_ID_PATHS_ALLOC
)
;
impl
-
>
kernels
.
copy
=
skc_device_acquire_kernel
(
runtime
-
>
device
SKC_DEVICE_KERNEL_ID_PATHS_COPY
)
;
struct
skc_config
const
*
const
config
=
runtime
-
>
config
;
impl
-
>
ring
.
subbufs
=
config
-
>
paths_copy
.
buffer
.
count
;
impl
-
>
ring
.
blocks_per
.
buffer
=
config
-
>
paths_copy
.
subbuf
.
count
*
config
-
>
paths_copy
.
buffer
.
count
;
impl
-
>
ring
.
blocks_per
.
subbuf
=
config
-
>
paths_copy
.
subbuf
.
count
;
cl_int
cl_err
;
impl
-
>
blocks
.
buffer
=
clCreateBuffer
(
runtime
-
>
cl
.
context
CL_MEM_READ_ONLY
|
CL_MEM_ALLOC_HOST_PTR
config
-
>
paths_copy
.
block
.
buffer
NULL
&
cl_err
)
;
cl_ok
(
cl_err
)
;
impl
-
>
blocks
.
subbufs
=
skc_runtime_host_perm_alloc
(
runtime
SKC_MEM_FLAGS_READ_WRITE
impl
-
>
ring
.
subbufs
*
sizeof
(
*
impl
-
>
blocks
.
subbufs
)
)
;
impl
-
>
cmds
.
buffer
=
clCreateBuffer
(
runtime
-
>
cl
.
context
CL_MEM_READ_ONLY
|
CL_MEM_ALLOC_HOST_PTR
config
-
>
paths_copy
.
command
.
buffer
NULL
&
cl_err
)
;
cl_ok
(
cl_err
)
;
impl
-
>
cmds
.
subbufs
=
skc_runtime_host_perm_alloc
(
runtime
SKC_MEM_FLAGS_READ_WRITE
impl
-
>
ring
.
subbufs
*
sizeof
(
*
impl
-
>
cmds
.
subbufs
)
)
;
impl
-
>
release
.
records
=
skc_runtime_host_perm_alloc
(
runtime
SKC_MEM_FLAGS_READ_WRITE
impl
-
>
ring
.
subbufs
*
sizeof
(
*
impl
-
>
release
.
records
)
)
;
impl
-
>
release
.
paths
=
skc_runtime_host_perm_alloc
(
runtime
SKC_MEM_FLAGS_READ_WRITE
impl
-
>
ring
.
blocks_per
.
buffer
*
sizeof
(
*
impl
-
>
release
.
paths
)
)
;
impl
-
>
reads
=
clCreateBuffer
(
runtime
-
>
cl
.
context
CL_MEM_READ_WRITE
|
CL_MEM_HOST_NO_ACCESS
sizeof
(
skc_uint
)
*
impl
-
>
ring
.
subbufs
NULL
&
cl_err
)
;
cl_ok
(
cl_err
)
;
for
(
skc_uint
ii
=
0
;
ii
<
impl
-
>
ring
.
subbufs
;
ii
+
+
)
{
struct
skc_release_record
*
record
=
impl
-
>
release
.
records
+
ii
;
record
-
>
impl
=
impl
;
record
-
>
grid
=
NULL
;
record
-
>
from
=
record
-
>
to
=
ii
*
impl
-
>
ring
.
blocks_per
.
subbuf
;
}
struct
skc_subbuffer_blocks
*
sb
=
impl
-
>
blocks
.
subbufs
;
struct
skc_subbuffer_cmds
*
sc
=
impl
-
>
cmds
.
subbufs
;
cl_buffer_region
rb
=
{
0
config
-
>
paths_copy
.
block
.
subbuf
}
;
cl_buffer_region
rc
=
{
0
config
-
>
paths_copy
.
command
.
subbuf
}
;
for
(
skc_uint
ii
=
0
;
ii
<
config
-
>
paths_copy
.
buffer
.
count
;
ii
+
+
)
{
sb
-
>
device
=
clCreateSubBuffer
(
impl
-
>
blocks
.
buffer
CL_MEM_HOST_WRITE_ONLY
CL_BUFFER_CREATE_TYPE_REGION
&
rb
&
cl_err
)
;
cl_ok
(
cl_err
)
;
sb
-
>
host
=
clEnqueueMapBuffer
(
impl
-
>
cq
sb
-
>
device
CL_FALSE
CL_MAP_WRITE_INVALIDATE_REGION
0
rb
.
size
0
NULL
NULL
&
cl_err
)
;
cl_ok
(
cl_err
)
;
sc
-
>
device
=
clCreateSubBuffer
(
impl
-
>
cmds
.
buffer
CL_MEM_HOST_WRITE_ONLY
CL_BUFFER_CREATE_TYPE_REGION
&
rc
&
cl_err
)
;
cl_ok
(
cl_err
)
;
sc
-
>
host
=
clEnqueueMapBuffer
(
impl
-
>
cq
sc
-
>
device
CL_FALSE
CL_MAP_WRITE_INVALIDATE_REGION
0
rc
.
size
0
NULL
&
sc
-
>
map
&
cl_err
)
;
cl_ok
(
cl_err
)
;
sb
+
=
1
;
sc
+
=
1
;
rb
.
origin
+
=
rb
.
size
;
rc
.
origin
+
=
rc
.
size
;
}
impl
-
>
prev
.
from
=
0
;
impl
-
>
prev
.
to
=
0
;
impl
-
>
prev
.
rolling
=
0
;
impl
-
>
curr
.
from
=
0
;
impl
-
>
curr
.
to
=
0
;
impl
-
>
wip
.
to
=
0
;
impl
-
>
wip
.
head
=
skc_runtime_host_perm_alloc
(
runtime
SKC_MEM_FLAGS_READ_WRITE
config
-
>
block
.
bytes
)
;
impl
-
>
wip
.
node
=
skc_runtime_host_perm_alloc
(
runtime
SKC_MEM_FLAGS_READ_WRITE
config
-
>
block
.
bytes
)
;
impl
-
>
wip
.
rolling
.
one
=
SKC_BLOCK_ID_TAG_COUNT
*
config
-
>
block
.
subblocks
;
impl
-
>
wip
.
rolling
.
next
=
0
;
cl
(
Finish
(
impl
-
>
cq
)
)
;
return
SKC_ERR_SUCCESS
;
}
