#
ifndef
SkAtomics_sync_DEFINED
#
define
SkAtomics_sync_DEFINED
static
inline
void
barrier
(
sk_memory_order
mo
)
{
asm
volatile
(
"
"
:
:
:
"
memory
"
)
;
/
/
Prevents
the
compiler
from
reordering
code
.
#
if
SK_CPU_X86
if
(
sk_memory_order_seq_cst
=
=
mo
)
{
__sync_synchronize
(
)
;
}
#
else
if
(
sk_memory_order_relaxed
!
=
mo
)
{
__sync_synchronize
(
)
;
}
#
endif
}
template
<
typename
T
>
T
sk_atomic_load
(
const
T
*
ptr
sk_memory_order
mo
)
{
T
val
=
*
ptr
;
barrier
(
mo
)
;
return
val
;
}
template
<
typename
T
>
void
sk_atomic_store
(
T
*
ptr
T
val
sk_memory_order
mo
)
{
barrier
(
mo
)
;
*
ptr
=
val
;
}
template
<
typename
T
>
T
sk_atomic_fetch_add
(
T
*
ptr
T
val
sk_memory_order
)
{
return
__sync_fetch_and_add
(
ptr
val
)
;
}
template
<
typename
T
>
T
sk_atomic_fetch_sub
(
T
*
ptr
T
val
sk_memory_order
)
{
return
__sync_fetch_and_sub
(
ptr
val
)
;
}
template
<
typename
T
>
bool
sk_atomic_compare_exchange
(
T
*
ptr
T
*
expected
T
desired
sk_memory_order
sk_memory_order
)
{
T
prev
=
__sync_val_compare_and_swap
(
ptr
*
expected
desired
)
;
if
(
prev
=
=
*
expected
)
{
return
true
;
}
*
expected
=
prev
;
return
false
;
}
template
<
typename
T
>
T
sk_atomic_exchange
(
T
*
ptr
T
val
sk_memory_order
)
{
T
prev
;
do
{
prev
=
sk_atomic_load
(
ptr
)
;
}
while
(
!
sk_atomic_compare_exchange
(
ptr
&
prev
val
)
)
;
return
prev
;
}
#
endif
