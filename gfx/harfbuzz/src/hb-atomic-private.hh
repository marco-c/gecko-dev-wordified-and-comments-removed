#
ifndef
HB_ATOMIC_PRIVATE_HH
#
define
HB_ATOMIC_PRIVATE_HH
#
include
"
hb
-
private
.
hh
"
#
if
defined
(
hb_atomic_int_impl_add
)
\
&
&
defined
(
hb_atomic_ptr_impl_get
)
\
&
&
defined
(
hb_atomic_ptr_impl_cmpexch
)
#
elif
!
defined
(
HB_NO_MT
)
&
&
defined
(
__ATOMIC_CONSUME
)
typedef
int
hb_atomic_int_impl_t
;
#
define
hb_atomic_int_impl_add
(
AI
V
)
__atomic_fetch_add
(
(
AI
)
(
V
)
__ATOMIC_ACQ_REL
)
#
define
hb_atomic_int_impl_set_relaxed
(
AI
V
)
__atomic_store_n
(
(
AI
)
(
V
)
__ATOMIC_RELAXED
)
#
define
hb_atomic_int_impl_get_relaxed
(
AI
)
__atomic_load_n
(
(
AI
)
__ATOMIC_RELAXED
)
#
define
hb_atomic_ptr_impl_get
(
P
)
__atomic_load_n
(
(
P
)
__ATOMIC_CONSUME
)
static
inline
bool
_hb_atomic_ptr_impl_cmplexch
(
const
void
*
*
P
const
void
*
O_
const
void
*
N
)
{
const
void
*
O
=
O_
;
return
__atomic_compare_exchange_n
(
(
void
*
*
)
P
(
void
*
*
)
&
O
(
void
*
)
N
true
__ATOMIC_ACQ_REL
__ATOMIC_RELAXED
)
;
}
#
define
hb_atomic_ptr_impl_cmpexch
(
P
O
N
)
_hb_atomic_ptr_impl_cmplexch
(
(
const
void
*
*
)
(
P
)
(
O
)
(
N
)
)
#
elif
!
defined
(
HB_NO_MT
)
&
&
__cplusplus
>
=
201103L
#
include
<
atomic
>
typedef
int
hb_atomic_int_impl_t
;
#
define
hb_atomic_int_impl_add
(
AI
V
)
(
reinterpret_cast
<
std
:
:
atomic
<
int
>
*
>
(
AI
)
-
>
fetch_add
(
(
V
)
std
:
:
memory_order_acq_rel
)
)
#
define
hb_atomic_int_impl_set_relaxed
(
AI
V
)
(
reinterpret_cast
<
std
:
:
atomic
<
int
>
*
>
(
AI
)
-
>
store
(
(
V
)
std
:
:
memory_order_relaxed
)
)
#
define
hb_atomic_int_impl_get_relaxed
(
AI
)
(
reinterpret_cast
<
std
:
:
atomic
<
int
>
*
>
(
AI
)
-
>
load
(
std
:
:
memory_order_relaxed
)
)
#
define
hb_atomic_ptr_impl_get
(
P
)
(
reinterpret_cast
<
std
:
:
atomic
<
void
*
>
*
>
(
P
)
-
>
load
(
std
:
:
memory_order_consume
)
)
static
inline
bool
_hb_atomic_ptr_impl_cmplexch
(
const
void
*
*
P
const
void
*
O_
const
void
*
N
)
{
const
void
*
O
=
O_
;
return
reinterpret_cast
<
std
:
:
atomic
<
const
void
*
>
*
>
(
P
)
-
>
compare_exchange_weak
(
O
N
std
:
:
memory_order_acq_rel
std
:
:
memory_order_relaxed
)
;
}
#
define
hb_atomic_ptr_impl_cmpexch
(
P
O
N
)
_hb_atomic_ptr_impl_cmplexch
(
(
const
void
*
*
)
(
P
)
(
O
)
(
N
)
)
#
elif
!
defined
(
HB_NO_MT
)
&
&
(
defined
(
_WIN32
)
|
|
defined
(
__CYGWIN__
)
)
#
include
<
windows
.
h
>
static
inline
void
_hb_memory_barrier
(
void
)
{
#
if
!
defined
(
MemoryBarrier
)
long
dummy
=
0
;
InterlockedExchange
(
&
dummy
1
)
;
#
else
MemoryBarrier
(
)
;
#
endif
}
#
define
_hb_memory_barrier
(
)
_hb_memory_barrier
(
)
typedef
LONG
hb_atomic_int_impl_t
;
#
define
hb_atomic_int_impl_add
(
AI
V
)
InterlockedExchangeAdd
(
(
AI
)
(
V
)
)
#
define
hb_atomic_ptr_impl_cmpexch
(
P
O
N
)
(
InterlockedCompareExchangePointer
(
(
void
*
*
)
(
P
)
(
void
*
)
(
N
)
(
void
*
)
(
O
)
)
=
=
(
void
*
)
(
O
)
)
#
elif
!
defined
(
HB_NO_MT
)
&
&
defined
(
HAVE_INTEL_ATOMIC_PRIMITIVES
)
#
define
_hb_memory_barrier
(
)
__sync_synchronize
(
)
typedef
int
hb_atomic_int_impl_t
;
#
define
hb_atomic_int_impl_add
(
AI
V
)
__sync_fetch_and_add
(
(
AI
)
(
V
)
)
#
define
hb_atomic_ptr_impl_cmpexch
(
P
O
N
)
__sync_bool_compare_and_swap
(
(
P
)
(
O
)
(
N
)
)
#
elif
!
defined
(
HB_NO_MT
)
&
&
defined
(
HAVE_SOLARIS_ATOMIC_OPS
)
#
include
<
atomic
.
h
>
#
include
<
mbarrier
.
h
>
#
define
_hb_memory_r_barrier
(
)
__machine_r_barrier
(
)
#
define
_hb_memory_w_barrier
(
)
__machine_w_barrier
(
)
#
define
_hb_memory_barrier
(
)
__machine_rw_barrier
(
)
typedef
unsigned
int
hb_atomic_int_impl_t
;
static
inline
int
_hb_fetch_and_add
(
hb_atomic_int_impl_t
*
AI
int
V
)
{
_hb_memory_w_barrier
(
)
;
int
result
=
atomic_add_int_nv
(
AI
V
)
;
_hb_memory_r_barrier
(
)
;
return
result
;
}
static
inline
bool
_hb_compare_and_swap_ptr
(
const
void
*
*
P
const
void
*
O
const
void
*
N
)
{
_hb_memory_w_barrier
(
)
;
int
result
=
atomic_cas_ptr
(
(
void
*
*
)
P
(
void
*
)
O
(
void
*
)
N
)
=
=
(
void
*
)
O
;
_hb_memory_r_barrier
(
)
;
return
result
;
}
#
define
hb_atomic_int_impl_add
(
AI
V
)
_hb_fetch_and_add
(
(
AI
)
(
V
)
)
#
define
hb_atomic_ptr_impl_cmpexch
(
P
O
N
)
_hb_compare_and_swap_ptr
(
(
const
void
*
*
)
(
P
)
(
O
)
(
N
)
)
#
elif
!
defined
(
HB_NO_MT
)
&
&
defined
(
__APPLE__
)
#
include
<
libkern
/
OSAtomic
.
h
>
#
ifdef
__MAC_OS_X_MIN_REQUIRED
#
include
<
AvailabilityMacros
.
h
>
#
elif
defined
(
__IPHONE_OS_MIN_REQUIRED
)
#
include
<
Availability
.
h
>
#
endif
#
define
_hb_memory_barrier
(
)
OSMemoryBarrier
(
)
typedef
int32_t
hb_atomic_int_impl_t
;
#
define
hb_atomic_int_impl_add
(
AI
V
)
(
OSAtomicAdd32Barrier
(
(
V
)
(
AI
)
)
-
(
V
)
)
#
if
(
MAC_OS_X_VERSION_MIN_REQUIRED
>
MAC_OS_X_VERSION_10_4
|
|
__IPHONE_VERSION_MIN_REQUIRED
>
=
20100
)
#
define
hb_atomic_ptr_impl_cmpexch
(
P
O
N
)
OSAtomicCompareAndSwapPtrBarrier
(
(
void
*
)
(
O
)
(
void
*
)
(
N
)
(
void
*
*
)
(
P
)
)
#
else
#
if
__ppc64__
|
|
__x86_64__
|
|
__aarch64__
#
define
hb_atomic_ptr_impl_cmpexch
(
P
O
N
)
OSAtomicCompareAndSwap64Barrier
(
(
int64_t
)
(
void
*
)
(
O
)
(
int64_t
)
(
void
*
)
(
N
)
(
int64_t
*
)
(
P
)
)
#
else
#
define
hb_atomic_ptr_impl_cmpexch
(
P
O
N
)
OSAtomicCompareAndSwap32Barrier
(
(
int32_t
)
(
void
*
)
(
O
)
(
int32_t
)
(
void
*
)
(
N
)
(
int32_t
*
)
(
P
)
)
#
endif
#
endif
#
elif
!
defined
(
HB_NO_MT
)
&
&
defined
(
_AIX
)
&
&
defined
(
__IBMCPP__
)
#
include
<
builtins
.
h
>
#
define
_hb_memory_barrier
(
)
__lwsync
(
)
typedef
int
hb_atomic_int_impl_t
;
static
inline
int
_hb_fetch_and_add
(
hb_atomic_int_impl_t
*
AI
int
V
)
{
_hb_memory_barrier
(
)
;
int
result
=
__fetch_and_add
(
AI
V
)
;
_hb_memory_barrier
(
)
;
return
result
;
}
static
inline
bool
_hb_compare_and_swaplp
(
long
*
P
long
O
long
N
)
{
_hb_memory_barrier
(
)
;
bool
result
=
__compare_and_swaplp
(
P
&
O
N
)
;
_hb_memory_barrier
(
)
;
return
result
;
}
#
define
hb_atomic_int_impl_add
(
AI
V
)
_hb_fetch_and_add
(
(
AI
)
(
V
)
)
#
define
hb_atomic_ptr_impl_cmpexch
(
P
O
N
)
_hb_compare_and_swaplp
(
(
long
*
)
(
P
)
(
long
)
(
O
)
(
long
)
(
N
)
)
static_assert
(
(
sizeof
(
long
)
=
=
sizeof
(
void
*
)
)
"
"
)
;
#
elif
!
defined
(
HB_NO_MT
)
#
define
HB_ATOMIC_INT_NIL
1
/
*
Warn
that
fallback
implementation
is
in
use
.
*
/
#
define
_hb_memory_barrier
(
)
typedef
volatile
int
hb_atomic_int_impl_t
;
#
define
hb_atomic_int_impl_add
(
AI
V
)
(
(
*
(
AI
)
+
=
(
V
)
)
-
(
V
)
)
#
define
hb_atomic_ptr_impl_cmpexch
(
P
O
N
)
(
*
(
void
*
volatile
*
)
(
P
)
=
=
(
void
*
)
(
O
)
?
(
*
(
void
*
volatile
*
)
(
P
)
=
(
void
*
)
(
N
)
true
)
:
false
)
#
else
typedef
int
hb_atomic_int_impl_t
;
#
define
hb_atomic_int_impl_add
(
AI
V
)
(
(
*
(
AI
)
+
=
(
V
)
)
-
(
V
)
)
#
define
_hb_memory_barrier
(
)
#
define
hb_atomic_ptr_impl_cmpexch
(
P
O
N
)
(
*
(
void
*
*
)
(
P
)
=
=
(
void
*
)
(
O
)
?
(
*
(
void
*
*
)
(
P
)
=
(
void
*
)
(
N
)
true
)
:
false
)
#
endif
#
ifndef
_hb_memory_r_barrier
#
define
_hb_memory_r_barrier
(
)
_hb_memory_barrier
(
)
#
endif
#
ifndef
_hb_memory_w_barrier
#
define
_hb_memory_w_barrier
(
)
_hb_memory_barrier
(
)
#
endif
#
ifndef
HB_ATOMIC_INT_INIT
#
define
HB_ATOMIC_INT_INIT
(
V
)
{
V
}
#
endif
#
ifndef
hb_atomic_int_impl_set_relaxed
#
define
hb_atomic_int_impl_set_relaxed
(
AI
V
)
(
*
(
AI
)
=
(
V
)
)
#
endif
#
ifndef
hb_atomic_int_impl_get_relaxed
#
define
hb_atomic_int_impl_get_relaxed
(
AI
)
(
*
(
AI
)
)
#
endif
#
ifndef
hb_atomic_ptr_impl_get
inline
void
*
hb_atomic_ptr_impl_get
(
void
*
*
P
)
{
void
*
v
=
*
P
;
_hb_memory_r_barrier
(
)
;
return
v
;
}
#
endif
struct
hb_atomic_int_t
{
mutable
hb_atomic_int_impl_t
v
;
inline
void
set_relaxed
(
int
v_
)
{
hb_atomic_int_impl_set_relaxed
(
&
v
v_
)
;
}
inline
int
get_relaxed
(
void
)
const
{
return
hb_atomic_int_impl_get_relaxed
(
&
v
)
;
}
inline
int
inc
(
void
)
{
return
hb_atomic_int_impl_add
(
&
v
1
)
;
}
inline
int
dec
(
void
)
{
return
hb_atomic_int_impl_add
(
&
v
-
1
)
;
}
}
;
#
define
hb_atomic_ptr_get
(
P
)
hb_atomic_ptr_impl_get
(
(
void
*
*
)
P
)
#
define
hb_atomic_ptr_cmpexch
(
P
O
N
)
hb_atomic_ptr_impl_cmpexch
(
(
P
)
(
O
)
(
N
)
)
#
endif
