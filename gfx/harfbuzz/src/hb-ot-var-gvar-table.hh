#
ifndef
HB_OT_VAR_GVAR_TABLE_HH
#
define
HB_OT_VAR_GVAR_TABLE_HH
#
include
"
hb
-
open
-
type
.
hh
"
#
include
"
hb
-
ot
-
var
-
common
.
hh
"
#
define
HB_OT_TAG_gvar
HB_TAG
(
'
g
'
'
v
'
'
a
'
'
r
'
)
namespace
OT
{
struct
contour_point_t
{
void
init
(
float
x_
=
0
.
f
float
y_
=
0
.
f
bool
is_end_point_
=
false
)
{
flag
=
0
;
x
=
x_
;
y
=
y_
;
is_end_point
=
is_end_point_
;
}
void
transform
(
const
float
(
&
matrix
)
[
4
]
)
{
float
x_
=
x
*
matrix
[
0
]
+
y
*
matrix
[
2
]
;
y
=
x
*
matrix
[
1
]
+
y
*
matrix
[
3
]
;
x
=
x_
;
}
void
translate
(
const
contour_point_t
&
p
)
{
x
+
=
p
.
x
;
y
+
=
p
.
y
;
}
float
x
=
0
.
f
;
float
y
=
0
.
f
;
uint8_t
flag
=
0
;
bool
is_end_point
=
false
;
}
;
struct
contour_point_vector_t
:
hb_vector_t
<
contour_point_t
>
{
void
extend
(
const
hb_array_t
<
contour_point_t
>
&
a
)
{
unsigned
int
old_len
=
length
;
if
(
unlikely
(
!
resize
(
old_len
+
a
.
length
false
)
)
)
return
;
auto
arrayZ
=
this
-
>
arrayZ
+
old_len
;
unsigned
count
=
a
.
length
;
hb_memcpy
(
arrayZ
a
.
arrayZ
count
*
sizeof
(
arrayZ
[
0
]
)
)
;
}
}
;
struct
GlyphVariationData
:
TupleVariationData
{
}
;
struct
gvar
{
static
constexpr
hb_tag_t
tableTag
=
HB_OT_TAG_gvar
;
bool
sanitize_shallow
(
hb_sanitize_context_t
*
c
)
const
{
TRACE_SANITIZE
(
this
)
;
return_trace
(
c
-
>
check_struct
(
this
)
&
&
(
version
.
major
=
=
1
)
&
&
sharedTuples
.
sanitize
(
c
this
axisCount
*
sharedTupleCount
)
&
&
(
is_long_offset
(
)
?
c
-
>
check_array
(
get_long_offset_array
(
)
c
-
>
get_num_glyphs
(
)
+
1
)
:
c
-
>
check_array
(
get_short_offset_array
(
)
c
-
>
get_num_glyphs
(
)
+
1
)
)
)
;
}
bool
sanitize
(
hb_sanitize_context_t
*
c
)
const
{
return
sanitize_shallow
(
c
)
;
}
bool
subset
(
hb_subset_context_t
*
c
)
const
{
TRACE_SUBSET
(
this
)
;
unsigned
glyph_count
=
version
.
to_int
(
)
?
c
-
>
plan
-
>
source
-
>
get_num_glyphs
(
)
:
0
;
gvar
*
out
=
c
-
>
serializer
-
>
allocate_min
<
gvar
>
(
)
;
if
(
unlikely
(
!
out
)
)
return_trace
(
false
)
;
out
-
>
version
.
major
=
1
;
out
-
>
version
.
minor
=
0
;
out
-
>
axisCount
=
axisCount
;
out
-
>
sharedTupleCount
=
sharedTupleCount
;
unsigned
int
num_glyphs
=
c
-
>
plan
-
>
num_output_glyphs
(
)
;
out
-
>
glyphCountX
=
hb_min
(
0xFFFFu
num_glyphs
)
;
unsigned
int
subset_data_size
=
0
;
for
(
hb_codepoint_t
gid
=
(
c
-
>
plan
-
>
flags
&
HB_SUBSET_FLAGS_NOTDEF_OUTLINE
)
?
0
:
1
;
gid
<
num_glyphs
;
gid
+
+
)
{
hb_codepoint_t
old_gid
;
if
(
!
c
-
>
plan
-
>
old_gid_for_new_gid
(
gid
&
old_gid
)
)
continue
;
subset_data_size
+
=
get_glyph_var_data_bytes
(
c
-
>
source_blob
glyph_count
old_gid
)
.
length
;
}
bool
long_offset
=
subset_data_size
&
~
0xFFFFu
;
out
-
>
flags
=
long_offset
?
1
:
0
;
HBUINT8
*
subset_offsets
=
c
-
>
serializer
-
>
allocate_size
<
HBUINT8
>
(
(
long_offset
?
4
:
2
)
*
(
num_glyphs
+
1
)
)
;
if
(
!
subset_offsets
)
return_trace
(
false
)
;
if
(
!
sharedTupleCount
|
|
!
sharedTuples
)
out
-
>
sharedTuples
=
0
;
else
{
unsigned
int
shared_tuple_size
=
F2DOT14
:
:
static_size
*
axisCount
*
sharedTupleCount
;
F2DOT14
*
tuples
=
c
-
>
serializer
-
>
allocate_size
<
F2DOT14
>
(
shared_tuple_size
)
;
if
(
!
tuples
)
return_trace
(
false
)
;
out
-
>
sharedTuples
=
(
char
*
)
tuples
-
(
char
*
)
out
;
hb_memcpy
(
tuples
this
+
sharedTuples
shared_tuple_size
)
;
}
char
*
subset_data
=
c
-
>
serializer
-
>
allocate_size
<
char
>
(
subset_data_size
)
;
if
(
!
subset_data
)
return_trace
(
false
)
;
out
-
>
dataZ
=
subset_data
-
(
char
*
)
out
;
unsigned
int
glyph_offset
=
0
;
for
(
hb_codepoint_t
gid
=
(
c
-
>
plan
-
>
flags
&
HB_SUBSET_FLAGS_NOTDEF_OUTLINE
)
?
0
:
1
;
gid
<
num_glyphs
;
gid
+
+
)
{
hb_codepoint_t
old_gid
;
hb_bytes_t
var_data_bytes
=
c
-
>
plan
-
>
old_gid_for_new_gid
(
gid
&
old_gid
)
?
get_glyph_var_data_bytes
(
c
-
>
source_blob
glyph_count
old_gid
)
:
hb_bytes_t
(
)
;
if
(
long_offset
)
(
(
HBUINT32
*
)
subset_offsets
)
[
gid
]
=
glyph_offset
;
else
(
(
HBUINT16
*
)
subset_offsets
)
[
gid
]
=
glyph_offset
/
2
;
if
(
var_data_bytes
.
length
>
0
)
hb_memcpy
(
subset_data
var_data_bytes
.
arrayZ
var_data_bytes
.
length
)
;
subset_data
+
=
var_data_bytes
.
length
;
glyph_offset
+
=
var_data_bytes
.
length
;
}
if
(
long_offset
)
(
(
HBUINT32
*
)
subset_offsets
)
[
num_glyphs
]
=
glyph_offset
;
else
(
(
HBUINT16
*
)
subset_offsets
)
[
num_glyphs
]
=
glyph_offset
/
2
;
return_trace
(
true
)
;
}
protected
:
const
hb_bytes_t
get_glyph_var_data_bytes
(
hb_blob_t
*
blob
unsigned
glyph_count
hb_codepoint_t
glyph
)
const
{
unsigned
start_offset
=
get_offset
(
glyph_count
glyph
)
;
unsigned
end_offset
=
get_offset
(
glyph_count
glyph
+
1
)
;
if
(
unlikely
(
end_offset
<
start_offset
)
)
return
hb_bytes_t
(
)
;
unsigned
length
=
end_offset
-
start_offset
;
hb_bytes_t
var_data
=
blob
-
>
as_bytes
(
)
.
sub_array
(
(
(
unsigned
)
dataZ
)
+
start_offset
length
)
;
return
likely
(
var_data
.
length
>
=
GlyphVariationData
:
:
min_size
)
?
var_data
:
hb_bytes_t
(
)
;
}
bool
is_long_offset
(
)
const
{
return
flags
&
1
;
}
unsigned
get_offset
(
unsigned
glyph_count
unsigned
i
)
const
{
if
(
unlikely
(
i
>
glyph_count
)
)
return
0
;
_hb_compiler_memory_r_barrier
(
)
;
return
is_long_offset
(
)
?
get_long_offset_array
(
)
[
i
]
:
get_short_offset_array
(
)
[
i
]
*
2
;
}
const
HBUINT32
*
get_long_offset_array
(
)
const
{
return
(
const
HBUINT32
*
)
&
offsetZ
;
}
const
HBUINT16
*
get_short_offset_array
(
)
const
{
return
(
const
HBUINT16
*
)
&
offsetZ
;
}
public
:
struct
accelerator_t
{
accelerator_t
(
hb_face_t
*
face
)
{
table
=
hb_sanitize_context_t
(
)
.
reference_table
<
gvar
>
(
face
)
;
glyphCount
=
table
-
>
version
.
to_int
(
)
?
face
-
>
get_num_glyphs
(
)
:
0
;
hb_array_t
<
const
F2DOT14
>
shared_tuples
=
(
table
+
table
-
>
sharedTuples
)
.
as_array
(
table
-
>
sharedTupleCount
*
table
-
>
axisCount
)
;
unsigned
count
=
table
-
>
sharedTupleCount
;
if
(
unlikely
(
!
shared_tuple_active_idx
.
resize
(
count
false
)
)
)
return
;
unsigned
axis_count
=
table
-
>
axisCount
;
for
(
unsigned
i
=
0
;
i
<
count
;
i
+
+
)
{
hb_array_t
<
const
F2DOT14
>
tuple
=
shared_tuples
.
sub_array
(
axis_count
*
i
axis_count
)
;
int
idx
=
-
1
;
for
(
unsigned
j
=
0
;
j
<
axis_count
;
j
+
+
)
{
const
F2DOT14
&
peak
=
tuple
.
arrayZ
[
j
]
;
if
(
peak
.
to_int
(
)
!
=
0
)
{
if
(
idx
!
=
-
1
)
{
idx
=
-
1
;
break
;
}
idx
=
j
;
}
}
shared_tuple_active_idx
.
arrayZ
[
i
]
=
idx
;
}
}
~
accelerator_t
(
)
{
table
.
destroy
(
)
;
}
private
:
static
float
infer_delta
(
const
hb_array_t
<
contour_point_t
>
points
const
hb_array_t
<
contour_point_t
>
deltas
unsigned
int
target
unsigned
int
prev
unsigned
int
next
float
contour_point_t
:
:
*
m
)
{
float
target_val
=
points
.
arrayZ
[
target
]
.
*
m
;
float
prev_val
=
points
.
arrayZ
[
prev
]
.
*
m
;
float
next_val
=
points
.
arrayZ
[
next
]
.
*
m
;
float
prev_delta
=
deltas
.
arrayZ
[
prev
]
.
*
m
;
float
next_delta
=
deltas
.
arrayZ
[
next
]
.
*
m
;
if
(
prev_val
=
=
next_val
)
return
(
prev_delta
=
=
next_delta
)
?
prev_delta
:
0
.
f
;
else
if
(
target_val
<
=
hb_min
(
prev_val
next_val
)
)
return
(
prev_val
<
next_val
)
?
prev_delta
:
next_delta
;
else
if
(
target_val
>
=
hb_max
(
prev_val
next_val
)
)
return
(
prev_val
>
next_val
)
?
prev_delta
:
next_delta
;
float
r
=
(
target_val
-
prev_val
)
/
(
next_val
-
prev_val
)
;
return
prev_delta
+
r
*
(
next_delta
-
prev_delta
)
;
}
static
unsigned
int
next_index
(
unsigned
int
i
unsigned
int
start
unsigned
int
end
)
{
return
(
i
>
=
end
)
?
start
:
(
i
+
1
)
;
}
public
:
bool
apply_deltas_to_points
(
hb_codepoint_t
glyph
hb_array_t
<
int
>
coords
const
hb_array_t
<
contour_point_t
>
points
)
const
{
if
(
!
coords
)
return
true
;
if
(
unlikely
(
glyph
>
=
glyphCount
)
)
return
true
;
hb_bytes_t
var_data_bytes
=
table
-
>
get_glyph_var_data_bytes
(
table
.
get_blob
(
)
glyphCount
glyph
)
;
if
(
!
var_data_bytes
.
as
<
GlyphVariationData
>
(
)
-
>
has_data
(
)
)
return
true
;
hb_vector_t
<
unsigned
int
>
shared_indices
;
GlyphVariationData
:
:
tuple_iterator_t
iterator
;
if
(
!
GlyphVariationData
:
:
get_tuple_iterator
(
var_data_bytes
table
-
>
axisCount
var_data_bytes
.
arrayZ
shared_indices
&
iterator
)
)
return
true
;
contour_point_vector_t
orig_points_vec
;
auto
orig_points
=
orig_points_vec
.
as_array
(
)
;
contour_point_vector_t
deltas_vec
;
auto
deltas
=
deltas_vec
.
as_array
(
)
;
hb_vector_t
<
unsigned
>
end_points
;
unsigned
num_coords
=
table
-
>
axisCount
;
hb_array_t
<
const
F2DOT14
>
shared_tuples
=
(
table
+
table
-
>
sharedTuples
)
.
as_array
(
table
-
>
sharedTupleCount
*
num_coords
)
;
hb_vector_t
<
unsigned
int
>
private_indices
;
hb_vector_t
<
int
>
x_deltas
;
hb_vector_t
<
int
>
y_deltas
;
bool
flush
=
false
;
do
{
float
scalar
=
iterator
.
current_tuple
-
>
calculate_scalar
(
coords
num_coords
shared_tuples
&
shared_tuple_active_idx
)
;
if
(
scalar
=
=
0
.
f
)
continue
;
const
HBUINT8
*
p
=
iterator
.
get_serialized_data
(
)
;
unsigned
int
length
=
iterator
.
current_tuple
-
>
get_data_size
(
)
;
if
(
unlikely
(
!
iterator
.
var_data_bytes
.
check_range
(
p
length
)
)
)
return
false
;
if
(
!
deltas
)
{
if
(
unlikely
(
!
deltas_vec
.
resize
(
points
.
length
false
)
)
)
return
false
;
deltas
=
deltas_vec
.
as_array
(
)
;
hb_memset
(
deltas
.
arrayZ
0
deltas
.
get_size
(
)
)
;
}
const
HBUINT8
*
end
=
p
+
length
;
bool
has_private_points
=
iterator
.
current_tuple
-
>
has_private_points
(
)
;
if
(
has_private_points
&
&
!
GlyphVariationData
:
:
unpack_points
(
p
private_indices
end
)
)
return
false
;
const
hb_array_t
<
unsigned
int
>
&
indices
=
has_private_points
?
private_indices
:
shared_indices
;
bool
apply_to_all
=
(
indices
.
length
=
=
0
)
;
unsigned
int
num_deltas
=
apply_to_all
?
points
.
length
:
indices
.
length
;
if
(
unlikely
(
!
x_deltas
.
resize
(
num_deltas
false
)
)
)
return
false
;
if
(
unlikely
(
!
GlyphVariationData
:
:
unpack_deltas
(
p
x_deltas
end
)
)
)
return
false
;
if
(
unlikely
(
!
y_deltas
.
resize
(
num_deltas
false
)
)
)
return
false
;
if
(
unlikely
(
!
GlyphVariationData
:
:
unpack_deltas
(
p
y_deltas
end
)
)
)
return
false
;
if
(
!
apply_to_all
)
{
if
(
!
orig_points
)
{
orig_points_vec
.
extend
(
points
)
;
if
(
unlikely
(
orig_points_vec
.
in_error
(
)
)
)
return
false
;
orig_points
=
orig_points_vec
.
as_array
(
)
;
}
if
(
flush
)
{
unsigned
count
=
points
.
length
;
for
(
unsigned
int
i
=
0
;
i
<
count
;
i
+
+
)
points
.
arrayZ
[
i
]
.
translate
(
deltas
.
arrayZ
[
i
]
)
;
flush
=
false
;
}
hb_memset
(
deltas
.
arrayZ
0
deltas
.
get_size
(
)
)
;
}
if
(
HB_OPTIMIZE_SIZE_VAL
)
{
for
(
unsigned
int
i
=
0
;
i
<
num_deltas
;
i
+
+
)
{
unsigned
int
pt_index
;
if
(
apply_to_all
)
pt_index
=
i
;
else
{
pt_index
=
indices
[
i
]
;
if
(
unlikely
(
pt_index
>
=
deltas
.
length
)
)
continue
;
}
auto
&
delta
=
deltas
.
arrayZ
[
pt_index
]
;
delta
.
flag
=
1
;
delta
.
x
+
=
x_deltas
.
arrayZ
[
i
]
*
scalar
;
delta
.
y
+
=
y_deltas
.
arrayZ
[
i
]
*
scalar
;
}
}
else
{
if
(
scalar
!
=
1
.
0f
)
{
if
(
apply_to_all
)
for
(
unsigned
int
i
=
0
;
i
<
num_deltas
;
i
+
+
)
{
unsigned
int
pt_index
=
i
;
auto
&
delta
=
deltas
.
arrayZ
[
pt_index
]
;
delta
.
x
+
=
x_deltas
.
arrayZ
[
i
]
*
scalar
;
delta
.
y
+
=
y_deltas
.
arrayZ
[
i
]
*
scalar
;
}
else
for
(
unsigned
int
i
=
0
;
i
<
num_deltas
;
i
+
+
)
{
unsigned
int
pt_index
=
indices
[
i
]
;
if
(
unlikely
(
pt_index
>
=
deltas
.
length
)
)
continue
;
auto
&
delta
=
deltas
.
arrayZ
[
pt_index
]
;
delta
.
flag
=
1
;
delta
.
x
+
=
x_deltas
.
arrayZ
[
i
]
*
scalar
;
delta
.
y
+
=
y_deltas
.
arrayZ
[
i
]
*
scalar
;
}
}
else
{
if
(
apply_to_all
)
for
(
unsigned
int
i
=
0
;
i
<
num_deltas
;
i
+
+
)
{
unsigned
int
pt_index
=
i
;
auto
&
delta
=
deltas
.
arrayZ
[
pt_index
]
;
delta
.
x
+
=
x_deltas
.
arrayZ
[
i
]
;
delta
.
y
+
=
y_deltas
.
arrayZ
[
i
]
;
}
else
for
(
unsigned
int
i
=
0
;
i
<
num_deltas
;
i
+
+
)
{
unsigned
int
pt_index
=
indices
[
i
]
;
if
(
unlikely
(
pt_index
>
=
deltas
.
length
)
)
continue
;
auto
&
delta
=
deltas
.
arrayZ
[
pt_index
]
;
delta
.
flag
=
1
;
delta
.
x
+
=
x_deltas
.
arrayZ
[
i
]
;
delta
.
y
+
=
y_deltas
.
arrayZ
[
i
]
;
}
}
}
if
(
!
apply_to_all
)
{
if
(
!
end_points
)
{
unsigned
count
=
points
.
length
;
for
(
unsigned
i
=
0
;
i
<
count
;
+
+
i
)
if
(
points
.
arrayZ
[
i
]
.
is_end_point
)
end_points
.
push
(
i
)
;
if
(
unlikely
(
end_points
.
in_error
(
)
)
)
return
false
;
}
unsigned
start_point
=
0
;
for
(
unsigned
end_point
:
end_points
)
{
unsigned
unref_count
=
0
;
for
(
unsigned
i
=
start_point
;
i
<
end_point
+
1
;
i
+
+
)
unref_count
+
=
deltas
.
arrayZ
[
i
]
.
flag
;
unref_count
=
(
end_point
-
start_point
+
1
)
-
unref_count
;
unsigned
j
=
start_point
;
if
(
unref_count
=
=
0
|
|
unref_count
>
end_point
-
start_point
)
goto
no_more_gaps
;
for
(
;
;
)
{
unsigned
int
prev
next
i
;
for
(
;
;
)
{
i
=
j
;
j
=
next_index
(
i
start_point
end_point
)
;
if
(
deltas
.
arrayZ
[
i
]
.
flag
&
&
!
deltas
.
arrayZ
[
j
]
.
flag
)
break
;
}
prev
=
j
=
i
;
for
(
;
;
)
{
i
=
j
;
j
=
next_index
(
i
start_point
end_point
)
;
if
(
!
deltas
.
arrayZ
[
i
]
.
flag
&
&
deltas
.
arrayZ
[
j
]
.
flag
)
break
;
}
next
=
j
;
i
=
prev
;
for
(
;
;
)
{
i
=
next_index
(
i
start_point
end_point
)
;
if
(
i
=
=
next
)
break
;
deltas
.
arrayZ
[
i
]
.
x
=
infer_delta
(
orig_points
deltas
i
prev
next
&
contour_point_t
:
:
x
)
;
deltas
.
arrayZ
[
i
]
.
y
=
infer_delta
(
orig_points
deltas
i
prev
next
&
contour_point_t
:
:
y
)
;
if
(
-
-
unref_count
=
=
0
)
goto
no_more_gaps
;
}
}
no_more_gaps
:
start_point
=
end_point
+
1
;
}
}
flush
=
true
;
}
while
(
iterator
.
move_to_next
(
)
)
;
if
(
flush
)
{
unsigned
count
=
points
.
length
;
for
(
unsigned
int
i
=
0
;
i
<
count
;
i
+
+
)
points
.
arrayZ
[
i
]
.
translate
(
deltas
.
arrayZ
[
i
]
)
;
}
return
true
;
}
unsigned
int
get_axis_count
(
)
const
{
return
table
-
>
axisCount
;
}
private
:
hb_blob_ptr_t
<
gvar
>
table
;
unsigned
glyphCount
;
hb_vector_t
<
signed
>
shared_tuple_active_idx
;
}
;
protected
:
FixedVersion
<
>
version
;
HBUINT16
axisCount
;
HBUINT16
sharedTupleCount
;
NNOffset32To
<
UnsizedArrayOf
<
F2DOT14
>
>
sharedTuples
;
HBUINT16
glyphCountX
;
HBUINT16
flags
;
Offset32To
<
GlyphVariationData
>
dataZ
;
UnsizedArrayOf
<
HBUINT8
>
offsetZ
;
public
:
DEFINE_SIZE_ARRAY
(
20
offsetZ
)
;
}
;
struct
gvar_accelerator_t
:
gvar
:
:
accelerator_t
{
gvar_accelerator_t
(
hb_face_t
*
face
)
:
gvar
:
:
accelerator_t
(
face
)
{
}
}
;
}
#
endif
