"
use
strict
"
;
const
{
ArchiveEncryptionState
}
=
ChromeUtils
.
importESModule
(
"
resource
:
/
/
/
modules
/
backup
/
ArchiveEncryptionState
.
sys
.
mjs
"
)
;
const
{
OSKeyStoreTestUtils
}
=
ChromeUtils
.
importESModule
(
"
resource
:
/
/
testing
-
common
/
OSKeyStoreTestUtils
.
sys
.
mjs
"
)
;
const
{
ArchiveUtils
}
=
ChromeUtils
.
importESModule
(
"
resource
:
/
/
/
modules
/
backup
/
ArchiveUtils
.
sys
.
mjs
"
)
;
const
{
ArchiveDecryptor
}
=
ChromeUtils
.
importESModule
(
"
resource
:
/
/
/
modules
/
backup
/
ArchiveEncryption
.
sys
.
mjs
"
)
;
const
{
DecoderDecryptorTransformer
FileWriterStream
}
=
ChromeUtils
.
importESModule
(
"
resource
:
/
/
/
modules
/
backup
/
BackupService
.
sys
.
mjs
"
)
;
let
testProfilePath
;
let
fakeCompressedStagingPath
;
let
archiveTemplateFile
=
do_get_file
(
"
data
/
test_archive
.
template
.
html
"
)
;
let
archiveTemplateURI
=
Services
.
io
.
newFileURI
(
archiveTemplateFile
)
.
spec
;
const
SIZE_IN_BYTES
=
125123
;
let
fakeBytes
;
async
function
assertExtractionsMatch
(
extractionPath
)
{
let
writtenBytes
=
await
IOUtils
.
read
(
extractionPath
)
;
assertUint8ArraysSimilarity
(
writtenBytes
fakeBytes
true
)
;
}
add_setup
(
async
(
)
=
>
{
testProfilePath
=
await
IOUtils
.
createUniqueDirectory
(
PathUtils
.
tempDir
"
testCreateArchive
"
)
;
fakeCompressedStagingPath
=
PathUtils
.
join
(
testProfilePath
"
fake
-
compressed
-
staging
.
zip
"
)
;
fakeBytes
=
new
Uint8Array
(
SIZE_IN_BYTES
)
;
let
gen
=
seededRandomNumberGenerator
(
)
;
for
(
let
i
=
0
;
i
<
SIZE_IN_BYTES
;
+
+
i
)
{
fakeBytes
.
set
(
gen
.
next
(
)
.
value
i
)
;
}
await
IOUtils
.
write
(
fakeCompressedStagingPath
fakeBytes
)
;
OSKeyStoreTestUtils
.
setup
(
)
;
registerCleanupFunction
(
async
(
)
=
>
{
await
OSKeyStoreTestUtils
.
cleanup
(
)
;
await
IOUtils
.
remove
(
testProfilePath
{
recursive
:
true
}
)
;
}
)
;
}
)
;
add_task
(
async
function
test_createArchive_unencrypted
(
)
{
let
bs
=
new
BackupService
(
)
;
const
FAKE_ARCHIVE_PATH
=
PathUtils
.
join
(
testProfilePath
"
fake
-
unencrypted
-
archive
.
html
"
)
;
await
bs
.
createArchive
(
FAKE_ARCHIVE_PATH
archiveTemplateURI
fakeCompressedStagingPath
null
FAKE_METADATA
)
;
let
{
isEncrypted
archiveJSON
}
=
await
bs
.
sampleArchive
(
FAKE_ARCHIVE_PATH
)
;
Assert
.
ok
(
!
isEncrypted
"
Should
not
be
considered
encrypted
.
"
)
;
Assert
.
deepEqual
(
archiveJSON
.
meta
FAKE_METADATA
"
Metadata
was
encoded
in
the
archive
JSON
block
.
"
)
;
const
EXTRACTION_PATH
=
PathUtils
.
join
(
testProfilePath
"
extraction
.
bin
"
)
;
await
bs
.
extractCompressedSnapshotFromArchive
(
FAKE_ARCHIVE_PATH
EXTRACTION_PATH
)
;
assertExtractionsMatch
(
EXTRACTION_PATH
)
;
await
IOUtils
.
remove
(
FAKE_ARCHIVE_PATH
)
;
await
IOUtils
.
remove
(
EXTRACTION_PATH
)
;
}
)
;
add_task
(
async
function
test_createArchive_encrypted
(
)
{
const
TEST_RECOVERY_CODE
=
"
This
is
some
recovery
code
.
"
;
let
bs
=
new
BackupService
(
)
;
let
{
instance
:
encState
}
=
await
ArchiveEncryptionState
.
initialize
(
TEST_RECOVERY_CODE
)
;
const
FAKE_ARCHIVE_PATH
=
PathUtils
.
join
(
testProfilePath
"
fake
-
encrypted
-
archive
.
html
"
)
;
await
bs
.
createArchive
(
FAKE_ARCHIVE_PATH
archiveTemplateURI
fakeCompressedStagingPath
encState
FAKE_METADATA
)
;
let
{
isEncrypted
archiveJSON
}
=
await
bs
.
sampleArchive
(
FAKE_ARCHIVE_PATH
)
;
Assert
.
ok
(
isEncrypted
"
Should
be
considered
encrypted
.
"
)
;
Assert
.
deepEqual
(
archiveJSON
.
meta
FAKE_METADATA
"
Metadata
was
encoded
in
the
archive
JSON
block
.
"
)
;
const
EXTRACTION_PATH
=
PathUtils
.
join
(
testProfilePath
"
extraction
.
bin
"
)
;
await
Assert
.
rejects
(
bs
.
extractCompressedSnapshotFromArchive
(
FAKE_ARCHIVE_PATH
EXTRACTION_PATH
)
/
recovery
code
is
required
/
)
;
await
bs
.
extractCompressedSnapshotFromArchive
(
FAKE_ARCHIVE_PATH
EXTRACTION_PATH
TEST_RECOVERY_CODE
)
;
assertExtractionsMatch
(
EXTRACTION_PATH
)
;
await
IOUtils
.
remove
(
FAKE_ARCHIVE_PATH
)
;
await
IOUtils
.
remove
(
EXTRACTION_PATH
)
;
}
)
;
add_task
(
async
function
test_createArchive_multiple_of_six_test
(
)
{
let
bs
=
new
BackupService
(
)
;
const
FAKE_ARCHIVE_PATH
=
PathUtils
.
join
(
testProfilePath
"
fake
-
unencrypted
-
archive
.
html
"
)
;
const
FAKE_COMPRESSED_FILE
=
PathUtils
.
join
(
testProfilePath
"
fake
-
compressed
-
staging
-
mul6
.
zip
"
)
;
const
NOT_MULTIPLE_OF_SIX_OVERRIDE_CHUNK_SIZE
=
500
;
const
MULTIPLE_OF_SIX_SIZE_IN_BYTES
=
6
*
500
;
let
multipleOfSixBytes
=
new
Uint8Array
(
MULTIPLE_OF_SIX_SIZE_IN_BYTES
)
;
let
gen
=
seededRandomNumberGenerator
(
)
;
for
(
let
i
=
0
;
i
<
MULTIPLE_OF_SIX_SIZE_IN_BYTES
;
+
+
i
)
{
multipleOfSixBytes
.
set
(
gen
.
next
(
)
.
value
i
)
;
}
await
IOUtils
.
write
(
FAKE_COMPRESSED_FILE
multipleOfSixBytes
)
;
await
bs
.
createArchive
(
FAKE_ARCHIVE_PATH
archiveTemplateURI
FAKE_COMPRESSED_FILE
null
FAKE_METADATA
{
chunkSize
:
NOT_MULTIPLE_OF_SIX_OVERRIDE_CHUNK_SIZE
}
)
;
const
EXTRACTION_PATH
=
PathUtils
.
join
(
testProfilePath
"
extraction
.
bin
"
)
;
await
bs
.
extractCompressedSnapshotFromArchive
(
FAKE_ARCHIVE_PATH
EXTRACTION_PATH
)
;
let
writtenBytes
=
await
IOUtils
.
read
(
EXTRACTION_PATH
)
;
assertUint8ArraysSimilarity
(
writtenBytes
multipleOfSixBytes
true
)
;
await
IOUtils
.
remove
(
FAKE_COMPRESSED_FILE
)
;
await
IOUtils
.
remove
(
FAKE_ARCHIVE_PATH
)
;
await
IOUtils
.
remove
(
EXTRACTION_PATH
)
;
}
)
;
add_task
(
async
function
test_createArchive_encrypted_truncated
(
)
{
const
TEST_RECOVERY_CODE
=
"
This
is
some
recovery
code
.
"
;
let
bs
=
new
BackupService
(
)
;
let
{
instance
:
encState
}
=
await
ArchiveEncryptionState
.
initialize
(
TEST_RECOVERY_CODE
)
;
const
FAKE_ARCHIVE_PATH
=
PathUtils
.
join
(
testProfilePath
"
fake
-
encrypted
-
archive
.
html
"
)
;
const
FAKE_COMPRESSED_FILE
=
PathUtils
.
join
(
testProfilePath
"
fake
-
compressed
-
staging
-
large
.
zip
"
)
;
const
MULTIPLE_OF_MAX_CHUNK_SIZE
=
2
*
ArchiveUtils
.
ARCHIVE_CHUNK_MAX_BYTES_SIZE
;
let
multipleOfMaxChunkSizeBytes
=
new
Uint8Array
(
MULTIPLE_OF_MAX_CHUNK_SIZE
)
;
let
gen
=
seededRandomNumberGenerator
(
)
;
for
(
let
i
=
0
;
i
<
MULTIPLE_OF_MAX_CHUNK_SIZE
;
+
+
i
)
{
multipleOfMaxChunkSizeBytes
.
set
(
gen
.
next
(
)
.
value
i
)
;
}
await
IOUtils
.
write
(
FAKE_COMPRESSED_FILE
multipleOfMaxChunkSizeBytes
)
;
await
bs
.
createArchive
(
FAKE_ARCHIVE_PATH
archiveTemplateURI
FAKE_COMPRESSED_FILE
encState
FAKE_METADATA
)
;
let
lines
=
(
await
IOUtils
.
readUTF8
(
FAKE_ARCHIVE_PATH
)
)
.
split
(
"
\
n
"
)
;
let
foundIndex
=
-
1
;
for
(
let
i
=
lines
.
length
-
1
;
i
>
=
0
;
i
-
-
)
{
if
(
lines
[
i
]
.
length
>
ArchiveUtils
.
ARCHIVE_CHUNK_MAX_BYTES_SIZE
)
{
foundIndex
=
i
;
break
;
}
}
Assert
.
notEqual
(
foundIndex
-
1
"
Should
have
found
a
long
line
"
)
;
lines
.
splice
(
foundIndex
1
)
;
await
IOUtils
.
writeUTF8
(
FAKE_ARCHIVE_PATH
lines
.
join
(
"
\
n
"
)
)
;
let
{
isEncrypted
}
=
await
bs
.
sampleArchive
(
FAKE_ARCHIVE_PATH
)
;
Assert
.
ok
(
isEncrypted
"
Should
be
considered
encrypted
.
"
)
;
const
EXTRACTION_PATH
=
PathUtils
.
join
(
testProfilePath
"
extraction
.
bin
"
)
;
await
Assert
.
rejects
(
bs
.
extractCompressedSnapshotFromArchive
(
FAKE_ARCHIVE_PATH
EXTRACTION_PATH
TEST_RECOVERY_CODE
)
/
Corrupted
archive
/
)
;
Assert
.
ok
(
!
(
await
IOUtils
.
exists
(
EXTRACTION_PATH
)
)
"
Extraction
should
have
been
automatically
destroyed
.
"
)
;
await
IOUtils
.
remove
(
FAKE_ARCHIVE_PATH
)
;
await
IOUtils
.
remove
(
FAKE_COMPRESSED_FILE
)
;
}
)
;
add_task
(
async
function
test_createArchive_early_binary_stream_close
(
)
{
const
TEST_RECOVERY_CODE
=
"
This
is
some
recovery
code
.
"
;
let
bs
=
new
BackupService
(
)
;
let
{
instance
:
encState
}
=
await
ArchiveEncryptionState
.
initialize
(
TEST_RECOVERY_CODE
)
;
const
FAKE_ARCHIVE_PATH
=
PathUtils
.
join
(
testProfilePath
"
fake
-
encrypted
-
archive
.
html
"
)
;
await
bs
.
createArchive
(
FAKE_ARCHIVE_PATH
archiveTemplateURI
fakeCompressedStagingPath
encState
FAKE_METADATA
)
;
let
{
isEncrypted
startByteOffset
contentType
archiveJSON
}
=
await
bs
.
sampleArchive
(
FAKE_ARCHIVE_PATH
)
;
Assert
.
ok
(
isEncrypted
"
Should
be
considered
encrypted
.
"
)
;
let
archiveFile
=
await
IOUtils
.
getFile
(
FAKE_ARCHIVE_PATH
)
;
let
archiveStream
=
await
bs
.
createBinaryReadableStream
(
archiveFile
startByteOffset
contentType
)
;
let
decryptor
=
await
ArchiveDecryptor
.
initialize
(
TEST_RECOVERY_CODE
archiveJSON
)
;
const
EXTRACTION_PATH
=
PathUtils
.
join
(
testProfilePath
"
extraction
.
bin
"
)
;
let
binaryDecoder
=
new
TransformStream
(
new
DecoderDecryptorTransformer
(
decryptor
)
)
;
let
fileWriter
=
new
WritableStream
(
new
FileWriterStream
(
EXTRACTION_PATH
decryptor
)
)
;
let
earlyAborter
=
new
TransformStream
(
{
async
transform
(
chunkPart
controller
)
{
controller
.
enqueue
(
chunkPart
.
substring
(
0
Math
.
floor
(
chunkPart
.
length
/
2
)
)
)
;
controller
.
error
(
"
We
'
re
done
.
Aborting
early
.
"
)
;
}
}
)
;
let
pipePromise
=
archiveStream
.
pipeThrough
(
earlyAborter
)
.
pipeThrough
(
binaryDecoder
)
.
pipeTo
(
fileWriter
)
;
await
Assert
.
rejects
(
pipePromise
/
Aborting
early
/
)
;
Assert
.
ok
(
!
(
await
IOUtils
.
exists
(
EXTRACTION_PATH
)
)
"
Extraction
should
have
been
automatically
destroyed
.
"
)
;
await
IOUtils
.
remove
(
FAKE_ARCHIVE_PATH
)
;
}
)
;
