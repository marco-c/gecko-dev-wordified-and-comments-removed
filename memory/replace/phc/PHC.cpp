#
include
"
PHC
.
h
"
#
include
<
stdlib
.
h
>
#
include
<
time
.
h
>
#
include
<
algorithm
>
#
ifdef
XP_WIN
#
include
<
process
.
h
>
#
else
#
include
<
sys
/
mman
.
h
>
#
include
<
sys
/
types
.
h
>
#
include
<
pthread
.
h
>
#
include
<
unistd
.
h
>
#
endif
#
include
"
replace_malloc
.
h
"
#
include
"
FdPrintf
.
h
"
#
include
"
Mutex
.
h
"
#
include
"
mozilla
/
Assertions
.
h
"
#
include
"
mozilla
/
Atomics
.
h
"
#
include
"
mozilla
/
Attributes
.
h
"
#
include
"
mozilla
/
CheckedInt
.
h
"
#
include
"
mozilla
/
Maybe
.
h
"
#
include
"
mozilla
/
StackWalk
.
h
"
#
include
"
mozilla
/
ThreadLocal
.
h
"
#
include
"
mozilla
/
XorShift128PlusRNG
.
h
"
using
namespace
mozilla
;
using
namespace
mozilla
:
:
recordreplay
;
#
ifdef
ANDROID
extern
"
C
"
MOZ_EXPORT
int
pthread_atfork
(
void
(
*
)
(
void
)
void
(
*
)
(
void
)
void
(
*
)
(
void
)
)
;
#
endif
#
ifndef
DISALLOW_COPY_AND_ASSIGN
#
define
DISALLOW_COPY_AND_ASSIGN
(
T
)
\
T
(
const
T
&
)
;
\
void
operator
=
(
const
T
&
)
#
endif
static
malloc_table_t
sMallocTable
;
class
InfallibleAllocPolicy
{
public
:
static
void
AbortOnFailure
(
const
void
*
aP
)
{
if
(
!
aP
)
{
MOZ_CRASH
(
"
PHC
failed
to
allocate
"
)
;
}
}
template
<
class
T
>
static
T
*
new_
(
)
{
void
*
p
=
sMallocTable
.
malloc
(
sizeof
(
T
)
)
;
AbortOnFailure
(
p
)
;
return
new
(
p
)
T
;
}
}
;
class
StackTrace
:
public
phc
:
:
StackTrace
{
public
:
StackTrace
(
)
:
phc
:
:
StackTrace
(
)
{
}
void
Clear
(
)
{
mLength
=
0
;
}
void
Fill
(
)
;
private
:
static
void
StackWalkCallback
(
uint32_t
aFrameNumber
void
*
aPc
void
*
aSp
void
*
aClosure
)
{
StackTrace
*
st
=
(
StackTrace
*
)
aClosure
;
MOZ_ASSERT
(
st
-
>
mLength
<
kMaxFrames
)
;
st
-
>
mPcs
[
st
-
>
mLength
]
=
aPc
;
st
-
>
mLength
+
+
;
MOZ_ASSERT
(
st
-
>
mLength
=
=
aFrameNumber
)
;
}
}
;
void
StackTrace
:
:
Fill
(
)
{
mLength
=
0
;
#
if
defined
(
XP_WIN
)
&
&
defined
(
_M_IX86
)
CONTEXT
context
;
RtlCaptureContext
(
&
context
)
;
void
*
*
fp
=
reinterpret_cast
<
void
*
*
>
(
context
.
Ebp
)
;
PNT_TIB
pTib
=
reinterpret_cast
<
PNT_TIB
>
(
NtCurrentTeb
(
)
)
;
void
*
stackEnd
=
static_cast
<
void
*
>
(
pTib
-
>
StackBase
)
;
FramePointerStackWalk
(
StackWalkCallback
0
kMaxFrames
this
fp
stackEnd
)
;
#
elif
defined
(
XP_MACOSX
)
void
*
*
fp
;
asm
(
"
movq
(
%
%
rbp
)
%
0
\
n
\
t
"
:
"
=
r
"
(
fp
)
)
;
void
*
stackEnd
=
pthread_get_stackaddr_np
(
pthread_self
(
)
)
;
FramePointerStackWalk
(
StackWalkCallback
0
kMaxFrames
this
fp
stackEnd
)
;
#
else
MozStackWalk
(
StackWalkCallback
0
kMaxFrames
this
)
;
#
endif
}
#
define
PHC_LOGGING
0
#
if
PHC_LOGGING
static
size_t
GetPid
(
)
{
return
size_t
(
getpid
(
)
)
;
}
static
size_t
GetTid
(
)
{
#
if
defined
(
XP_WIN
)
return
size_t
(
GetCurrentThreadId
(
)
)
;
#
else
return
size_t
(
pthread_self
(
)
)
;
#
endif
}
#
if
defined
(
XP_WIN
)
#
define
LOG_STDERR
\
reinterpret_cast
<
intptr_t
>
(
GetStdHandle
(
STD_ERROR_HANDLE
)
)
#
else
#
define
LOG_STDERR
2
#
endif
#
define
LOG
(
fmt
.
.
.
)
\
FdPrintf
(
LOG_STDERR
"
PHC
[
%
zu
%
zu
~
%
zu
]
"
fmt
GetPid
(
)
GetTid
(
)
\
size_t
(
GAtomic
:
:
Now
(
)
)
__VA_ARGS__
)
#
else
#
define
LOG
(
fmt
.
.
.
)
#
endif
using
Time
=
uint64_t
;
using
Delay
=
uint32_t
;
static
const
size_t
kPageSize
=
4096
;
static
const
size_t
kMaxPageAllocs
=
64
;
static
const
size_t
kAllPagesSize
=
kPageSize
*
kMaxPageAllocs
;
const
uint8_t
kAllocJunk
=
0xe4
;
static
const
Time
kMaxTime
=
~
(
Time
(
0
)
)
;
static
const
Delay
kAvgFirstAllocDelay
=
512
*
1024
;
static
const
Delay
kAvgAllocDelay
=
2
*
1024
;
static
const
Delay
kAvgPageReuseDelay
=
32
*
1024
;
template
<
Delay
AvgDelay
>
constexpr
Delay
Rnd64ToDelay
(
uint64_t
aRnd
)
{
static_assert
(
IsPowerOfTwo
(
AvgDelay
)
"
must
be
a
power
of
two
"
)
;
return
aRnd
%
(
AvgDelay
*
2
)
+
1
;
}
class
GAtomic
{
public
:
static
void
Init
(
Delay
aFirstDelay
)
{
sAllocDelay
=
aFirstDelay
;
LOG
(
"
Initial
sAllocDelay
<
-
%
zu
\
n
"
size_t
(
aFirstDelay
)
)
;
}
static
Time
Now
(
)
{
return
sNow
;
}
static
void
IncrementNow
(
)
{
sNow
+
+
;
}
static
int32_t
DecrementDelay
(
)
{
return
-
-
sAllocDelay
;
}
static
void
SetAllocDelay
(
Delay
aAllocDelay
)
{
sAllocDelay
=
aAllocDelay
;
}
private
:
static
Atomic
<
Time
Relaxed
Behavior
:
:
DontPreserve
>
sNow
;
static
Atomic
<
Delay
ReleaseAcquire
Behavior
:
:
DontPreserve
>
sAllocDelay
;
}
;
Atomic
<
Time
Relaxed
Behavior
:
:
DontPreserve
>
GAtomic
:
:
sNow
;
Atomic
<
Delay
ReleaseAcquire
Behavior
:
:
DontPreserve
>
GAtomic
:
:
sAllocDelay
;
class
GConst
{
private
:
const
uintptr_t
mPagesStart
;
const
uintptr_t
mPagesLimit
;
uintptr_t
AllocPages
(
)
{
void
*
pages
=
#
ifdef
XP_WIN
VirtualAlloc
(
nullptr
kAllPagesSize
MEM_RESERVE
PAGE_NOACCESS
)
;
#
else
mmap
(
nullptr
kAllPagesSize
PROT_NONE
MAP_ANONYMOUS
|
MAP_PRIVATE
-
1
0
)
;
#
endif
if
(
!
pages
)
{
MOZ_CRASH
(
)
;
}
return
reinterpret_cast
<
uintptr_t
>
(
pages
)
;
}
public
:
GConst
(
)
:
mPagesStart
(
AllocPages
(
)
)
mPagesLimit
(
mPagesStart
+
kAllPagesSize
)
{
LOG
(
"
AllocPages
at
%
p
.
.
%
p
\
n
"
(
void
*
)
mPagesStart
(
void
*
)
mPagesLimit
)
;
}
Maybe
<
uintptr_t
>
PageIndex
(
const
void
*
aPtr
)
{
auto
ptr
=
reinterpret_cast
<
uintptr_t
>
(
aPtr
)
;
if
(
!
(
mPagesStart
<
=
ptr
&
&
ptr
<
mPagesLimit
)
)
{
return
Nothing
(
)
;
}
size_t
i
=
(
ptr
-
mPagesStart
)
/
kPageSize
;
MOZ_ASSERT
(
i
<
kMaxPageAllocs
)
;
return
Some
(
i
)
;
}
void
*
PagePtr
(
size_t
aIndex
)
{
MOZ_ASSERT
(
aIndex
<
kMaxPageAllocs
)
;
return
reinterpret_cast
<
void
*
>
(
mPagesStart
+
kPageSize
*
aIndex
)
;
}
}
;
static
GConst
*
gConst
;
#
if
!
defined
(
XP_DARWIN
)
#
define
PHC_THREAD_LOCAL
(
T
)
MOZ_THREAD_LOCAL
(
T
)
#
else
#
define
PHC_THREAD_LOCAL
(
T
)
\
detail
:
:
ThreadLocal
<
T
detail
:
:
ThreadLocalKeyStorage
>
#
endif
class
GTls
{
DISALLOW_COPY_AND_ASSIGN
(
GTls
)
;
static
PHC_THREAD_LOCAL
(
bool
)
tlsIsDisabled
;
public
:
static
void
Init
(
)
{
if
(
!
tlsIsDisabled
.
init
(
)
)
{
MOZ_CRASH
(
)
;
}
}
static
void
DisableOnCurrentThread
(
)
{
MOZ_ASSERT
(
!
GTls
:
:
tlsIsDisabled
.
get
(
)
)
;
tlsIsDisabled
.
set
(
true
)
;
}
static
void
EnableOnCurrentThread
(
)
{
MOZ_ASSERT
(
GTls
:
:
tlsIsDisabled
.
get
(
)
)
;
tlsIsDisabled
.
set
(
false
)
;
}
static
bool
IsDisabledOnCurrentThread
(
)
{
return
tlsIsDisabled
.
get
(
)
;
}
}
;
PHC_THREAD_LOCAL
(
bool
)
GTls
:
:
tlsIsDisabled
;
class
AutoDisableOnCurrentThread
{
DISALLOW_COPY_AND_ASSIGN
(
AutoDisableOnCurrentThread
)
;
public
:
explicit
AutoDisableOnCurrentThread
(
)
{
GTls
:
:
DisableOnCurrentThread
(
)
;
}
~
AutoDisableOnCurrentThread
(
)
{
GTls
:
:
EnableOnCurrentThread
(
)
;
}
}
;
using
GMutLock
=
const
MutexAutoLock
&
;
class
GMut
{
enum
class
PageState
{
NeverAllocated
=
0
InUse
=
1
Freed
=
2
}
;
class
PageInfo
{
public
:
PageInfo
(
)
:
mState
(
PageState
:
:
NeverAllocated
)
mArenaId
(
)
mUsableSize
(
0
)
mAllocStack
(
)
mFreeStack
(
)
mReuseTime
(
0
)
{
}
PageState
mState
;
Maybe
<
arena_id_t
>
mArenaId
;
size_t
mUsableSize
;
Maybe
<
StackTrace
>
mAllocStack
;
Maybe
<
StackTrace
>
mFreeStack
;
Time
mReuseTime
;
}
;
public
:
static
Mutex
sMutex
;
GMut
(
)
:
mRNG
(
RandomSeed
<
0
>
(
)
RandomSeed
<
1
>
(
)
)
mPages
(
)
{
sMutex
.
Init
(
)
;
}
uint64_t
Random64
(
GMutLock
)
{
return
mRNG
.
next
(
)
;
}
bool
IsPageInUse
(
GMutLock
uintptr_t
aIndex
)
{
return
mPages
[
aIndex
]
.
mState
=
=
PageState
:
:
InUse
;
}
bool
IsPageAllocatable
(
GMutLock
uintptr_t
aIndex
Time
aNow
)
{
const
PageInfo
&
page
=
mPages
[
aIndex
]
;
return
page
.
mState
!
=
PageState
:
:
InUse
&
&
aNow
>
=
page
.
mReuseTime
;
}
Maybe
<
arena_id_t
>
PageArena
(
GMutLock
aLock
uintptr_t
aIndex
)
{
const
PageInfo
&
page
=
mPages
[
aIndex
]
;
AssertPageInUse
(
aLock
page
)
;
return
page
.
mArenaId
;
}
size_t
PageUsableSize
(
GMutLock
aLock
uintptr_t
aIndex
)
{
const
PageInfo
&
page
=
mPages
[
aIndex
]
;
AssertPageInUse
(
aLock
page
)
;
return
page
.
mUsableSize
;
}
void
SetPageInUse
(
GMutLock
aLock
uintptr_t
aIndex
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aUsableSize
const
StackTrace
&
aAllocStack
)
{
MOZ_ASSERT
(
aUsableSize
=
=
sMallocTable
.
malloc_good_size
(
aUsableSize
)
)
;
PageInfo
&
page
=
mPages
[
aIndex
]
;
AssertPageNotInUse
(
aLock
page
)
;
page
.
mState
=
PageState
:
:
InUse
;
page
.
mArenaId
=
aArenaId
;
page
.
mUsableSize
=
aUsableSize
;
page
.
mAllocStack
=
Some
(
aAllocStack
)
;
page
.
mFreeStack
=
Nothing
(
)
;
page
.
mReuseTime
=
kMaxTime
;
}
void
ResizePageInUse
(
GMutLock
aLock
uintptr_t
aIndex
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aNewUsableSize
const
StackTrace
&
aAllocStack
)
{
MOZ_ASSERT
(
aNewUsableSize
=
=
sMallocTable
.
malloc_good_size
(
aNewUsableSize
)
)
;
PageInfo
&
page
=
mPages
[
aIndex
]
;
AssertPageInUse
(
aLock
page
)
;
if
(
aArenaId
.
isSome
(
)
)
{
MOZ_RELEASE_ASSERT
(
page
.
mArenaId
=
=
aArenaId
)
;
}
page
.
mUsableSize
=
aNewUsableSize
;
page
.
mAllocStack
=
Some
(
aAllocStack
)
;
}
;
void
SetPageFreed
(
GMutLock
aLock
uintptr_t
aIndex
const
Maybe
<
arena_id_t
>
&
aArenaId
const
StackTrace
&
aFreeStack
Delay
aReuseDelay
)
{
PageInfo
&
page
=
mPages
[
aIndex
]
;
AssertPageInUse
(
aLock
page
)
;
page
.
mState
=
PageState
:
:
Freed
;
if
(
aArenaId
.
isSome
(
)
)
{
MOZ_RELEASE_ASSERT
(
page
.
mArenaId
=
=
aArenaId
)
;
}
page
.
mFreeStack
=
Some
(
aFreeStack
)
;
page
.
mReuseTime
=
GAtomic
:
:
Now
(
)
+
aReuseDelay
;
}
void
EnsureInUse
(
GMutLock
void
*
aPtr
uintptr_t
aIndex
)
{
const
PageInfo
&
page
=
mPages
[
aIndex
]
;
MOZ_RELEASE_ASSERT
(
page
.
mState
!
=
PageState
:
:
NeverAllocated
)
;
if
(
page
.
mState
=
=
PageState
:
:
Freed
)
{
LOG
(
"
EnsureInUse
(
%
p
)
failure
\
n
"
aPtr
)
;
sMutex
.
Unlock
(
)
;
*
static_cast
<
char
*
>
(
aPtr
)
=
0
;
MOZ_CRASH
(
"
unreachable
"
)
;
}
}
void
FillAddrInfo
(
GMutLock
uintptr_t
aIndex
const
void
*
aBaseAddr
phc
:
:
AddrInfo
&
aOut
)
{
const
PageInfo
&
page
=
mPages
[
aIndex
]
;
switch
(
page
.
mState
)
{
case
PageState
:
:
NeverAllocated
:
aOut
.
mKind
=
phc
:
:
AddrInfo
:
:
Kind
:
:
NeverAllocatedPage
;
break
;
case
PageState
:
:
InUse
:
aOut
.
mKind
=
phc
:
:
AddrInfo
:
:
Kind
:
:
InUsePage
;
break
;
case
PageState
:
:
Freed
:
aOut
.
mKind
=
phc
:
:
AddrInfo
:
:
Kind
:
:
FreedPage
;
break
;
default
:
MOZ_CRASH
(
)
;
}
aOut
.
mBaseAddr
=
const_cast
<
const
void
*
>
(
gConst
-
>
PagePtr
(
aIndex
)
)
;
aOut
.
mUsableSize
=
page
.
mUsableSize
;
aOut
.
mAllocStack
=
page
.
mAllocStack
;
aOut
.
mFreeStack
=
page
.
mFreeStack
;
}
void
FillJemallocPtrInfo
(
GMutLock
const
void
*
aPtr
uintptr_t
aIndex
jemalloc_ptr_info_t
*
aInfo
)
{
const
PageInfo
&
page
=
mPages
[
aIndex
]
;
switch
(
page
.
mState
)
{
case
PageState
:
:
NeverAllocated
:
break
;
case
PageState
:
:
InUse
:
{
char
*
pagePtr
=
static_cast
<
char
*
>
(
gConst
-
>
PagePtr
(
aIndex
)
)
;
if
(
aPtr
<
pagePtr
+
page
.
mUsableSize
)
{
*
aInfo
=
{
TagLiveAlloc
pagePtr
page
.
mUsableSize
page
.
mArenaId
.
valueOr
(
0
)
}
;
return
;
}
break
;
}
case
PageState
:
:
Freed
:
{
char
*
pagePtr
=
static_cast
<
char
*
>
(
gConst
-
>
PagePtr
(
aIndex
)
)
;
if
(
aPtr
<
pagePtr
+
page
.
mUsableSize
)
{
*
aInfo
=
{
TagFreedAlloc
gConst
-
>
PagePtr
(
aIndex
)
page
.
mUsableSize
page
.
mArenaId
.
valueOr
(
0
)
}
;
return
;
}
break
;
}
default
:
MOZ_CRASH
(
)
;
}
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
}
static
void
prefork
(
)
{
sMutex
.
Lock
(
)
;
}
static
void
postfork
(
)
{
sMutex
.
Unlock
(
)
;
}
private
:
template
<
int
N
>
uint64_t
RandomSeed
(
)
{
static_assert
(
N
=
=
0
|
|
N
=
=
1
"
must
be
0
or
1
"
)
;
uint64_t
seed
;
if
(
N
=
=
0
)
{
time_t
t
=
time
(
nullptr
)
;
seed
=
t
^
(
t
<
<
32
)
;
}
else
{
seed
=
uintptr_t
(
&
seed
)
^
(
uintptr_t
(
&
seed
)
<
<
32
)
;
}
return
seed
;
}
void
AssertPageInUse
(
GMutLock
const
PageInfo
&
aPage
)
{
MOZ_ASSERT
(
aPage
.
mState
=
=
PageState
:
:
InUse
)
;
MOZ_ASSERT
(
aPage
.
mUsableSize
>
0
)
;
MOZ_ASSERT
(
aPage
.
mAllocStack
.
isSome
(
)
)
;
MOZ_ASSERT
(
aPage
.
mFreeStack
.
isNothing
(
)
)
;
MOZ_ASSERT
(
aPage
.
mReuseTime
=
=
kMaxTime
)
;
}
void
AssertPageNotInUse
(
GMutLock
const
PageInfo
&
aPage
)
{
#
ifdef
DEBUG
bool
isFresh
=
aPage
.
mState
=
=
PageState
:
:
NeverAllocated
;
MOZ_ASSERT
(
isFresh
|
|
aPage
.
mState
=
=
PageState
:
:
Freed
)
;
MOZ_ASSERT_IF
(
isFresh
aPage
.
mArenaId
=
=
Nothing
(
)
)
;
MOZ_ASSERT
(
isFresh
=
=
(
aPage
.
mUsableSize
=
=
0
)
)
;
MOZ_ASSERT
(
isFresh
=
=
(
aPage
.
mAllocStack
.
isNothing
(
)
)
)
;
MOZ_ASSERT
(
isFresh
=
=
(
aPage
.
mFreeStack
.
isNothing
(
)
)
)
;
MOZ_ASSERT
(
aPage
.
mReuseTime
!
=
kMaxTime
)
;
#
endif
}
non_crypto
:
:
XorShift128PlusRNG
mRNG
;
PageInfo
mPages
[
kMaxPageAllocs
]
;
}
;
Mutex
GMut
:
:
sMutex
;
static
GMut
*
gMut
;
static
void
*
MaybePageAlloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aReqSize
bool
aZero
)
{
if
(
aReqSize
>
kPageSize
)
{
return
nullptr
;
}
GAtomic
:
:
IncrementNow
(
)
;
int32_t
newDelay
=
GAtomic
:
:
DecrementDelay
(
)
;
if
(
newDelay
!
=
0
)
{
return
nullptr
;
}
if
(
GTls
:
:
IsDisabledOnCurrentThread
(
)
)
{
return
nullptr
;
}
AutoDisableOnCurrentThread
disable
;
StackTrace
allocStack
;
allocStack
.
Fill
(
)
;
MutexAutoLock
lock
(
GMut
:
:
sMutex
)
;
Time
now
=
GAtomic
:
:
Now
(
)
;
Delay
newAllocDelay
=
Rnd64ToDelay
<
kAvgAllocDelay
>
(
gMut
-
>
Random64
(
lock
)
)
;
void
*
ptr
=
nullptr
;
for
(
uintptr_t
n
=
0
i
=
size_t
(
gMut
-
>
Random64
(
lock
)
)
%
kMaxPageAllocs
;
n
<
kMaxPageAllocs
;
n
+
+
i
=
(
i
+
1
)
%
kMaxPageAllocs
)
{
if
(
gMut
-
>
IsPageAllocatable
(
lock
i
now
)
)
{
void
*
pagePtr
=
gConst
-
>
PagePtr
(
i
)
;
bool
ok
=
#
ifdef
XP_WIN
!
!
VirtualAlloc
(
pagePtr
kPageSize
MEM_COMMIT
PAGE_READWRITE
)
;
#
else
mprotect
(
pagePtr
kPageSize
PROT_READ
|
PROT_WRITE
)
=
=
0
;
#
endif
size_t
usableSize
=
sMallocTable
.
malloc_good_size
(
aReqSize
)
;
if
(
ok
)
{
gMut
-
>
SetPageInUse
(
lock
i
aArenaId
usableSize
allocStack
)
;
ptr
=
pagePtr
;
if
(
aZero
)
{
memset
(
ptr
0
usableSize
)
;
}
else
{
#
ifdef
DEBUG
memset
(
ptr
kAllocJunk
usableSize
)
;
#
endif
}
}
LOG
(
"
PageAlloc
(
%
zu
)
-
>
%
p
[
%
zu
]
(
%
zu
)
(
z
%
zu
)
sAllocDelay
<
-
%
zu
\
n
"
aReqSize
ptr
i
usableSize
size_t
(
aZero
)
size_t
(
newAllocDelay
)
)
;
break
;
}
}
if
(
!
ptr
)
{
LOG
(
"
No
PageAlloc
(
%
zu
)
sAllocDelay
<
-
%
zu
\
n
"
aReqSize
size_t
(
newAllocDelay
)
)
;
}
GAtomic
:
:
SetAllocDelay
(
newAllocDelay
)
;
return
ptr
;
}
static
void
FreePage
(
GMutLock
aLock
size_t
aIndex
const
Maybe
<
arena_id_t
>
&
aArenaId
const
StackTrace
&
aFreeStack
Delay
aReuseDelay
)
{
void
*
pagePtr
=
gConst
-
>
PagePtr
(
aIndex
)
;
#
ifdef
XP_WIN
if
(
!
VirtualFree
(
pagePtr
kPageSize
MEM_DECOMMIT
)
)
{
return
;
}
#
else
if
(
!
mmap
(
pagePtr
kPageSize
PROT_NONE
MAP_FIXED
|
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
)
{
return
;
}
#
endif
gMut
-
>
SetPageFreed
(
aLock
aIndex
aArenaId
aFreeStack
aReuseDelay
)
;
}
MOZ_ALWAYS_INLINE
static
void
*
PageMalloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aReqSize
)
{
void
*
ptr
=
MaybePageAlloc
(
aArenaId
aReqSize
false
)
;
return
ptr
?
ptr
:
(
aArenaId
.
isSome
(
)
?
sMallocTable
.
moz_arena_malloc
(
*
aArenaId
aReqSize
)
:
sMallocTable
.
malloc
(
aReqSize
)
)
;
}
static
void
*
replace_malloc
(
size_t
aReqSize
)
{
return
PageMalloc
(
Nothing
(
)
aReqSize
)
;
}
MOZ_ALWAYS_INLINE
static
void
*
PageCalloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aNum
size_t
aReqSize
)
{
CheckedInt
<
size_t
>
checkedSize
=
CheckedInt
<
size_t
>
(
aNum
)
*
aReqSize
;
if
(
!
checkedSize
.
isValid
(
)
)
{
return
nullptr
;
}
void
*
ptr
=
MaybePageAlloc
(
aArenaId
checkedSize
.
value
(
)
true
)
;
return
ptr
?
ptr
:
(
aArenaId
.
isSome
(
)
?
sMallocTable
.
moz_arena_calloc
(
*
aArenaId
aNum
aReqSize
)
:
sMallocTable
.
calloc
(
aNum
aReqSize
)
)
;
}
static
void
*
replace_calloc
(
size_t
aNum
size_t
aReqSize
)
{
return
PageCalloc
(
Nothing
(
)
aNum
aReqSize
)
;
}
MOZ_ALWAYS_INLINE
static
void
*
PageRealloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aOldPtr
size_t
aNewSize
)
{
if
(
!
aOldPtr
)
{
return
PageMalloc
(
aArenaId
aNewSize
)
;
}
Maybe
<
uintptr_t
>
i
=
gConst
-
>
PageIndex
(
aOldPtr
)
;
if
(
i
.
isNothing
(
)
)
{
return
aArenaId
.
isSome
(
)
?
sMallocTable
.
moz_arena_realloc
(
*
aArenaId
aOldPtr
aNewSize
)
:
sMallocTable
.
realloc
(
aOldPtr
aNewSize
)
;
}
Maybe
<
AutoDisableOnCurrentThread
>
disable
;
StackTrace
stack
;
if
(
GTls
:
:
IsDisabledOnCurrentThread
(
)
)
{
}
else
{
disable
.
emplace
(
)
;
stack
.
Fill
(
)
;
}
MutexAutoLock
lock
(
GMut
:
:
sMutex
)
;
gMut
-
>
EnsureInUse
(
lock
aOldPtr
*
i
)
;
if
(
aNewSize
<
=
kPageSize
)
{
size_t
newUsableSize
=
sMallocTable
.
malloc_good_size
(
aNewSize
)
;
gMut
-
>
ResizePageInUse
(
lock
*
i
aArenaId
newUsableSize
stack
)
;
LOG
(
"
PageRealloc
-
Reuse
(
%
p
%
zu
)
\
n
"
aOldPtr
aNewSize
)
;
return
aOldPtr
;
}
void
*
newPtr
;
if
(
aArenaId
.
isSome
(
)
)
{
newPtr
=
sMallocTable
.
moz_arena_malloc
(
*
aArenaId
aNewSize
)
;
}
else
{
Maybe
<
arena_id_t
>
oldArenaId
=
gMut
-
>
PageArena
(
lock
*
i
)
;
newPtr
=
(
oldArenaId
.
isSome
(
)
?
sMallocTable
.
moz_arena_malloc
(
*
oldArenaId
aNewSize
)
:
sMallocTable
.
malloc
(
aNewSize
)
)
;
}
if
(
!
newPtr
)
{
return
nullptr
;
}
MOZ_ASSERT
(
aNewSize
>
kPageSize
)
;
Delay
reuseDelay
=
Rnd64ToDelay
<
kAvgPageReuseDelay
>
(
gMut
-
>
Random64
(
lock
)
)
;
size_t
oldUsableSize
=
gMut
-
>
PageUsableSize
(
lock
*
i
)
;
memcpy
(
newPtr
aOldPtr
std
:
:
min
(
oldUsableSize
aNewSize
)
)
;
FreePage
(
lock
*
i
aArenaId
stack
reuseDelay
)
;
LOG
(
"
PageRealloc
-
Free
(
%
p
[
%
zu
]
%
zu
)
-
>
%
p
%
zu
delay
reuse
at
~
%
zu
\
n
"
aOldPtr
*
i
aNewSize
newPtr
size_t
(
reuseDelay
)
size_t
(
GAtomic
:
:
Now
(
)
)
+
reuseDelay
)
;
return
newPtr
;
}
static
void
*
replace_realloc
(
void
*
aOldPtr
size_t
aNewSize
)
{
return
PageRealloc
(
Nothing
(
)
aOldPtr
aNewSize
)
;
}
MOZ_ALWAYS_INLINE
static
void
PageFree
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aPtr
)
{
Maybe
<
uintptr_t
>
i
=
gConst
-
>
PageIndex
(
aPtr
)
;
if
(
i
.
isNothing
(
)
)
{
return
aArenaId
.
isSome
(
)
?
sMallocTable
.
moz_arena_free
(
*
aArenaId
aPtr
)
:
sMallocTable
.
free
(
aPtr
)
;
}
Maybe
<
AutoDisableOnCurrentThread
>
disable
;
StackTrace
freeStack
;
if
(
GTls
:
:
IsDisabledOnCurrentThread
(
)
)
{
}
else
{
disable
.
emplace
(
)
;
freeStack
.
Fill
(
)
;
}
MutexAutoLock
lock
(
GMut
:
:
sMutex
)
;
gMut
-
>
EnsureInUse
(
lock
aPtr
*
i
)
;
Delay
reuseDelay
=
Rnd64ToDelay
<
kAvgPageReuseDelay
>
(
gMut
-
>
Random64
(
lock
)
)
;
FreePage
(
lock
*
i
aArenaId
freeStack
reuseDelay
)
;
LOG
(
"
PageFree
(
%
p
[
%
zu
]
)
%
zu
delay
reuse
at
~
%
zu
\
n
"
aPtr
*
i
size_t
(
reuseDelay
)
size_t
(
GAtomic
:
:
Now
(
)
)
+
reuseDelay
)
;
}
static
void
replace_free
(
void
*
aPtr
)
{
return
PageFree
(
Nothing
(
)
aPtr
)
;
}
MOZ_ALWAYS_INLINE
static
void
*
PageMemalign
(
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aAlignment
size_t
aReqSize
)
{
MOZ_ASSERT
(
IsPowerOfTwo
(
aAlignment
)
)
;
void
*
ptr
=
nullptr
;
if
(
aAlignment
<
=
kPageSize
)
{
ptr
=
MaybePageAlloc
(
aArenaId
aReqSize
false
)
;
}
return
ptr
?
ptr
:
(
aArenaId
.
isSome
(
)
?
sMallocTable
.
moz_arena_memalign
(
*
aArenaId
aAlignment
aReqSize
)
:
sMallocTable
.
memalign
(
aAlignment
aReqSize
)
)
;
}
static
void
*
replace_memalign
(
size_t
aAlignment
size_t
aReqSize
)
{
return
PageMemalign
(
Nothing
(
)
aAlignment
aReqSize
)
;
}
static
size_t
replace_malloc_usable_size
(
usable_ptr_t
aPtr
)
{
Maybe
<
uintptr_t
>
i
=
gConst
-
>
PageIndex
(
aPtr
)
;
if
(
i
.
isNothing
(
)
)
{
return
sMallocTable
.
malloc_usable_size
(
aPtr
)
;
}
MutexAutoLock
lock
(
GMut
:
:
sMutex
)
;
gMut
-
>
EnsureInUse
(
lock
const_cast
<
void
*
>
(
aPtr
)
*
i
)
;
return
gMut
-
>
PageUsableSize
(
lock
*
i
)
;
}
void
replace_jemalloc_stats
(
jemalloc_stats_t
*
aStats
)
{
sMallocTable
.
jemalloc_stats
(
aStats
)
;
size_t
mapped
=
kAllPagesSize
;
aStats
-
>
mapped
+
=
mapped
;
size_t
allocated
=
0
;
{
MutexAutoLock
lock
(
GMut
:
:
sMutex
)
;
for
(
size_t
i
=
0
;
i
<
kMaxPageAllocs
;
i
+
+
)
{
if
(
gMut
-
>
IsPageInUse
(
lock
i
)
)
{
allocated
+
=
gMut
-
>
PageUsableSize
(
lock
i
)
;
}
}
}
aStats
-
>
allocated
+
=
allocated
;
size_t
waste
=
mapped
-
allocated
;
aStats
-
>
waste
+
=
waste
;
size_t
bookkeeping
=
sMallocTable
.
malloc_usable_size
(
gConst
)
+
sMallocTable
.
malloc_usable_size
(
gMut
)
;
aStats
-
>
allocated
-
=
bookkeeping
;
aStats
-
>
bookkeeping
+
=
bookkeeping
;
}
void
replace_jemalloc_ptr_info
(
const
void
*
aPtr
jemalloc_ptr_info_t
*
aInfo
)
{
Maybe
<
uintptr_t
>
i
=
gConst
-
>
PageIndex
(
aPtr
)
;
if
(
i
.
isNothing
(
)
)
{
return
sMallocTable
.
jemalloc_ptr_info
(
aPtr
aInfo
)
;
}
MutexAutoLock
lock
(
GMut
:
:
sMutex
)
;
gMut
-
>
FillJemallocPtrInfo
(
lock
aPtr
*
i
aInfo
)
;
#
if
DEBUG
LOG
(
"
JemallocPtrInfo
(
%
p
[
%
zu
]
)
-
>
{
%
zu
%
p
%
zu
%
zu
}
\
n
"
aPtr
*
i
size_t
(
aInfo
-
>
tag
)
aInfo
-
>
addr
aInfo
-
>
size
aInfo
-
>
arenaId
)
;
#
else
LOG
(
"
JemallocPtrInfo
(
%
p
[
%
zu
]
)
-
>
{
%
zu
%
p
%
zu
}
\
n
"
aPtr
*
i
size_t
(
aInfo
-
>
tag
)
aInfo
-
>
addr
aInfo
-
>
size
)
;
#
endif
}
arena_id_t
replace_moz_create_arena_with_params
(
arena_params_t
*
aParams
)
{
return
sMallocTable
.
moz_create_arena_with_params
(
aParams
)
;
}
void
replace_moz_dispose_arena
(
arena_id_t
aArenaId
)
{
return
sMallocTable
.
moz_dispose_arena
(
aArenaId
)
;
}
void
*
replace_moz_arena_malloc
(
arena_id_t
aArenaId
size_t
aReqSize
)
{
return
PageMalloc
(
Some
(
aArenaId
)
aReqSize
)
;
}
void
*
replace_moz_arena_calloc
(
arena_id_t
aArenaId
size_t
aNum
size_t
aReqSize
)
{
return
PageCalloc
(
Some
(
aArenaId
)
aNum
aReqSize
)
;
}
void
*
replace_moz_arena_realloc
(
arena_id_t
aArenaId
void
*
aOldPtr
size_t
aNewSize
)
{
return
PageRealloc
(
Some
(
aArenaId
)
aOldPtr
aNewSize
)
;
}
void
replace_moz_arena_free
(
arena_id_t
aArenaId
void
*
aPtr
)
{
return
PageFree
(
Some
(
aArenaId
)
aPtr
)
;
}
void
*
replace_moz_arena_memalign
(
arena_id_t
aArenaId
size_t
aAlignment
size_t
aReqSize
)
{
return
PageMemalign
(
Some
(
aArenaId
)
aAlignment
aReqSize
)
;
}
class
PHCBridge
:
public
ReplaceMallocBridge
{
virtual
bool
IsPHCAllocation
(
const
void
*
aPtr
phc
:
:
AddrInfo
*
aOut
)
override
{
Maybe
<
uintptr_t
>
i
=
gConst
-
>
PageIndex
(
aPtr
)
;
if
(
i
.
isNothing
(
)
)
{
return
false
;
}
if
(
aOut
)
{
MutexAutoLock
lock
(
GMut
:
:
sMutex
)
;
gMut
-
>
FillAddrInfo
(
lock
*
i
aPtr
*
aOut
)
;
LOG
(
"
IsPHCAllocation
:
%
zu
%
p
%
zu
%
zu
%
zu
\
n
"
size_t
(
aOut
-
>
mKind
)
aOut
-
>
mBaseAddr
aOut
-
>
mUsableSize
aOut
-
>
mAllocStack
.
isSome
(
)
?
aOut
-
>
mAllocStack
-
>
mLength
:
0
aOut
-
>
mFreeStack
.
isSome
(
)
?
aOut
-
>
mFreeStack
-
>
mLength
:
0
)
;
}
return
true
;
}
virtual
void
DisablePHCOnCurrentThread
(
)
override
{
GTls
:
:
DisableOnCurrentThread
(
)
;
LOG
(
"
DisablePHCOnCurrentThread
:
%
zu
\
n
"
0ul
)
;
}
virtual
void
ReenablePHCOnCurrentThread
(
)
override
{
GTls
:
:
EnableOnCurrentThread
(
)
;
LOG
(
"
ReenablePHCOnCurrentThread
:
%
zu
\
n
"
0ul
)
;
}
virtual
bool
IsPHCEnabledOnCurrentThread
(
)
override
{
bool
enabled
=
!
GTls
:
:
IsDisabledOnCurrentThread
(
)
;
LOG
(
"
IsPHCEnabledOnCurrentThread
:
%
zu
\
n
"
size_t
(
enabled
)
)
;
return
enabled
;
}
}
;
void
replace_init
(
malloc_table_t
*
aMallocTable
ReplaceMallocBridge
*
*
aBridge
)
{
jemalloc_stats_t
stats
;
aMallocTable
-
>
jemalloc_stats
(
&
stats
)
;
if
(
stats
.
page_size
!
=
kPageSize
)
{
return
;
}
sMallocTable
=
*
aMallocTable
;
aMallocTable
-
>
malloc
=
replace_malloc
;
aMallocTable
-
>
calloc
=
replace_calloc
;
aMallocTable
-
>
realloc
=
replace_realloc
;
aMallocTable
-
>
free
=
replace_free
;
aMallocTable
-
>
memalign
=
replace_memalign
;
aMallocTable
-
>
malloc_usable_size
=
replace_malloc_usable_size
;
aMallocTable
-
>
jemalloc_stats
=
replace_jemalloc_stats
;
aMallocTable
-
>
jemalloc_ptr_info
=
replace_jemalloc_ptr_info
;
aMallocTable
-
>
moz_create_arena_with_params
=
replace_moz_create_arena_with_params
;
aMallocTable
-
>
moz_dispose_arena
=
replace_moz_dispose_arena
;
aMallocTable
-
>
moz_arena_malloc
=
replace_moz_arena_malloc
;
aMallocTable
-
>
moz_arena_calloc
=
replace_moz_arena_calloc
;
aMallocTable
-
>
moz_arena_realloc
=
replace_moz_arena_realloc
;
aMallocTable
-
>
moz_arena_free
=
replace_moz_arena_free
;
aMallocTable
-
>
moz_arena_memalign
=
replace_moz_arena_memalign
;
static
PHCBridge
bridge
;
*
aBridge
=
&
bridge
;
#
ifndef
XP_WIN
sMallocTable
.
malloc
(
-
1
)
;
pthread_atfork
(
GMut
:
:
prefork
GMut
:
:
postfork
GMut
:
:
postfork
)
;
#
endif
gConst
=
InfallibleAllocPolicy
:
:
new_
<
GConst
>
(
)
;
GTls
:
:
Init
(
)
;
gMut
=
InfallibleAllocPolicy
:
:
new_
<
GMut
>
(
)
;
{
MutexAutoLock
lock
(
GMut
:
:
sMutex
)
;
Delay
firstAllocDelay
=
Rnd64ToDelay
<
kAvgFirstAllocDelay
>
(
gMut
-
>
Random64
(
lock
)
)
;
GAtomic
:
:
Init
(
firstAllocDelay
)
;
}
}
