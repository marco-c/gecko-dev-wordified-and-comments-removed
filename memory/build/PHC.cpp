#
include
"
PHC
.
h
"
#
include
<
stdlib
.
h
>
#
include
<
time
.
h
>
#
include
<
algorithm
>
#
ifdef
XP_WIN
#
include
<
process
.
h
>
#
else
#
include
<
sys
/
mman
.
h
>
#
include
<
sys
/
types
.
h
>
#
include
<
pthread
.
h
>
#
include
<
unistd
.
h
>
#
endif
#
include
"
mozjemalloc
.
h
"
#
include
"
mozjemalloc
.
h
"
#
include
"
FdPrintf
.
h
"
#
include
"
Mutex
.
h
"
#
include
"
mozilla
/
Array
.
h
"
#
include
"
mozilla
/
Assertions
.
h
"
#
include
"
mozilla
/
Atomics
.
h
"
#
include
"
mozilla
/
Attributes
.
h
"
#
include
"
mozilla
/
CheckedInt
.
h
"
#
include
"
mozilla
/
Maybe
.
h
"
#
include
"
mozilla
/
StackWalk
.
h
"
#
include
"
mozilla
/
ThreadLocal
.
h
"
#
include
"
mozilla
/
XorShift128PlusRNG
.
h
"
using
namespace
mozilla
;
#
ifdef
ANDROID
extern
"
C
"
MOZ_EXPORT
int
pthread_atfork
(
void
(
*
)
(
void
)
void
(
*
)
(
void
)
void
(
*
)
(
void
)
)
;
#
endif
#
ifndef
DISALLOW_COPY_AND_ASSIGN
#
define
DISALLOW_COPY_AND_ASSIGN
(
T
)
\
T
(
const
T
&
)
;
\
void
operator
=
(
const
T
&
)
#
endif
class
InfallibleAllocPolicy
{
public
:
static
void
AbortOnFailure
(
const
void
*
aP
)
{
if
(
!
aP
)
{
MOZ_CRASH
(
"
PHC
failed
to
allocate
"
)
;
}
}
template
<
class
T
>
static
T
*
new_
(
)
{
void
*
p
=
MozJemalloc
:
:
malloc
(
sizeof
(
T
)
)
;
AbortOnFailure
(
p
)
;
return
new
(
p
)
T
;
}
}
;
class
StackTrace
:
public
phc
:
:
StackTrace
{
public
:
StackTrace
(
)
=
default
;
void
Clear
(
)
{
mLength
=
0
;
}
void
Fill
(
)
;
private
:
static
void
StackWalkCallback
(
uint32_t
aFrameNumber
void
*
aPc
void
*
aSp
void
*
aClosure
)
{
StackTrace
*
st
=
(
StackTrace
*
)
aClosure
;
MOZ_ASSERT
(
st
-
>
mLength
<
kMaxFrames
)
;
st
-
>
mPcs
[
st
-
>
mLength
]
=
aPc
;
st
-
>
mLength
+
+
;
MOZ_ASSERT
(
st
-
>
mLength
=
=
aFrameNumber
)
;
}
}
;
void
StackTrace
:
:
Fill
(
)
{
mLength
=
0
;
#
if
defined
(
XP_WIN
)
&
&
defined
(
_M_IX86
)
CONTEXT
context
;
RtlCaptureContext
(
&
context
)
;
void
*
*
fp
=
reinterpret_cast
<
void
*
*
>
(
context
.
Ebp
)
;
PNT_TIB
pTib
=
reinterpret_cast
<
PNT_TIB
>
(
NtCurrentTeb
(
)
)
;
void
*
stackEnd
=
static_cast
<
void
*
>
(
pTib
-
>
StackBase
)
;
FramePointerStackWalk
(
StackWalkCallback
kMaxFrames
this
fp
stackEnd
)
;
#
elif
defined
(
XP_DARWIN
)
#
pragma
GCC
diagnostic
push
#
pragma
GCC
diagnostic
ignored
"
-
Wframe
-
address
"
void
*
*
fp
=
reinterpret_cast
<
void
*
*
>
(
__builtin_frame_address
(
1
)
)
;
#
pragma
GCC
diagnostic
pop
void
*
stackEnd
=
pthread_get_stackaddr_np
(
pthread_self
(
)
)
;
FramePointerStackWalk
(
StackWalkCallback
kMaxFrames
this
fp
stackEnd
)
;
#
else
MozStackWalk
(
StackWalkCallback
nullptr
kMaxFrames
this
)
;
#
endif
}
#
define
PHC_LOGGING
0
static
void
Log
(
const
char
*
fmt
.
.
.
)
;
using
Time
=
uint64_t
;
using
Delay
=
uint32_t
;
static
constexpr
Delay
DELAY_MAX
=
UINT32_MAX
/
2
;
static
const
size_t
kPageSize
=
#
if
defined
(
XP_DARWIN
)
&
&
defined
(
__aarch64__
)
16384
#
else
4096
#
endif
;
static
const
size_t
kPhcAlign
=
1024
*
1024
;
static_assert
(
IsPowerOfTwo
(
kPhcAlign
)
)
;
static_assert
(
(
kPhcAlign
%
kPageSize
)
=
=
0
)
;
#
ifdef
EARLY_BETA_OR_EARLIER
static
const
size_t
kNumAllocPages
=
kPageSize
=
=
4096
?
4096
:
1024
;
#
else
static
const
size_t
kNumAllocPages
=
kPageSize
=
=
4096
?
256
:
64
;
#
endif
static
const
size_t
kNumAllPages
=
kNumAllocPages
*
2
+
1
;
static
const
size_t
kAllPagesSize
=
kNumAllPages
*
kPageSize
;
static
const
size_t
kAllPagesJemallocSize
=
kAllPagesSize
-
kPageSize
;
static
const
Delay
kDelayDecrementAmount
=
256
;
static
const
Delay
kDelayBackoffAmount
=
64
;
static
const
Delay
kDelayResetWhenDisabled
=
64
*
1024
;
#
define
DEFAULT_STATE
mozilla
:
:
phc
:
:
OnlyFree
static
const
Time
kMaxTime
=
~
(
Time
(
0
)
)
;
constexpr
Delay
Rnd64ToDelay
(
Delay
aAvgDelay
uint64_t
aRnd
)
{
MOZ_ASSERT
(
IsPowerOfTwo
(
aAvgDelay
)
"
must
be
a
power
of
two
"
)
;
return
(
aRnd
&
(
uint64_t
(
aAvgDelay
)
*
2
-
1
)
)
+
1
;
}
static
Delay
CheckProbability
(
int64_t
aProb
)
{
return
RoundUpPow2
(
std
:
:
clamp
(
aProb
int64_t
(
2
)
int64_t
(
0x80000000
)
)
)
;
}
class
PtrKind
{
private
:
enum
class
Tag
:
uint8_t
{
Nothing
GuardPage
AllocPage
}
;
Tag
mTag
;
uintptr_t
mIndex
;
public
:
PtrKind
(
const
void
*
aPtr
const
uint8_t
*
aPagesStart
const
uint8_t
*
aPagesLimit
)
{
if
(
!
(
aPagesStart
<
=
aPtr
&
&
aPtr
<
aPagesLimit
)
)
{
mTag
=
Tag
:
:
Nothing
;
}
else
{
uintptr_t
offset
=
static_cast
<
const
uint8_t
*
>
(
aPtr
)
-
aPagesStart
;
uintptr_t
allPageIndex
=
offset
/
kPageSize
;
MOZ_ASSERT
(
allPageIndex
<
kNumAllPages
)
;
if
(
allPageIndex
&
1
)
{
uintptr_t
allocPageIndex
=
allPageIndex
/
2
;
MOZ_ASSERT
(
allocPageIndex
<
kNumAllocPages
)
;
mTag
=
Tag
:
:
AllocPage
;
mIndex
=
allocPageIndex
;
}
else
{
mTag
=
Tag
:
:
GuardPage
;
}
}
}
bool
IsNothing
(
)
const
{
return
mTag
=
=
Tag
:
:
Nothing
;
}
bool
IsGuardPage
(
)
const
{
return
mTag
=
=
Tag
:
:
GuardPage
;
}
uintptr_t
AllocPageIndex
(
)
const
{
MOZ_RELEASE_ASSERT
(
mTag
=
=
Tag
:
:
AllocPage
)
;
return
mIndex
;
}
}
;
#
if
!
defined
(
XP_DARWIN
)
#
define
PHC_THREAD_LOCAL
(
T
)
MOZ_THREAD_LOCAL
(
T
)
#
else
#
define
PHC_THREAD_LOCAL
(
T
)
\
detail
:
:
ThreadLocal
<
T
detail
:
:
ThreadLocalKeyStorage
>
#
endif
enum
class
AllocPageState
{
NeverAllocated
=
0
InUse
=
1
Freed
=
2
}
;
class
AllocPageInfo
{
public
:
AllocPageInfo
(
)
:
mState
(
AllocPageState
:
:
NeverAllocated
)
mBaseAddr
(
nullptr
)
mReuseTime
(
0
)
{
}
AllocPageState
mState
;
Maybe
<
arena_id_t
>
mArenaId
;
uint8_t
*
mBaseAddr
;
size_t
UsableSize
(
)
const
{
return
mState
=
=
AllocPageState
:
:
NeverAllocated
?
0
:
kPageSize
-
(
reinterpret_cast
<
uintptr_t
>
(
mBaseAddr
)
&
(
kPageSize
-
1
)
)
;
}
size_t
FragmentationBytes
(
)
const
{
MOZ_ASSERT
(
kPageSize
>
=
UsableSize
(
)
)
;
return
mState
=
=
AllocPageState
:
:
InUse
?
kPageSize
-
UsableSize
(
)
:
0
;
}
Maybe
<
StackTrace
>
mAllocStack
;
Maybe
<
StackTrace
>
mFreeStack
;
Time
mReuseTime
;
#
if
PHC_LOGGING
Time
mFreeTime
;
#
endif
Maybe
<
uintptr_t
>
mNextPage
;
void
AssertInUse
(
)
const
{
MOZ_ASSERT
(
mState
=
=
AllocPageState
:
:
InUse
)
;
MOZ_ASSERT
(
mBaseAddr
)
;
MOZ_ASSERT
(
UsableSize
(
)
>
0
)
;
MOZ_ASSERT
(
mAllocStack
.
isSome
(
)
)
;
MOZ_ASSERT
(
mFreeStack
.
isNothing
(
)
)
;
MOZ_ASSERT
(
mReuseTime
=
=
kMaxTime
)
;
MOZ_ASSERT
(
!
mNextPage
)
;
}
void
AssertNotInUse
(
)
const
{
#
ifdef
DEBUG
bool
isFresh
=
mState
=
=
AllocPageState
:
:
NeverAllocated
;
MOZ_ASSERT
(
isFresh
|
|
mState
=
=
AllocPageState
:
:
Freed
)
;
MOZ_ASSERT_IF
(
isFresh
mArenaId
=
=
Nothing
(
)
)
;
MOZ_ASSERT
(
isFresh
=
=
(
mBaseAddr
=
=
nullptr
)
)
;
MOZ_ASSERT
(
isFresh
=
=
(
mAllocStack
.
isNothing
(
)
)
)
;
MOZ_ASSERT
(
isFresh
=
=
(
mFreeStack
.
isNothing
(
)
)
)
;
MOZ_ASSERT
(
mReuseTime
!
=
kMaxTime
)
;
#
endif
}
bool
IsPageInUse
(
)
const
{
return
mState
=
=
AllocPageState
:
:
InUse
;
}
bool
IsPageFreed
(
)
const
{
return
mState
=
=
AllocPageState
:
:
Freed
;
}
bool
IsPageAllocatable
(
Time
aNow
)
const
{
return
!
IsPageInUse
(
)
&
&
aNow
>
=
mReuseTime
;
}
void
SetInUse
(
const
Maybe
<
arena_id_t
>
&
aArenaId
uint8_t
*
aBaseAddr
const
StackTrace
&
aAllocStack
)
{
AssertNotInUse
(
)
;
mState
=
AllocPageState
:
:
InUse
;
mArenaId
=
aArenaId
;
mBaseAddr
=
aBaseAddr
;
mAllocStack
=
Some
(
aAllocStack
)
;
mFreeStack
=
Nothing
(
)
;
mReuseTime
=
kMaxTime
;
MOZ_ASSERT
(
!
mNextPage
)
;
}
void
ResizeInUse
(
const
Maybe
<
arena_id_t
>
&
aArenaId
uint8_t
*
aNewBaseAddr
const
StackTrace
&
aAllocStack
)
{
AssertInUse
(
)
;
if
(
aArenaId
.
isSome
(
)
)
{
MOZ_RELEASE_ASSERT
(
mArenaId
=
=
aArenaId
)
;
}
mBaseAddr
=
aNewBaseAddr
;
mAllocStack
=
Some
(
aAllocStack
)
;
}
void
SetPageFreed
(
const
Maybe
<
arena_id_t
>
&
aArenaId
const
StackTrace
&
aFreeStack
Delay
aReuseDelay
Time
aNow
)
{
AssertInUse
(
)
;
mState
=
AllocPageState
:
:
Freed
;
if
(
aArenaId
.
isSome
(
)
)
{
MOZ_RELEASE_ASSERT
(
mArenaId
=
=
aArenaId
)
;
}
mFreeStack
=
Some
(
aFreeStack
)
;
#
if
PHC_LOGGING
mFreeTime
=
aNow
;
#
endif
mReuseTime
=
aNow
+
aReuseDelay
;
}
}
;
class
PHCRegion
{
private
:
uint8_t
*
mPagesStart
=
nullptr
;
uint8_t
*
mPagesLimit
=
nullptr
;
public
:
void
AllocVirtualAddresses
(
)
{
MOZ_ASSERT
(
!
mPagesStart
|
|
!
mPagesLimit
)
;
void
*
pages
=
MozJemalloc
:
:
memalign
(
kPhcAlign
kAllPagesJemallocSize
)
;
if
(
!
pages
)
{
MOZ_CRASH
(
)
;
}
#
ifdef
XP_WIN
if
(
!
VirtualFree
(
pages
kAllPagesJemallocSize
MEM_DECOMMIT
)
)
{
MOZ_CRASH
(
"
VirtualFree
failed
"
)
;
}
#
else
if
(
mmap
(
pages
kAllPagesJemallocSize
PROT_NONE
MAP_FIXED
|
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
=
=
MAP_FAILED
)
{
MOZ_CRASH
(
"
mmap
failed
"
)
;
}
#
endif
mPagesStart
=
static_cast
<
uint8_t
*
>
(
pages
)
;
mPagesLimit
=
mPagesStart
+
kAllPagesSize
;
Log
(
"
AllocVirtualAddresses
at
%
p
.
.
%
p
\
n
"
mPagesStart
mPagesLimit
)
;
}
constexpr
PHCRegion
(
)
{
}
class
PtrKind
PtrKind
(
const
void
*
aPtr
)
{
MOZ_ASSERT
(
mPagesStart
!
=
nullptr
&
&
mPagesLimit
!
=
nullptr
)
;
class
PtrKind
pk
(
aPtr
mPagesStart
mPagesLimit
)
;
return
pk
;
}
bool
IsInFirstGuardPage
(
const
void
*
aPtr
)
{
MOZ_ASSERT
(
mPagesStart
!
=
nullptr
&
&
mPagesLimit
!
=
nullptr
)
;
return
mPagesStart
<
=
aPtr
&
&
aPtr
<
mPagesStart
+
kPageSize
;
}
uint8_t
*
AllocPagePtr
(
uintptr_t
aIndex
)
{
MOZ_ASSERT
(
mPagesStart
!
=
nullptr
&
&
mPagesLimit
!
=
nullptr
)
;
MOZ_ASSERT
(
aIndex
<
kNumAllocPages
)
;
return
mPagesStart
+
(
2
*
aIndex
+
1
)
*
kPageSize
;
}
}
;
class
PHC
{
public
:
PHC
(
)
:
mRNG
(
RandomSeed
<
1
>
(
)
RandomSeed
<
2
>
(
)
)
{
mMutex
.
Init
(
)
;
if
(
!
tlsIsDisabled
.
init
(
)
)
{
MOZ_CRASH
(
)
;
}
if
(
!
tlsAllocDelay
.
init
(
)
)
{
MOZ_CRASH
(
)
;
}
if
(
!
tlsLastDelay
.
init
(
)
)
{
MOZ_CRASH
(
)
;
}
MutexAutoLock
lock
(
mMutex
)
;
ForceSetNewAllocDelay
(
Rnd64ToDelay
(
mAvgFirstAllocDelay
Random64
(
)
)
)
;
for
(
uintptr_t
i
=
0
;
i
<
kNumAllocPages
;
i
+
+
)
{
AppendPageToFreeList
(
i
)
;
}
}
uint64_t
Random64
(
)
MOZ_REQUIRES
(
mMutex
)
{
return
mRNG
.
next
(
)
;
}
uint8_t
*
AllocPageBaseAddr
(
uintptr_t
aIndex
)
MOZ_REQUIRES
(
mMutex
)
{
return
mAllocPages
[
aIndex
]
.
mBaseAddr
;
}
Maybe
<
arena_id_t
>
PageArena
(
uintptr_t
aIndex
)
MOZ_REQUIRES
(
mMutex
)
{
const
AllocPageInfo
&
page
=
mAllocPages
[
aIndex
]
;
page
.
AssertInUse
(
)
;
return
page
.
mArenaId
;
}
size_t
PageUsableSize
(
uintptr_t
aIndex
)
MOZ_REQUIRES
(
mMutex
)
{
const
AllocPageInfo
&
page
=
mAllocPages
[
aIndex
]
;
page
.
AssertInUse
(
)
;
return
page
.
UsableSize
(
)
;
}
size_t
FragmentationBytes
(
)
MOZ_EXCLUDES
(
mMutex
)
{
MutexAutoLock
lock
(
mMutex
)
;
size_t
sum
=
0
;
for
(
const
auto
&
page
:
mAllocPages
)
{
sum
+
=
page
.
FragmentationBytes
(
)
;
}
return
sum
;
}
size_t
AllocatedBytes
(
)
MOZ_EXCLUDES
(
mMutex
)
{
MutexAutoLock
lock
(
mMutex
)
;
size_t
allocated
=
0
;
for
(
const
auto
&
page
:
mAllocPages
)
{
if
(
page
.
IsPageInUse
(
)
)
{
allocated
+
=
page
.
UsableSize
(
)
;
}
}
return
allocated
;
}
void
SetPageInUse
(
uintptr_t
aIndex
const
Maybe
<
arena_id_t
>
&
aArenaId
uint8_t
*
aBaseAddr
const
StackTrace
&
aAllocStack
)
MOZ_REQUIRES
(
mMutex
)
{
mAllocPages
[
aIndex
]
.
SetInUse
(
aArenaId
aBaseAddr
aAllocStack
)
;
}
#
if
PHC_LOGGING
Time
GetFreeTime
(
uintptr_t
aIndex
)
const
MOZ_REQUIRES
(
mMutex
)
{
return
mAllocPages
[
aIndex
]
.
mFreeTime
;
}
#
endif
void
ResizePageInUse
(
uintptr_t
aIndex
const
Maybe
<
arena_id_t
>
&
aArenaId
uint8_t
*
aNewBaseAddr
const
StackTrace
&
aAllocStack
)
MOZ_REQUIRES
(
mMutex
)
{
mAllocPages
[
aIndex
]
.
ResizeInUse
(
aArenaId
aNewBaseAddr
aAllocStack
)
;
}
;
void
SetPageFreed
(
uintptr_t
aIndex
const
Maybe
<
arena_id_t
>
&
aArenaId
const
StackTrace
&
aFreeStack
Delay
aReuseDelay
)
MOZ_REQUIRES
(
mMutex
)
{
AllocPageInfo
&
page
=
mAllocPages
[
aIndex
]
;
page
.
SetPageFreed
(
aArenaId
aFreeStack
aReuseDelay
Now
(
)
)
;
MOZ_ASSERT
(
!
page
.
mNextPage
)
;
AppendPageToFreeList
(
aIndex
)
;
}
static
void
CrashOnGuardPage
(
void
*
aPtr
)
{
Log
(
"
CrashOnGuardPage
(
%
p
)
bounds
violation
\
n
"
aPtr
)
;
*
static_cast
<
uint8_t
*
>
(
aPtr
)
=
0
;
MOZ_CRASH
(
"
unreachable
"
)
;
}
void
EnsureValidAndInUse
(
void
*
aPtr
uintptr_t
aIndex
)
MOZ_REQUIRES
(
mMutex
)
{
const
AllocPageInfo
&
page
=
mAllocPages
[
aIndex
]
;
MOZ_RELEASE_ASSERT
(
page
.
mBaseAddr
=
=
aPtr
)
;
if
(
page
.
mState
=
=
AllocPageState
:
:
Freed
)
{
Log
(
"
EnsureValidAndInUse
(
%
p
)
use
-
after
-
free
\
n
"
aPtr
)
;
mMutex
.
Unlock
(
)
;
*
static_cast
<
uint8_t
*
>
(
aPtr
)
=
0
;
MOZ_CRASH
(
"
unreachable
"
)
;
}
}
void
FillAddrInfo
(
uintptr_t
aIndex
const
void
*
aBaseAddr
bool
isGuardPage
phc
:
:
AddrInfo
&
aOut
)
MOZ_REQUIRES
(
mMutex
)
{
const
AllocPageInfo
&
page
=
mAllocPages
[
aIndex
]
;
if
(
isGuardPage
)
{
aOut
.
mKind
=
phc
:
:
AddrInfo
:
:
Kind
:
:
GuardPage
;
}
else
{
switch
(
page
.
mState
)
{
case
AllocPageState
:
:
NeverAllocated
:
aOut
.
mKind
=
phc
:
:
AddrInfo
:
:
Kind
:
:
NeverAllocatedPage
;
break
;
case
AllocPageState
:
:
InUse
:
aOut
.
mKind
=
phc
:
:
AddrInfo
:
:
Kind
:
:
InUsePage
;
break
;
case
AllocPageState
:
:
Freed
:
aOut
.
mKind
=
phc
:
:
AddrInfo
:
:
Kind
:
:
FreedPage
;
break
;
default
:
MOZ_CRASH
(
)
;
}
}
aOut
.
mBaseAddr
=
page
.
mBaseAddr
;
aOut
.
mUsableSize
=
page
.
UsableSize
(
)
;
aOut
.
mAllocStack
=
page
.
mAllocStack
;
aOut
.
mFreeStack
=
page
.
mFreeStack
;
}
void
FillJemallocPtrInfo
(
const
void
*
aPtr
uintptr_t
aIndex
jemalloc_ptr_info_t
*
aInfo
)
MOZ_REQUIRES
(
mMutex
)
{
const
AllocPageInfo
&
page
=
mAllocPages
[
aIndex
]
;
switch
(
page
.
mState
)
{
case
AllocPageState
:
:
NeverAllocated
:
break
;
case
AllocPageState
:
:
InUse
:
{
uint8_t
*
base
=
page
.
mBaseAddr
;
uint8_t
*
limit
=
base
+
page
.
UsableSize
(
)
;
if
(
base
<
=
aPtr
&
&
aPtr
<
limit
)
{
*
aInfo
=
{
TagLiveAlloc
page
.
mBaseAddr
page
.
UsableSize
(
)
page
.
mArenaId
.
valueOr
(
0
)
}
;
return
;
}
break
;
}
case
AllocPageState
:
:
Freed
:
{
uint8_t
*
base
=
page
.
mBaseAddr
;
uint8_t
*
limit
=
base
+
page
.
UsableSize
(
)
;
if
(
base
<
=
aPtr
&
&
aPtr
<
limit
)
{
*
aInfo
=
{
TagFreedAlloc
page
.
mBaseAddr
page
.
UsableSize
(
)
page
.
mArenaId
.
valueOr
(
0
)
}
;
return
;
}
break
;
}
default
:
MOZ_CRASH
(
)
;
}
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
}
#
ifndef
XP_WIN
static
void
prefork
(
)
MOZ_NO_THREAD_SAFETY_ANALYSIS
{
PHC
:
:
sPHC
-
>
mMutex
.
Lock
(
)
;
}
static
void
postfork_parent
(
)
MOZ_NO_THREAD_SAFETY_ANALYSIS
{
PHC
:
:
sPHC
-
>
mMutex
.
Unlock
(
)
;
}
static
void
postfork_child
(
)
{
PHC
:
:
sPHC
-
>
mMutex
.
Init
(
)
;
}
#
endif
void
IncPageAllocHits
(
)
MOZ_REQUIRES
(
mMutex
)
{
#
if
PHC_LOGGING
mPageAllocHits
+
+
;
#
endif
}
void
IncPageAllocMisses
(
)
MOZ_REQUIRES
(
mMutex
)
{
#
if
PHC_LOGGING
mPageAllocMisses
+
+
;
#
endif
}
phc
:
:
PHCStats
GetPageStatsLocked
(
)
MOZ_REQUIRES
(
mMutex
)
{
phc
:
:
PHCStats
stats
;
for
(
const
auto
&
page
:
mAllocPages
)
{
stats
.
mSlotsAllocated
+
=
page
.
IsPageInUse
(
)
?
1
:
0
;
stats
.
mSlotsFreed
+
=
page
.
IsPageFreed
(
)
?
1
:
0
;
}
stats
.
mSlotsUnused
=
kNumAllocPages
-
stats
.
mSlotsAllocated
-
stats
.
mSlotsFreed
;
return
stats
;
}
phc
:
:
PHCStats
GetPageStats
(
)
MOZ_EXCLUDES
(
mMutex
)
{
MutexAutoLock
lock
(
mMutex
)
;
return
GetPageStatsLocked
(
)
;
}
#
if
PHC_LOGGING
size_t
PageAllocHits
(
)
MOZ_REQUIRES
(
mMutex
)
{
return
mPageAllocHits
;
}
size_t
PageAllocAttempts
(
)
MOZ_REQUIRES
(
mMutex
)
{
return
mPageAllocHits
+
mPageAllocMisses
;
}
size_t
PageAllocHitRate
(
)
MOZ_REQUIRES
(
mMutex
)
{
return
mPageAllocHits
*
100
/
(
mPageAllocHits
+
mPageAllocMisses
)
;
}
#
endif
void
LogNoAlloc
(
size_t
aReqSize
size_t
aAlignment
Delay
newAllocDelay
)
;
bool
ShouldMakeNewAllocations
(
)
const
{
return
mPhcState
=
=
mozilla
:
:
phc
:
:
Enabled
;
}
using
PHCState
=
mozilla
:
:
phc
:
:
PHCState
;
void
SetState
(
PHCState
aState
)
{
if
(
mPhcState
!
=
PHCState
:
:
Enabled
&
&
aState
=
=
PHCState
:
:
Enabled
)
{
MutexAutoLock
lock
(
mMutex
)
;
ResetRNG
(
)
;
ForceSetNewAllocDelay
(
Rnd64ToDelay
(
mAvgFirstAllocDelay
Random64
(
)
)
)
;
}
mPhcState
=
aState
;
}
void
ResetRNG
(
)
MOZ_REQUIRES
(
mMutex
)
{
mRNG
=
non_crypto
:
:
XorShift128PlusRNG
(
RandomSeed
<
0
>
(
)
RandomSeed
<
1
>
(
)
)
;
}
void
SetProbabilities
(
int64_t
aAvgDelayFirst
int64_t
aAvgDelayNormal
int64_t
aAvgDelayPageReuse
)
MOZ_EXCLUDES
(
mMutex
)
{
MutexAutoLock
lock
(
mMutex
)
;
mAvgFirstAllocDelay
=
CheckProbability
(
aAvgDelayFirst
)
;
mAvgAllocDelay
=
CheckProbability
(
aAvgDelayNormal
)
;
mAvgPageReuseDelay
=
CheckProbability
(
aAvgDelayPageReuse
)
;
}
static
void
DisableOnCurrentThread
(
)
{
MOZ_ASSERT
(
!
tlsIsDisabled
.
get
(
)
)
;
tlsIsDisabled
.
set
(
true
)
;
}
void
EnableOnCurrentThread
(
)
{
MOZ_ASSERT
(
tlsIsDisabled
.
get
(
)
)
;
tlsIsDisabled
.
set
(
false
)
;
}
static
bool
IsDisabledOnCurrentThread
(
)
{
return
tlsIsDisabled
.
get
(
)
;
}
static
Time
Now
(
)
{
if
(
!
sPHC
)
{
return
0
;
}
return
sPHC
-
>
mNow
;
}
void
AdvanceNow
(
uint32_t
delay
=
0
)
{
mNow
+
=
tlsLastDelay
.
get
(
)
-
delay
;
tlsLastDelay
.
set
(
delay
)
;
}
static
bool
DecrementDelay
(
)
{
const
Delay
alloc_delay
=
tlsAllocDelay
.
get
(
)
;
if
(
MOZ_LIKELY
(
alloc_delay
>
0
)
)
{
tlsAllocDelay
.
set
(
alloc_delay
-
1
)
;
return
false
;
}
MOZ_ASSERT
(
sPHC
)
;
sPHC
-
>
AdvanceNow
(
)
;
Delay
new_delay
=
(
sAllocDelay
-
=
kDelayDecrementAmount
)
;
Delay
old_delay
=
new_delay
+
kDelayDecrementAmount
;
if
(
MOZ_LIKELY
(
new_delay
<
DELAY_MAX
)
)
{
tlsAllocDelay
.
set
(
kDelayDecrementAmount
)
;
tlsLastDelay
.
set
(
kDelayDecrementAmount
)
;
Log
(
"
Update
sAllocDelay
<
-
%
zu
tlsAllocDelay
<
-
%
zu
\
n
"
size_t
(
new_delay
)
size_t
(
kDelayDecrementAmount
)
)
;
return
false
;
}
if
(
old_delay
<
new_delay
)
{
Log
(
"
Update
sAllocDelay
<
-
%
zu
tlsAllocDelay
<
-
%
zu
\
n
"
size_t
(
new_delay
)
size_t
(
old_delay
)
)
;
if
(
old_delay
=
=
0
)
{
return
true
;
}
tlsAllocDelay
.
set
(
old_delay
)
;
tlsLastDelay
.
set
(
old_delay
)
;
return
false
;
}
Log
(
"
Update
sAllocDelay
<
-
%
zu
tlsAllocDelay
<
-
%
zu
\
n
"
size_t
(
new_delay
)
size_t
(
alloc_delay
)
)
;
return
true
;
}
static
void
ResetLocalAllocDelay
(
Delay
aDelay
=
0
)
{
tlsAllocDelay
.
set
(
aDelay
)
;
tlsLastDelay
.
set
(
aDelay
)
;
}
static
void
ForceSetNewAllocDelay
(
Delay
aNewAllocDelay
)
{
Log
(
"
Setting
sAllocDelay
<
-
%
zu
\
n
"
size_t
(
aNewAllocDelay
)
)
;
sAllocDelay
=
aNewAllocDelay
;
ResetLocalAllocDelay
(
)
;
}
static
bool
SetNewAllocDelay
(
Delay
aNewAllocDelay
)
{
bool
cas_retry
;
do
{
Delay
read_delay
=
sAllocDelay
;
if
(
read_delay
<
DELAY_MAX
)
{
Log
(
"
Observe
delay
%
zu
this
thread
lost
the
race
\
n
"
size_t
(
read_delay
)
)
;
ResetLocalAllocDelay
(
)
;
return
false
;
}
else
{
Log
(
"
Preparing
for
CAS
read
sAllocDelay
%
zu
\
n
"
size_t
(
read_delay
)
)
;
}
cas_retry
=
!
sAllocDelay
.
compareExchange
(
read_delay
aNewAllocDelay
)
;
if
(
cas_retry
)
{
Log
(
"
Lost
the
CAS
sAllocDelay
is
now
%
zu
\
n
"
size_t
(
sAllocDelay
)
)
;
cpu_pause
(
)
;
}
}
while
(
cas_retry
)
;
Log
(
"
Won
the
CAS
set
sAllocDelay
=
%
zu
\
n
"
size_t
(
sAllocDelay
)
)
;
ResetLocalAllocDelay
(
)
;
return
true
;
}
static
Delay
LocalAllocDelay
(
)
{
return
tlsAllocDelay
.
get
(
)
;
}
static
Delay
SharedAllocDelay
(
)
{
return
sAllocDelay
;
}
static
Delay
LastDelay
(
)
{
return
tlsLastDelay
.
get
(
)
;
}
Maybe
<
uintptr_t
>
PopNextFreeIfAllocatable
(
Time
now
)
MOZ_REQUIRES
(
mMutex
)
{
if
(
!
mFreePageListHead
)
{
return
Nothing
(
)
;
}
uintptr_t
index
=
mFreePageListHead
.
value
(
)
;
MOZ_RELEASE_ASSERT
(
index
<
kNumAllocPages
)
;
AllocPageInfo
&
page
=
mAllocPages
[
index
]
;
page
.
AssertNotInUse
(
)
;
if
(
!
page
.
IsPageAllocatable
(
now
)
)
{
return
Nothing
(
)
;
}
mFreePageListHead
=
page
.
mNextPage
;
page
.
mNextPage
=
Nothing
(
)
;
if
(
!
mFreePageListHead
)
{
mFreePageListTail
=
Nothing
(
)
;
}
return
Some
(
index
)
;
}
void
UnpopNextFree
(
uintptr_t
index
)
MOZ_REQUIRES
(
mMutex
)
{
AllocPageInfo
&
page
=
mAllocPages
[
index
]
;
MOZ_ASSERT
(
!
page
.
mNextPage
)
;
page
.
mNextPage
=
mFreePageListHead
;
mFreePageListHead
=
Some
(
index
)
;
if
(
!
mFreePageListTail
)
{
mFreePageListTail
=
Some
(
index
)
;
}
}
void
AppendPageToFreeList
(
uintptr_t
aIndex
)
MOZ_REQUIRES
(
mMutex
)
{
MOZ_RELEASE_ASSERT
(
aIndex
<
kNumAllocPages
)
;
AllocPageInfo
&
page
=
mAllocPages
[
aIndex
]
;
MOZ_ASSERT
(
!
page
.
mNextPage
)
;
MOZ_ASSERT
(
mFreePageListHead
!
=
Some
(
aIndex
)
&
&
mFreePageListTail
!
=
Some
(
aIndex
)
)
;
if
(
!
mFreePageListTail
)
{
MOZ_ASSERT
(
!
mFreePageListHead
)
;
mFreePageListHead
=
Some
(
aIndex
)
;
}
else
{
MOZ_ASSERT
(
mFreePageListTail
.
value
(
)
<
kNumAllocPages
)
;
AllocPageInfo
&
tail_page
=
mAllocPages
[
mFreePageListTail
.
value
(
)
]
;
MOZ_ASSERT
(
!
tail_page
.
mNextPage
)
;
tail_page
.
mNextPage
=
Some
(
aIndex
)
;
}
page
.
mNextPage
=
Nothing
(
)
;
mFreePageListTail
=
Some
(
aIndex
)
;
}
private
:
template
<
int
N
>
uint64_t
RandomSeed
(
)
{
static_assert
(
N
=
=
0
|
|
N
=
=
1
|
|
N
=
=
2
"
must
be
0
1
or
2
"
)
;
uint64_t
seed
;
if
(
N
=
=
0
)
{
time_t
t
=
time
(
nullptr
)
;
seed
=
t
^
(
t
<
<
32
)
;
}
else
if
(
N
=
=
1
)
{
seed
=
uintptr_t
(
&
seed
)
^
(
uintptr_t
(
&
seed
)
<
<
32
)
;
}
else
{
seed
=
uintptr_t
(
&
sRegion
)
^
(
uintptr_t
(
&
sRegion
)
<
<
32
)
;
}
return
seed
;
}
public
:
void
*
MaybePageAlloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aReqSize
size_t
aAlignment
bool
aZero
)
;
void
FreePage
(
uintptr_t
aIndex
const
Maybe
<
arena_id_t
>
&
aArenaId
const
StackTrace
&
aFreeStack
Delay
aReuseDelay
)
;
void
PageFree
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aPtr
)
;
Maybe
<
void
*
>
PageRealloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aOldPtr
size_t
aNewSize
)
;
void
PagePtrInfo
(
const
void
*
aPtr
jemalloc_ptr_info_t
*
aInfo
)
;
size_t
PtrUsableSize
(
usable_ptr_t
aPtr
)
;
bool
IsPHCAllocation
(
const
void
*
aPtr
mozilla
:
:
phc
:
:
AddrInfo
*
aOut
)
;
void
Crash
(
const
char
*
aMessage
)
;
private
:
alignas
(
kCacheLineSize
)
Mutex
mMutex
MOZ_UNANNOTATED
;
Atomic
<
Time
ReleaseAcquire
>
mNow
;
Atomic
<
PHCState
Relaxed
>
mPhcState
=
Atomic
<
PHCState
Relaxed
>
(
DEFAULT_STATE
)
;
non_crypto
:
:
XorShift128PlusRNG
mRNG
MOZ_GUARDED_BY
(
mMutex
)
;
Maybe
<
uintptr_t
>
mFreePageListHead
MOZ_GUARDED_BY
(
mMutex
)
;
Maybe
<
uintptr_t
>
mFreePageListTail
MOZ_GUARDED_BY
(
mMutex
)
;
#
if
PHC_LOGGING
size_t
mPageAllocHits
MOZ_GUARDED_BY
(
mMutex
)
=
0
;
size_t
mPageAllocMisses
MOZ_GUARDED_BY
(
mMutex
)
=
0
;
#
endif
alignas
(
kCacheLineSize
)
Delay
mAvgFirstAllocDelay
MOZ_GUARDED_BY
(
mMutex
)
=
64
*
1024
;
Delay
mAvgAllocDelay
MOZ_GUARDED_BY
(
mMutex
)
=
16
*
1024
;
Delay
mAvgPageReuseDelay
MOZ_GUARDED_BY
(
mMutex
)
=
256
*
1024
;
static
PHC_THREAD_LOCAL
(
bool
)
tlsIsDisabled
;
static
Atomic
<
Delay
ReleaseAcquire
>
sAllocDelay
;
static
PHC_THREAD_LOCAL
(
Delay
)
tlsAllocDelay
;
static
PHC_THREAD_LOCAL
(
Delay
)
tlsLastDelay
;
Array
<
AllocPageInfo
kNumAllocPages
>
mAllocPages
MOZ_GUARDED_BY
(
mMutex
)
;
public
:
Delay
GetAvgAllocDelay
(
)
MOZ_REQUIRES
(
mMutex
)
{
return
mAvgAllocDelay
;
}
Delay
GetAvgFirstAllocDelay
(
)
MOZ_REQUIRES
(
mMutex
)
{
return
mAvgFirstAllocDelay
;
}
Delay
GetAvgPageReuseDelay
(
)
MOZ_REQUIRES
(
mMutex
)
{
return
mAvgPageReuseDelay
;
}
Delay
ReuseDelay
(
)
MOZ_REQUIRES
(
mMutex
)
{
Delay
avg_reuse_delay
=
GetAvgPageReuseDelay
(
)
;
return
(
avg_reuse_delay
/
2
)
+
Rnd64ToDelay
(
avg_reuse_delay
/
2
Random64
(
)
)
;
}
static
PHCRegion
sRegion
;
static
PHC
*
sPHC
;
}
;
alignas
(
kCacheLineSize
)
PHCRegion
PHC
:
:
sRegion
;
PHC
*
PHC
:
:
sPHC
;
PHC_THREAD_LOCAL
(
bool
)
PHC
:
:
tlsIsDisabled
;
PHC_THREAD_LOCAL
(
Delay
)
PHC
:
:
tlsAllocDelay
;
Atomic
<
Delay
ReleaseAcquire
>
PHC
:
:
sAllocDelay
;
PHC_THREAD_LOCAL
(
Delay
)
PHC
:
:
tlsLastDelay
;
void
PHC
:
:
Crash
(
const
char
*
aMessage
)
MOZ_REQUIRES
(
mMutex
)
{
mMutex
.
Unlock
(
)
;
MOZ_CRASH_UNSAFE
(
aMessage
)
;
}
class
AutoDisableOnCurrentThread
{
public
:
AutoDisableOnCurrentThread
(
const
AutoDisableOnCurrentThread
&
)
=
delete
;
const
AutoDisableOnCurrentThread
&
operator
=
(
const
AutoDisableOnCurrentThread
&
)
=
delete
;
explicit
AutoDisableOnCurrentThread
(
)
{
PHC
:
:
DisableOnCurrentThread
(
)
;
}
~
AutoDisableOnCurrentThread
(
)
{
PHC
:
:
sPHC
-
>
EnableOnCurrentThread
(
)
;
}
}
;
static
bool
phc_init
(
)
{
if
(
GetKernelPageSize
(
)
!
=
kPageSize
)
{
return
false
;
}
PHC
:
:
sRegion
.
AllocVirtualAddresses
(
)
;
PHC
:
:
sPHC
=
InfallibleAllocPolicy
:
:
new_
<
PHC
>
(
)
;
#
ifndef
XP_WIN
pthread_atfork
(
PHC
:
:
prefork
PHC
:
:
postfork_parent
PHC
:
:
postfork_child
)
;
#
endif
return
true
;
}
static
inline
bool
maybe_init
(
)
{
if
(
MOZ_UNLIKELY
(
!
PHC
:
:
sPHC
)
)
{
static
bool
sInitSuccess
=
[
]
(
)
{
return
phc_init
(
)
;
}
(
)
;
return
sInitSuccess
;
}
return
true
;
}
static
MOZ_ALWAYS_INLINE
bool
ShouldPageAllocHot
(
size_t
aReqSize
)
{
if
(
MOZ_UNLIKELY
(
!
maybe_init
(
)
)
)
{
return
false
;
}
if
(
MOZ_UNLIKELY
(
aReqSize
>
kPageSize
)
)
{
return
false
;
}
if
(
MOZ_LIKELY
(
!
PHC
:
:
DecrementDelay
(
)
)
)
{
return
false
;
}
return
true
;
}
void
PHC
:
:
LogNoAlloc
(
size_t
aReqSize
size_t
aAlignment
Delay
newAllocDelay
)
MOZ_REQUIRES
(
mMutex
)
{
#
if
PHC_LOGGING
phc
:
:
PHCStats
stats
=
GetPageStatsLocked
(
)
;
Log
(
"
No
PageAlloc
(
%
zu
%
zu
)
sAllocDelay
<
-
%
zu
fullness
%
zu
/
%
zu
/
%
zu
"
"
hits
%
zu
/
%
zu
(
%
zu
%
%
)
\
n
"
aReqSize
aAlignment
size_t
(
newAllocDelay
)
stats
.
mSlotsAllocated
stats
.
mSlotsFreed
kNumAllocPages
PageAllocHits
(
)
PageAllocAttempts
(
)
PageAllocHitRate
(
)
)
;
#
endif
}
void
*
PHC
:
:
MaybePageAlloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aReqSize
size_t
aAlignment
bool
aZero
)
{
MOZ_ASSERT
(
IsPowerOfTwo
(
aAlignment
)
)
;
if
(
!
ShouldMakeNewAllocations
(
)
)
{
ForceSetNewAllocDelay
(
kDelayResetWhenDisabled
)
;
return
nullptr
;
}
if
(
IsDisabledOnCurrentThread
(
)
)
{
ResetLocalAllocDelay
(
kDelayBackoffAmount
)
;
return
nullptr
;
}
AutoDisableOnCurrentThread
disable
;
StackTrace
allocStack
;
allocStack
.
Fill
(
)
;
MutexAutoLock
lock
(
mMutex
)
;
Time
now
=
Now
(
)
;
Delay
newAllocDelay
=
Rnd64ToDelay
(
GetAvgAllocDelay
(
)
Random64
(
)
)
;
if
(
!
SetNewAllocDelay
(
newAllocDelay
)
)
{
return
nullptr
;
}
Maybe
<
uintptr_t
>
mb_index
=
PopNextFreeIfAllocatable
(
now
)
;
if
(
!
mb_index
)
{
IncPageAllocMisses
(
)
;
LogNoAlloc
(
aReqSize
aAlignment
newAllocDelay
)
;
return
nullptr
;
}
uintptr_t
index
=
mb_index
.
value
(
)
;
#
if
PHC_LOGGING
Time
lifetime
=
0
;
#
endif
uint8_t
*
pagePtr
=
sRegion
.
AllocPagePtr
(
index
)
;
MOZ_ASSERT
(
pagePtr
)
;
bool
ok
=
#
ifdef
XP_WIN
!
!
VirtualAlloc
(
pagePtr
kPageSize
MEM_COMMIT
PAGE_READWRITE
)
;
#
else
mprotect
(
pagePtr
kPageSize
PROT_READ
|
PROT_WRITE
)
=
=
0
;
#
endif
if
(
!
ok
)
{
UnpopNextFree
(
index
)
;
IncPageAllocMisses
(
)
;
LogNoAlloc
(
aReqSize
aAlignment
newAllocDelay
)
;
return
nullptr
;
}
size_t
usableSize
=
MozJemalloc
:
:
malloc_good_size
(
aReqSize
)
;
MOZ_ASSERT
(
usableSize
>
0
)
;
uint8_t
*
ptr
=
pagePtr
+
kPageSize
-
usableSize
;
if
(
aAlignment
!
=
1
)
{
ptr
=
reinterpret_cast
<
uint8_t
*
>
(
(
reinterpret_cast
<
uintptr_t
>
(
ptr
)
&
~
(
aAlignment
-
1
)
)
)
;
}
#
if
PHC_LOGGING
Time
then
=
GetFreeTime
(
index
)
;
lifetime
=
then
!
=
0
?
now
-
then
:
0
;
#
endif
SetPageInUse
(
index
aArenaId
ptr
allocStack
)
;
if
(
aZero
)
{
memset
(
ptr
0
usableSize
)
;
}
else
{
#
ifdef
DEBUG
memset
(
ptr
kAllocJunk
usableSize
)
;
#
endif
}
IncPageAllocHits
(
)
;
#
if
PHC_LOGGING
phc
:
:
PHCStats
stats
=
GetPageStatsLocked
(
)
;
Log
(
"
PageAlloc
(
%
zu
%
zu
)
-
>
%
p
[
%
zu
]
/
%
p
(
%
zu
)
(
z
%
zu
)
sAllocDelay
<
-
%
zu
"
"
fullness
%
zu
/
%
zu
/
%
zu
hits
%
zu
/
%
zu
(
%
zu
%
%
)
lifetime
%
zu
\
n
"
aReqSize
aAlignment
pagePtr
index
ptr
usableSize
size_t
(
newAllocDelay
)
size_t
(
SharedAllocDelay
(
)
)
stats
.
mSlotsAllocated
stats
.
mSlotsFreed
kNumAllocPages
PageAllocHits
(
)
PageAllocAttempts
(
)
PageAllocHitRate
(
)
lifetime
)
;
#
endif
return
ptr
;
}
void
PHC
:
:
FreePage
(
uintptr_t
aIndex
const
Maybe
<
arena_id_t
>
&
aArenaId
const
StackTrace
&
aFreeStack
Delay
aReuseDelay
)
MOZ_REQUIRES
(
mMutex
)
{
void
*
pagePtr
=
sRegion
.
AllocPagePtr
(
aIndex
)
;
#
ifdef
XP_WIN
if
(
!
VirtualFree
(
pagePtr
kPageSize
MEM_DECOMMIT
)
)
{
Crash
(
"
VirtualFree
failed
"
)
;
}
#
else
if
(
mmap
(
pagePtr
kPageSize
PROT_NONE
MAP_FIXED
|
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
=
=
MAP_FAILED
)
{
Crash
(
"
mmap
failed
"
)
;
}
#
endif
SetPageFreed
(
aIndex
aArenaId
aFreeStack
aReuseDelay
)
;
}
MOZ_ALWAYS_INLINE
static
void
*
PageMalloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aReqSize
)
{
void
*
ptr
=
ShouldPageAllocHot
(
aReqSize
)
?
PHC
:
:
sPHC
-
>
MaybePageAlloc
(
aArenaId
.
isSome
(
)
?
aArenaId
:
Nothing
(
)
aReqSize
1
false
)
:
nullptr
;
return
ptr
?
ptr
:
(
aArenaId
.
isSome
(
)
?
MozJemalloc
:
:
moz_arena_malloc
(
*
aArenaId
aReqSize
)
:
MozJemalloc
:
:
malloc
(
aReqSize
)
)
;
}
inline
void
*
MozJemallocPHC
:
:
malloc
(
size_t
aReqSize
)
{
return
PageMalloc
(
Nothing
(
)
aReqSize
)
;
}
MOZ_ALWAYS_INLINE
static
void
*
PageCalloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aNum
size_t
aReqSize
)
{
CheckedInt
<
size_t
>
checkedSize
=
CheckedInt
<
size_t
>
(
aNum
)
*
aReqSize
;
if
(
!
checkedSize
.
isValid
(
)
)
{
return
nullptr
;
}
void
*
ptr
=
ShouldPageAllocHot
(
checkedSize
.
value
(
)
)
?
PHC
:
:
sPHC
-
>
MaybePageAlloc
(
aArenaId
.
isSome
(
)
?
aArenaId
:
Nothing
(
)
checkedSize
.
value
(
)
1
true
)
:
nullptr
;
return
ptr
?
ptr
:
(
aArenaId
.
isSome
(
)
?
MozJemalloc
:
:
moz_arena_calloc
(
*
aArenaId
aNum
aReqSize
)
:
MozJemalloc
:
:
calloc
(
aNum
aReqSize
)
)
;
}
inline
void
*
MozJemallocPHC
:
:
calloc
(
size_t
aNum
size_t
aReqSize
)
{
return
PageCalloc
(
Nothing
(
)
aNum
aReqSize
)
;
}
MOZ_ALWAYS_INLINE
static
bool
FastIsPHCPtr
(
void
*
aPtr
)
{
if
(
MOZ_UNLIKELY
(
!
maybe_init
(
)
)
)
{
return
false
;
}
PtrKind
pk
=
PHC
:
:
sRegion
.
PtrKind
(
aPtr
)
;
return
!
pk
.
IsNothing
(
)
;
}
MOZ_ALWAYS_INLINE
static
Maybe
<
void
*
>
MaybePageRealloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aOldPtr
size_t
aNewSize
)
{
if
(
!
aOldPtr
)
{
return
Some
(
PageMalloc
(
aArenaId
aNewSize
)
)
;
}
if
(
!
FastIsPHCPtr
(
aOldPtr
)
)
{
return
Nothing
(
)
;
}
return
PHC
:
:
sPHC
-
>
PageRealloc
(
aArenaId
aOldPtr
aNewSize
)
;
}
Maybe
<
void
*
>
PHC
:
:
PageRealloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aOldPtr
size_t
aNewSize
)
MOZ_EXCLUDES
(
mMutex
)
{
PtrKind
pk
=
sRegion
.
PtrKind
(
aOldPtr
)
;
MOZ_ASSERT
(
!
pk
.
IsNothing
(
)
)
;
if
(
pk
.
IsGuardPage
(
)
)
{
CrashOnGuardPage
(
aOldPtr
)
;
}
uintptr_t
index
=
pk
.
AllocPageIndex
(
)
;
AdvanceNow
(
LocalAllocDelay
(
)
)
;
Maybe
<
AutoDisableOnCurrentThread
>
disable
;
StackTrace
stack
;
if
(
IsDisabledOnCurrentThread
(
)
)
{
}
else
{
disable
.
emplace
(
)
;
stack
.
Fill
(
)
;
}
MutexAutoLock
lock
(
mMutex
)
;
EnsureValidAndInUse
(
aOldPtr
index
)
;
if
(
aNewSize
<
=
kPageSize
&
&
ShouldMakeNewAllocations
(
)
)
{
size_t
oldUsableSize
=
PageUsableSize
(
index
)
;
size_t
newUsableSize
=
MozJemalloc
:
:
malloc_good_size
(
aNewSize
)
;
uint8_t
*
pagePtr
=
sRegion
.
AllocPagePtr
(
index
)
;
uint8_t
*
newPtr
=
pagePtr
+
kPageSize
-
newUsableSize
;
memmove
(
newPtr
aOldPtr
std
:
:
min
(
oldUsableSize
aNewSize
)
)
;
ResizePageInUse
(
index
aArenaId
newPtr
stack
)
;
Log
(
"
PageRealloc
-
Reuse
(
%
p
%
zu
)
-
>
%
p
\
n
"
aOldPtr
aNewSize
newPtr
)
;
return
Some
(
newPtr
)
;
}
void
*
newPtr
;
if
(
aArenaId
.
isSome
(
)
)
{
newPtr
=
MozJemalloc
:
:
moz_arena_malloc
(
*
aArenaId
aNewSize
)
;
}
else
{
Maybe
<
arena_id_t
>
oldArenaId
=
PageArena
(
index
)
;
newPtr
=
(
oldArenaId
.
isSome
(
)
?
MozJemalloc
:
:
moz_arena_malloc
(
*
oldArenaId
aNewSize
)
:
MozJemalloc
:
:
malloc
(
aNewSize
)
)
;
}
if
(
!
newPtr
)
{
return
Some
(
nullptr
)
;
}
Delay
reuseDelay
=
ReuseDelay
(
)
;
size_t
oldUsableSize
=
PageUsableSize
(
index
)
;
memcpy
(
newPtr
aOldPtr
std
:
:
min
(
oldUsableSize
aNewSize
)
)
;
FreePage
(
index
aArenaId
stack
reuseDelay
)
;
Log
(
"
PageRealloc
-
Free
(
%
p
[
%
zu
]
%
zu
)
-
>
%
p
%
zu
delay
reuse
at
~
%
zu
\
n
"
aOldPtr
index
aNewSize
newPtr
size_t
(
reuseDelay
)
size_t
(
Now
(
)
)
+
reuseDelay
)
;
return
Some
(
newPtr
)
;
}
MOZ_ALWAYS_INLINE
static
void
*
PageRealloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aOldPtr
size_t
aNewSize
)
{
Maybe
<
void
*
>
ptr
=
MaybePageRealloc
(
aArenaId
aOldPtr
aNewSize
)
;
return
ptr
.
isSome
(
)
?
*
ptr
:
(
aArenaId
.
isSome
(
)
?
MozJemalloc
:
:
moz_arena_realloc
(
*
aArenaId
aOldPtr
aNewSize
)
:
MozJemalloc
:
:
realloc
(
aOldPtr
aNewSize
)
)
;
}
inline
void
*
MozJemallocPHC
:
:
realloc
(
void
*
aOldPtr
size_t
aNewSize
)
{
return
PageRealloc
(
Nothing
(
)
aOldPtr
aNewSize
)
;
}
void
PHC
:
:
PageFree
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aPtr
)
MOZ_EXCLUDES
(
mMutex
)
{
PtrKind
pk
=
sRegion
.
PtrKind
(
aPtr
)
;
MOZ_ASSERT
(
!
pk
.
IsNothing
(
)
)
;
if
(
pk
.
IsGuardPage
(
)
)
{
PHC
:
:
CrashOnGuardPage
(
aPtr
)
;
}
AdvanceNow
(
LocalAllocDelay
(
)
)
;
uintptr_t
index
=
pk
.
AllocPageIndex
(
)
;
Maybe
<
AutoDisableOnCurrentThread
>
disable
;
StackTrace
freeStack
;
if
(
IsDisabledOnCurrentThread
(
)
)
{
}
else
{
disable
.
emplace
(
)
;
freeStack
.
Fill
(
)
;
}
MutexAutoLock
lock
(
mMutex
)
;
EnsureValidAndInUse
(
aPtr
index
)
;
Delay
reuseDelay
=
ReuseDelay
(
)
;
FreePage
(
index
aArenaId
freeStack
reuseDelay
)
;
#
if
PHC_LOGGING
phc
:
:
PHCStats
stats
=
GetPageStatsLocked
(
)
;
Log
(
"
PageFree
(
%
p
[
%
zu
]
)
%
zu
delay
reuse
at
~
%
zu
fullness
%
zu
/
%
zu
/
%
zu
\
n
"
aPtr
index
size_t
(
reuseDelay
)
size_t
(
Now
(
)
)
+
reuseDelay
stats
.
mSlotsAllocated
stats
.
mSlotsFreed
kNumAllocPages
)
;
#
endif
}
MOZ_ALWAYS_INLINE
static
void
PageFree
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aPtr
)
{
if
(
MOZ_UNLIKELY
(
FastIsPHCPtr
(
aPtr
)
)
)
{
PHC
:
:
sPHC
-
>
PageFree
(
aArenaId
.
isSome
(
)
?
aArenaId
:
Nothing
(
)
aPtr
)
;
return
;
}
aArenaId
.
isSome
(
)
?
MozJemalloc
:
:
moz_arena_free
(
*
aArenaId
aPtr
)
:
MozJemalloc
:
:
free
(
aPtr
)
;
}
inline
void
MozJemallocPHC
:
:
free
(
void
*
aPtr
)
{
PageFree
(
Nothing
(
)
aPtr
)
;
}
MOZ_ALWAYS_INLINE
static
void
*
PageMemalign
(
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aAlignment
size_t
aReqSize
)
{
MOZ_RELEASE_ASSERT
(
IsPowerOfTwo
(
aAlignment
)
)
;
void
*
ptr
=
nullptr
;
if
(
ShouldPageAllocHot
(
aReqSize
)
&
&
aAlignment
<
=
kPageSize
)
{
ptr
=
PHC
:
:
sPHC
-
>
MaybePageAlloc
(
aArenaId
.
isSome
(
)
?
aArenaId
:
Nothing
(
)
aReqSize
aAlignment
false
)
;
}
return
ptr
?
ptr
:
(
aArenaId
.
isSome
(
)
?
MozJemalloc
:
:
moz_arena_memalign
(
*
aArenaId
aAlignment
aReqSize
)
:
MozJemalloc
:
:
memalign
(
aAlignment
aReqSize
)
)
;
}
inline
void
*
MozJemallocPHC
:
:
memalign
(
size_t
aAlignment
size_t
aReqSize
)
{
return
PageMemalign
(
Nothing
(
)
aAlignment
aReqSize
)
;
}
inline
size_t
MozJemallocPHC
:
:
malloc_usable_size
(
usable_ptr_t
aPtr
)
{
if
(
!
maybe_init
(
)
)
{
return
MozJemalloc
:
:
malloc_usable_size
(
aPtr
)
;
}
PtrKind
pk
=
PHC
:
:
sRegion
.
PtrKind
(
aPtr
)
;
if
(
pk
.
IsNothing
(
)
)
{
return
MozJemalloc
:
:
malloc_usable_size
(
aPtr
)
;
}
return
PHC
:
:
sPHC
-
>
PtrUsableSize
(
aPtr
)
;
}
size_t
PHC
:
:
PtrUsableSize
(
usable_ptr_t
aPtr
)
MOZ_EXCLUDES
(
mMutex
)
{
PtrKind
pk
=
sRegion
.
PtrKind
(
aPtr
)
;
if
(
pk
.
IsGuardPage
(
)
)
{
CrashOnGuardPage
(
const_cast
<
void
*
>
(
aPtr
)
)
;
}
uintptr_t
index
=
pk
.
AllocPageIndex
(
)
;
MutexAutoLock
lock
(
mMutex
)
;
void
*
pageBaseAddr
=
AllocPageBaseAddr
(
index
)
;
if
(
MOZ_UNLIKELY
(
aPtr
<
pageBaseAddr
)
)
{
return
0
;
}
return
PageUsableSize
(
index
)
;
}
static
size_t
metadata_size
(
)
{
return
MozJemalloc
:
:
malloc_usable_size
(
PHC
:
:
sPHC
)
;
}
inline
void
MozJemallocPHC
:
:
jemalloc_stats_internal
(
jemalloc_stats_t
*
aStats
jemalloc_bin_stats_t
*
aBinStats
)
{
MozJemalloc
:
:
jemalloc_stats_internal
(
aStats
aBinStats
)
;
if
(
!
maybe_init
(
)
)
{
return
;
}
aStats
-
>
allocated
-
=
kAllPagesJemallocSize
;
aStats
-
>
allocated
+
=
PHC
:
:
sPHC
-
>
AllocatedBytes
(
)
;
size_t
bookkeeping
=
metadata_size
(
)
;
aStats
-
>
allocated
-
=
bookkeeping
;
aStats
-
>
bookkeeping
+
=
bookkeeping
;
}
inline
void
MozJemallocPHC
:
:
jemalloc_stats_lite
(
jemalloc_stats_lite_t
*
aStats
)
{
MozJemalloc
:
:
jemalloc_stats_lite
(
aStats
)
;
}
inline
void
MozJemallocPHC
:
:
jemalloc_ptr_info
(
const
void
*
aPtr
jemalloc_ptr_info_t
*
aInfo
)
{
if
(
!
maybe_init
(
)
)
{
MozJemalloc
:
:
jemalloc_ptr_info
(
aPtr
aInfo
)
;
return
;
}
PtrKind
pk
=
PHC
:
:
sRegion
.
PtrKind
(
aPtr
)
;
if
(
pk
.
IsNothing
(
)
)
{
MozJemalloc
:
:
jemalloc_ptr_info
(
aPtr
aInfo
)
;
return
;
}
PHC
:
:
sPHC
-
>
PagePtrInfo
(
aPtr
aInfo
)
;
}
void
PHC
:
:
PagePtrInfo
(
const
void
*
aPtr
jemalloc_ptr_info_t
*
aInfo
)
MOZ_EXCLUDES
(
mMutex
)
{
PtrKind
pk
=
sRegion
.
PtrKind
(
aPtr
)
;
if
(
pk
.
IsGuardPage
(
)
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
return
;
}
uintptr_t
index
=
pk
.
AllocPageIndex
(
)
;
MutexAutoLock
lock
(
mMutex
)
;
FillJemallocPtrInfo
(
aPtr
index
aInfo
)
;
#
if
DEBUG
Log
(
"
JemallocPtrInfo
(
%
p
[
%
zu
]
)
-
>
{
%
zu
%
p
%
zu
%
zu
}
\
n
"
aPtr
index
size_t
(
aInfo
-
>
tag
)
aInfo
-
>
addr
aInfo
-
>
size
aInfo
-
>
arenaId
)
;
#
else
Log
(
"
JemallocPtrInfo
(
%
p
[
%
zu
]
)
-
>
{
%
zu
%
p
%
zu
}
\
n
"
aPtr
index
size_t
(
aInfo
-
>
tag
)
aInfo
-
>
addr
aInfo
-
>
size
)
;
#
endif
}
inline
void
*
MozJemallocPHC
:
:
moz_arena_malloc
(
arena_id_t
aArenaId
size_t
aReqSize
)
{
return
PageMalloc
(
Some
(
aArenaId
)
aReqSize
)
;
}
inline
void
*
MozJemallocPHC
:
:
moz_arena_calloc
(
arena_id_t
aArenaId
size_t
aNum
size_t
aReqSize
)
{
return
PageCalloc
(
Some
(
aArenaId
)
aNum
aReqSize
)
;
}
inline
void
*
MozJemallocPHC
:
:
moz_arena_realloc
(
arena_id_t
aArenaId
void
*
aOldPtr
size_t
aNewSize
)
{
return
PageRealloc
(
Some
(
aArenaId
)
aOldPtr
aNewSize
)
;
}
inline
void
MozJemallocPHC
:
:
moz_arena_free
(
arena_id_t
aArenaId
void
*
aPtr
)
{
return
PageFree
(
Some
(
aArenaId
)
aPtr
)
;
}
inline
void
*
MozJemallocPHC
:
:
moz_arena_memalign
(
arena_id_t
aArenaId
size_t
aAlignment
size_t
aReqSize
)
{
return
PageMemalign
(
Some
(
aArenaId
)
aAlignment
aReqSize
)
;
}
bool
PHC
:
:
IsPHCAllocation
(
const
void
*
aPtr
mozilla
:
:
phc
:
:
AddrInfo
*
aOut
)
{
PtrKind
pk
=
sRegion
.
PtrKind
(
aPtr
)
;
if
(
pk
.
IsNothing
(
)
)
{
return
false
;
}
bool
isGuardPage
=
false
;
if
(
pk
.
IsGuardPage
(
)
)
{
if
(
(
uintptr_t
(
aPtr
)
%
kPageSize
)
<
(
kPageSize
/
2
)
)
{
if
(
sRegion
.
IsInFirstGuardPage
(
aPtr
)
)
{
return
false
;
}
pk
=
sRegion
.
PtrKind
(
static_cast
<
const
uint8_t
*
>
(
aPtr
)
-
kPageSize
)
;
}
else
{
pk
=
sRegion
.
PtrKind
(
static_cast
<
const
uint8_t
*
>
(
aPtr
)
+
kPageSize
)
;
}
isGuardPage
=
true
;
}
uintptr_t
index
=
pk
.
AllocPageIndex
(
)
;
if
(
aOut
)
{
if
(
mMutex
.
TryLock
(
)
)
{
FillAddrInfo
(
index
aPtr
isGuardPage
*
aOut
)
;
Log
(
"
IsPHCAllocation
:
%
zu
%
p
%
zu
%
zu
%
zu
\
n
"
size_t
(
aOut
-
>
mKind
)
aOut
-
>
mBaseAddr
aOut
-
>
mUsableSize
aOut
-
>
mAllocStack
.
isSome
(
)
?
aOut
-
>
mAllocStack
-
>
mLength
:
0
aOut
-
>
mFreeStack
.
isSome
(
)
?
aOut
-
>
mFreeStack
-
>
mLength
:
0
)
;
mMutex
.
Unlock
(
)
;
}
else
{
Log
(
"
IsPHCAllocation
:
PHC
is
locked
\
n
"
)
;
aOut
-
>
mPhcWasLocked
=
true
;
}
}
return
true
;
}
namespace
mozilla
:
:
phc
{
bool
IsPHCAllocation
(
const
void
*
aPtr
AddrInfo
*
aOut
)
{
if
(
!
maybe_init
(
)
)
{
return
false
;
}
return
PHC
:
:
sPHC
-
>
IsPHCAllocation
(
aPtr
aOut
)
;
}
void
DisablePHCOnCurrentThread
(
)
{
PHC
:
:
DisableOnCurrentThread
(
)
;
Log
(
"
DisablePHCOnCurrentThread
:
%
zu
\
n
"
0ul
)
;
}
void
ReenablePHCOnCurrentThread
(
)
{
PHC
:
:
sPHC
-
>
EnableOnCurrentThread
(
)
;
Log
(
"
ReenablePHCOnCurrentThread
:
%
zu
\
n
"
0ul
)
;
}
bool
IsPHCEnabledOnCurrentThread
(
)
{
bool
enabled
=
!
PHC
:
:
IsDisabledOnCurrentThread
(
)
;
Log
(
"
IsPHCEnabledOnCurrentThread
:
%
zu
\
n
"
size_t
(
enabled
)
)
;
return
enabled
;
}
void
PHCMemoryUsage
(
MemoryUsage
&
aMemoryUsage
)
{
if
(
!
maybe_init
(
)
)
{
aMemoryUsage
=
MemoryUsage
(
)
;
return
;
}
aMemoryUsage
.
mMetadataBytes
=
metadata_size
(
)
;
if
(
PHC
:
:
sPHC
)
{
aMemoryUsage
.
mFragmentationBytes
=
PHC
:
:
sPHC
-
>
FragmentationBytes
(
)
;
}
else
{
aMemoryUsage
.
mFragmentationBytes
=
0
;
}
}
void
GetPHCStats
(
PHCStats
&
aStats
)
{
if
(
!
maybe_init
(
)
)
{
aStats
=
PHCStats
(
)
;
return
;
}
aStats
=
PHC
:
:
sPHC
-
>
GetPageStats
(
)
;
}
void
SetPHCState
(
PHCState
aState
)
{
if
(
!
maybe_init
(
)
)
{
return
;
}
PHC
:
:
sPHC
-
>
SetState
(
aState
)
;
}
void
SetPHCProbabilities
(
int64_t
aAvgDelayFirst
int64_t
aAvgDelayNormal
int64_t
aAvgDelayPageReuse
)
{
if
(
!
maybe_init
(
)
)
{
return
;
}
PHC
:
:
sPHC
-
>
SetProbabilities
(
aAvgDelayFirst
aAvgDelayNormal
aAvgDelayPageReuse
)
;
}
}
#
if
PHC_LOGGING
static
size_t
GetPid
(
)
{
return
size_t
(
getpid
(
)
)
;
}
static
size_t
GetTid
(
)
{
#
if
defined
(
XP_WIN
)
return
size_t
(
GetCurrentThreadId
(
)
)
;
#
else
return
size_t
(
pthread_self
(
)
)
;
#
endif
}
#
endif
static
void
Log
(
const
char
*
fmt
.
.
.
)
{
#
if
PHC_LOGGING
#
if
defined
(
XP_WIN
)
#
define
LOG_STDERR
\
reinterpret_cast
<
intptr_t
>
(
GetStdHandle
(
STD_ERROR_HANDLE
)
)
#
else
#
define
LOG_STDERR
2
#
endif
char
buf
[
256
]
;
size_t
pos
=
SNPrintf
(
buf
sizeof
(
buf
)
"
PHC
[
%
zu
%
zu
~
%
zu
]
"
GetPid
(
)
GetTid
(
)
size_t
(
PHC
:
:
Now
(
)
)
)
;
va_list
vargs
;
va_start
(
vargs
fmt
)
;
pos
+
=
VSNPrintf
(
&
buf
[
pos
]
sizeof
(
buf
)
-
pos
fmt
vargs
)
;
MOZ_ASSERT
(
pos
<
sizeof
(
buf
)
)
;
va_end
(
vargs
)
;
FdPuts
(
LOG_STDERR
buf
pos
)
;
#
endif
}
