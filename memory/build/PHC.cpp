#
include
"
PHC
.
h
"
#
include
<
stdlib
.
h
>
#
include
<
time
.
h
>
#
include
<
algorithm
>
#
ifdef
XP_WIN
#
include
<
process
.
h
>
#
else
#
include
<
sys
/
mman
.
h
>
#
include
<
sys
/
types
.
h
>
#
include
<
pthread
.
h
>
#
include
<
unistd
.
h
>
#
endif
#
include
"
mozjemalloc
.
h
"
#
include
"
FdPrintf
.
h
"
#
include
"
Mutex
.
h
"
#
include
"
mozilla
/
Assertions
.
h
"
#
include
"
mozilla
/
Atomics
.
h
"
#
include
"
mozilla
/
Attributes
.
h
"
#
include
"
mozilla
/
CheckedInt
.
h
"
#
include
"
mozilla
/
Maybe
.
h
"
#
include
"
mozilla
/
StackWalk
.
h
"
#
include
"
mozilla
/
ThreadLocal
.
h
"
#
include
"
mozilla
/
XorShift128PlusRNG
.
h
"
using
namespace
mozilla
;
#
ifdef
ANDROID
extern
"
C
"
MOZ_EXPORT
int
pthread_atfork
(
void
(
*
)
(
void
)
void
(
*
)
(
void
)
void
(
*
)
(
void
)
)
;
#
endif
#
ifndef
DISALLOW_COPY_AND_ASSIGN
#
define
DISALLOW_COPY_AND_ASSIGN
(
T
)
\
T
(
const
T
&
)
;
\
void
operator
=
(
const
T
&
)
#
endif
class
InfallibleAllocPolicy
{
public
:
static
void
AbortOnFailure
(
const
void
*
aP
)
{
if
(
!
aP
)
{
MOZ_CRASH
(
"
PHC
failed
to
allocate
"
)
;
}
}
template
<
class
T
>
static
T
*
new_
(
)
{
void
*
p
=
MozJemalloc
:
:
malloc
(
sizeof
(
T
)
)
;
AbortOnFailure
(
p
)
;
return
new
(
p
)
T
;
}
template
<
class
T
>
static
T
*
new_
(
size_t
n
)
{
void
*
p
=
MozJemalloc
:
:
malloc
(
sizeof
(
T
)
*
n
)
;
AbortOnFailure
(
p
)
;
return
new
(
p
)
T
[
n
]
;
}
template
<
class
T
>
static
T
*
realloc
(
T
*
aOldArray
size_t
n
)
{
void
*
p
=
MozJemalloc
:
:
realloc
(
aOldArray
sizeof
(
T
)
*
n
)
;
AbortOnFailure
(
p
)
;
return
reinterpret_cast
<
T
*
>
(
p
)
;
}
}
;
class
StackTrace
:
public
phc
:
:
StackTrace
{
public
:
StackTrace
(
)
=
default
;
void
Clear
(
)
{
mLength
=
0
;
}
void
Fill
(
)
;
private
:
static
void
StackWalkCallback
(
uint32_t
aFrameNumber
void
*
aPc
void
*
aSp
void
*
aClosure
)
{
StackTrace
*
st
=
(
StackTrace
*
)
aClosure
;
MOZ_ASSERT
(
st
-
>
mLength
<
kMaxFrames
)
;
st
-
>
mPcs
[
st
-
>
mLength
]
=
aPc
;
st
-
>
mLength
+
+
;
MOZ_ASSERT
(
st
-
>
mLength
=
=
aFrameNumber
)
;
}
}
;
void
StackTrace
:
:
Fill
(
)
{
mLength
=
0
;
#
if
defined
(
XP_WIN
)
&
&
defined
(
_M_IX86
)
CONTEXT
context
;
RtlCaptureContext
(
&
context
)
;
void
*
*
fp
=
reinterpret_cast
<
void
*
*
>
(
context
.
Ebp
)
;
PNT_TIB
pTib
=
reinterpret_cast
<
PNT_TIB
>
(
NtCurrentTeb
(
)
)
;
void
*
stackEnd
=
static_cast
<
void
*
>
(
pTib
-
>
StackBase
)
;
FramePointerStackWalk
(
StackWalkCallback
kMaxFrames
this
fp
stackEnd
)
;
#
elif
defined
(
XP_DARWIN
)
#
pragma
GCC
diagnostic
push
#
pragma
GCC
diagnostic
ignored
"
-
Wframe
-
address
"
void
*
*
fp
=
reinterpret_cast
<
void
*
*
>
(
__builtin_frame_address
(
1
)
)
;
#
pragma
GCC
diagnostic
pop
void
*
stackEnd
=
pthread_get_stackaddr_np
(
pthread_self
(
)
)
;
FramePointerStackWalk
(
StackWalkCallback
kMaxFrames
this
fp
stackEnd
)
;
#
else
MozStackWalk
(
StackWalkCallback
nullptr
kMaxFrames
this
)
;
#
endif
}
#
define
PHC_LOGGING
0
static
void
Log
(
const
char
*
fmt
.
.
.
)
;
template
<
typename
T
>
class
PHCArray
{
private
:
size_t
mCapacity
=
0
;
T
*
mArray
=
nullptr
;
public
:
PHCArray
(
)
{
}
~
PHCArray
(
)
{
for
(
size_t
i
=
0
;
i
<
mCapacity
;
i
+
+
)
{
mArray
[
i
]
.
~
T
(
)
;
}
MozJemalloc
:
:
free
(
mArray
)
;
}
const
T
&
operator
[
]
(
size_t
aIndex
)
const
{
MOZ_ASSERT
(
aIndex
<
mCapacity
)
;
return
mArray
[
aIndex
]
;
}
T
&
operator
[
]
(
size_t
aIndex
)
{
MOZ_ASSERT
(
aIndex
<
mCapacity
)
;
return
mArray
[
aIndex
]
;
}
T
*
begin
(
)
{
return
mArray
;
}
const
T
*
begin
(
)
const
{
return
mArray
;
}
const
T
*
end
(
)
const
{
return
&
mArray
[
mCapacity
]
;
}
void
Init
(
size_t
aCapacity
)
{
MOZ_ASSERT
(
mCapacity
=
=
0
)
;
MOZ_ASSERT
(
mArray
=
=
nullptr
)
;
mArray
=
InfallibleAllocPolicy
:
:
new_
<
T
>
(
aCapacity
)
;
mCapacity
=
aCapacity
;
}
size_t
Capacity
(
)
const
{
return
mCapacity
;
}
void
GrowTo
(
size_t
aNewCapacity
)
{
MOZ_ASSERT
(
aNewCapacity
>
mCapacity
)
;
if
(
mCapacity
=
=
0
)
{
Init
(
aNewCapacity
)
;
return
;
}
mArray
=
InfallibleAllocPolicy
:
:
realloc
<
T
>
(
mArray
aNewCapacity
)
;
for
(
size_t
i
=
mCapacity
;
i
<
aNewCapacity
;
i
+
+
)
{
new
(
&
mArray
[
i
]
)
T
(
)
;
}
mCapacity
=
aNewCapacity
;
}
size_t
SizeOfExcludingThis
(
)
{
return
MozJemalloc
:
:
malloc_usable_size
(
mArray
)
;
}
}
;
using
Time
=
uint64_t
;
using
Delay
=
uint32_t
;
static
constexpr
Delay
DELAY_MAX
=
UINT32_MAX
/
2
;
static
const
size_t
kPageSize
=
#
if
defined
(
XP_DARWIN
)
&
&
defined
(
__aarch64__
)
16384
#
else
4096
#
endif
;
static
const
size_t
kPhcAlign
=
1024
*
1024
;
static_assert
(
IsPowerOfTwo
(
kPhcAlign
)
)
;
static_assert
(
(
kPhcAlign
%
kPageSize
)
=
=
0
)
;
#
ifdef
HAVE_64BIT_BUILD
static
const
size_t
kPhcVirtualReservation
=
1024
*
1024
*
1024
;
#
else
static
const
size_t
kPhcVirtualReservation
=
2
*
1024
*
1024
;
#
endif
static
const
Delay
kDelayDecrementAmount
=
256
;
static
const
Delay
kDelayBackoffAmount
=
64
;
static
const
Delay
kDelayResetWhenDisabled
=
64
*
1024
;
#
define
DEFAULT_STATE
mozilla
:
:
phc
:
:
OnlyFree
static
const
Time
kMaxTime
=
~
(
Time
(
0
)
)
;
constexpr
Delay
Rnd64ToDelay
(
Delay
aAvgDelay
uint64_t
aRnd
)
{
MOZ_ASSERT
(
IsPowerOfTwo
(
aAvgDelay
)
"
must
be
a
power
of
two
"
)
;
return
(
aRnd
&
(
uint64_t
(
aAvgDelay
)
*
2
-
1
)
)
+
1
;
}
static
Delay
CheckProbability
(
int64_t
aProb
)
{
return
RoundUpPow2
(
std
:
:
clamp
(
aProb
int64_t
(
2
)
int64_t
(
0x80000000
)
)
)
;
}
#
if
!
defined
(
XP_DARWIN
)
#
define
PHC_THREAD_LOCAL
(
T
)
MOZ_THREAD_LOCAL
(
T
)
#
else
#
define
PHC_THREAD_LOCAL
(
T
)
\
detail
:
:
ThreadLocal
<
T
detail
:
:
ThreadLocalKeyStorage
>
#
endif
enum
class
AllocPageState
{
NeverAllocated
=
0
InUse
=
1
Freed
=
2
}
;
class
AllocPageInfo
{
public
:
AllocPageInfo
(
)
:
mState
(
AllocPageState
:
:
NeverAllocated
)
mBaseAddr
(
nullptr
)
mReuseTime
(
0
)
{
}
AllocPageState
mState
;
Maybe
<
arena_id_t
>
mArenaId
;
uint8_t
*
mBaseAddr
;
size_t
UsableSize
(
)
const
{
return
mState
=
=
AllocPageState
:
:
NeverAllocated
?
0
:
kPageSize
-
(
reinterpret_cast
<
uintptr_t
>
(
mBaseAddr
)
&
(
kPageSize
-
1
)
)
;
}
size_t
FragmentationBytes
(
)
const
{
MOZ_ASSERT
(
kPageSize
>
=
UsableSize
(
)
)
;
return
mState
=
=
AllocPageState
:
:
InUse
?
kPageSize
-
UsableSize
(
)
:
0
;
}
Maybe
<
StackTrace
>
mAllocStack
;
Maybe
<
StackTrace
>
mFreeStack
;
Time
mReuseTime
;
#
if
PHC_LOGGING
Time
mFreeTime
;
#
endif
Maybe
<
uintptr_t
>
mNextPage
;
void
AssertInUse
(
)
const
{
MOZ_ASSERT
(
mState
=
=
AllocPageState
:
:
InUse
)
;
MOZ_ASSERT
(
mBaseAddr
)
;
MOZ_ASSERT
(
UsableSize
(
)
>
0
)
;
MOZ_ASSERT
(
mAllocStack
.
isSome
(
)
)
;
MOZ_ASSERT
(
mFreeStack
.
isNothing
(
)
)
;
MOZ_ASSERT
(
mReuseTime
=
=
kMaxTime
)
;
MOZ_ASSERT
(
!
mNextPage
)
;
}
void
AssertNotInUse
(
)
const
{
#
ifdef
DEBUG
bool
isFresh
=
mState
=
=
AllocPageState
:
:
NeverAllocated
;
MOZ_ASSERT
(
isFresh
|
|
mState
=
=
AllocPageState
:
:
Freed
)
;
MOZ_ASSERT_IF
(
isFresh
mArenaId
=
=
Nothing
(
)
)
;
MOZ_ASSERT
(
isFresh
=
=
(
mBaseAddr
=
=
nullptr
)
)
;
MOZ_ASSERT
(
isFresh
=
=
(
mAllocStack
.
isNothing
(
)
)
)
;
MOZ_ASSERT
(
isFresh
=
=
(
mFreeStack
.
isNothing
(
)
)
)
;
MOZ_ASSERT
(
mReuseTime
!
=
kMaxTime
)
;
#
endif
}
bool
IsPageInUse
(
)
const
{
return
mState
=
=
AllocPageState
:
:
InUse
;
}
bool
IsPageFreed
(
)
const
{
return
mState
=
=
AllocPageState
:
:
Freed
;
}
bool
IsPageAllocatable
(
Time
aNow
)
const
{
return
!
IsPageInUse
(
)
&
&
aNow
>
=
mReuseTime
;
}
void
SetInUse
(
const
Maybe
<
arena_id_t
>
&
aArenaId
uint8_t
*
aBaseAddr
const
StackTrace
&
aAllocStack
)
{
AssertNotInUse
(
)
;
mState
=
AllocPageState
:
:
InUse
;
mArenaId
=
aArenaId
;
mBaseAddr
=
aBaseAddr
;
mAllocStack
=
Some
(
aAllocStack
)
;
mFreeStack
=
Nothing
(
)
;
mReuseTime
=
kMaxTime
;
MOZ_ASSERT
(
!
mNextPage
)
;
}
void
ResizeInUse
(
const
Maybe
<
arena_id_t
>
&
aArenaId
uint8_t
*
aNewBaseAddr
const
StackTrace
&
aAllocStack
)
{
AssertInUse
(
)
;
if
(
aArenaId
.
isSome
(
)
)
{
MOZ_RELEASE_ASSERT
(
mArenaId
=
=
aArenaId
)
;
}
mBaseAddr
=
aNewBaseAddr
;
mAllocStack
=
Some
(
aAllocStack
)
;
}
void
SetPageFreed
(
const
Maybe
<
arena_id_t
>
&
aArenaId
const
StackTrace
&
aFreeStack
Delay
aReuseDelay
Time
aNow
)
{
AssertInUse
(
)
;
mState
=
AllocPageState
:
:
Freed
;
if
(
aArenaId
.
isSome
(
)
)
{
MOZ_RELEASE_ASSERT
(
mArenaId
=
=
aArenaId
)
;
}
mFreeStack
=
Some
(
aFreeStack
)
;
#
if
PHC_LOGGING
mFreeTime
=
aNow
;
#
endif
mReuseTime
=
aNow
+
aReuseDelay
;
}
}
;
class
PHCRegion
{
private
:
uint8_t
*
mPagesStart
=
nullptr
;
uint8_t
*
mPagesLimit
=
nullptr
;
public
:
bool
AllocVirtualAddresses
(
)
{
MOZ_ASSERT
(
!
mPagesStart
|
|
!
mPagesLimit
)
;
size_t
jemalloc_allocation
=
kPhcVirtualReservation
-
kPageSize
;
void
*
pages
=
MozJemalloc
:
:
memalign
(
kPhcAlign
jemalloc_allocation
)
;
if
(
!
pages
)
{
return
false
;
}
#
ifdef
XP_WIN
if
(
!
VirtualFree
(
pages
jemalloc_allocation
MEM_DECOMMIT
)
)
{
return
false
;
}
#
else
if
(
mmap
(
pages
jemalloc_allocation
PROT_NONE
MAP_FIXED
|
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
=
=
MAP_FAILED
)
{
return
false
;
}
#
endif
mPagesStart
=
static_cast
<
uint8_t
*
>
(
pages
)
;
mPagesLimit
=
mPagesStart
+
kPhcVirtualReservation
;
Log
(
"
AllocVirtualAddresses
at
%
p
.
.
%
p
\
n
"
mPagesStart
mPagesLimit
)
;
return
true
;
}
constexpr
PHCRegion
(
)
{
}
bool
IsInFirstGuardPage
(
const
void
*
aPtr
)
{
MOZ_ASSERT
(
mPagesStart
!
=
nullptr
&
&
mPagesLimit
!
=
nullptr
)
;
return
mPagesStart
<
=
aPtr
&
&
aPtr
<
mPagesStart
+
kPageSize
;
}
uint8_t
*
AllocPagePtr
(
uintptr_t
aIndex
)
{
MOZ_ASSERT
(
mPagesStart
!
=
nullptr
&
&
mPagesLimit
!
=
nullptr
)
;
return
mPagesStart
+
(
2
*
aIndex
+
1
)
*
kPageSize
;
}
MOZ_ALWAYS_INLINE
bool
WithinBounds
(
const
void
*
aPtr
)
const
{
MOZ_ASSERT
(
mPagesStart
&
&
mPagesLimit
)
;
return
aPtr
>
=
mPagesStart
&
&
aPtr
<
mPagesLimit
;
}
const
uint8_t
*
PagesStart
(
)
const
{
return
mPagesStart
;
}
size_t
ReservedBytes
(
)
const
{
return
mPagesStart
?
kPhcVirtualReservation
-
kPageSize
:
0
;
}
}
;
class
PtrKind
;
class
PHC
{
public
:
PHC
(
)
:
mRNG
(
RandomSeed
<
1
>
(
)
RandomSeed
<
2
>
(
)
)
{
mMutex
.
Init
(
)
;
if
(
!
tlsIsDisabled
.
init
(
)
)
{
MOZ_CRASH
(
)
;
}
if
(
!
tlsAllocDelay
.
init
(
)
)
{
MOZ_CRASH
(
)
;
}
if
(
!
tlsLastDelay
.
init
(
)
)
{
MOZ_CRASH
(
)
;
}
#
ifdef
EARLY_BETA_OR_EARLIER
Resize
(
16
*
1024
*
1024
)
;
#
else
Resize
(
(
1024
+
128
)
*
1024
)
;
#
endif
{
MutexAutoLock
lock
(
mMutex
)
;
ForceSetNewAllocDelay
(
Rnd64ToDelay
(
mAvgFirstAllocDelay
Random64
(
)
)
)
;
}
}
void
Resize
(
size_t
aSizeBytes
)
{
size_t
max_pages
=
(
kPhcVirtualReservation
/
kPageSize
/
2
)
-
1
;
size_t
size_pages
=
aSizeBytes
/
kPageSize
;
size_pages
=
std
:
:
min
(
size_pages
max_pages
)
;
MutexAutoLock
lock
(
mMutex
)
;
size_t
old_size_pages
=
NumAllocPages
(
)
;
if
(
size_pages
>
old_size_pages
)
{
Log
(
"
Growing
PHC
storage
from
%
zu
to
%
zu
\
n
"
old_size_pages
size_pages
)
;
mAllocPages
.
GrowTo
(
size_pages
)
;
for
(
size_t
i
=
old_size_pages
;
i
<
size_pages
;
i
+
+
)
{
AppendPageToFreeList
(
i
)
;
}
}
else
if
(
size_pages
<
old_size_pages
)
{
Log
(
"
Shrink
requested
and
ignored
.
"
)
;
}
}
uint64_t
Random64
(
)
MOZ_REQUIRES
(
mMutex
)
{
return
mRNG
.
next
(
)
;
}
PtrKind
GetPtrKind
(
const
void
*
aPtr
)
;
uint8_t
*
AllocPageBaseAddr
(
uintptr_t
aIndex
)
MOZ_REQUIRES
(
mMutex
)
{
return
mAllocPages
[
aIndex
]
.
mBaseAddr
;
}
Maybe
<
arena_id_t
>
PageArena
(
uintptr_t
aIndex
)
MOZ_REQUIRES
(
mMutex
)
{
const
AllocPageInfo
&
page
=
mAllocPages
[
aIndex
]
;
page
.
AssertInUse
(
)
;
return
page
.
mArenaId
;
}
size_t
PageUsableSize
(
uintptr_t
aIndex
)
MOZ_REQUIRES
(
mMutex
)
{
const
AllocPageInfo
&
page
=
mAllocPages
[
aIndex
]
;
page
.
AssertInUse
(
)
;
return
page
.
UsableSize
(
)
;
}
void
GetMemoryUsage
(
phc
:
:
MemoryUsage
&
aInfo
)
MOZ_EXCLUDES
(
mMutex
)
{
MutexAutoLock
lock
(
mMutex
)
;
aInfo
=
phc
:
:
MemoryUsage
(
)
;
for
(
const
auto
&
page
:
mAllocPages
)
{
if
(
page
.
IsPageInUse
(
)
)
{
aInfo
.
mAllocatedBytes
+
=
page
.
UsableSize
(
)
;
aInfo
.
mFragmentationBytes
+
=
page
.
FragmentationBytes
(
)
;
}
}
aInfo
.
mMetadataBytes
=
MozJemalloc
:
:
malloc_usable_size
(
this
)
+
mAllocPages
.
SizeOfExcludingThis
(
)
;
}
void
SetPageInUse
(
uintptr_t
aIndex
const
Maybe
<
arena_id_t
>
&
aArenaId
uint8_t
*
aBaseAddr
const
StackTrace
&
aAllocStack
)
MOZ_REQUIRES
(
mMutex
)
{
mAllocPages
[
aIndex
]
.
SetInUse
(
aArenaId
aBaseAddr
aAllocStack
)
;
}
#
if
PHC_LOGGING
Time
GetFreeTime
(
uintptr_t
aIndex
)
const
MOZ_REQUIRES
(
mMutex
)
{
return
mAllocPages
[
aIndex
]
.
mFreeTime
;
}
#
endif
void
ResizePageInUse
(
uintptr_t
aIndex
const
Maybe
<
arena_id_t
>
&
aArenaId
uint8_t
*
aNewBaseAddr
const
StackTrace
&
aAllocStack
)
MOZ_REQUIRES
(
mMutex
)
{
mAllocPages
[
aIndex
]
.
ResizeInUse
(
aArenaId
aNewBaseAddr
aAllocStack
)
;
}
;
void
SetPageFreed
(
uintptr_t
aIndex
const
Maybe
<
arena_id_t
>
&
aArenaId
const
StackTrace
&
aFreeStack
Delay
aReuseDelay
)
MOZ_REQUIRES
(
mMutex
)
{
AllocPageInfo
&
page
=
mAllocPages
[
aIndex
]
;
page
.
SetPageFreed
(
aArenaId
aFreeStack
aReuseDelay
Now
(
)
)
;
MOZ_ASSERT
(
!
page
.
mNextPage
)
;
AppendPageToFreeList
(
aIndex
)
;
}
static
void
CrashOnGuardPage
(
void
*
aPtr
)
{
Log
(
"
CrashOnGuardPage
(
%
p
)
bounds
violation
\
n
"
aPtr
)
;
*
static_cast
<
uint8_t
*
>
(
aPtr
)
=
0
;
MOZ_CRASH
(
"
unreachable
"
)
;
}
void
EnsureValidAndInUse
(
void
*
aPtr
uintptr_t
aIndex
)
MOZ_REQUIRES
(
mMutex
)
{
const
AllocPageInfo
&
page
=
mAllocPages
[
aIndex
]
;
MOZ_RELEASE_ASSERT
(
page
.
mBaseAddr
=
=
aPtr
)
;
if
(
page
.
mState
=
=
AllocPageState
:
:
Freed
)
{
Log
(
"
EnsureValidAndInUse
(
%
p
)
use
-
after
-
free
\
n
"
aPtr
)
;
mMutex
.
Unlock
(
)
;
*
static_cast
<
uint8_t
*
>
(
aPtr
)
=
0
;
MOZ_CRASH
(
"
unreachable
"
)
;
}
}
void
FillAddrInfo
(
uintptr_t
aIndex
const
void
*
aBaseAddr
bool
isGuardPage
phc
:
:
AddrInfo
&
aOut
)
MOZ_REQUIRES
(
mMutex
)
{
const
AllocPageInfo
&
page
=
mAllocPages
[
aIndex
]
;
if
(
isGuardPage
)
{
aOut
.
mKind
=
phc
:
:
AddrInfo
:
:
Kind
:
:
GuardPage
;
}
else
{
switch
(
page
.
mState
)
{
case
AllocPageState
:
:
NeverAllocated
:
aOut
.
mKind
=
phc
:
:
AddrInfo
:
:
Kind
:
:
NeverAllocatedPage
;
break
;
case
AllocPageState
:
:
InUse
:
aOut
.
mKind
=
phc
:
:
AddrInfo
:
:
Kind
:
:
InUsePage
;
break
;
case
AllocPageState
:
:
Freed
:
aOut
.
mKind
=
phc
:
:
AddrInfo
:
:
Kind
:
:
FreedPage
;
break
;
default
:
MOZ_CRASH
(
)
;
}
}
aOut
.
mBaseAddr
=
page
.
mBaseAddr
;
aOut
.
mUsableSize
=
page
.
UsableSize
(
)
;
aOut
.
mAllocStack
=
page
.
mAllocStack
;
aOut
.
mFreeStack
=
page
.
mFreeStack
;
}
void
FillJemallocPtrInfo
(
const
void
*
aPtr
uintptr_t
aIndex
jemalloc_ptr_info_t
*
aInfo
)
MOZ_REQUIRES
(
mMutex
)
{
const
AllocPageInfo
&
page
=
mAllocPages
[
aIndex
]
;
switch
(
page
.
mState
)
{
case
AllocPageState
:
:
NeverAllocated
:
break
;
case
AllocPageState
:
:
InUse
:
{
uint8_t
*
base
=
page
.
mBaseAddr
;
uint8_t
*
limit
=
base
+
page
.
UsableSize
(
)
;
if
(
base
<
=
aPtr
&
&
aPtr
<
limit
)
{
*
aInfo
=
{
TagLiveAlloc
page
.
mBaseAddr
page
.
UsableSize
(
)
page
.
mArenaId
.
valueOr
(
0
)
}
;
return
;
}
break
;
}
case
AllocPageState
:
:
Freed
:
{
uint8_t
*
base
=
page
.
mBaseAddr
;
uint8_t
*
limit
=
base
+
page
.
UsableSize
(
)
;
if
(
base
<
=
aPtr
&
&
aPtr
<
limit
)
{
*
aInfo
=
{
TagFreedAlloc
page
.
mBaseAddr
page
.
UsableSize
(
)
page
.
mArenaId
.
valueOr
(
0
)
}
;
return
;
}
break
;
}
default
:
MOZ_CRASH
(
)
;
}
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
}
#
ifndef
XP_WIN
static
void
prefork
(
)
MOZ_NO_THREAD_SAFETY_ANALYSIS
{
PHC
:
:
sPHC
-
>
mMutex
.
Lock
(
)
;
}
static
void
postfork_parent
(
)
MOZ_NO_THREAD_SAFETY_ANALYSIS
{
PHC
:
:
sPHC
-
>
mMutex
.
Unlock
(
)
;
}
static
void
postfork_child
(
)
{
PHC
:
:
sPHC
-
>
mMutex
.
Init
(
)
;
}
#
endif
void
IncPageAllocHits
(
)
MOZ_REQUIRES
(
mMutex
)
{
#
if
PHC_LOGGING
mPageAllocHits
+
+
;
#
endif
}
void
IncPageAllocMisses
(
)
MOZ_REQUIRES
(
mMutex
)
{
#
if
PHC_LOGGING
mPageAllocMisses
+
+
;
#
endif
}
phc
:
:
PHCStats
GetPageStatsLocked
(
)
MOZ_REQUIRES
(
mMutex
)
{
phc
:
:
PHCStats
stats
;
for
(
const
auto
&
page
:
mAllocPages
)
{
stats
.
mSlotsAllocated
+
=
page
.
IsPageInUse
(
)
?
1
:
0
;
stats
.
mSlotsFreed
+
=
page
.
IsPageFreed
(
)
?
1
:
0
;
}
stats
.
mSlotsUnused
=
NumAllocPages
(
)
-
stats
.
mSlotsAllocated
-
stats
.
mSlotsFreed
;
return
stats
;
}
phc
:
:
PHCStats
GetPageStats
(
)
MOZ_EXCLUDES
(
mMutex
)
{
MutexAutoLock
lock
(
mMutex
)
;
return
GetPageStatsLocked
(
)
;
}
#
if
PHC_LOGGING
size_t
PageAllocHits
(
)
MOZ_REQUIRES
(
mMutex
)
{
return
mPageAllocHits
;
}
size_t
PageAllocAttempts
(
)
MOZ_REQUIRES
(
mMutex
)
{
return
mPageAllocHits
+
mPageAllocMisses
;
}
size_t
PageAllocHitRate
(
)
MOZ_REQUIRES
(
mMutex
)
{
return
mPageAllocHits
*
100
/
(
mPageAllocHits
+
mPageAllocMisses
)
;
}
#
endif
void
LogNoAlloc
(
size_t
aReqSize
size_t
aAlignment
Delay
newAllocDelay
)
;
bool
ShouldMakeNewAllocations
(
)
const
{
return
mPhcState
=
=
mozilla
:
:
phc
:
:
Enabled
;
}
using
PHCState
=
mozilla
:
:
phc
:
:
PHCState
;
void
SetState
(
PHCState
aState
)
{
if
(
mPhcState
!
=
PHCState
:
:
Enabled
&
&
aState
=
=
PHCState
:
:
Enabled
)
{
MutexAutoLock
lock
(
mMutex
)
;
ResetRNG
(
)
;
ForceSetNewAllocDelay
(
Rnd64ToDelay
(
mAvgFirstAllocDelay
Random64
(
)
)
)
;
}
mPhcState
=
aState
;
}
void
ResetRNG
(
)
MOZ_REQUIRES
(
mMutex
)
{
mRNG
=
non_crypto
:
:
XorShift128PlusRNG
(
RandomSeed
<
0
>
(
)
RandomSeed
<
1
>
(
)
)
;
}
void
SetProbabilities
(
int64_t
aAvgDelayFirst
int64_t
aAvgDelayNormal
int64_t
aAvgDelayPageReuse
)
MOZ_EXCLUDES
(
mMutex
)
{
MutexAutoLock
lock
(
mMutex
)
;
mAvgFirstAllocDelay
=
CheckProbability
(
aAvgDelayFirst
)
;
mAvgAllocDelay
=
CheckProbability
(
aAvgDelayNormal
)
;
mAvgPageReuseDelay
=
CheckProbability
(
aAvgDelayPageReuse
)
;
}
static
void
DisableOnCurrentThread
(
)
{
MOZ_ASSERT
(
!
tlsIsDisabled
.
get
(
)
)
;
tlsIsDisabled
.
set
(
true
)
;
}
void
EnableOnCurrentThread
(
)
{
MOZ_ASSERT
(
tlsIsDisabled
.
get
(
)
)
;
tlsIsDisabled
.
set
(
false
)
;
}
static
bool
IsDisabledOnCurrentThread
(
)
{
return
tlsIsDisabled
.
get
(
)
;
}
static
Time
Now
(
)
{
if
(
!
sPHC
)
{
return
0
;
}
return
sPHC
-
>
mNow
;
}
void
AdvanceNow
(
uint32_t
delay
=
0
)
{
mNow
+
=
tlsLastDelay
.
get
(
)
-
delay
;
tlsLastDelay
.
set
(
delay
)
;
}
static
bool
DecrementDelay
(
)
{
const
Delay
alloc_delay
=
tlsAllocDelay
.
get
(
)
;
if
(
MOZ_LIKELY
(
alloc_delay
>
0
)
)
{
tlsAllocDelay
.
set
(
alloc_delay
-
1
)
;
return
false
;
}
MOZ_ASSERT
(
sPHC
)
;
sPHC
-
>
AdvanceNow
(
)
;
Delay
new_delay
=
(
sAllocDelay
-
=
kDelayDecrementAmount
)
;
Delay
old_delay
=
new_delay
+
kDelayDecrementAmount
;
if
(
MOZ_LIKELY
(
new_delay
<
DELAY_MAX
)
)
{
tlsAllocDelay
.
set
(
kDelayDecrementAmount
)
;
tlsLastDelay
.
set
(
kDelayDecrementAmount
)
;
Log
(
"
Update
sAllocDelay
<
-
%
zu
tlsAllocDelay
<
-
%
zu
\
n
"
size_t
(
new_delay
)
size_t
(
kDelayDecrementAmount
)
)
;
return
false
;
}
if
(
old_delay
<
new_delay
)
{
Log
(
"
Update
sAllocDelay
<
-
%
zu
tlsAllocDelay
<
-
%
zu
\
n
"
size_t
(
new_delay
)
size_t
(
old_delay
)
)
;
if
(
old_delay
=
=
0
)
{
return
true
;
}
tlsAllocDelay
.
set
(
old_delay
)
;
tlsLastDelay
.
set
(
old_delay
)
;
return
false
;
}
Log
(
"
Update
sAllocDelay
<
-
%
zu
tlsAllocDelay
<
-
%
zu
\
n
"
size_t
(
new_delay
)
size_t
(
alloc_delay
)
)
;
return
true
;
}
static
void
ResetLocalAllocDelay
(
Delay
aDelay
=
0
)
{
tlsAllocDelay
.
set
(
aDelay
)
;
tlsLastDelay
.
set
(
aDelay
)
;
}
static
void
ForceSetNewAllocDelay
(
Delay
aNewAllocDelay
)
{
Log
(
"
Setting
sAllocDelay
<
-
%
zu
\
n
"
size_t
(
aNewAllocDelay
)
)
;
sAllocDelay
=
aNewAllocDelay
;
ResetLocalAllocDelay
(
)
;
}
static
bool
SetNewAllocDelay
(
Delay
aNewAllocDelay
)
{
bool
cas_retry
;
do
{
Delay
read_delay
=
sAllocDelay
;
if
(
read_delay
<
DELAY_MAX
)
{
Log
(
"
Observe
delay
%
zu
this
thread
lost
the
race
\
n
"
size_t
(
read_delay
)
)
;
ResetLocalAllocDelay
(
)
;
return
false
;
}
else
{
Log
(
"
Preparing
for
CAS
read
sAllocDelay
%
zu
\
n
"
size_t
(
read_delay
)
)
;
}
cas_retry
=
!
sAllocDelay
.
compareExchange
(
read_delay
aNewAllocDelay
)
;
if
(
cas_retry
)
{
Log
(
"
Lost
the
CAS
sAllocDelay
is
now
%
zu
\
n
"
size_t
(
sAllocDelay
)
)
;
cpu_pause
(
)
;
}
}
while
(
cas_retry
)
;
Log
(
"
Won
the
CAS
set
sAllocDelay
=
%
zu
\
n
"
size_t
(
sAllocDelay
)
)
;
ResetLocalAllocDelay
(
)
;
return
true
;
}
static
Delay
LocalAllocDelay
(
)
{
return
tlsAllocDelay
.
get
(
)
;
}
static
Delay
SharedAllocDelay
(
)
{
return
sAllocDelay
;
}
static
Delay
LastDelay
(
)
{
return
tlsLastDelay
.
get
(
)
;
}
Maybe
<
uintptr_t
>
PopNextFreeIfAllocatable
(
Time
now
)
MOZ_REQUIRES
(
mMutex
)
{
if
(
!
mFreePageListHead
)
{
return
Nothing
(
)
;
}
uintptr_t
index
=
mFreePageListHead
.
value
(
)
;
MOZ_RELEASE_ASSERT
(
index
<
NumAllocPages
(
)
)
;
AllocPageInfo
&
page
=
mAllocPages
[
index
]
;
page
.
AssertNotInUse
(
)
;
if
(
!
page
.
IsPageAllocatable
(
now
)
)
{
return
Nothing
(
)
;
}
mFreePageListHead
=
page
.
mNextPage
;
page
.
mNextPage
=
Nothing
(
)
;
if
(
!
mFreePageListHead
)
{
mFreePageListTail
=
Nothing
(
)
;
}
return
Some
(
index
)
;
}
void
UnpopNextFree
(
uintptr_t
index
)
MOZ_REQUIRES
(
mMutex
)
{
AllocPageInfo
&
page
=
mAllocPages
[
index
]
;
MOZ_ASSERT
(
!
page
.
mNextPage
)
;
page
.
mNextPage
=
mFreePageListHead
;
mFreePageListHead
=
Some
(
index
)
;
if
(
!
mFreePageListTail
)
{
mFreePageListTail
=
Some
(
index
)
;
}
}
void
AppendPageToFreeList
(
uintptr_t
aIndex
)
MOZ_REQUIRES
(
mMutex
)
{
MOZ_RELEASE_ASSERT
(
aIndex
<
NumAllocPages
(
)
)
;
AllocPageInfo
&
page
=
mAllocPages
[
aIndex
]
;
MOZ_ASSERT
(
!
page
.
mNextPage
)
;
MOZ_ASSERT
(
mFreePageListHead
!
=
Some
(
aIndex
)
&
&
mFreePageListTail
!
=
Some
(
aIndex
)
)
;
if
(
!
mFreePageListTail
)
{
MOZ_ASSERT
(
!
mFreePageListHead
)
;
mFreePageListHead
=
Some
(
aIndex
)
;
}
else
{
MOZ_ASSERT
(
mFreePageListTail
.
value
(
)
<
NumAllocPages
(
)
)
;
AllocPageInfo
&
tail_page
=
mAllocPages
[
mFreePageListTail
.
value
(
)
]
;
MOZ_ASSERT
(
!
tail_page
.
mNextPage
)
;
tail_page
.
mNextPage
=
Some
(
aIndex
)
;
}
page
.
mNextPage
=
Nothing
(
)
;
mFreePageListTail
=
Some
(
aIndex
)
;
}
private
:
template
<
int
N
>
uint64_t
RandomSeed
(
)
{
static_assert
(
N
=
=
0
|
|
N
=
=
1
|
|
N
=
=
2
"
must
be
0
1
or
2
"
)
;
uint64_t
seed
;
if
(
N
=
=
0
)
{
time_t
t
=
time
(
nullptr
)
;
seed
=
t
^
(
t
<
<
32
)
;
}
else
if
(
N
=
=
1
)
{
seed
=
uintptr_t
(
&
seed
)
^
(
uintptr_t
(
&
seed
)
<
<
32
)
;
}
else
{
seed
=
uintptr_t
(
&
sRegion
)
^
(
uintptr_t
(
&
sRegion
)
<
<
32
)
;
}
return
seed
;
}
public
:
void
*
MaybePageAlloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aReqSize
size_t
aAlignment
bool
aZero
)
;
void
FreePage
(
uintptr_t
aIndex
const
Maybe
<
arena_id_t
>
&
aArenaId
const
StackTrace
&
aFreeStack
Delay
aReuseDelay
)
;
void
PageFree
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aPtr
)
;
Maybe
<
void
*
>
PageRealloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aOldPtr
size_t
aNewSize
)
;
void
PagePtrInfo
(
const
void
*
aPtr
jemalloc_ptr_info_t
*
aInfo
)
;
size_t
PtrUsableSize
(
usable_ptr_t
aPtr
)
;
bool
IsPHCAllocation
(
const
void
*
aPtr
mozilla
:
:
phc
:
:
AddrInfo
*
aOut
)
;
void
Crash
(
const
char
*
aMessage
)
;
private
:
alignas
(
kCacheLineSize
)
Mutex
mMutex
MOZ_UNANNOTATED
;
Atomic
<
Time
ReleaseAcquire
>
mNow
;
Atomic
<
PHCState
Relaxed
>
mPhcState
=
Atomic
<
PHCState
Relaxed
>
(
DEFAULT_STATE
)
;
non_crypto
:
:
XorShift128PlusRNG
mRNG
MOZ_GUARDED_BY
(
mMutex
)
;
Maybe
<
uintptr_t
>
mFreePageListHead
MOZ_GUARDED_BY
(
mMutex
)
;
Maybe
<
uintptr_t
>
mFreePageListTail
MOZ_GUARDED_BY
(
mMutex
)
;
#
if
PHC_LOGGING
size_t
mPageAllocHits
MOZ_GUARDED_BY
(
mMutex
)
=
0
;
size_t
mPageAllocMisses
MOZ_GUARDED_BY
(
mMutex
)
=
0
;
#
endif
alignas
(
kCacheLineSize
)
Delay
mAvgFirstAllocDelay
MOZ_GUARDED_BY
(
mMutex
)
=
64
*
1024
;
Delay
mAvgAllocDelay
MOZ_GUARDED_BY
(
mMutex
)
=
16
*
1024
;
Delay
mAvgPageReuseDelay
MOZ_GUARDED_BY
(
mMutex
)
=
256
*
1024
;
static
PHC_THREAD_LOCAL
(
bool
)
tlsIsDisabled
;
static
Atomic
<
Delay
ReleaseAcquire
>
sAllocDelay
;
static
PHC_THREAD_LOCAL
(
Delay
)
tlsAllocDelay
;
static
PHC_THREAD_LOCAL
(
Delay
)
tlsLastDelay
;
PHCArray
<
AllocPageInfo
>
mAllocPages
MOZ_GUARDED_BY
(
mMutex
)
;
public
:
size_t
NumAllocPages
(
)
const
MOZ_REQUIRES
(
mMutex
)
{
return
mAllocPages
.
Capacity
(
)
;
}
size_t
NumAllPages
(
)
const
MOZ_REQUIRES
(
mMutex
)
{
return
NumAllocPages
(
)
*
2
+
1
;
}
Delay
GetAvgAllocDelay
(
)
MOZ_REQUIRES
(
mMutex
)
{
return
mAvgAllocDelay
;
}
Delay
GetAvgFirstAllocDelay
(
)
MOZ_REQUIRES
(
mMutex
)
{
return
mAvgFirstAllocDelay
;
}
Delay
GetAvgPageReuseDelay
(
)
MOZ_REQUIRES
(
mMutex
)
{
return
mAvgPageReuseDelay
;
}
Delay
ReuseDelay
(
)
MOZ_REQUIRES
(
mMutex
)
{
Delay
avg_reuse_delay
=
GetAvgPageReuseDelay
(
)
;
return
(
avg_reuse_delay
/
2
)
+
Rnd64ToDelay
(
avg_reuse_delay
/
2
Random64
(
)
)
;
}
static
PHCRegion
sRegion
;
static
PHC
*
sPHC
;
}
;
class
PtrKind
{
private
:
enum
class
Tag
:
uint8_t
{
GuardPage
AllocPage
}
;
Tag
mTag
;
uintptr_t
mIndex
;
protected
:
PtrKind
(
const
void
*
aPtr
const
uint8_t
*
aPagesStart
)
{
uintptr_t
offset
=
static_cast
<
const
uint8_t
*
>
(
aPtr
)
-
aPagesStart
;
uintptr_t
allPageIndex
=
offset
/
kPageSize
;
if
(
allPageIndex
&
1
)
{
uintptr_t
allocPageIndex
=
allPageIndex
/
2
;
mTag
=
Tag
:
:
AllocPage
;
mIndex
=
allocPageIndex
;
}
else
{
mTag
=
Tag
:
:
GuardPage
;
}
}
friend
PtrKind
PHC
:
:
GetPtrKind
(
const
void
*
aPtr
)
;
public
:
bool
IsGuardPage
(
)
const
{
return
mTag
=
=
Tag
:
:
GuardPage
;
}
Maybe
<
uintptr_t
>
AllocPageIndex
(
uintptr_t
aNumPages
)
const
{
MOZ_RELEASE_ASSERT
(
mTag
=
=
Tag
:
:
AllocPage
)
;
if
(
mIndex
<
aNumPages
)
{
return
Some
(
mIndex
)
;
}
else
{
return
Nothing
(
)
;
}
}
}
;
PtrKind
PHC
:
:
GetPtrKind
(
const
void
*
aPtr
)
{
MOZ_ASSERT
(
sRegion
.
WithinBounds
(
aPtr
)
)
;
return
PtrKind
(
aPtr
sRegion
.
PagesStart
(
)
)
;
}
alignas
(
kCacheLineSize
)
PHCRegion
PHC
:
:
sRegion
;
PHC
*
PHC
:
:
sPHC
;
PHC_THREAD_LOCAL
(
bool
)
PHC
:
:
tlsIsDisabled
;
PHC_THREAD_LOCAL
(
Delay
)
PHC
:
:
tlsAllocDelay
;
Atomic
<
Delay
ReleaseAcquire
>
PHC
:
:
sAllocDelay
;
PHC_THREAD_LOCAL
(
Delay
)
PHC
:
:
tlsLastDelay
;
void
PHC
:
:
Crash
(
const
char
*
aMessage
)
MOZ_REQUIRES
(
mMutex
)
{
mMutex
.
Unlock
(
)
;
MOZ_CRASH_UNSAFE
(
aMessage
)
;
}
class
AutoDisableOnCurrentThread
{
public
:
AutoDisableOnCurrentThread
(
const
AutoDisableOnCurrentThread
&
)
=
delete
;
const
AutoDisableOnCurrentThread
&
operator
=
(
const
AutoDisableOnCurrentThread
&
)
=
delete
;
explicit
AutoDisableOnCurrentThread
(
)
{
PHC
:
:
DisableOnCurrentThread
(
)
;
}
~
AutoDisableOnCurrentThread
(
)
{
PHC
:
:
sPHC
-
>
EnableOnCurrentThread
(
)
;
}
}
;
void
phc_init
(
)
{
MOZ_ASSERT
(
!
PHC
:
:
sPHC
)
;
if
(
GetKernelPageSize
(
)
!
=
kPageSize
)
{
return
;
}
if
(
!
PHC
:
:
sRegion
.
AllocVirtualAddresses
(
)
)
{
return
;
}
PHC
:
:
sPHC
=
InfallibleAllocPolicy
:
:
new_
<
PHC
>
(
)
;
#
ifndef
XP_WIN
pthread_atfork
(
PHC
:
:
prefork
PHC
:
:
postfork_parent
PHC
:
:
postfork_child
)
;
#
endif
}
static
MOZ_ALWAYS_INLINE
bool
ShouldPageAllocHot
(
size_t
aReqSize
)
{
if
(
MOZ_UNLIKELY
(
!
PHC
:
:
sPHC
)
)
{
return
false
;
}
if
(
MOZ_UNLIKELY
(
aReqSize
>
kPageSize
)
)
{
return
false
;
}
if
(
MOZ_LIKELY
(
!
PHC
:
:
DecrementDelay
(
)
)
)
{
return
false
;
}
return
true
;
}
void
PHC
:
:
LogNoAlloc
(
size_t
aReqSize
size_t
aAlignment
Delay
newAllocDelay
)
MOZ_REQUIRES
(
mMutex
)
{
#
if
PHC_LOGGING
phc
:
:
PHCStats
stats
=
GetPageStatsLocked
(
)
;
Log
(
"
No
PageAlloc
(
%
zu
%
zu
)
sAllocDelay
<
-
%
zu
fullness
%
zu
/
%
zu
/
%
zu
"
"
hits
%
zu
/
%
zu
(
%
zu
%
%
)
\
n
"
aReqSize
aAlignment
size_t
(
newAllocDelay
)
stats
.
mSlotsAllocated
stats
.
mSlotsFreed
NumAllocPages
(
)
PageAllocHits
(
)
PageAllocAttempts
(
)
PageAllocHitRate
(
)
)
;
#
endif
}
void
*
PHC
:
:
MaybePageAlloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aReqSize
size_t
aAlignment
bool
aZero
)
{
MOZ_ASSERT
(
IsPowerOfTwo
(
aAlignment
)
)
;
if
(
!
ShouldMakeNewAllocations
(
)
)
{
ForceSetNewAllocDelay
(
kDelayResetWhenDisabled
)
;
return
nullptr
;
}
if
(
IsDisabledOnCurrentThread
(
)
)
{
ResetLocalAllocDelay
(
kDelayBackoffAmount
)
;
return
nullptr
;
}
AutoDisableOnCurrentThread
disable
;
StackTrace
allocStack
;
allocStack
.
Fill
(
)
;
MutexAutoLock
lock
(
mMutex
)
;
Time
now
=
Now
(
)
;
Delay
newAllocDelay
=
Rnd64ToDelay
(
GetAvgAllocDelay
(
)
Random64
(
)
)
;
if
(
!
SetNewAllocDelay
(
newAllocDelay
)
)
{
return
nullptr
;
}
Maybe
<
uintptr_t
>
mb_index
=
PopNextFreeIfAllocatable
(
now
)
;
if
(
!
mb_index
)
{
IncPageAllocMisses
(
)
;
LogNoAlloc
(
aReqSize
aAlignment
newAllocDelay
)
;
return
nullptr
;
}
uintptr_t
index
=
mb_index
.
value
(
)
;
#
if
PHC_LOGGING
Time
lifetime
=
0
;
#
endif
uint8_t
*
pagePtr
=
sRegion
.
AllocPagePtr
(
index
)
;
MOZ_ASSERT
(
pagePtr
)
;
bool
ok
=
#
ifdef
XP_WIN
!
!
VirtualAlloc
(
pagePtr
kPageSize
MEM_COMMIT
PAGE_READWRITE
)
;
#
else
mprotect
(
pagePtr
kPageSize
PROT_READ
|
PROT_WRITE
)
=
=
0
;
#
endif
if
(
!
ok
)
{
UnpopNextFree
(
index
)
;
IncPageAllocMisses
(
)
;
LogNoAlloc
(
aReqSize
aAlignment
newAllocDelay
)
;
return
nullptr
;
}
size_t
usableSize
=
MozJemalloc
:
:
malloc_good_size
(
aReqSize
)
;
MOZ_ASSERT
(
usableSize
>
0
)
;
uint8_t
*
ptr
=
pagePtr
+
kPageSize
-
usableSize
;
if
(
aAlignment
!
=
1
)
{
ptr
=
reinterpret_cast
<
uint8_t
*
>
(
(
reinterpret_cast
<
uintptr_t
>
(
ptr
)
&
~
(
aAlignment
-
1
)
)
)
;
}
#
if
PHC_LOGGING
Time
then
=
GetFreeTime
(
index
)
;
lifetime
=
then
!
=
0
?
now
-
then
:
0
;
#
endif
SetPageInUse
(
index
aArenaId
ptr
allocStack
)
;
if
(
aZero
)
{
memset
(
ptr
0
usableSize
)
;
}
else
{
#
ifdef
DEBUG
memset
(
ptr
kAllocJunk
usableSize
)
;
#
endif
}
IncPageAllocHits
(
)
;
#
if
PHC_LOGGING
phc
:
:
PHCStats
stats
=
GetPageStatsLocked
(
)
;
Log
(
"
PageAlloc
(
%
zu
%
zu
)
-
>
%
p
[
%
zu
]
/
%
p
(
%
zu
)
(
z
%
zu
)
sAllocDelay
<
-
%
zu
"
"
fullness
%
zu
/
%
zu
/
%
zu
hits
%
zu
/
%
zu
(
%
zu
%
%
)
lifetime
%
zu
\
n
"
aReqSize
aAlignment
pagePtr
index
ptr
usableSize
size_t
(
newAllocDelay
)
size_t
(
SharedAllocDelay
(
)
)
stats
.
mSlotsAllocated
stats
.
mSlotsFreed
NumAllocPages
(
)
PageAllocHits
(
)
PageAllocAttempts
(
)
PageAllocHitRate
(
)
lifetime
)
;
#
endif
return
ptr
;
}
void
PHC
:
:
FreePage
(
uintptr_t
aIndex
const
Maybe
<
arena_id_t
>
&
aArenaId
const
StackTrace
&
aFreeStack
Delay
aReuseDelay
)
MOZ_REQUIRES
(
mMutex
)
{
void
*
pagePtr
=
sRegion
.
AllocPagePtr
(
aIndex
)
;
#
ifdef
XP_WIN
if
(
!
VirtualFree
(
pagePtr
kPageSize
MEM_DECOMMIT
)
)
{
Crash
(
"
VirtualFree
failed
"
)
;
}
#
else
if
(
mmap
(
pagePtr
kPageSize
PROT_NONE
MAP_FIXED
|
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
=
=
MAP_FAILED
)
{
Crash
(
"
mmap
failed
"
)
;
}
#
endif
SetPageFreed
(
aIndex
aArenaId
aFreeStack
aReuseDelay
)
;
}
MOZ_ALWAYS_INLINE
static
void
*
PageMalloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aReqSize
)
{
void
*
ptr
=
ShouldPageAllocHot
(
aReqSize
)
?
PHC
:
:
sPHC
-
>
MaybePageAlloc
(
aArenaId
.
isSome
(
)
?
aArenaId
:
Nothing
(
)
aReqSize
1
false
)
:
nullptr
;
return
ptr
?
ptr
:
(
aArenaId
.
isSome
(
)
?
MozJemalloc
:
:
moz_arena_malloc
(
*
aArenaId
aReqSize
)
:
MozJemalloc
:
:
malloc
(
aReqSize
)
)
;
}
inline
void
*
MozJemallocPHC
:
:
malloc
(
size_t
aReqSize
)
{
return
PageMalloc
(
Nothing
(
)
aReqSize
)
;
}
MOZ_ALWAYS_INLINE
static
void
*
PageCalloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aNum
size_t
aReqSize
)
{
CheckedInt
<
size_t
>
checkedSize
=
CheckedInt
<
size_t
>
(
aNum
)
*
aReqSize
;
if
(
!
checkedSize
.
isValid
(
)
)
{
return
nullptr
;
}
void
*
ptr
=
ShouldPageAllocHot
(
checkedSize
.
value
(
)
)
?
PHC
:
:
sPHC
-
>
MaybePageAlloc
(
aArenaId
.
isSome
(
)
?
aArenaId
:
Nothing
(
)
checkedSize
.
value
(
)
1
true
)
:
nullptr
;
return
ptr
?
ptr
:
(
aArenaId
.
isSome
(
)
?
MozJemalloc
:
:
moz_arena_calloc
(
*
aArenaId
aNum
aReqSize
)
:
MozJemalloc
:
:
calloc
(
aNum
aReqSize
)
)
;
}
inline
void
*
MozJemallocPHC
:
:
calloc
(
size_t
aNum
size_t
aReqSize
)
{
return
PageCalloc
(
Nothing
(
)
aNum
aReqSize
)
;
}
MOZ_ALWAYS_INLINE
static
bool
FastIsPHCPtr
(
const
void
*
aPtr
)
{
if
(
MOZ_UNLIKELY
(
!
PHC
:
:
sPHC
)
)
{
return
false
;
}
return
PHC
:
:
sRegion
.
WithinBounds
(
aPtr
)
;
}
MOZ_ALWAYS_INLINE
static
Maybe
<
void
*
>
MaybePageRealloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aOldPtr
size_t
aNewSize
)
{
if
(
!
aOldPtr
)
{
return
Some
(
PageMalloc
(
aArenaId
aNewSize
)
)
;
}
if
(
MOZ_UNLIKELY
(
!
FastIsPHCPtr
(
aOldPtr
)
)
)
{
return
Nothing
(
)
;
}
return
PHC
:
:
sPHC
-
>
PageRealloc
(
aArenaId
aOldPtr
aNewSize
)
;
}
Maybe
<
void
*
>
PHC
:
:
PageRealloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aOldPtr
size_t
aNewSize
)
MOZ_EXCLUDES
(
mMutex
)
{
PtrKind
pk
=
GetPtrKind
(
aOldPtr
)
;
if
(
pk
.
IsGuardPage
(
)
)
{
CrashOnGuardPage
(
aOldPtr
)
;
}
AdvanceNow
(
LocalAllocDelay
(
)
)
;
Maybe
<
AutoDisableOnCurrentThread
>
disable
;
StackTrace
stack
;
if
(
IsDisabledOnCurrentThread
(
)
)
{
}
else
{
disable
.
emplace
(
)
;
stack
.
Fill
(
)
;
}
MutexAutoLock
lock
(
mMutex
)
;
Maybe
<
uintptr_t
>
mb_index
=
pk
.
AllocPageIndex
(
NumAllocPages
(
)
)
;
if
(
!
mb_index
)
{
Crash
(
"
Realloc
of
invalid
pointer
"
)
;
}
uintptr_t
index
=
mb_index
.
value
(
)
;
EnsureValidAndInUse
(
aOldPtr
index
)
;
if
(
aNewSize
<
=
kPageSize
&
&
ShouldMakeNewAllocations
(
)
)
{
size_t
oldUsableSize
=
PageUsableSize
(
index
)
;
size_t
newUsableSize
=
MozJemalloc
:
:
malloc_good_size
(
aNewSize
)
;
uint8_t
*
pagePtr
=
sRegion
.
AllocPagePtr
(
index
)
;
uint8_t
*
newPtr
=
pagePtr
+
kPageSize
-
newUsableSize
;
memmove
(
newPtr
aOldPtr
std
:
:
min
(
oldUsableSize
aNewSize
)
)
;
ResizePageInUse
(
index
aArenaId
newPtr
stack
)
;
Log
(
"
PageRealloc
-
Reuse
(
%
p
%
zu
)
-
>
%
p
\
n
"
aOldPtr
aNewSize
newPtr
)
;
return
Some
(
newPtr
)
;
}
void
*
newPtr
;
if
(
aArenaId
.
isSome
(
)
)
{
newPtr
=
MozJemalloc
:
:
moz_arena_malloc
(
*
aArenaId
aNewSize
)
;
}
else
{
Maybe
<
arena_id_t
>
oldArenaId
=
PageArena
(
index
)
;
newPtr
=
(
oldArenaId
.
isSome
(
)
?
MozJemalloc
:
:
moz_arena_malloc
(
*
oldArenaId
aNewSize
)
:
MozJemalloc
:
:
malloc
(
aNewSize
)
)
;
}
if
(
!
newPtr
)
{
return
Some
(
nullptr
)
;
}
Delay
reuseDelay
=
ReuseDelay
(
)
;
size_t
oldUsableSize
=
PageUsableSize
(
index
)
;
memcpy
(
newPtr
aOldPtr
std
:
:
min
(
oldUsableSize
aNewSize
)
)
;
FreePage
(
index
aArenaId
stack
reuseDelay
)
;
Log
(
"
PageRealloc
-
Free
(
%
p
[
%
zu
]
%
zu
)
-
>
%
p
%
zu
delay
reuse
at
~
%
zu
\
n
"
aOldPtr
index
aNewSize
newPtr
size_t
(
reuseDelay
)
size_t
(
Now
(
)
)
+
reuseDelay
)
;
return
Some
(
newPtr
)
;
}
MOZ_ALWAYS_INLINE
static
void
*
PageRealloc
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aOldPtr
size_t
aNewSize
)
{
Maybe
<
void
*
>
ptr
=
MaybePageRealloc
(
aArenaId
aOldPtr
aNewSize
)
;
return
ptr
.
isSome
(
)
?
*
ptr
:
(
aArenaId
.
isSome
(
)
?
MozJemalloc
:
:
moz_arena_realloc
(
*
aArenaId
aOldPtr
aNewSize
)
:
MozJemalloc
:
:
realloc
(
aOldPtr
aNewSize
)
)
;
}
inline
void
*
MozJemallocPHC
:
:
realloc
(
void
*
aOldPtr
size_t
aNewSize
)
{
return
PageRealloc
(
Nothing
(
)
aOldPtr
aNewSize
)
;
}
void
PHC
:
:
PageFree
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aPtr
)
MOZ_EXCLUDES
(
mMutex
)
{
PtrKind
pk
=
GetPtrKind
(
aPtr
)
;
if
(
pk
.
IsGuardPage
(
)
)
{
PHC
:
:
CrashOnGuardPage
(
aPtr
)
;
}
AdvanceNow
(
LocalAllocDelay
(
)
)
;
Maybe
<
AutoDisableOnCurrentThread
>
disable
;
StackTrace
freeStack
;
if
(
IsDisabledOnCurrentThread
(
)
)
{
}
else
{
disable
.
emplace
(
)
;
freeStack
.
Fill
(
)
;
}
MutexAutoLock
lock
(
mMutex
)
;
Maybe
<
uintptr_t
>
mb_index
=
pk
.
AllocPageIndex
(
NumAllocPages
(
)
)
;
if
(
!
mb_index
)
{
Crash
(
"
free
of
invalid
pointer
"
)
;
}
uintptr_t
index
=
mb_index
.
value
(
)
;
EnsureValidAndInUse
(
aPtr
index
)
;
Delay
reuseDelay
=
ReuseDelay
(
)
;
FreePage
(
index
aArenaId
freeStack
reuseDelay
)
;
#
if
PHC_LOGGING
phc
:
:
PHCStats
stats
=
GetPageStatsLocked
(
)
;
Log
(
"
PageFree
(
%
p
[
%
zu
]
)
%
zu
delay
reuse
at
~
%
zu
fullness
%
zu
/
%
zu
/
%
zu
\
n
"
aPtr
index
size_t
(
reuseDelay
)
size_t
(
Now
(
)
)
+
reuseDelay
stats
.
mSlotsAllocated
stats
.
mSlotsFreed
NumAllocPages
(
)
)
;
#
endif
}
MOZ_ALWAYS_INLINE
static
void
PageFree
(
const
Maybe
<
arena_id_t
>
&
aArenaId
void
*
aPtr
)
{
if
(
MOZ_UNLIKELY
(
FastIsPHCPtr
(
aPtr
)
)
)
{
PHC
:
:
sPHC
-
>
PageFree
(
aArenaId
.
isSome
(
)
?
aArenaId
:
Nothing
(
)
aPtr
)
;
return
;
}
aArenaId
.
isSome
(
)
?
MozJemalloc
:
:
moz_arena_free
(
*
aArenaId
aPtr
)
:
MozJemalloc
:
:
free
(
aPtr
)
;
}
inline
void
MozJemallocPHC
:
:
free
(
void
*
aPtr
)
{
PageFree
(
Nothing
(
)
aPtr
)
;
}
MOZ_ALWAYS_INLINE
static
void
*
PageMemalign
(
const
Maybe
<
arena_id_t
>
&
aArenaId
size_t
aAlignment
size_t
aReqSize
)
{
MOZ_RELEASE_ASSERT
(
IsPowerOfTwo
(
aAlignment
)
)
;
void
*
ptr
=
nullptr
;
if
(
ShouldPageAllocHot
(
aReqSize
)
&
&
aAlignment
<
=
kPageSize
)
{
ptr
=
PHC
:
:
sPHC
-
>
MaybePageAlloc
(
aArenaId
.
isSome
(
)
?
aArenaId
:
Nothing
(
)
aReqSize
aAlignment
false
)
;
}
return
ptr
?
ptr
:
(
aArenaId
.
isSome
(
)
?
MozJemalloc
:
:
moz_arena_memalign
(
*
aArenaId
aAlignment
aReqSize
)
:
MozJemalloc
:
:
memalign
(
aAlignment
aReqSize
)
)
;
}
inline
void
*
MozJemallocPHC
:
:
memalign
(
size_t
aAlignment
size_t
aReqSize
)
{
return
PageMemalign
(
Nothing
(
)
aAlignment
aReqSize
)
;
}
inline
size_t
MozJemallocPHC
:
:
malloc_usable_size
(
usable_ptr_t
aPtr
)
{
if
(
MOZ_LIKELY
(
!
FastIsPHCPtr
(
aPtr
)
)
)
{
return
MozJemalloc
:
:
malloc_usable_size
(
aPtr
)
;
}
return
PHC
:
:
sPHC
-
>
PtrUsableSize
(
aPtr
)
;
}
size_t
PHC
:
:
PtrUsableSize
(
usable_ptr_t
aPtr
)
MOZ_EXCLUDES
(
mMutex
)
{
PtrKind
pk
=
GetPtrKind
(
aPtr
)
;
if
(
pk
.
IsGuardPage
(
)
)
{
CrashOnGuardPage
(
const_cast
<
void
*
>
(
aPtr
)
)
;
}
MutexAutoLock
lock
(
mMutex
)
;
Maybe
<
uintptr_t
>
index
=
pk
.
AllocPageIndex
(
NumAllocPages
(
)
)
;
if
(
!
index
)
{
Crash
(
"
PtrUsableSize
(
)
of
invalid
pointer
"
)
;
}
void
*
pageBaseAddr
=
AllocPageBaseAddr
(
index
.
value
(
)
)
;
if
(
MOZ_UNLIKELY
(
aPtr
<
pageBaseAddr
)
)
{
return
0
;
}
return
PageUsableSize
(
index
.
value
(
)
)
;
}
inline
void
MozJemallocPHC
:
:
jemalloc_stats_internal
(
jemalloc_stats_t
*
aStats
jemalloc_bin_stats_t
*
aBinStats
)
{
MozJemalloc
:
:
jemalloc_stats_internal
(
aStats
aBinStats
)
;
if
(
!
PHC
:
:
sPHC
)
{
return
;
}
aStats
-
>
allocated
-
=
PHC
:
:
sRegion
.
ReservedBytes
(
)
;
phc
:
:
MemoryUsage
mem_info
;
PHC
:
:
sPHC
-
>
GetMemoryUsage
(
mem_info
)
;
aStats
-
>
allocated
+
=
mem_info
.
mAllocatedBytes
;
aStats
-
>
allocated
-
=
mem_info
.
mMetadataBytes
;
aStats
-
>
bookkeeping
+
=
mem_info
.
mMetadataBytes
;
}
inline
void
MozJemallocPHC
:
:
jemalloc_stats_lite
(
jemalloc_stats_lite_t
*
aStats
)
{
MozJemalloc
:
:
jemalloc_stats_lite
(
aStats
)
;
}
inline
void
MozJemallocPHC
:
:
jemalloc_ptr_info
(
const
void
*
aPtr
jemalloc_ptr_info_t
*
aInfo
)
{
if
(
MOZ_LIKELY
(
!
FastIsPHCPtr
(
aPtr
)
)
)
{
MozJemalloc
:
:
jemalloc_ptr_info
(
aPtr
aInfo
)
;
return
;
}
PHC
:
:
sPHC
-
>
PagePtrInfo
(
aPtr
aInfo
)
;
}
void
PHC
:
:
PagePtrInfo
(
const
void
*
aPtr
jemalloc_ptr_info_t
*
aInfo
)
MOZ_EXCLUDES
(
mMutex
)
{
PtrKind
pk
=
GetPtrKind
(
aPtr
)
;
if
(
pk
.
IsGuardPage
(
)
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
return
;
}
MutexAutoLock
lock
(
mMutex
)
;
Maybe
<
uintptr_t
>
index
=
pk
.
AllocPageIndex
(
NumAllocPages
(
)
)
;
if
(
!
index
)
{
Crash
(
"
JemallocPtrInfo
of
invalid
pointer
"
)
;
}
FillJemallocPtrInfo
(
aPtr
index
.
value
(
)
aInfo
)
;
#
if
DEBUG
Log
(
"
JemallocPtrInfo
(
%
p
[
%
zu
]
)
-
>
{
%
zu
%
p
%
zu
%
zu
}
\
n
"
aPtr
index
.
value
(
)
size_t
(
aInfo
-
>
tag
)
aInfo
-
>
addr
aInfo
-
>
size
aInfo
-
>
arenaId
)
;
#
else
Log
(
"
JemallocPtrInfo
(
%
p
[
%
zu
]
)
-
>
{
%
zu
%
p
%
zu
}
\
n
"
aPtr
index
.
value
(
)
size_t
(
aInfo
-
>
tag
)
aInfo
-
>
addr
aInfo
-
>
size
)
;
#
endif
}
inline
void
*
MozJemallocPHC
:
:
moz_arena_malloc
(
arena_id_t
aArenaId
size_t
aReqSize
)
{
return
PageMalloc
(
Some
(
aArenaId
)
aReqSize
)
;
}
inline
void
*
MozJemallocPHC
:
:
moz_arena_calloc
(
arena_id_t
aArenaId
size_t
aNum
size_t
aReqSize
)
{
return
PageCalloc
(
Some
(
aArenaId
)
aNum
aReqSize
)
;
}
inline
void
*
MozJemallocPHC
:
:
moz_arena_realloc
(
arena_id_t
aArenaId
void
*
aOldPtr
size_t
aNewSize
)
{
return
PageRealloc
(
Some
(
aArenaId
)
aOldPtr
aNewSize
)
;
}
inline
void
MozJemallocPHC
:
:
moz_arena_free
(
arena_id_t
aArenaId
void
*
aPtr
)
{
return
PageFree
(
Some
(
aArenaId
)
aPtr
)
;
}
inline
void
*
MozJemallocPHC
:
:
moz_arena_memalign
(
arena_id_t
aArenaId
size_t
aAlignment
size_t
aReqSize
)
{
return
PageMemalign
(
Some
(
aArenaId
)
aAlignment
aReqSize
)
;
}
bool
PHC
:
:
IsPHCAllocation
(
const
void
*
aPtr
mozilla
:
:
phc
:
:
AddrInfo
*
aOut
)
{
PtrKind
pk
=
GetPtrKind
(
aPtr
)
;
bool
isGuardPage
=
false
;
if
(
pk
.
IsGuardPage
(
)
)
{
if
(
(
uintptr_t
(
aPtr
)
%
kPageSize
)
<
(
kPageSize
/
2
)
)
{
if
(
sRegion
.
IsInFirstGuardPage
(
aPtr
)
)
{
return
false
;
}
pk
=
GetPtrKind
(
static_cast
<
const
uint8_t
*
>
(
aPtr
)
-
kPageSize
)
;
}
else
{
pk
=
GetPtrKind
(
static_cast
<
const
uint8_t
*
>
(
aPtr
)
+
kPageSize
)
;
}
isGuardPage
=
true
;
}
if
(
aOut
)
{
if
(
mMutex
.
TryLock
(
)
)
{
Maybe
<
uintptr_t
>
index
=
pk
.
AllocPageIndex
(
NumAllocPages
(
)
)
;
if
(
!
index
)
{
mMutex
.
Unlock
(
)
;
return
false
;
}
FillAddrInfo
(
index
.
value
(
)
aPtr
isGuardPage
*
aOut
)
;
Log
(
"
IsPHCAllocation
:
%
zu
%
p
%
zu
%
zu
%
zu
\
n
"
size_t
(
aOut
-
>
mKind
)
aOut
-
>
mBaseAddr
aOut
-
>
mUsableSize
aOut
-
>
mAllocStack
.
isSome
(
)
?
aOut
-
>
mAllocStack
-
>
mLength
:
0
aOut
-
>
mFreeStack
.
isSome
(
)
?
aOut
-
>
mFreeStack
-
>
mLength
:
0
)
;
mMutex
.
Unlock
(
)
;
}
else
{
Log
(
"
IsPHCAllocation
:
PHC
is
locked
\
n
"
)
;
aOut
-
>
mPhcWasLocked
=
true
;
}
}
return
true
;
}
namespace
mozilla
:
:
phc
{
bool
IsPHCAllocation
(
const
void
*
aPtr
AddrInfo
*
aOut
)
{
if
(
MOZ_LIKELY
(
!
FastIsPHCPtr
(
aPtr
)
)
)
{
return
false
;
}
return
PHC
:
:
sPHC
-
>
IsPHCAllocation
(
aPtr
aOut
)
;
}
void
DisablePHCOnCurrentThread
(
)
{
PHC
:
:
DisableOnCurrentThread
(
)
;
Log
(
"
DisablePHCOnCurrentThread
:
%
zu
\
n
"
0ul
)
;
}
void
ReenablePHCOnCurrentThread
(
)
{
PHC
:
:
sPHC
-
>
EnableOnCurrentThread
(
)
;
Log
(
"
ReenablePHCOnCurrentThread
:
%
zu
\
n
"
0ul
)
;
}
bool
IsPHCEnabledOnCurrentThread
(
)
{
bool
enabled
=
!
PHC
:
:
IsDisabledOnCurrentThread
(
)
;
Log
(
"
IsPHCEnabledOnCurrentThread
:
%
zu
\
n
"
size_t
(
enabled
)
)
;
return
enabled
;
}
void
PHCMemoryUsage
(
MemoryUsage
&
aMemoryUsage
)
{
aMemoryUsage
=
MemoryUsage
(
)
;
if
(
PHC
:
:
sPHC
)
{
PHC
:
:
sPHC
-
>
GetMemoryUsage
(
aMemoryUsage
)
;
}
}
void
SetPHCSize
(
size_t
aSizeBytes
)
{
if
(
PHC
:
:
sPHC
)
{
PHC
:
:
sPHC
-
>
Resize
(
aSizeBytes
)
;
}
}
void
GetPHCStats
(
PHCStats
&
aStats
)
{
if
(
!
PHC
:
:
sPHC
)
{
aStats
=
PHCStats
(
)
;
return
;
}
aStats
=
PHC
:
:
sPHC
-
>
GetPageStats
(
)
;
}
void
SetPHCState
(
PHCState
aState
)
{
if
(
!
PHC
:
:
sPHC
)
{
return
;
}
PHC
:
:
sPHC
-
>
SetState
(
aState
)
;
}
void
SetPHCProbabilities
(
int64_t
aAvgDelayFirst
int64_t
aAvgDelayNormal
int64_t
aAvgDelayPageReuse
)
{
if
(
!
PHC
:
:
sPHC
)
{
return
;
}
PHC
:
:
sPHC
-
>
SetProbabilities
(
aAvgDelayFirst
aAvgDelayNormal
aAvgDelayPageReuse
)
;
}
}
#
if
PHC_LOGGING
static
size_t
GetPid
(
)
{
return
size_t
(
getpid
(
)
)
;
}
static
size_t
GetTid
(
)
{
#
if
defined
(
XP_WIN
)
return
size_t
(
GetCurrentThreadId
(
)
)
;
#
else
return
size_t
(
pthread_self
(
)
)
;
#
endif
}
#
endif
static
void
Log
(
const
char
*
fmt
.
.
.
)
{
#
if
PHC_LOGGING
#
if
defined
(
XP_WIN
)
#
define
LOG_STDERR
\
reinterpret_cast
<
intptr_t
>
(
GetStdHandle
(
STD_ERROR_HANDLE
)
)
#
else
#
define
LOG_STDERR
2
#
endif
char
buf
[
256
]
;
size_t
pos
=
SNPrintf
(
buf
sizeof
(
buf
)
"
PHC
[
%
zu
%
zu
~
%
zu
]
"
GetPid
(
)
GetTid
(
)
size_t
(
PHC
:
:
Now
(
)
)
)
;
va_list
vargs
;
va_start
(
vargs
fmt
)
;
pos
+
=
VSNPrintf
(
&
buf
[
pos
]
sizeof
(
buf
)
-
pos
fmt
vargs
)
;
MOZ_ASSERT
(
pos
<
sizeof
(
buf
)
)
;
va_end
(
vargs
)
;
FdPuts
(
LOG_STDERR
buf
pos
)
;
#
endif
}
