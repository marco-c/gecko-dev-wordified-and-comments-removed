#
include
"
mozmemory_wrap
.
h
"
#
include
"
mozjemalloc
.
h
"
#
include
"
mozjemalloc_types
.
h
"
#
include
<
cstring
>
#
include
<
cerrno
>
#
include
<
optional
>
#
include
<
type_traits
>
#
ifdef
XP_WIN
#
include
<
io
.
h
>
#
include
<
windows
.
h
>
#
else
#
include
<
sys
/
mman
.
h
>
#
include
<
unistd
.
h
>
#
endif
#
ifdef
XP_DARWIN
#
include
<
libkern
/
OSAtomic
.
h
>
#
include
<
mach
/
mach_init
.
h
>
#
include
<
mach
/
vm_map
.
h
>
#
endif
#
include
"
mozilla
/
Atomics
.
h
"
#
include
"
mozilla
/
Alignment
.
h
"
#
include
"
mozilla
/
ArrayUtils
.
h
"
#
include
"
mozilla
/
Assertions
.
h
"
#
include
"
mozilla
/
CheckedInt
.
h
"
#
include
"
mozilla
/
DoublyLinkedList
.
h
"
#
include
"
mozilla
/
HelperMacros
.
h
"
#
include
"
mozilla
/
Likely
.
h
"
#
include
"
mozilla
/
Literals
.
h
"
#
include
"
mozilla
/
MathAlgorithms
.
h
"
#
include
"
mozilla
/
RandomNum
.
h
"
#
include
"
mozilla
/
TaggedAnonymousMemory
.
h
"
#
include
"
mozilla
/
ThreadLocal
.
h
"
#
include
"
mozilla
/
UniquePtr
.
h
"
#
include
"
mozilla
/
Unused
.
h
"
#
include
"
mozilla
/
XorShift128PlusRNG
.
h
"
#
include
"
mozilla
/
fallible
.
h
"
#
include
"
rb
.
h
"
#
include
"
Mutex
.
h
"
#
include
"
PHC
.
h
"
#
include
"
Utils
.
h
"
#
if
defined
(
XP_WIN
)
#
include
"
mozmemory_utils
.
h
"
#
endif
#
if
defined
(
XP_WIN
)
&
&
!
defined
(
JS_STANDALONE
)
#
include
"
mozilla
/
ProcessType
.
h
"
#
endif
using
namespace
mozilla
;
#
ifdef
XP_DARWIN
#
define
MALLOC_DOUBLE_PURGE
#
endif
#
ifdef
XP_WIN
#
define
MALLOC_DECOMMIT
#
endif
#
ifdef
MOZ_DEBUG
#
define
MALLOC_RUNTIME_CONFIG
#
endif
#
ifndef
MALLOC_RUNTIME_CONFIG
#
if
!
defined
(
__ia64__
)
&
&
!
defined
(
__sparc__
)
&
&
!
defined
(
__mips__
)
&
&
\
!
defined
(
__aarch64__
)
&
&
!
defined
(
__powerpc__
)
&
&
!
defined
(
XP_MACOSX
)
&
&
\
!
defined
(
__loongarch__
)
#
define
MALLOC_STATIC_PAGESIZE
1
#
endif
#
endif
#
ifdef
XP_WIN
#
define
STDERR_FILENO
2
static
char
mozillaMallocOptionsBuf
[
64
]
;
#
define
getenv
xgetenv
static
char
*
getenv
(
const
char
*
name
)
{
if
(
GetEnvironmentVariableA
(
name
mozillaMallocOptionsBuf
sizeof
(
mozillaMallocOptionsBuf
)
)
>
0
)
{
return
mozillaMallocOptionsBuf
;
}
return
nullptr
;
}
#
endif
#
ifndef
XP_WIN
#
if
defined
(
XP_LINUX
)
&
&
defined
(
MADV_FREE
)
#
undef
MADV_FREE
#
endif
#
ifndef
MADV_FREE
#
define
MADV_FREE
MADV_DONTNEED
#
endif
#
endif
#
if
(
defined
(
XP_LINUX
)
&
&
!
defined
(
__alpha__
)
)
|
|
\
(
defined
(
__FreeBSD_kernel__
)
&
&
defined
(
__GLIBC__
)
)
#
include
<
sys
/
syscall
.
h
>
#
if
defined
(
SYS_mmap
)
|
|
defined
(
SYS_mmap2
)
static
inline
void
*
_mmap
(
void
*
addr
size_t
length
int
prot
int
flags
int
fd
off_t
offset
)
{
#
ifdef
__s390__
struct
{
void
*
addr
;
size_t
length
;
long
prot
;
long
flags
;
long
fd
;
off_t
offset
;
}
args
=
{
addr
length
prot
flags
fd
offset
}
;
return
(
void
*
)
syscall
(
SYS_mmap
&
args
)
;
#
else
#
if
defined
(
ANDROID
)
&
&
defined
(
__aarch64__
)
&
&
defined
(
SYS_mmap2
)
#
undef
SYS_mmap2
#
endif
#
ifdef
SYS_mmap2
return
(
void
*
)
syscall
(
SYS_mmap2
addr
length
prot
flags
fd
offset
>
>
12
)
;
#
else
return
(
void
*
)
syscall
(
SYS_mmap
addr
length
prot
flags
fd
offset
)
;
#
endif
#
endif
}
#
define
mmap
_mmap
#
define
munmap
(
a
l
)
syscall
(
SYS_munmap
a
l
)
#
endif
#
endif
struct
arena_t
;
struct
arena_chunk_map_t
{
RedBlackTreeNode
<
arena_chunk_map_t
>
link
;
size_t
bits
;
#
define
CHUNK_MAP_FRESH
(
(
size_t
)
0x80U
)
#
define
CHUNK_MAP_MADVISED
(
(
size_t
)
0x40U
)
#
define
CHUNK_MAP_DECOMMITTED
(
(
size_t
)
0x20U
)
#
define
CHUNK_MAP_MADVISED_OR_DECOMMITTED
\
(
CHUNK_MAP_MADVISED
|
CHUNK_MAP_DECOMMITTED
)
#
define
CHUNK_MAP_FRESH_MADVISED_OR_DECOMMITTED
\
(
CHUNK_MAP_FRESH
|
CHUNK_MAP_MADVISED
|
CHUNK_MAP_DECOMMITTED
)
#
define
CHUNK_MAP_KEY
(
(
size_t
)
0x10U
)
#
define
CHUNK_MAP_DIRTY
(
(
size_t
)
0x08U
)
#
define
CHUNK_MAP_ZEROED
(
(
size_t
)
0x04U
)
#
define
CHUNK_MAP_LARGE
(
(
size_t
)
0x02U
)
#
define
CHUNK_MAP_ALLOCATED
(
(
size_t
)
0x01U
)
}
;
struct
arena_chunk_t
{
arena_t
*
arena
;
RedBlackTreeNode
<
arena_chunk_t
>
link_dirty
;
#
ifdef
MALLOC_DOUBLE_PURGE
DoublyLinkedListElement
<
arena_chunk_t
>
chunks_madvised_elem
;
#
endif
size_t
ndirty
;
arena_chunk_map_t
map
[
]
;
}
;
#
ifdef
XP_WIN
static
const
size_t
kMinTinyClass
=
sizeof
(
void
*
)
*
2
;
#
else
static
const
size_t
kMinTinyClass
=
sizeof
(
void
*
)
;
#
endif
static
const
size_t
kMaxTinyClass
=
8
;
static
const
size_t
kMinQuantumClass
=
kMaxTinyClass
*
2
;
static
const
size_t
kMinQuantumWideClass
=
512
;
static
const
size_t
kMinSubPageClass
=
4_KiB
;
static
const
size_t
kQuantum
=
16
;
static
const
size_t
kQuantumMask
=
kQuantum
-
1
;
static
const
size_t
kQuantumWide
=
256
;
static
const
size_t
kQuantumWideMask
=
kQuantumWide
-
1
;
static
const
size_t
kMaxQuantumClass
=
kMinQuantumWideClass
-
kQuantum
;
static
const
size_t
kMaxQuantumWideClass
=
kMinSubPageClass
-
kQuantumWide
;
static_assert
(
mozilla
:
:
IsPowerOfTwo
(
kQuantum
)
"
kQuantum
is
not
a
power
of
two
"
)
;
static_assert
(
mozilla
:
:
IsPowerOfTwo
(
kQuantumWide
)
"
kQuantumWide
is
not
a
power
of
two
"
)
;
static_assert
(
kMaxQuantumClass
%
kQuantum
=
=
0
"
kMaxQuantumClass
is
not
a
multiple
of
kQuantum
"
)
;
static_assert
(
kMaxQuantumWideClass
%
kQuantumWide
=
=
0
"
kMaxQuantumWideClass
is
not
a
multiple
of
kQuantumWide
"
)
;
static_assert
(
kQuantum
<
kQuantumWide
"
kQuantum
must
be
smaller
than
kQuantumWide
"
)
;
static_assert
(
mozilla
:
:
IsPowerOfTwo
(
kMinSubPageClass
)
"
kMinSubPageClass
is
not
a
power
of
two
"
)
;
static
const
size_t
kNumTinyClasses
=
LOG2
(
kMaxTinyClass
)
-
LOG2
(
kMinTinyClass
)
+
1
;
static
const
size_t
kNumQuantumClasses
=
(
kMaxQuantumClass
+
kQuantum
-
kMinQuantumClass
)
/
kQuantum
;
static
const
size_t
kNumQuantumWideClasses
=
(
kMaxQuantumWideClass
+
kQuantumWide
-
kMinQuantumWideClass
)
/
kQuantumWide
;
static
const
size_t
kChunkSize
=
1_MiB
;
static
const
size_t
kChunkSizeMask
=
kChunkSize
-
1
;
#
ifdef
MALLOC_STATIC_PAGESIZE
#
if
defined
(
__powerpc64__
)
static
const
size_t
gPageSize
=
64_KiB
;
#
elif
defined
(
__loongarch64
)
static
const
size_t
gPageSize
=
16_KiB
;
#
else
static
const
size_t
gPageSize
=
4_KiB
;
#
endif
static
const
size_t
gRealPageSize
=
gPageSize
;
#
else
static
size_t
gRealPageSize
;
static
size_t
gPageSize
;
#
endif
#
ifdef
MALLOC_STATIC_PAGESIZE
#
define
DECLARE_GLOBAL
(
type
name
)
#
define
DEFINE_GLOBALS
#
define
END_GLOBALS
#
define
DEFINE_GLOBAL
(
type
)
static
const
type
#
define
GLOBAL_LOG2
LOG2
#
define
GLOBAL_ASSERT_HELPER1
(
x
)
static_assert
(
x
#
x
)
#
define
GLOBAL_ASSERT_HELPER2
(
x
y
)
static_assert
(
x
y
)
#
define
GLOBAL_ASSERT
(
.
.
.
)
\
MACRO_CALL
(
\
MOZ_PASTE_PREFIX_AND_ARG_COUNT
(
GLOBAL_ASSERT_HELPER
__VA_ARGS__
)
\
(
__VA_ARGS__
)
)
#
define
GLOBAL_CONSTEXPR
constexpr
#
else
#
define
DECLARE_GLOBAL
(
type
name
)
static
type
name
;
#
define
DEFINE_GLOBALS
static
void
DefineGlobals
(
)
{
#
define
END_GLOBALS
}
#
define
DEFINE_GLOBAL
(
type
)
#
define
GLOBAL_LOG2
FloorLog2
#
define
GLOBAL_ASSERT
MOZ_RELEASE_ASSERT
#
define
GLOBAL_CONSTEXPR
#
endif
DECLARE_GLOBAL
(
size_t
gMaxSubPageClass
)
DECLARE_GLOBAL
(
uint8_t
gNumSubPageClasses
)
DECLARE_GLOBAL
(
uint8_t
gPageSize2Pow
)
DECLARE_GLOBAL
(
size_t
gPageSizeMask
)
DECLARE_GLOBAL
(
size_t
gChunkNumPages
)
DECLARE_GLOBAL
(
size_t
gChunkHeaderNumPages
)
DECLARE_GLOBAL
(
size_t
gMaxLargeClass
)
DEFINE_GLOBALS
DEFINE_GLOBAL
(
size_t
)
gMaxSubPageClass
=
gPageSize
/
2
>
=
kMinSubPageClass
?
gPageSize
/
2
:
0
;
#
define
gMaxBinClass
\
(
gMaxSubPageClass
?
gMaxSubPageClass
:
kMaxQuantumWideClass
)
DEFINE_GLOBAL
(
uint8_t
)
gNumSubPageClasses
=
[
]
(
)
GLOBAL_CONSTEXPR
-
>
uint8_t
{
if
GLOBAL_CONSTEXPR
(
gMaxSubPageClass
!
=
0
)
{
return
FloorLog2
(
gMaxSubPageClass
)
-
LOG2
(
kMinSubPageClass
)
+
1
;
}
return
0
;
}
(
)
;
DEFINE_GLOBAL
(
uint8_t
)
gPageSize2Pow
=
GLOBAL_LOG2
(
gPageSize
)
;
DEFINE_GLOBAL
(
size_t
)
gPageSizeMask
=
gPageSize
-
1
;
DEFINE_GLOBAL
(
size_t
)
gChunkNumPages
=
kChunkSize
>
>
gPageSize2Pow
;
DEFINE_GLOBAL
(
size_t
)
gChunkHeaderNumPages
=
1
+
(
(
(
sizeof
(
arena_chunk_t
)
+
sizeof
(
arena_chunk_map_t
)
*
gChunkNumPages
+
gPageSizeMask
)
&
~
gPageSizeMask
)
>
>
gPageSize2Pow
)
;
DEFINE_GLOBAL
(
size_t
)
gMaxLargeClass
=
kChunkSize
-
gPageSize
-
(
gChunkHeaderNumPages
<
<
gPageSize2Pow
)
;
GLOBAL_ASSERT
(
1ULL
<
<
gPageSize2Pow
=
=
gPageSize
"
Page
size
is
not
a
power
of
two
"
)
;
GLOBAL_ASSERT
(
kQuantum
>
=
sizeof
(
void
*
)
)
;
GLOBAL_ASSERT
(
kQuantum
<
=
kQuantumWide
)
;
GLOBAL_ASSERT
(
!
kNumQuantumWideClasses
|
|
kQuantumWide
<
=
(
kMinSubPageClass
-
kMaxQuantumClass
)
)
;
GLOBAL_ASSERT
(
kQuantumWide
<
=
kMaxQuantumClass
)
;
GLOBAL_ASSERT
(
gMaxSubPageClass
>
=
kMinSubPageClass
|
|
gMaxSubPageClass
=
=
0
)
;
GLOBAL_ASSERT
(
gMaxLargeClass
>
=
gMaxSubPageClass
)
;
GLOBAL_ASSERT
(
kChunkSize
>
=
gPageSize
)
;
GLOBAL_ASSERT
(
kQuantum
*
4
<
=
kChunkSize
)
;
END_GLOBALS
static
const
size_t
gRecycleLimit
=
128_MiB
;
static
Atomic
<
size_t
ReleaseAcquire
>
gRecycledSize
;
#
define
DIRTY_MAX_DEFAULT
(
1U
<
<
8
)
static
size_t
opt_dirty_max
=
DIRTY_MAX_DEFAULT
;
#
define
CHUNK_CEILING
(
s
)
(
(
(
s
)
+
kChunkSizeMask
)
&
~
kChunkSizeMask
)
#
define
CACHELINE_CEILING
(
s
)
\
(
(
(
s
)
+
(
kCacheLineSize
-
1
)
)
&
~
(
kCacheLineSize
-
1
)
)
#
define
QUANTUM_CEILING
(
a
)
(
(
(
a
)
+
(
kQuantumMask
)
)
&
~
(
kQuantumMask
)
)
#
define
QUANTUM_WIDE_CEILING
(
a
)
\
(
(
(
a
)
+
(
kQuantumWideMask
)
)
&
~
(
kQuantumWideMask
)
)
#
define
SUBPAGE_CEILING
(
a
)
(
RoundUpPow2
(
a
)
)
#
define
PAGE_CEILING
(
s
)
(
(
(
s
)
+
gPageSizeMask
)
&
~
gPageSizeMask
)
#
define
NUM_SMALL_CLASSES
\
(
kNumTinyClasses
+
kNumQuantumClasses
+
kNumQuantumWideClasses
+
\
gNumSubPageClasses
)
#
if
defined
(
MALLOC_DECOMMIT
)
&
&
defined
(
MALLOC_DOUBLE_PURGE
)
#
error
MALLOC_DECOMMIT
and
MALLOC_DOUBLE_PURGE
are
mutually
exclusive
.
#
endif
static
void
*
base_alloc
(
size_t
aSize
)
;
#
if
defined
(
_MSC_VER
)
&
&
!
defined
(
__clang__
)
static
bool
malloc_initialized
;
#
else
static
Atomic
<
bool
MemoryOrdering
:
:
ReleaseAcquire
>
malloc_initialized
;
#
endif
static
StaticMutex
gInitLock
MOZ_UNANNOTATED
=
{
STATIC_MUTEX_INIT
}
;
struct
arena_stats_t
{
size_t
mapped
;
size_t
committed
;
size_t
allocated_small
;
size_t
allocated_large
;
}
;
enum
ChunkType
{
UNKNOWN_CHUNK
ZEROED_CHUNK
ARENA_CHUNK
HUGE_CHUNK
RECYCLED_CHUNK
}
;
struct
extent_node_t
{
union
{
RedBlackTreeNode
<
extent_node_t
>
mLinkBySize
;
arena_id_t
mArenaId
;
}
;
RedBlackTreeNode
<
extent_node_t
>
mLinkByAddr
;
void
*
mAddr
;
size_t
mSize
;
union
{
ChunkType
mChunkType
;
arena_t
*
mArena
;
}
;
}
;
struct
ExtentTreeSzTrait
{
static
RedBlackTreeNode
<
extent_node_t
>
&
GetTreeNode
(
extent_node_t
*
aThis
)
{
return
aThis
-
>
mLinkBySize
;
}
static
inline
Order
Compare
(
extent_node_t
*
aNode
extent_node_t
*
aOther
)
{
Order
ret
=
CompareInt
(
aNode
-
>
mSize
aOther
-
>
mSize
)
;
return
(
ret
!
=
Order
:
:
eEqual
)
?
ret
:
CompareAddr
(
aNode
-
>
mAddr
aOther
-
>
mAddr
)
;
}
}
;
struct
ExtentTreeTrait
{
static
RedBlackTreeNode
<
extent_node_t
>
&
GetTreeNode
(
extent_node_t
*
aThis
)
{
return
aThis
-
>
mLinkByAddr
;
}
static
inline
Order
Compare
(
extent_node_t
*
aNode
extent_node_t
*
aOther
)
{
return
CompareAddr
(
aNode
-
>
mAddr
aOther
-
>
mAddr
)
;
}
}
;
struct
ExtentTreeBoundsTrait
:
public
ExtentTreeTrait
{
static
inline
Order
Compare
(
extent_node_t
*
aKey
extent_node_t
*
aNode
)
{
uintptr_t
key_addr
=
reinterpret_cast
<
uintptr_t
>
(
aKey
-
>
mAddr
)
;
uintptr_t
node_addr
=
reinterpret_cast
<
uintptr_t
>
(
aNode
-
>
mAddr
)
;
size_t
node_size
=
aNode
-
>
mSize
;
if
(
node_addr
<
=
key_addr
&
&
key_addr
<
node_addr
+
node_size
)
{
return
Order
:
:
eEqual
;
}
return
CompareAddr
(
aKey
-
>
mAddr
aNode
-
>
mAddr
)
;
}
}
;
class
SizeClass
{
public
:
enum
ClassType
{
Tiny
Quantum
QuantumWide
SubPage
Large
}
;
explicit
inline
SizeClass
(
size_t
aSize
)
{
if
(
aSize
<
=
kMaxTinyClass
)
{
mType
=
Tiny
;
mSize
=
std
:
:
max
(
RoundUpPow2
(
aSize
)
kMinTinyClass
)
;
}
else
if
(
aSize
<
=
kMaxQuantumClass
)
{
mType
=
Quantum
;
mSize
=
QUANTUM_CEILING
(
aSize
)
;
}
else
if
(
aSize
<
=
kMaxQuantumWideClass
)
{
mType
=
QuantumWide
;
mSize
=
QUANTUM_WIDE_CEILING
(
aSize
)
;
}
else
if
(
aSize
<
=
gMaxSubPageClass
)
{
mType
=
SubPage
;
mSize
=
SUBPAGE_CEILING
(
aSize
)
;
}
else
if
(
aSize
<
=
gMaxLargeClass
)
{
mType
=
Large
;
mSize
=
PAGE_CEILING
(
aSize
)
;
}
else
{
MOZ_MAKE_COMPILER_ASSUME_IS_UNREACHABLE
(
"
Invalid
size
"
)
;
}
}
SizeClass
&
operator
=
(
const
SizeClass
&
aOther
)
=
default
;
bool
operator
=
=
(
const
SizeClass
&
aOther
)
{
return
aOther
.
mSize
=
=
mSize
;
}
size_t
Size
(
)
{
return
mSize
;
}
ClassType
Type
(
)
{
return
mType
;
}
SizeClass
Next
(
)
{
return
SizeClass
(
mSize
+
1
)
;
}
private
:
ClassType
mType
;
size_t
mSize
;
}
;
template
<
typename
T
>
class
FastDivisor
{
private
:
static
const
unsigned
p
=
17
;
T
m
;
public
:
FastDivisor
(
)
:
m
(
0
)
{
}
FastDivisor
(
unsigned
div
unsigned
max
)
{
MOZ_ASSERT
(
div
<
=
max
)
;
MOZ_ASSERT
(
(
1U
<
<
p
)
>
=
div
)
;
unsigned
m_
=
(
(
1U
<
<
p
)
+
div
-
1
-
(
(
(
1U
<
<
p
)
-
1
)
%
div
)
)
/
div
;
MOZ_DIAGNOSTIC_ASSERT
(
max
<
UINT_MAX
/
m_
)
;
MOZ_ASSERT
(
m_
<
=
std
:
:
numeric_limits
<
T
>
:
:
max
(
)
)
;
m
=
static_cast
<
T
>
(
m_
)
;
MOZ_ASSERT
(
m
)
;
#
ifdef
MOZ_DEBUG
for
(
unsigned
num
=
0
;
num
<
max
;
num
+
=
div
)
{
MOZ_ASSERT
(
num
/
div
=
=
divide
(
num
)
)
;
}
#
endif
}
inline
uint32_t
divide
(
uint32_t
num
)
const
{
MOZ_ASSERT
(
m
)
;
return
(
num
*
m
)
>
>
p
;
}
}
;
template
<
typename
T
>
unsigned
inline
operator
/
(
unsigned
num
FastDivisor
<
T
>
divisor
)
{
return
divisor
.
divide
(
num
)
;
}
template
<
size_t
Bits
>
class
AddressRadixTree
{
#
ifdef
HAVE_64BIT_BUILD
static
const
size_t
kNodeSize
=
kCacheLineSize
;
#
else
static
const
size_t
kNodeSize
=
16_KiB
;
#
endif
static
const
size_t
kBitsPerLevel
=
LOG2
(
kNodeSize
)
-
LOG2
(
sizeof
(
void
*
)
)
;
static
const
size_t
kBitsAtLevel1
=
(
Bits
%
kBitsPerLevel
)
?
Bits
%
kBitsPerLevel
:
kBitsPerLevel
;
static
const
size_t
kHeight
=
(
Bits
+
kBitsPerLevel
-
1
)
/
kBitsPerLevel
;
static_assert
(
kBitsAtLevel1
+
(
kHeight
-
1
)
*
kBitsPerLevel
=
=
Bits
"
AddressRadixTree
parameters
don
'
t
work
out
"
)
;
Mutex
mLock
MOZ_UNANNOTATED
;
void
*
*
mRoot
;
public
:
bool
Init
(
)
;
inline
void
*
Get
(
void
*
aAddr
)
;
inline
bool
Set
(
void
*
aAddr
void
*
aValue
)
;
inline
bool
Unset
(
void
*
aAddr
)
{
return
Set
(
aAddr
nullptr
)
;
}
private
:
inline
void
*
*
GetSlot
(
void
*
aAddr
bool
aCreate
=
false
)
;
}
;
struct
arena_bin_t
;
struct
ArenaChunkMapLink
{
static
RedBlackTreeNode
<
arena_chunk_map_t
>
&
GetTreeNode
(
arena_chunk_map_t
*
aThis
)
{
return
aThis
-
>
link
;
}
}
;
struct
ArenaRunTreeTrait
:
public
ArenaChunkMapLink
{
static
inline
Order
Compare
(
arena_chunk_map_t
*
aNode
arena_chunk_map_t
*
aOther
)
{
MOZ_ASSERT
(
aNode
)
;
MOZ_ASSERT
(
aOther
)
;
return
CompareAddr
(
aNode
aOther
)
;
}
}
;
struct
ArenaAvailTreeTrait
:
public
ArenaChunkMapLink
{
static
inline
Order
Compare
(
arena_chunk_map_t
*
aNode
arena_chunk_map_t
*
aOther
)
{
size_t
size1
=
aNode
-
>
bits
&
~
gPageSizeMask
;
size_t
size2
=
aOther
-
>
bits
&
~
gPageSizeMask
;
Order
ret
=
CompareInt
(
size1
size2
)
;
return
(
ret
!
=
Order
:
:
eEqual
)
?
ret
:
CompareAddr
(
(
aNode
-
>
bits
&
CHUNK_MAP_KEY
)
?
nullptr
:
aNode
aOther
)
;
}
}
;
struct
ArenaDirtyChunkTrait
{
static
RedBlackTreeNode
<
arena_chunk_t
>
&
GetTreeNode
(
arena_chunk_t
*
aThis
)
{
return
aThis
-
>
link_dirty
;
}
static
inline
Order
Compare
(
arena_chunk_t
*
aNode
arena_chunk_t
*
aOther
)
{
MOZ_ASSERT
(
aNode
)
;
MOZ_ASSERT
(
aOther
)
;
return
CompareAddr
(
aNode
aOther
)
;
}
}
;
#
ifdef
MALLOC_DOUBLE_PURGE
namespace
mozilla
{
template
<
>
struct
GetDoublyLinkedListElement
<
arena_chunk_t
>
{
static
DoublyLinkedListElement
<
arena_chunk_t
>
&
Get
(
arena_chunk_t
*
aThis
)
{
return
aThis
-
>
chunks_madvised_elem
;
}
}
;
}
#
endif
struct
arena_run_t
{
#
if
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
uint32_t
mMagic
;
#
define
ARENA_RUN_MAGIC
0x384adf93
unsigned
mNumFree
;
#
endif
arena_bin_t
*
mBin
;
unsigned
mRegionsMinElement
;
#
if
!
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
unsigned
mNumFree
;
#
endif
unsigned
mRegionsMask
[
]
;
}
;
struct
arena_bin_t
{
arena_run_t
*
mCurrentRun
;
RedBlackTree
<
arena_chunk_map_t
ArenaRunTreeTrait
>
mNonFullRuns
;
size_t
mSizeClass
;
uint32_t
mRunNumRegions
;
uint32_t
mRunNumRegionsMask
;
uint32_t
mRunFirstRegionOffset
;
uint32_t
mNumRuns
;
FastDivisor
<
uint16_t
>
mSizeDivisor
;
uint8_t
mRunSizePages
;
static
constexpr
double
kRunOverhead
=
1
.
6_percent
;
static
constexpr
double
kRunRelaxedOverhead
=
2
.
4_percent
;
inline
void
Init
(
SizeClass
aSizeClass
)
;
}
;
#
if
defined
(
__x86_64__
)
|
|
defined
(
__aarch64__
)
static_assert
(
sizeof
(
arena_bin_t
)
=
=
48
)
;
#
elif
defined
(
__x86__
)
|
|
defined
(
__arm__
)
static_assert
(
sizeof
(
arena_bin_t
)
=
=
32
)
;
#
endif
struct
arena_t
{
#
if
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
uint32_t
mMagic
;
#
define
ARENA_MAGIC
0x947d3d24
#
endif
RedBlackTreeNode
<
arena_t
>
mLink
;
arena_id_t
mId
;
MaybeMutex
mLock
MOZ_UNANNOTATED
;
arena_stats_t
mStats
;
private
:
RedBlackTree
<
arena_chunk_t
ArenaDirtyChunkTrait
>
mChunksDirty
;
#
ifdef
MALLOC_DOUBLE_PURGE
DoublyLinkedList
<
arena_chunk_t
>
mChunksMAdvised
;
#
endif
arena_chunk_t
*
mSpare
;
bool
mRandomizeSmallAllocations
;
bool
mIsPrivate
;
mozilla
:
:
non_crypto
:
:
XorShift128PlusRNG
*
mPRNG
;
bool
mIsPRNGInitializing
;
public
:
size_t
mNumDirty
;
size_t
mNumMAdvised
;
size_t
mNumFresh
;
size_t
mMaxDirty
;
int32_t
mMaxDirtyIncreaseOverride
;
int32_t
mMaxDirtyDecreaseOverride
;
private
:
RedBlackTree
<
arena_chunk_map_t
ArenaAvailTreeTrait
>
mRunsAvail
;
public
:
arena_bin_t
mBins
[
]
;
explicit
arena_t
(
arena_params_t
*
aParams
bool
aIsPrivate
)
;
~
arena_t
(
)
;
private
:
void
InitChunk
(
arena_chunk_t
*
aChunk
size_t
aMinCommittedPages
)
;
[
[
nodiscard
]
]
arena_chunk_t
*
DeallocChunk
(
arena_chunk_t
*
aChunk
)
;
arena_run_t
*
AllocRun
(
size_t
aSize
bool
aLarge
bool
aZero
)
;
arena_chunk_t
*
DallocRun
(
arena_run_t
*
aRun
bool
aDirty
)
;
[
[
nodiscard
]
]
bool
SplitRun
(
arena_run_t
*
aRun
size_t
aSize
bool
aLarge
bool
aZero
)
;
void
TrimRunHead
(
arena_chunk_t
*
aChunk
arena_run_t
*
aRun
size_t
aOldSize
size_t
aNewSize
)
;
void
TrimRunTail
(
arena_chunk_t
*
aChunk
arena_run_t
*
aRun
size_t
aOldSize
size_t
aNewSize
bool
dirty
)
;
arena_run_t
*
GetNonFullBinRun
(
arena_bin_t
*
aBin
)
;
inline
uint8_t
FindFreeBitInMask
(
uint32_t
aMask
uint32_t
&
aRng
)
;
inline
void
*
ArenaRunRegAlloc
(
arena_run_t
*
aRun
arena_bin_t
*
aBin
)
;
inline
void
*
MallocSmall
(
size_t
aSize
bool
aZero
)
;
void
*
MallocLarge
(
size_t
aSize
bool
aZero
)
;
void
*
MallocHuge
(
size_t
aSize
bool
aZero
)
;
void
*
PallocLarge
(
size_t
aAlignment
size_t
aSize
size_t
aAllocSize
)
;
void
*
PallocHuge
(
size_t
aSize
size_t
aAlignment
bool
aZero
)
;
void
RallocShrinkLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
;
bool
RallocGrowLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
;
void
*
RallocSmallOrLarge
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
;
void
*
RallocHuge
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
;
public
:
inline
void
*
Malloc
(
size_t
aSize
bool
aZero
)
;
void
*
Palloc
(
size_t
aAlignment
size_t
aSize
)
;
[
[
nodiscard
]
]
inline
arena_chunk_t
*
DallocSmall
(
arena_chunk_t
*
aChunk
void
*
aPtr
arena_chunk_map_t
*
aMapElm
)
;
[
[
nodiscard
]
]
arena_chunk_t
*
DallocLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
)
;
void
*
Ralloc
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
;
size_t
EffectiveMaxDirty
(
)
;
#
ifdef
MALLOC_DECOMMIT
size_t
ExtraCommitPages
(
size_t
aReqPages
size_t
aRemainingPages
)
;
#
endif
void
Purge
(
size_t
aMaxDirty
)
;
void
HardPurge
(
)
;
bool
IsMainThreadOnly
(
)
const
{
return
!
mLock
.
LockIsEnabled
(
)
;
}
void
*
operator
new
(
size_t
aCount
)
=
delete
;
void
*
operator
new
(
size_t
aCount
const
fallible_t
&
)
noexcept
;
void
operator
delete
(
void
*
)
;
}
;
struct
ArenaTreeTrait
{
static
RedBlackTreeNode
<
arena_t
>
&
GetTreeNode
(
arena_t
*
aThis
)
{
return
aThis
-
>
mLink
;
}
static
inline
Order
Compare
(
arena_t
*
aNode
arena_t
*
aOther
)
{
MOZ_ASSERT
(
aNode
)
;
MOZ_ASSERT
(
aOther
)
;
return
CompareInt
(
aNode
-
>
mId
aOther
-
>
mId
)
;
}
}
;
class
ArenaCollection
{
public
:
bool
Init
(
)
{
mArenas
.
Init
(
)
;
mPrivateArenas
.
Init
(
)
;
mMainThreadArenas
.
Init
(
)
;
arena_params_t
params
;
params
.
mMaxDirty
=
opt_dirty_max
;
mDefaultArena
=
mLock
.
Init
(
)
?
CreateArena
(
false
&
params
)
:
nullptr
;
return
bool
(
mDefaultArena
)
;
}
inline
arena_t
*
GetById
(
arena_id_t
aArenaId
bool
aIsPrivate
)
;
arena_t
*
CreateArena
(
bool
aIsPrivate
arena_params_t
*
aParams
)
;
void
DisposeArena
(
arena_t
*
aArena
)
{
MutexAutoLock
lock
(
mLock
)
;
Tree
&
tree
=
aArena
-
>
IsMainThreadOnly
(
)
?
mMainThreadArenas
:
mPrivateArenas
;
MOZ_RELEASE_ASSERT
(
tree
.
Search
(
aArena
)
"
Arena
not
in
tree
"
)
;
tree
.
Remove
(
aArena
)
;
delete
aArena
;
}
void
SetDefaultMaxDirtyPageModifier
(
int32_t
aModifier
)
{
mDefaultMaxDirtyPageModifier
=
aModifier
;
}
int32_t
DefaultMaxDirtyPageModifier
(
)
{
return
mDefaultMaxDirtyPageModifier
;
}
using
Tree
=
RedBlackTree
<
arena_t
ArenaTreeTrait
>
;
struct
Iterator
:
Tree
:
:
Iterator
{
explicit
Iterator
(
Tree
*
aTree
Tree
*
aSecondTree
Tree
*
aThirdTree
=
nullptr
)
:
Tree
:
:
Iterator
(
aTree
)
mSecondTree
(
aSecondTree
)
mThirdTree
(
aThirdTree
)
{
}
Item
<
Iterator
>
begin
(
)
{
return
Item
<
Iterator
>
(
this
*
Tree
:
:
Iterator
:
:
begin
(
)
)
;
}
Item
<
Iterator
>
end
(
)
{
return
Item
<
Iterator
>
(
this
nullptr
)
;
}
arena_t
*
Next
(
)
{
arena_t
*
result
=
Tree
:
:
Iterator
:
:
Next
(
)
;
if
(
!
result
&
&
mSecondTree
)
{
new
(
this
)
Iterator
(
mSecondTree
mThirdTree
)
;
result
=
*
Tree
:
:
Iterator
:
:
begin
(
)
;
}
return
result
;
}
private
:
Tree
*
mSecondTree
;
Tree
*
mThirdTree
;
}
;
Iterator
iter
(
)
{
if
(
IsOnMainThreadWeak
(
)
)
{
return
Iterator
(
&
mArenas
&
mPrivateArenas
&
mMainThreadArenas
)
;
}
return
Iterator
(
&
mArenas
&
mPrivateArenas
)
;
}
inline
arena_t
*
GetDefault
(
)
{
return
mDefaultArena
;
}
Mutex
mLock
MOZ_UNANNOTATED
;
bool
IsOnMainThread
(
)
const
{
return
mMainThreadId
.
isSome
(
)
&
&
ThreadIdEqual
(
mMainThreadId
.
value
(
)
GetThreadId
(
)
)
;
}
bool
IsOnMainThreadWeak
(
)
const
{
return
mMainThreadId
.
isNothing
(
)
|
|
IsOnMainThread
(
)
;
}
void
ResetMainThread
(
)
{
mMainThreadId
=
Nothing
(
)
;
}
void
SetMainThread
(
)
{
MutexAutoLock
lock
(
mLock
)
;
MOZ_ASSERT
(
mMainThreadId
.
isNothing
(
)
)
;
mMainThreadId
=
Some
(
GetThreadId
(
)
)
;
}
private
:
const
static
arena_id_t
MAIN_THREAD_ARENA_BIT
=
0x1
;
inline
arena_t
*
GetByIdInternal
(
Tree
&
aTree
arena_id_t
aArenaId
)
;
arena_id_t
MakeRandArenaId
(
bool
aIsMainThreadOnly
)
const
;
static
bool
ArenaIdIsMainThreadOnly
(
arena_id_t
aArenaId
)
{
return
aArenaId
&
MAIN_THREAD_ARENA_BIT
;
}
arena_t
*
mDefaultArena
;
arena_id_t
mLastPublicArenaId
;
Tree
mArenas
;
Tree
mPrivateArenas
;
Tree
mMainThreadArenas
;
Atomic
<
int32_t
MemoryOrdering
:
:
Relaxed
>
mDefaultMaxDirtyPageModifier
;
Maybe
<
ThreadId
>
mMainThreadId
;
}
;
static
ArenaCollection
gArenas
;
static
AddressRadixTree
<
(
sizeof
(
void
*
)
<
<
3
)
-
LOG2
(
kChunkSize
)
>
gChunkRTree
;
static
Mutex
chunks_mtx
;
static
RedBlackTree
<
extent_node_t
ExtentTreeSzTrait
>
gChunksBySize
MOZ_GUARDED_BY
(
chunks_mtx
)
;
static
RedBlackTree
<
extent_node_t
ExtentTreeTrait
>
gChunksByAddress
MOZ_GUARDED_BY
(
chunks_mtx
)
;
static
Mutex
huge_mtx
;
static
RedBlackTree
<
extent_node_t
ExtentTreeTrait
>
huge
MOZ_GUARDED_BY
(
huge_mtx
)
;
static
size_t
huge_allocated
MOZ_GUARDED_BY
(
huge_mtx
)
;
static
size_t
huge_mapped
MOZ_GUARDED_BY
(
huge_mtx
)
;
static
Mutex
base_mtx
;
static
void
*
base_pages
MOZ_GUARDED_BY
(
base_mtx
)
;
static
void
*
base_next_addr
MOZ_GUARDED_BY
(
base_mtx
)
;
static
void
*
base_next_decommitted
MOZ_GUARDED_BY
(
base_mtx
)
;
static
void
*
base_past_addr
MOZ_GUARDED_BY
(
base_mtx
)
;
static
size_t
base_mapped
MOZ_GUARDED_BY
(
base_mtx
)
;
static
size_t
base_committed
MOZ_GUARDED_BY
(
base_mtx
)
;
#
if
!
defined
(
XP_DARWIN
)
static
MOZ_THREAD_LOCAL
(
arena_t
*
)
thread_arena
;
#
else
static
detail
:
:
ThreadLocal
<
arena_t
*
detail
:
:
ThreadLocalKeyStorage
>
thread_arena
;
#
endif
#
ifdef
MALLOC_RUNTIME_CONFIG
#
define
MALLOC_RUNTIME_VAR
static
#
else
#
define
MALLOC_RUNTIME_VAR
static
const
#
endif
enum
PoisonType
{
NONE
SOME
ALL
}
;
MALLOC_RUNTIME_VAR
bool
opt_junk
=
false
;
MALLOC_RUNTIME_VAR
bool
opt_zero
=
false
;
#
ifdef
EARLY_BETA_OR_EARLIER
MALLOC_RUNTIME_VAR
PoisonType
opt_poison
=
ALL
;
#
else
MALLOC_RUNTIME_VAR
PoisonType
opt_poison
=
SOME
;
#
endif
MALLOC_RUNTIME_VAR
size_t
opt_poison_size
=
256
;
#
ifndef
MALLOC_RUNTIME_CONFIG
static_assert
(
opt_poison_size
>
=
kCacheLineSize
)
;
static_assert
(
(
opt_poison_size
%
kCacheLineSize
)
=
=
0
)
;
#
endif
static
bool
opt_randomize_small
=
true
;
static
void
*
chunk_alloc
(
size_t
aSize
size_t
aAlignment
bool
aBase
)
;
static
void
chunk_dealloc
(
void
*
aChunk
size_t
aSize
ChunkType
aType
)
;
#
ifdef
MOZ_DEBUG
static
void
chunk_assert_zero
(
void
*
aPtr
size_t
aSize
)
;
#
endif
static
void
huge_dalloc
(
void
*
aPtr
arena_t
*
aArena
)
;
static
bool
malloc_init_hard
(
)
;
#
ifndef
XP_WIN
#
ifdef
XP_DARWIN
#
define
FORK_HOOK
extern
"
C
"
#
else
#
define
FORK_HOOK
static
#
endif
FORK_HOOK
void
_malloc_prefork
(
void
)
;
FORK_HOOK
void
_malloc_postfork_parent
(
void
)
;
FORK_HOOK
void
_malloc_postfork_child
(
void
)
;
#
ifdef
XP_DARWIN
FORK_HOOK
void
_malloc_postfork
(
void
)
;
#
endif
#
endif
static
inline
bool
malloc_init
(
)
{
if
(
!
malloc_initialized
)
{
return
malloc_init_hard
(
)
;
}
return
true
;
}
static
void
_malloc_message
(
const
char
*
p
)
{
#
if
!
defined
(
XP_WIN
)
#
define
_write
write
#
endif
if
(
_write
(
STDERR_FILENO
p
(
unsigned
int
)
strlen
(
p
)
)
<
0
)
{
return
;
}
}
template
<
typename
.
.
.
Args
>
static
void
_malloc_message
(
const
char
*
p
Args
.
.
.
args
)
{
_malloc_message
(
p
)
;
_malloc_message
(
args
.
.
.
)
;
}
#
ifdef
ANDROID
extern
"
C
"
MOZ_EXPORT
int
pthread_atfork
(
void
(
*
)
(
void
)
void
(
*
)
(
void
)
void
(
*
)
(
void
)
)
;
#
endif
static
inline
arena_chunk_t
*
GetChunkForPtr
(
const
void
*
aPtr
)
{
return
(
arena_chunk_t
*
)
(
uintptr_t
(
aPtr
)
&
~
kChunkSizeMask
)
;
}
static
inline
size_t
GetChunkOffsetForPtr
(
const
void
*
aPtr
)
{
return
(
size_t
)
(
uintptr_t
(
aPtr
)
&
kChunkSizeMask
)
;
}
static
inline
const
char
*
_getprogname
(
void
)
{
return
"
<
jemalloc
>
"
;
}
static
inline
void
MaybePoison
(
void
*
aPtr
size_t
aSize
)
{
size_t
size
;
switch
(
opt_poison
)
{
case
NONE
:
return
;
case
SOME
:
size
=
std
:
:
min
(
aSize
opt_poison_size
)
;
break
;
case
ALL
:
size
=
aSize
;
break
;
}
MOZ_ASSERT
(
size
!
=
0
&
&
size
<
=
aSize
)
;
memset
(
aPtr
kAllocPoison
size
)
;
}
static
inline
void
ApplyZeroOrJunk
(
void
*
aPtr
size_t
aSize
)
{
if
(
opt_junk
)
{
memset
(
aPtr
kAllocJunk
aSize
)
;
}
else
if
(
opt_zero
)
{
memset
(
aPtr
0
aSize
)
;
}
}
#
ifdef
XP_WIN
namespace
MozAllocRetries
{
constexpr
size_t
kMaxAttempts
=
10
;
constexpr
size_t
kDelayMs
=
50
;
using
StallSpecs
=
:
:
mozilla
:
:
StallSpecs
;
static
constexpr
StallSpecs
maxStall
=
{
.
maxAttempts
=
kMaxAttempts
.
delayMs
=
kDelayMs
}
;
static
inline
StallSpecs
GetStallSpecs
(
)
{
#
if
defined
(
JS_STANDALONE
)
return
maxStall
;
#
else
switch
(
GetGeckoProcessType
(
)
)
{
case
GeckoProcessType
:
:
GeckoProcessType_Default
:
return
maxStall
;
default
:
return
{
.
maxAttempts
=
maxStall
.
maxAttempts
/
2
.
delayMs
=
maxStall
.
delayMs
}
;
}
#
endif
}
[
[
nodiscard
]
]
void
*
MozVirtualAlloc
(
LPVOID
lpAddress
SIZE_T
dwSize
DWORD
flAllocationType
DWORD
flProtect
)
{
DWORD
const
lastError
=
:
:
GetLastError
(
)
;
constexpr
auto
IsOOMError
=
[
]
{
switch
(
:
:
GetLastError
(
)
)
{
case
ERROR_COMMITMENT_LIMIT
:
case
ERROR_NOT_ENOUGH_MEMORY
:
return
true
;
}
return
false
;
}
;
{
void
*
ptr
=
:
:
VirtualAlloc
(
lpAddress
dwSize
flAllocationType
flProtect
)
;
if
(
MOZ_LIKELY
(
ptr
)
)
return
ptr
;
if
(
!
IsOOMError
(
)
)
return
nullptr
;
if
(
!
(
flAllocationType
&
MEM_COMMIT
)
)
return
nullptr
;
}
const
StallSpecs
stallSpecs
=
GetStallSpecs
(
)
;
const
auto
ret
=
stallSpecs
.
StallAndRetry
(
&
:
:
Sleep
[
&
]
(
)
-
>
std
:
:
optional
<
void
*
>
{
void
*
ptr
=
:
:
VirtualAlloc
(
lpAddress
dwSize
flAllocationType
flProtect
)
;
if
(
ptr
)
{
if
(
IsOOMError
(
)
)
{
:
:
SetLastError
(
lastError
)
;
}
return
ptr
;
}
if
(
!
IsOOMError
(
)
)
{
return
nullptr
;
}
return
std
:
:
nullopt
;
}
)
;
return
ret
.
value_or
(
nullptr
)
;
}
}
using
MozAllocRetries
:
:
MozVirtualAlloc
;
namespace
mozilla
{
MOZ_JEMALLOC_API
StallSpecs
GetAllocatorStallSpecs
(
)
{
return
:
:
MozAllocRetries
:
:
GetStallSpecs
(
)
;
}
}
#
endif
static
inline
void
pages_decommit
(
void
*
aAddr
size_t
aSize
)
{
#
ifdef
XP_WIN
size_t
pages_size
=
std
:
:
min
(
aSize
kChunkSize
-
GetChunkOffsetForPtr
(
aAddr
)
)
;
while
(
aSize
>
0
)
{
if
(
!
VirtualFree
(
aAddr
pages_size
MEM_DECOMMIT
)
)
{
MOZ_CRASH
(
)
;
}
aAddr
=
(
void
*
)
(
(
uintptr_t
)
aAddr
+
pages_size
)
;
aSize
-
=
pages_size
;
pages_size
=
std
:
:
min
(
aSize
kChunkSize
)
;
}
#
else
if
(
mmap
(
aAddr
aSize
PROT_NONE
MAP_FIXED
|
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
=
=
MAP_FAILED
)
{
const
char
out_of_mappings
[
]
=
"
[
unhandlable
oom
]
Failed
to
mmap
likely
no
more
mappings
"
"
available
"
__FILE__
"
:
"
MOZ_STRINGIFY
(
__LINE__
)
;
if
(
errno
=
=
ENOMEM
)
{
#
ifndef
ANDROID
fputs
(
out_of_mappings
stderr
)
;
fflush
(
stderr
)
;
#
endif
MOZ_CRASH_ANNOTATE
(
out_of_mappings
)
;
}
MOZ_REALLY_CRASH
(
__LINE__
)
;
}
MozTagAnonymousMemory
(
aAddr
aSize
"
jemalloc
-
decommitted
"
)
;
#
endif
}
[
[
nodiscard
]
]
static
inline
bool
pages_commit
(
void
*
aAddr
size_t
aSize
)
{
#
ifdef
XP_WIN
size_t
pages_size
=
std
:
:
min
(
aSize
kChunkSize
-
GetChunkOffsetForPtr
(
aAddr
)
)
;
while
(
aSize
>
0
)
{
if
(
!
MozVirtualAlloc
(
aAddr
pages_size
MEM_COMMIT
PAGE_READWRITE
)
)
{
return
false
;
}
aAddr
=
(
void
*
)
(
(
uintptr_t
)
aAddr
+
pages_size
)
;
aSize
-
=
pages_size
;
pages_size
=
std
:
:
min
(
aSize
kChunkSize
)
;
}
#
else
if
(
mmap
(
aAddr
aSize
PROT_READ
|
PROT_WRITE
MAP_FIXED
|
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
=
=
MAP_FAILED
)
{
return
false
;
}
MozTagAnonymousMemory
(
aAddr
aSize
"
jemalloc
"
)
;
#
endif
return
true
;
}
static
bool
base_pages_alloc
(
size_t
minsize
)
MOZ_REQUIRES
(
base_mtx
)
{
size_t
csize
;
size_t
pminsize
;
MOZ_ASSERT
(
minsize
!
=
0
)
;
csize
=
CHUNK_CEILING
(
minsize
)
;
base_pages
=
chunk_alloc
(
csize
kChunkSize
true
)
;
if
(
!
base_pages
)
{
return
true
;
}
base_next_addr
=
base_pages
;
base_past_addr
=
(
void
*
)
(
(
uintptr_t
)
base_pages
+
csize
)
;
pminsize
=
PAGE_CEILING
(
minsize
)
;
base_next_decommitted
=
(
void
*
)
(
(
uintptr_t
)
base_pages
+
pminsize
)
;
if
(
pminsize
<
csize
)
{
pages_decommit
(
base_next_decommitted
csize
-
pminsize
)
;
}
base_mapped
+
=
csize
;
base_committed
+
=
pminsize
;
return
false
;
}
static
void
*
base_alloc
(
size_t
aSize
)
{
void
*
ret
;
size_t
csize
;
csize
=
CACHELINE_CEILING
(
aSize
)
;
MutexAutoLock
lock
(
base_mtx
)
;
if
(
(
uintptr_t
)
base_next_addr
+
csize
>
(
uintptr_t
)
base_past_addr
)
{
if
(
base_pages_alloc
(
csize
)
)
{
return
nullptr
;
}
}
ret
=
base_next_addr
;
base_next_addr
=
(
void
*
)
(
(
uintptr_t
)
base_next_addr
+
csize
)
;
if
(
(
uintptr_t
)
base_next_addr
>
(
uintptr_t
)
base_next_decommitted
)
{
void
*
pbase_next_addr
=
(
void
*
)
(
PAGE_CEILING
(
(
uintptr_t
)
base_next_addr
)
)
;
if
(
!
pages_commit
(
base_next_decommitted
(
uintptr_t
)
pbase_next_addr
-
(
uintptr_t
)
base_next_decommitted
)
)
{
return
nullptr
;
}
base_committed
+
=
(
uintptr_t
)
pbase_next_addr
-
(
uintptr_t
)
base_next_decommitted
;
base_next_decommitted
=
pbase_next_addr
;
}
return
ret
;
}
static
void
*
base_calloc
(
size_t
aNumber
size_t
aSize
)
{
void
*
ret
=
base_alloc
(
aNumber
*
aSize
)
;
if
(
ret
)
{
memset
(
ret
0
aNumber
*
aSize
)
;
}
return
ret
;
}
template
<
typename
T
>
struct
TypedBaseAlloc
{
static
T
*
sFirstFree
;
static
size_t
size_of
(
)
{
return
sizeof
(
T
)
;
}
static
T
*
alloc
(
)
{
T
*
ret
;
base_mtx
.
Lock
(
)
;
if
(
sFirstFree
)
{
ret
=
sFirstFree
;
sFirstFree
=
*
(
T
*
*
)
ret
;
base_mtx
.
Unlock
(
)
;
}
else
{
base_mtx
.
Unlock
(
)
;
ret
=
(
T
*
)
base_alloc
(
size_of
(
)
)
;
}
return
ret
;
}
static
void
dealloc
(
T
*
aNode
)
{
MutexAutoLock
lock
(
base_mtx
)
;
*
(
T
*
*
)
aNode
=
sFirstFree
;
sFirstFree
=
aNode
;
}
}
;
using
ExtentAlloc
=
TypedBaseAlloc
<
extent_node_t
>
;
template
<
>
extent_node_t
*
ExtentAlloc
:
:
sFirstFree
=
nullptr
;
template
<
>
arena_t
*
TypedBaseAlloc
<
arena_t
>
:
:
sFirstFree
=
nullptr
;
template
<
>
size_t
TypedBaseAlloc
<
arena_t
>
:
:
size_of
(
)
{
return
sizeof
(
arena_t
)
+
(
sizeof
(
arena_bin_t
)
*
NUM_SMALL_CLASSES
)
;
}
template
<
typename
T
>
struct
BaseAllocFreePolicy
{
void
operator
(
)
(
T
*
aPtr
)
{
TypedBaseAlloc
<
T
>
:
:
dealloc
(
aPtr
)
;
}
}
;
using
UniqueBaseNode
=
UniquePtr
<
extent_node_t
BaseAllocFreePolicy
<
extent_node_t
>
>
;
#
ifdef
XP_WIN
static
void
*
pages_map
(
void
*
aAddr
size_t
aSize
)
{
void
*
ret
=
nullptr
;
ret
=
MozVirtualAlloc
(
aAddr
aSize
MEM_COMMIT
|
MEM_RESERVE
PAGE_READWRITE
)
;
return
ret
;
}
static
void
pages_unmap
(
void
*
aAddr
size_t
aSize
)
{
if
(
VirtualFree
(
aAddr
0
MEM_RELEASE
)
=
=
0
)
{
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Error
in
VirtualFree
(
)
\
n
"
)
;
}
}
#
else
static
void
pages_unmap
(
void
*
aAddr
size_t
aSize
)
{
if
(
munmap
(
aAddr
aSize
)
=
=
-
1
)
{
char
buf
[
64
]
;
if
(
strerror_r
(
errno
buf
sizeof
(
buf
)
)
=
=
0
)
{
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Error
in
munmap
(
)
:
"
buf
"
\
n
"
)
;
}
}
}
static
void
*
pages_map
(
void
*
aAddr
size_t
aSize
)
{
void
*
ret
;
#
if
defined
(
__ia64__
)
|
|
\
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
)
bool
check_placement
=
true
;
if
(
!
aAddr
)
{
aAddr
=
(
void
*
)
0x0000070000000000
;
check_placement
=
false
;
}
#
endif
#
if
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
const
uintptr_t
start
=
0x0000070000000000ULL
;
const
uintptr_t
end
=
0x0000800000000000ULL
;
uintptr_t
hint
;
void
*
region
=
MAP_FAILED
;
for
(
hint
=
start
;
region
=
=
MAP_FAILED
&
&
hint
+
aSize
<
=
end
;
hint
+
=
kChunkSize
)
{
region
=
mmap
(
(
void
*
)
hint
aSize
PROT_READ
|
PROT_WRITE
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
;
if
(
region
!
=
MAP_FAILED
)
{
if
(
(
(
size_t
)
region
+
(
aSize
-
1
)
)
&
0xffff800000000000
)
{
if
(
munmap
(
region
aSize
)
)
{
MOZ_ASSERT
(
errno
=
=
ENOMEM
)
;
}
region
=
MAP_FAILED
;
}
}
}
ret
=
region
;
#
else
ret
=
mmap
(
aAddr
aSize
PROT_READ
|
PROT_WRITE
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
;
MOZ_ASSERT
(
ret
)
;
#
endif
if
(
ret
=
=
MAP_FAILED
)
{
ret
=
nullptr
;
}
#
if
defined
(
__ia64__
)
|
|
\
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
)
else
if
(
(
long
long
)
ret
&
0xffff800000000000
)
{
munmap
(
ret
aSize
)
;
ret
=
nullptr
;
}
else
if
(
check_placement
&
&
ret
!
=
aAddr
)
{
#
else
else
if
(
aAddr
&
&
ret
!
=
aAddr
)
{
#
endif
pages_unmap
(
ret
aSize
)
;
ret
=
nullptr
;
}
if
(
ret
)
{
MozTagAnonymousMemory
(
ret
aSize
"
jemalloc
"
)
;
}
#
if
defined
(
__ia64__
)
|
|
\
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
)
MOZ_ASSERT
(
!
ret
|
|
(
!
check_placement
&
&
ret
)
|
|
(
check_placement
&
&
ret
=
=
aAddr
)
)
;
#
else
MOZ_ASSERT
(
!
ret
|
|
(
!
aAddr
&
&
ret
!
=
aAddr
)
|
|
(
aAddr
&
&
ret
=
=
aAddr
)
)
;
#
endif
return
ret
;
}
#
endif
#
ifdef
XP_DARWIN
#
define
VM_COPY_MIN
kChunkSize
static
inline
void
pages_copy
(
void
*
dest
const
void
*
src
size_t
n
)
{
MOZ_ASSERT
(
(
void
*
)
(
(
uintptr_t
)
dest
&
~
gPageSizeMask
)
=
=
dest
)
;
MOZ_ASSERT
(
n
>
=
VM_COPY_MIN
)
;
MOZ_ASSERT
(
(
void
*
)
(
(
uintptr_t
)
src
&
~
gPageSizeMask
)
=
=
src
)
;
kern_return_t
r
=
vm_copy
(
mach_task_self
(
)
(
vm_address_t
)
src
(
vm_size_t
)
n
(
vm_address_t
)
dest
)
;
if
(
r
!
=
KERN_SUCCESS
)
{
MOZ_CRASH
(
"
vm_copy
(
)
failed
"
)
;
}
}
#
endif
template
<
size_t
Bits
>
bool
AddressRadixTree
<
Bits
>
:
:
Init
(
)
{
mLock
.
Init
(
)
;
mRoot
=
(
void
*
*
)
base_calloc
(
1
<
<
kBitsAtLevel1
sizeof
(
void
*
)
)
;
return
mRoot
;
}
template
<
size_t
Bits
>
void
*
*
AddressRadixTree
<
Bits
>
:
:
GetSlot
(
void
*
aKey
bool
aCreate
)
{
uintptr_t
key
=
reinterpret_cast
<
uintptr_t
>
(
aKey
)
;
uintptr_t
subkey
;
unsigned
i
lshift
height
bits
;
void
*
*
node
;
void
*
*
child
;
for
(
i
=
lshift
=
0
height
=
kHeight
node
=
mRoot
;
i
<
height
-
1
;
i
+
+
lshift
+
=
bits
node
=
child
)
{
bits
=
i
?
kBitsPerLevel
:
kBitsAtLevel1
;
subkey
=
(
key
<
<
lshift
)
>
>
(
(
sizeof
(
void
*
)
<
<
3
)
-
bits
)
;
child
=
(
void
*
*
)
node
[
subkey
]
;
if
(
!
child
&
&
aCreate
)
{
child
=
(
void
*
*
)
base_calloc
(
1
<
<
kBitsPerLevel
sizeof
(
void
*
)
)
;
if
(
child
)
{
node
[
subkey
]
=
child
;
}
}
if
(
!
child
)
{
return
nullptr
;
}
}
bits
=
i
?
kBitsPerLevel
:
kBitsAtLevel1
;
subkey
=
(
key
<
<
lshift
)
>
>
(
(
sizeof
(
void
*
)
<
<
3
)
-
bits
)
;
return
&
node
[
subkey
]
;
}
template
<
size_t
Bits
>
void
*
AddressRadixTree
<
Bits
>
:
:
Get
(
void
*
aKey
)
{
void
*
ret
=
nullptr
;
void
*
*
slot
=
GetSlot
(
aKey
)
;
if
(
slot
)
{
ret
=
*
slot
;
}
#
ifdef
MOZ_DEBUG
MutexAutoLock
lock
(
mLock
)
;
if
(
!
slot
)
{
slot
=
GetSlot
(
aKey
)
;
}
if
(
slot
)
{
MOZ_ASSERT
(
ret
=
=
*
slot
)
;
}
else
{
MOZ_ASSERT
(
ret
=
=
nullptr
)
;
}
#
endif
return
ret
;
}
template
<
size_t
Bits
>
bool
AddressRadixTree
<
Bits
>
:
:
Set
(
void
*
aKey
void
*
aValue
)
{
MutexAutoLock
lock
(
mLock
)
;
void
*
*
slot
=
GetSlot
(
aKey
true
)
;
if
(
slot
)
{
*
slot
=
aValue
;
}
return
slot
;
}
#
define
ALIGNMENT_ADDR2OFFSET
(
a
alignment
)
\
(
(
size_t
)
(
(
uintptr_t
)
(
a
)
&
(
(
alignment
)
-
1
)
)
)
#
define
ALIGNMENT_CEILING
(
s
alignment
)
\
(
(
(
s
)
+
(
(
alignment
)
-
1
)
)
&
(
~
(
(
alignment
)
-
1
)
)
)
static
void
*
pages_trim
(
void
*
addr
size_t
alloc_size
size_t
leadsize
size_t
size
)
{
void
*
ret
=
(
void
*
)
(
(
uintptr_t
)
addr
+
leadsize
)
;
MOZ_ASSERT
(
alloc_size
>
=
leadsize
+
size
)
;
#
ifdef
XP_WIN
{
void
*
new_addr
;
pages_unmap
(
addr
alloc_size
)
;
new_addr
=
pages_map
(
ret
size
)
;
if
(
new_addr
=
=
ret
)
{
return
ret
;
}
if
(
new_addr
)
{
pages_unmap
(
new_addr
size
)
;
}
return
nullptr
;
}
#
else
{
size_t
trailsize
=
alloc_size
-
leadsize
-
size
;
if
(
leadsize
!
=
0
)
{
pages_unmap
(
addr
leadsize
)
;
}
if
(
trailsize
!
=
0
)
{
pages_unmap
(
(
void
*
)
(
(
uintptr_t
)
ret
+
size
)
trailsize
)
;
}
return
ret
;
}
#
endif
}
static
void
*
chunk_alloc_mmap_slow
(
size_t
size
size_t
alignment
)
{
void
*
ret
*
pages
;
size_t
alloc_size
leadsize
;
alloc_size
=
size
+
alignment
-
gRealPageSize
;
if
(
alloc_size
<
size
)
{
return
nullptr
;
}
do
{
pages
=
pages_map
(
nullptr
alloc_size
)
;
if
(
!
pages
)
{
return
nullptr
;
}
leadsize
=
ALIGNMENT_CEILING
(
(
uintptr_t
)
pages
alignment
)
-
(
uintptr_t
)
pages
;
ret
=
pages_trim
(
pages
alloc_size
leadsize
size
)
;
}
while
(
!
ret
)
;
MOZ_ASSERT
(
ret
)
;
return
ret
;
}
static
void
*
chunk_alloc_mmap
(
size_t
size
size_t
alignment
)
{
void
*
ret
;
size_t
offset
;
ret
=
pages_map
(
nullptr
size
)
;
if
(
!
ret
)
{
return
nullptr
;
}
offset
=
ALIGNMENT_ADDR2OFFSET
(
ret
alignment
)
;
if
(
offset
!
=
0
)
{
pages_unmap
(
ret
size
)
;
return
chunk_alloc_mmap_slow
(
size
alignment
)
;
}
MOZ_ASSERT
(
ret
)
;
return
ret
;
}
static
bool
pages_purge
(
void
*
addr
size_t
length
bool
force_zero
)
{
pages_decommit
(
addr
length
)
;
return
true
;
}
static
void
*
chunk_recycle
(
size_t
aSize
size_t
aAlignment
)
{
extent_node_t
key
;
size_t
alloc_size
=
aSize
+
aAlignment
-
kChunkSize
;
if
(
alloc_size
<
aSize
)
{
return
nullptr
;
}
key
.
mAddr
=
nullptr
;
key
.
mSize
=
alloc_size
;
chunks_mtx
.
Lock
(
)
;
extent_node_t
*
node
=
gChunksBySize
.
SearchOrNext
(
&
key
)
;
if
(
!
node
)
{
chunks_mtx
.
Unlock
(
)
;
return
nullptr
;
}
size_t
leadsize
=
ALIGNMENT_CEILING
(
(
uintptr_t
)
node
-
>
mAddr
aAlignment
)
-
(
uintptr_t
)
node
-
>
mAddr
;
MOZ_ASSERT
(
node
-
>
mSize
>
=
leadsize
+
aSize
)
;
size_t
trailsize
=
node
-
>
mSize
-
leadsize
-
aSize
;
void
*
ret
=
(
void
*
)
(
(
uintptr_t
)
node
-
>
mAddr
+
leadsize
)
;
MOZ_ASSERT
(
node
-
>
mChunkType
=
=
ZEROED_CHUNK
)
;
gChunksBySize
.
Remove
(
node
)
;
gChunksByAddress
.
Remove
(
node
)
;
if
(
leadsize
!
=
0
)
{
node
-
>
mSize
=
leadsize
;
gChunksBySize
.
Insert
(
node
)
;
gChunksByAddress
.
Insert
(
node
)
;
node
=
nullptr
;
}
if
(
trailsize
!
=
0
)
{
if
(
!
node
)
{
chunks_mtx
.
Unlock
(
)
;
node
=
ExtentAlloc
:
:
alloc
(
)
;
if
(
!
node
)
{
chunk_dealloc
(
ret
aSize
ZEROED_CHUNK
)
;
return
nullptr
;
}
chunks_mtx
.
Lock
(
)
;
}
node
-
>
mAddr
=
(
void
*
)
(
(
uintptr_t
)
(
ret
)
+
aSize
)
;
node
-
>
mSize
=
trailsize
;
node
-
>
mChunkType
=
ZEROED_CHUNK
;
gChunksBySize
.
Insert
(
node
)
;
gChunksByAddress
.
Insert
(
node
)
;
node
=
nullptr
;
}
gRecycledSize
-
=
aSize
;
chunks_mtx
.
Unlock
(
)
;
if
(
node
)
{
ExtentAlloc
:
:
dealloc
(
node
)
;
}
if
(
!
pages_commit
(
ret
aSize
)
)
{
return
nullptr
;
}
return
ret
;
}
#
ifdef
XP_WIN
#
define
CAN_RECYCLE
(
size
)
(
(
size
)
=
=
kChunkSize
)
#
else
#
define
CAN_RECYCLE
(
size
)
true
#
endif
static
void
*
chunk_alloc
(
size_t
aSize
size_t
aAlignment
bool
aBase
)
{
void
*
ret
=
nullptr
;
MOZ_ASSERT
(
aSize
!
=
0
)
;
MOZ_ASSERT
(
(
aSize
&
kChunkSizeMask
)
=
=
0
)
;
MOZ_ASSERT
(
aAlignment
!
=
0
)
;
MOZ_ASSERT
(
(
aAlignment
&
kChunkSizeMask
)
=
=
0
)
;
if
(
CAN_RECYCLE
(
aSize
)
&
&
!
aBase
)
{
ret
=
chunk_recycle
(
aSize
aAlignment
)
;
}
if
(
!
ret
)
{
ret
=
chunk_alloc_mmap
(
aSize
aAlignment
)
;
}
if
(
ret
&
&
!
aBase
)
{
if
(
!
gChunkRTree
.
Set
(
ret
ret
)
)
{
chunk_dealloc
(
ret
aSize
UNKNOWN_CHUNK
)
;
return
nullptr
;
}
}
MOZ_ASSERT
(
GetChunkOffsetForPtr
(
ret
)
=
=
0
)
;
return
ret
;
}
#
ifdef
MOZ_DEBUG
static
void
chunk_assert_zero
(
void
*
aPtr
size_t
aSize
)
{
size_t
i
;
size_t
*
p
=
(
size_t
*
)
(
uintptr_t
)
aPtr
;
for
(
i
=
0
;
i
<
aSize
/
sizeof
(
size_t
)
;
i
+
+
)
{
MOZ_ASSERT
(
p
[
i
]
=
=
0
)
;
}
}
#
endif
static
void
chunk_record
(
void
*
aChunk
size_t
aSize
ChunkType
aType
)
{
extent_node_t
key
;
if
(
aType
!
=
ZEROED_CHUNK
)
{
if
(
pages_purge
(
aChunk
aSize
aType
=
=
HUGE_CHUNK
)
)
{
aType
=
ZEROED_CHUNK
;
}
}
UniqueBaseNode
xnode
(
ExtentAlloc
:
:
alloc
(
)
)
;
UniqueBaseNode
xprev
;
MutexAutoLock
lock
(
chunks_mtx
)
;
key
.
mAddr
=
(
void
*
)
(
(
uintptr_t
)
aChunk
+
aSize
)
;
extent_node_t
*
node
=
gChunksByAddress
.
SearchOrNext
(
&
key
)
;
if
(
node
&
&
node
-
>
mAddr
=
=
key
.
mAddr
)
{
gChunksBySize
.
Remove
(
node
)
;
node
-
>
mAddr
=
aChunk
;
node
-
>
mSize
+
=
aSize
;
if
(
node
-
>
mChunkType
!
=
aType
)
{
node
-
>
mChunkType
=
RECYCLED_CHUNK
;
}
gChunksBySize
.
Insert
(
node
)
;
}
else
{
if
(
!
xnode
)
{
return
;
}
node
=
xnode
.
release
(
)
;
node
-
>
mAddr
=
aChunk
;
node
-
>
mSize
=
aSize
;
node
-
>
mChunkType
=
aType
;
gChunksByAddress
.
Insert
(
node
)
;
gChunksBySize
.
Insert
(
node
)
;
}
extent_node_t
*
prev
=
gChunksByAddress
.
Prev
(
node
)
;
if
(
prev
&
&
(
void
*
)
(
(
uintptr_t
)
prev
-
>
mAddr
+
prev
-
>
mSize
)
=
=
aChunk
)
{
gChunksBySize
.
Remove
(
prev
)
;
gChunksByAddress
.
Remove
(
prev
)
;
gChunksBySize
.
Remove
(
node
)
;
node
-
>
mAddr
=
prev
-
>
mAddr
;
node
-
>
mSize
+
=
prev
-
>
mSize
;
if
(
node
-
>
mChunkType
!
=
prev
-
>
mChunkType
)
{
node
-
>
mChunkType
=
RECYCLED_CHUNK
;
}
gChunksBySize
.
Insert
(
node
)
;
xprev
.
reset
(
prev
)
;
}
gRecycledSize
+
=
aSize
;
}
static
void
chunk_dealloc
(
void
*
aChunk
size_t
aSize
ChunkType
aType
)
{
MOZ_ASSERT
(
aChunk
)
;
MOZ_ASSERT
(
GetChunkOffsetForPtr
(
aChunk
)
=
=
0
)
;
MOZ_ASSERT
(
aSize
!
=
0
)
;
MOZ_ASSERT
(
(
aSize
&
kChunkSizeMask
)
=
=
0
)
;
gChunkRTree
.
Unset
(
aChunk
)
;
if
(
CAN_RECYCLE
(
aSize
)
)
{
size_t
recycled_so_far
=
gRecycledSize
;
if
(
recycled_so_far
<
gRecycleLimit
)
{
size_t
recycle_remaining
=
gRecycleLimit
-
recycled_so_far
;
size_t
to_recycle
;
if
(
aSize
>
recycle_remaining
)
{
to_recycle
=
recycle_remaining
;
pages_trim
(
aChunk
aSize
0
to_recycle
)
;
}
else
{
to_recycle
=
aSize
;
}
chunk_record
(
aChunk
to_recycle
aType
)
;
return
;
}
}
pages_unmap
(
aChunk
aSize
)
;
}
#
undef
CAN_RECYCLE
static
inline
arena_t
*
thread_local_arena
(
bool
enabled
)
{
arena_t
*
arena
;
if
(
enabled
)
{
arena
=
gArenas
.
CreateArena
(
false
nullptr
)
;
}
else
{
arena
=
gArenas
.
GetDefault
(
)
;
}
thread_arena
.
set
(
arena
)
;
return
arena
;
}
inline
void
MozJemalloc
:
:
jemalloc_thread_local_arena
(
bool
aEnabled
)
{
if
(
malloc_init
(
)
)
{
thread_local_arena
(
aEnabled
)
;
}
}
static
inline
arena_t
*
choose_arena
(
size_t
size
)
{
arena_t
*
ret
=
nullptr
;
if
(
size
>
kMaxQuantumClass
)
{
ret
=
gArenas
.
GetDefault
(
)
;
}
else
{
ret
=
thread_arena
.
get
(
)
;
MOZ_DIAGNOSTIC_ASSERT_IF
(
ret
(
size_t
)
ret
>
=
gPageSize
)
;
if
(
!
ret
)
{
ret
=
thread_local_arena
(
false
)
;
}
}
MOZ_DIAGNOSTIC_ASSERT
(
ret
)
;
return
ret
;
}
inline
uint8_t
arena_t
:
:
FindFreeBitInMask
(
uint32_t
aMask
uint32_t
&
aRng
)
{
if
(
mPRNG
!
=
nullptr
)
{
if
(
aRng
=
=
UINT_MAX
)
{
aRng
=
mPRNG
-
>
next
(
)
%
32
;
}
uint8_t
bitIndex
;
aMask
=
aRng
?
RotateRight
(
aMask
aRng
)
:
aMask
;
bitIndex
=
CountTrailingZeroes32
(
aMask
)
;
return
(
bitIndex
+
aRng
)
%
32
;
}
return
CountTrailingZeroes32
(
aMask
)
;
}
inline
void
*
arena_t
:
:
ArenaRunRegAlloc
(
arena_run_t
*
aRun
arena_bin_t
*
aBin
)
{
void
*
ret
;
unsigned
i
mask
bit
regind
;
uint32_t
rndPos
=
UINT_MAX
;
MOZ_DIAGNOSTIC_ASSERT
(
aRun
-
>
mMagic
=
=
ARENA_RUN_MAGIC
)
;
MOZ_ASSERT
(
aRun
-
>
mRegionsMinElement
<
aBin
-
>
mRunNumRegionsMask
)
;
i
=
aRun
-
>
mRegionsMinElement
;
mask
=
aRun
-
>
mRegionsMask
[
i
]
;
if
(
mask
!
=
0
)
{
bit
=
FindFreeBitInMask
(
mask
rndPos
)
;
regind
=
(
(
i
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
+
bit
)
;
MOZ_ASSERT
(
regind
<
aBin
-
>
mRunNumRegions
)
;
ret
=
(
void
*
)
(
(
(
uintptr_t
)
aRun
)
+
aBin
-
>
mRunFirstRegionOffset
+
(
aBin
-
>
mSizeClass
*
regind
)
)
;
mask
^
=
(
1U
<
<
bit
)
;
aRun
-
>
mRegionsMask
[
i
]
=
mask
;
return
ret
;
}
for
(
i
+
+
;
i
<
aBin
-
>
mRunNumRegionsMask
;
i
+
+
)
{
mask
=
aRun
-
>
mRegionsMask
[
i
]
;
if
(
mask
!
=
0
)
{
bit
=
FindFreeBitInMask
(
mask
rndPos
)
;
regind
=
(
(
i
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
+
bit
)
;
MOZ_ASSERT
(
regind
<
aBin
-
>
mRunNumRegions
)
;
ret
=
(
void
*
)
(
(
(
uintptr_t
)
aRun
)
+
aBin
-
>
mRunFirstRegionOffset
+
(
aBin
-
>
mSizeClass
*
regind
)
)
;
mask
^
=
(
1U
<
<
bit
)
;
aRun
-
>
mRegionsMask
[
i
]
=
mask
;
aRun
-
>
mRegionsMinElement
=
i
;
return
ret
;
}
}
MOZ_DIAGNOSTIC_ASSERT
(
0
)
;
return
nullptr
;
}
static
inline
void
arena_run_reg_dalloc
(
arena_run_t
*
run
arena_bin_t
*
bin
void
*
ptr
size_t
size
)
{
uint32_t
diff
regind
;
unsigned
elm
bit
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
mMagic
=
=
ARENA_RUN_MAGIC
)
;
diff
=
(
uint32_t
)
(
(
uintptr_t
)
ptr
-
(
uintptr_t
)
run
-
bin
-
>
mRunFirstRegionOffset
)
;
MOZ_ASSERT
(
diff
<
=
(
static_cast
<
unsigned
>
(
bin
-
>
mRunSizePages
)
<
<
gPageSize2Pow
)
)
;
regind
=
diff
/
bin
-
>
mSizeDivisor
;
MOZ_DIAGNOSTIC_ASSERT
(
diff
=
=
regind
*
size
)
;
MOZ_DIAGNOSTIC_ASSERT
(
regind
<
bin
-
>
mRunNumRegions
)
;
elm
=
regind
>
>
(
LOG2
(
sizeof
(
int
)
)
+
3
)
;
if
(
elm
<
run
-
>
mRegionsMinElement
)
{
run
-
>
mRegionsMinElement
=
elm
;
}
bit
=
regind
-
(
elm
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
;
MOZ_RELEASE_ASSERT
(
(
run
-
>
mRegionsMask
[
elm
]
&
(
1U
<
<
bit
)
)
=
=
0
"
Double
-
free
?
"
)
;
run
-
>
mRegionsMask
[
elm
]
|
=
(
1U
<
<
bit
)
;
}
bool
arena_t
:
:
SplitRun
(
arena_run_t
*
aRun
size_t
aSize
bool
aLarge
bool
aZero
)
{
arena_chunk_t
*
chunk
=
GetChunkForPtr
(
aRun
)
;
size_t
old_ndirty
=
chunk
-
>
ndirty
;
size_t
run_ind
=
(
unsigned
)
(
(
uintptr_t
(
aRun
)
-
uintptr_t
(
chunk
)
)
>
>
gPageSize2Pow
)
;
size_t
total_pages
=
(
chunk
-
>
map
[
run_ind
]
.
bits
&
~
gPageSizeMask
)
>
>
gPageSize2Pow
;
size_t
need_pages
=
(
aSize
>
>
gPageSize2Pow
)
;
MOZ_ASSERT
(
need_pages
>
0
)
;
MOZ_ASSERT
(
need_pages
<
=
total_pages
)
;
size_t
rem_pages
=
total_pages
-
need_pages
;
#
ifdef
MALLOC_DECOMMIT
size_t
i
=
0
;
while
(
i
<
need_pages
)
{
if
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_DECOMMITTED
)
{
size_t
j
;
for
(
j
=
0
;
i
+
j
<
need_pages
&
&
(
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
CHUNK_MAP_DECOMMITTED
)
;
j
+
+
)
{
MOZ_ASSERT
(
(
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
(
CHUNK_MAP_FRESH
|
CHUNK_MAP_MADVISED
)
)
=
=
0
)
;
}
if
(
i
+
j
=
=
need_pages
)
{
size_t
extra_commit
=
ExtraCommitPages
(
j
rem_pages
)
;
for
(
;
i
+
j
<
need_pages
+
extra_commit
&
&
(
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
CHUNK_MAP_MADVISED_OR_DECOMMITTED
)
;
j
+
+
)
{
MOZ_ASSERT
(
(
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
(
CHUNK_MAP_FRESH
|
CHUNK_MAP_MADVISED
)
)
=
=
0
)
;
}
}
if
(
!
pages_commit
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
(
run_ind
+
i
)
<
<
gPageSize2Pow
)
)
j
<
<
gPageSize2Pow
)
)
{
return
false
;
}
for
(
size_t
k
=
0
;
k
<
j
;
k
+
+
)
{
chunk
-
>
map
[
run_ind
+
i
+
k
]
.
bits
=
(
chunk
-
>
map
[
run_ind
+
i
+
k
]
.
bits
&
~
CHUNK_MAP_DECOMMITTED
)
|
CHUNK_MAP_ZEROED
|
CHUNK_MAP_FRESH
;
}
mNumFresh
+
=
j
;
i
+
=
j
;
}
else
{
i
+
+
;
}
}
#
endif
mRunsAvail
.
Remove
(
&
chunk
-
>
map
[
run_ind
]
)
;
if
(
rem_pages
>
0
)
{
chunk
-
>
map
[
run_ind
+
need_pages
]
.
bits
=
(
rem_pages
<
<
gPageSize2Pow
)
|
(
chunk
-
>
map
[
run_ind
+
need_pages
]
.
bits
&
gPageSizeMask
)
;
chunk
-
>
map
[
run_ind
+
total_pages
-
1
]
.
bits
=
(
rem_pages
<
<
gPageSize2Pow
)
|
(
chunk
-
>
map
[
run_ind
+
total_pages
-
1
]
.
bits
&
gPageSizeMask
)
;
mRunsAvail
.
Insert
(
&
chunk
-
>
map
[
run_ind
+
need_pages
]
)
;
}
for
(
size_t
i
=
0
;
i
<
need_pages
;
i
+
+
)
{
if
(
aZero
)
{
if
(
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_ZEROED
)
=
=
0
)
{
memset
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
(
run_ind
+
i
)
<
<
gPageSize2Pow
)
)
0
gPageSize
)
;
}
}
if
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_DIRTY
)
{
chunk
-
>
ndirty
-
-
;
mNumDirty
-
-
;
}
else
if
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_MADVISED
)
{
mStats
.
committed
+
+
;
mNumMAdvised
-
-
;
}
if
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_FRESH
)
{
mStats
.
committed
+
+
;
mNumFresh
-
-
;
}
MOZ_ASSERT
(
!
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_DECOMMITTED
)
)
;
if
(
aLarge
)
{
chunk
-
>
map
[
run_ind
+
i
]
.
bits
=
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
}
else
{
chunk
-
>
map
[
run_ind
+
i
]
.
bits
=
size_t
(
aRun
)
|
CHUNK_MAP_ALLOCATED
;
}
}
if
(
aLarge
)
{
chunk
-
>
map
[
run_ind
]
.
bits
|
=
aSize
;
}
if
(
chunk
-
>
ndirty
=
=
0
&
&
old_ndirty
>
0
)
{
mChunksDirty
.
Remove
(
chunk
)
;
}
return
true
;
}
void
arena_t
:
:
InitChunk
(
arena_chunk_t
*
aChunk
size_t
aMinCommittedPages
)
{
mStats
.
mapped
+
=
kChunkSize
;
aChunk
-
>
arena
=
this
;
aChunk
-
>
ndirty
=
0
;
size_t
i
;
for
(
i
=
0
;
i
<
gChunkHeaderNumPages
-
1
;
i
+
+
)
{
aChunk
-
>
map
[
i
]
.
bits
=
0
;
}
mStats
.
committed
+
=
gChunkHeaderNumPages
-
1
;
pages_decommit
(
(
void
*
)
(
uintptr_t
(
aChunk
)
+
(
i
<
<
gPageSize2Pow
)
)
gPageSize
)
;
aChunk
-
>
map
[
i
+
+
]
.
bits
=
CHUNK_MAP_DECOMMITTED
;
#
ifdef
MALLOC_DECOMMIT
size_t
n_fresh_pages
=
aMinCommittedPages
+
ExtraCommitPages
(
aMinCommittedPages
gChunkNumPages
-
gChunkHeaderNumPages
-
aMinCommittedPages
-
1
)
;
#
else
size_t
n_fresh_pages
=
gChunkNumPages
-
1
-
gChunkHeaderNumPages
;
#
endif
for
(
size_t
j
=
0
;
j
<
n_fresh_pages
;
j
+
+
)
{
aChunk
-
>
map
[
i
+
j
]
.
bits
=
CHUNK_MAP_ZEROED
|
CHUNK_MAP_FRESH
;
}
i
+
=
n_fresh_pages
;
mNumFresh
+
=
n_fresh_pages
;
#
ifndef
MALLOC_DECOMMIT
MOZ_ASSERT
(
i
=
=
gChunkNumPages
-
1
)
;
#
endif
pages_decommit
(
(
void
*
)
(
uintptr_t
(
aChunk
)
+
(
i
<
<
gPageSize2Pow
)
)
(
gChunkNumPages
-
i
)
<
<
gPageSize2Pow
)
;
for
(
;
i
<
gChunkNumPages
;
i
+
+
)
{
aChunk
-
>
map
[
i
]
.
bits
=
CHUNK_MAP_DECOMMITTED
;
}
MOZ_ASSERT
(
aMinCommittedPages
>
0
)
;
MOZ_ASSERT
(
aMinCommittedPages
<
=
gChunkNumPages
-
gChunkHeaderNumPages
-
1
)
;
aChunk
-
>
map
[
gChunkHeaderNumPages
]
.
bits
|
=
gMaxLargeClass
;
aChunk
-
>
map
[
gChunkNumPages
-
2
]
.
bits
|
=
gMaxLargeClass
;
mRunsAvail
.
Insert
(
&
aChunk
-
>
map
[
gChunkHeaderNumPages
]
)
;
#
ifdef
MALLOC_DOUBLE_PURGE
new
(
&
aChunk
-
>
chunks_madvised_elem
)
DoublyLinkedListElement
<
arena_chunk_t
>
(
)
;
#
endif
}
arena_chunk_t
*
arena_t
:
:
DeallocChunk
(
arena_chunk_t
*
aChunk
)
{
if
(
mSpare
)
{
if
(
mSpare
-
>
ndirty
>
0
)
{
aChunk
-
>
arena
-
>
mChunksDirty
.
Remove
(
mSpare
)
;
mNumDirty
-
=
mSpare
-
>
ndirty
;
mStats
.
committed
-
=
mSpare
-
>
ndirty
;
}
size_t
madvised
=
0
;
size_t
fresh
=
0
;
for
(
size_t
i
=
gChunkHeaderNumPages
;
i
<
gChunkNumPages
-
1
;
i
+
+
)
{
MOZ_ASSERT
(
mSpare
-
>
map
[
i
]
.
bits
&
(
CHUNK_MAP_FRESH_MADVISED_OR_DECOMMITTED
|
CHUNK_MAP_DIRTY
)
)
;
if
(
mSpare
-
>
map
[
i
]
.
bits
&
CHUNK_MAP_MADVISED
)
{
madvised
+
+
;
}
else
if
(
mSpare
-
>
map
[
i
]
.
bits
&
CHUNK_MAP_FRESH
)
{
fresh
+
+
;
}
}
mNumMAdvised
-
=
madvised
;
mNumFresh
-
=
fresh
;
#
ifdef
MALLOC_DOUBLE_PURGE
if
(
mChunksMAdvised
.
ElementProbablyInList
(
mSpare
)
)
{
mChunksMAdvised
.
remove
(
mSpare
)
;
}
#
endif
mStats
.
mapped
-
=
kChunkSize
;
mStats
.
committed
-
=
gChunkHeaderNumPages
-
1
;
}
mRunsAvail
.
Remove
(
&
aChunk
-
>
map
[
gChunkHeaderNumPages
]
)
;
arena_chunk_t
*
chunk_dealloc
=
mSpare
;
mSpare
=
aChunk
;
return
chunk_dealloc
;
}
arena_run_t
*
arena_t
:
:
AllocRun
(
size_t
aSize
bool
aLarge
bool
aZero
)
{
arena_run_t
*
run
;
arena_chunk_map_t
*
mapelm
;
arena_chunk_map_t
key
;
MOZ_ASSERT
(
aSize
<
=
gMaxLargeClass
)
;
MOZ_ASSERT
(
(
aSize
&
gPageSizeMask
)
=
=
0
)
;
key
.
bits
=
aSize
|
CHUNK_MAP_KEY
;
mapelm
=
mRunsAvail
.
SearchOrNext
(
&
key
)
;
if
(
mapelm
)
{
arena_chunk_t
*
chunk
=
GetChunkForPtr
(
mapelm
)
;
size_t
pageind
=
(
uintptr_t
(
mapelm
)
-
uintptr_t
(
chunk
-
>
map
)
)
/
sizeof
(
arena_chunk_map_t
)
;
run
=
(
arena_run_t
*
)
(
uintptr_t
(
chunk
)
+
(
pageind
<
<
gPageSize2Pow
)
)
;
}
else
if
(
mSpare
)
{
arena_chunk_t
*
chunk
=
mSpare
;
mSpare
=
nullptr
;
run
=
(
arena_run_t
*
)
(
uintptr_t
(
chunk
)
+
(
gChunkHeaderNumPages
<
<
gPageSize2Pow
)
)
;
mRunsAvail
.
Insert
(
&
chunk
-
>
map
[
gChunkHeaderNumPages
]
)
;
}
else
{
arena_chunk_t
*
chunk
=
(
arena_chunk_t
*
)
chunk_alloc
(
kChunkSize
kChunkSize
false
)
;
if
(
!
chunk
)
{
return
nullptr
;
}
InitChunk
(
chunk
aSize
>
>
gPageSize2Pow
)
;
run
=
(
arena_run_t
*
)
(
uintptr_t
(
chunk
)
+
(
gChunkHeaderNumPages
<
<
gPageSize2Pow
)
)
;
}
return
SplitRun
(
run
aSize
aLarge
aZero
)
?
run
:
nullptr
;
}
size_t
arena_t
:
:
EffectiveMaxDirty
(
)
{
int32_t
modifier
=
gArenas
.
DefaultMaxDirtyPageModifier
(
)
;
if
(
modifier
)
{
int32_t
arenaOverride
=
modifier
>
0
?
mMaxDirtyIncreaseOverride
:
mMaxDirtyDecreaseOverride
;
if
(
arenaOverride
)
{
modifier
=
arenaOverride
;
}
}
return
modifier
>
=
0
?
mMaxDirty
<
<
modifier
:
mMaxDirty
>
>
-
modifier
;
}
#
ifdef
MALLOC_DECOMMIT
size_t
arena_t
:
:
ExtraCommitPages
(
size_t
aReqPages
size_t
aRemainingPages
)
{
const
int32_t
modifier
=
gArenas
.
DefaultMaxDirtyPageModifier
(
)
;
if
(
modifier
<
0
)
{
return
0
;
}
const
size_t
max_page_cache
=
EffectiveMaxDirty
(
)
;
const
size_t
page_cache
=
mNumDirty
+
mNumFresh
+
mNumMAdvised
;
if
(
page_cache
>
max_page_cache
)
{
return
0
;
}
if
(
modifier
>
0
)
{
return
std
:
:
min
(
aRemainingPages
max_page_cache
-
page_cache
)
;
}
const
size_t
min
=
max_page_cache
/
4
;
const
size_t
max
=
3
*
max_page_cache
/
4
;
size_t
amortisation_threshold
=
32
;
size_t
extra_pages
=
aReqPages
<
amortisation_threshold
?
amortisation_threshold
-
aReqPages
:
0
;
if
(
page_cache
+
extra_pages
<
min
)
{
extra_pages
=
min
-
page_cache
;
}
else
if
(
page_cache
+
extra_pages
>
max
)
{
amortisation_threshold
/
=
2
;
extra_pages
=
std
:
:
min
(
aReqPages
<
amortisation_threshold
?
amortisation_threshold
-
aReqPages
:
0
max_page_cache
-
page_cache
)
;
}
extra_pages
=
std
:
:
min
(
extra_pages
aRemainingPages
)
;
if
(
(
aRemainingPages
-
extra_pages
)
<
amortisation_threshold
/
2
&
&
(
page_cache
+
aRemainingPages
)
<
max_page_cache
)
{
return
aRemainingPages
;
}
return
extra_pages
;
}
#
endif
void
arena_t
:
:
Purge
(
size_t
aMaxDirty
)
{
arena_chunk_t
*
chunk
;
size_t
i
npages
;
#
ifdef
MOZ_DEBUG
size_t
ndirty
=
0
;
for
(
auto
chunk
:
mChunksDirty
.
iter
(
)
)
{
ndirty
+
=
chunk
-
>
ndirty
;
}
MOZ_ASSERT
(
ndirty
=
=
mNumDirty
)
;
#
endif
MOZ_DIAGNOSTIC_ASSERT
(
aMaxDirty
=
=
1
|
|
(
mNumDirty
>
aMaxDirty
)
)
;
while
(
mNumDirty
>
(
aMaxDirty
>
>
1
)
)
{
#
ifdef
MALLOC_DOUBLE_PURGE
bool
madvised
=
false
;
#
endif
chunk
=
mChunksDirty
.
Last
(
)
;
MOZ_DIAGNOSTIC_ASSERT
(
chunk
)
;
MOZ_ASSERT
(
(
chunk
-
>
map
[
gChunkNumPages
-
1
]
.
bits
&
CHUNK_MAP_DECOMMITTED
)
!
=
0
)
;
for
(
i
=
gChunkNumPages
-
2
;
chunk
-
>
ndirty
>
0
;
i
-
-
)
{
MOZ_DIAGNOSTIC_ASSERT
(
i
>
=
gChunkHeaderNumPages
)
;
if
(
chunk
-
>
map
[
i
]
.
bits
&
CHUNK_MAP_DIRTY
)
{
#
ifdef
MALLOC_DECOMMIT
const
size_t
free_operation
=
CHUNK_MAP_DECOMMITTED
;
#
else
const
size_t
free_operation
=
CHUNK_MAP_MADVISED
;
#
endif
MOZ_ASSERT
(
(
chunk
-
>
map
[
i
]
.
bits
&
CHUNK_MAP_FRESH_MADVISED_OR_DECOMMITTED
)
=
=
0
)
;
chunk
-
>
map
[
i
]
.
bits
^
=
free_operation
|
CHUNK_MAP_DIRTY
;
for
(
npages
=
1
;
i
>
gChunkHeaderNumPages
&
&
(
chunk
-
>
map
[
i
-
1
]
.
bits
&
CHUNK_MAP_DIRTY
)
;
npages
+
+
)
{
i
-
-
;
MOZ_ASSERT
(
(
chunk
-
>
map
[
i
]
.
bits
&
CHUNK_MAP_FRESH_MADVISED_OR_DECOMMITTED
)
=
=
0
)
;
chunk
-
>
map
[
i
]
.
bits
^
=
free_operation
|
CHUNK_MAP_DIRTY
;
}
chunk
-
>
ndirty
-
=
npages
;
mNumDirty
-
=
npages
;
#
ifdef
MALLOC_DECOMMIT
pages_decommit
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
i
<
<
gPageSize2Pow
)
)
(
npages
<
<
gPageSize2Pow
)
)
;
#
else
#
ifdef
XP_SOLARIS
posix_madvise
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
i
<
<
gPageSize2Pow
)
)
(
npages
<
<
gPageSize2Pow
)
MADV_FREE
)
;
#
else
madvise
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
i
<
<
gPageSize2Pow
)
)
(
npages
<
<
gPageSize2Pow
)
MADV_FREE
)
;
#
endif
mNumMAdvised
+
=
npages
;
#
ifdef
MALLOC_DOUBLE_PURGE
madvised
=
true
;
#
endif
#
endif
mStats
.
committed
-
=
npages
;
if
(
mNumDirty
<
=
(
aMaxDirty
>
>
1
)
)
{
break
;
}
}
}
if
(
chunk
-
>
ndirty
=
=
0
)
{
mChunksDirty
.
Remove
(
chunk
)
;
}
#
ifdef
MALLOC_DOUBLE_PURGE
if
(
madvised
)
{
if
(
mChunksMAdvised
.
ElementProbablyInList
(
chunk
)
)
{
mChunksMAdvised
.
remove
(
chunk
)
;
}
mChunksMAdvised
.
pushFront
(
chunk
)
;
}
#
endif
}
}
arena_chunk_t
*
arena_t
:
:
DallocRun
(
arena_run_t
*
aRun
bool
aDirty
)
{
arena_chunk_t
*
chunk
;
size_t
size
run_ind
run_pages
;
chunk
=
GetChunkForPtr
(
aRun
)
;
run_ind
=
(
size_t
)
(
(
uintptr_t
(
aRun
)
-
uintptr_t
(
chunk
)
)
>
>
gPageSize2Pow
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run_ind
>
=
gChunkHeaderNumPages
)
;
MOZ_RELEASE_ASSERT
(
run_ind
<
gChunkNumPages
-
1
)
;
if
(
(
chunk
-
>
map
[
run_ind
]
.
bits
&
CHUNK_MAP_LARGE
)
!
=
0
)
{
size
=
chunk
-
>
map
[
run_ind
]
.
bits
&
~
gPageSizeMask
;
run_pages
=
(
size
>
>
gPageSize2Pow
)
;
}
else
{
run_pages
=
aRun
-
>
mBin
-
>
mRunSizePages
;
size
=
run_pages
<
<
gPageSize2Pow
;
}
if
(
aDirty
)
{
size_t
i
;
for
(
i
=
0
;
i
<
run_pages
;
i
+
+
)
{
MOZ_DIAGNOSTIC_ASSERT
(
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_DIRTY
)
=
=
0
)
;
chunk
-
>
map
[
run_ind
+
i
]
.
bits
=
CHUNK_MAP_DIRTY
;
}
if
(
chunk
-
>
ndirty
=
=
0
)
{
mChunksDirty
.
Insert
(
chunk
)
;
}
chunk
-
>
ndirty
+
=
run_pages
;
mNumDirty
+
=
run_pages
;
}
else
{
size_t
i
;
for
(
i
=
0
;
i
<
run_pages
;
i
+
+
)
{
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
=
~
(
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
)
;
}
}
chunk
-
>
map
[
run_ind
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
]
.
bits
&
gPageSizeMask
)
;
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
gPageSizeMask
)
;
if
(
run_ind
+
run_pages
<
gChunkNumPages
-
1
&
&
(
chunk
-
>
map
[
run_ind
+
run_pages
]
.
bits
&
CHUNK_MAP_ALLOCATED
)
=
=
0
)
{
size_t
nrun_size
=
chunk
-
>
map
[
run_ind
+
run_pages
]
.
bits
&
~
gPageSizeMask
;
mRunsAvail
.
Remove
(
&
chunk
-
>
map
[
run_ind
+
run_pages
]
)
;
size
+
=
nrun_size
;
run_pages
=
size
>
>
gPageSize2Pow
;
MOZ_DIAGNOSTIC_ASSERT
(
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
~
gPageSizeMask
)
=
=
nrun_size
)
;
chunk
-
>
map
[
run_ind
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
]
.
bits
&
gPageSizeMask
)
;
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
gPageSizeMask
)
;
}
if
(
run_ind
>
gChunkHeaderNumPages
&
&
(
chunk
-
>
map
[
run_ind
-
1
]
.
bits
&
CHUNK_MAP_ALLOCATED
)
=
=
0
)
{
size_t
prun_size
=
chunk
-
>
map
[
run_ind
-
1
]
.
bits
&
~
gPageSizeMask
;
run_ind
-
=
prun_size
>
>
gPageSize2Pow
;
mRunsAvail
.
Remove
(
&
chunk
-
>
map
[
run_ind
]
)
;
size
+
=
prun_size
;
run_pages
=
size
>
>
gPageSize2Pow
;
MOZ_DIAGNOSTIC_ASSERT
(
(
chunk
-
>
map
[
run_ind
]
.
bits
&
~
gPageSizeMask
)
=
=
prun_size
)
;
chunk
-
>
map
[
run_ind
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
]
.
bits
&
gPageSizeMask
)
;
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
gPageSizeMask
)
;
}
mRunsAvail
.
Insert
(
&
chunk
-
>
map
[
run_ind
]
)
;
arena_chunk_t
*
chunk_dealloc
=
nullptr
;
if
(
(
chunk
-
>
map
[
gChunkHeaderNumPages
]
.
bits
&
(
~
gPageSizeMask
|
CHUNK_MAP_ALLOCATED
)
)
=
=
gMaxLargeClass
)
{
chunk_dealloc
=
DeallocChunk
(
chunk
)
;
}
size_t
maxDirty
=
EffectiveMaxDirty
(
)
;
if
(
mNumDirty
>
maxDirty
)
{
Purge
(
maxDirty
)
;
}
return
chunk_dealloc
;
}
void
arena_t
:
:
TrimRunHead
(
arena_chunk_t
*
aChunk
arena_run_t
*
aRun
size_t
aOldSize
size_t
aNewSize
)
{
size_t
pageind
=
(
uintptr_t
(
aRun
)
-
uintptr_t
(
aChunk
)
)
>
>
gPageSize2Pow
;
size_t
head_npages
=
(
aOldSize
-
aNewSize
)
>
>
gPageSize2Pow
;
MOZ_ASSERT
(
aOldSize
>
aNewSize
)
;
aChunk
-
>
map
[
pageind
]
.
bits
=
(
aOldSize
-
aNewSize
)
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
aChunk
-
>
map
[
pageind
+
head_npages
]
.
bits
=
aNewSize
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
#
ifdef
MOZ_DEBUG
arena_chunk_t
*
no_chunk
=
#
endif
DallocRun
(
aRun
false
)
;
MOZ_ASSERT
(
!
no_chunk
)
;
}
void
arena_t
:
:
TrimRunTail
(
arena_chunk_t
*
aChunk
arena_run_t
*
aRun
size_t
aOldSize
size_t
aNewSize
bool
aDirty
)
{
size_t
pageind
=
(
uintptr_t
(
aRun
)
-
uintptr_t
(
aChunk
)
)
>
>
gPageSize2Pow
;
size_t
npages
=
aNewSize
>
>
gPageSize2Pow
;
MOZ_ASSERT
(
aOldSize
>
aNewSize
)
;
aChunk
-
>
map
[
pageind
]
.
bits
=
aNewSize
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
aChunk
-
>
map
[
pageind
+
npages
]
.
bits
=
(
aOldSize
-
aNewSize
)
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
#
ifdef
MOZ_DEBUG
arena_chunk_t
*
no_chunk
=
#
endif
DallocRun
(
(
arena_run_t
*
)
(
uintptr_t
(
aRun
)
+
aNewSize
)
aDirty
)
;
MOZ_ASSERT
(
!
no_chunk
)
;
}
arena_run_t
*
arena_t
:
:
GetNonFullBinRun
(
arena_bin_t
*
aBin
)
{
arena_chunk_map_t
*
mapelm
;
arena_run_t
*
run
;
unsigned
i
remainder
;
mapelm
=
aBin
-
>
mNonFullRuns
.
First
(
)
;
if
(
mapelm
)
{
aBin
-
>
mNonFullRuns
.
Remove
(
mapelm
)
;
run
=
(
arena_run_t
*
)
(
mapelm
-
>
bits
&
~
gPageSizeMask
)
;
return
run
;
}
run
=
AllocRun
(
static_cast
<
size_t
>
(
aBin
-
>
mRunSizePages
)
<
<
gPageSize2Pow
false
false
)
;
if
(
!
run
)
{
return
nullptr
;
}
if
(
run
=
=
aBin
-
>
mCurrentRun
)
{
return
run
;
}
run
-
>
mBin
=
aBin
;
for
(
i
=
0
;
i
<
aBin
-
>
mRunNumRegionsMask
-
1
;
i
+
+
)
{
run
-
>
mRegionsMask
[
i
]
=
UINT_MAX
;
}
remainder
=
aBin
-
>
mRunNumRegions
&
(
(
1U
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
-
1
)
;
if
(
remainder
=
=
0
)
{
run
-
>
mRegionsMask
[
i
]
=
UINT_MAX
;
}
else
{
run
-
>
mRegionsMask
[
i
]
=
(
UINT_MAX
>
>
(
(
1U
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
-
remainder
)
)
;
}
run
-
>
mRegionsMinElement
=
0
;
run
-
>
mNumFree
=
aBin
-
>
mRunNumRegions
;
#
if
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
run
-
>
mMagic
=
ARENA_RUN_MAGIC
;
#
endif
aBin
-
>
mNumRuns
+
+
;
return
run
;
}
void
arena_bin_t
:
:
Init
(
SizeClass
aSizeClass
)
{
size_t
try_run_size
;
unsigned
try_nregs
try_mask_nelms
try_reg0_offset
;
static
const
size_t
kFixedHeaderSize
=
offsetof
(
arena_run_t
mRegionsMask
)
;
MOZ_ASSERT
(
aSizeClass
.
Size
(
)
<
=
gMaxBinClass
)
;
try_run_size
=
gPageSize
;
mCurrentRun
=
nullptr
;
mNonFullRuns
.
Init
(
)
;
mSizeClass
=
aSizeClass
.
Size
(
)
;
mNumRuns
=
0
;
while
(
true
)
{
try_nregs
=
(
(
try_run_size
-
kFixedHeaderSize
)
/
mSizeClass
)
+
1
;
do
{
try_nregs
-
-
;
try_mask_nelms
=
(
try_nregs
>
>
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
+
(
(
try_nregs
&
(
(
1U
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
-
1
)
)
?
1
:
0
)
;
try_reg0_offset
=
try_run_size
-
(
try_nregs
*
mSizeClass
)
;
}
while
(
kFixedHeaderSize
+
(
sizeof
(
unsigned
)
*
try_mask_nelms
)
>
try_reg0_offset
)
;
if
(
Fraction
(
try_reg0_offset
try_run_size
)
<
=
kRunOverhead
)
{
break
;
}
if
(
try_reg0_offset
>
mSizeClass
)
{
if
(
Fraction
(
try_reg0_offset
try_run_size
)
<
=
kRunRelaxedOverhead
)
{
break
;
}
}
if
(
try_mask_nelms
*
sizeof
(
unsigned
)
>
=
kFixedHeaderSize
)
{
break
;
}
if
(
try_run_size
+
gPageSize
>
gMaxLargeClass
)
{
break
;
}
try_run_size
+
=
gPageSize
;
}
MOZ_ASSERT
(
kFixedHeaderSize
+
(
sizeof
(
unsigned
)
*
try_mask_nelms
)
<
=
try_reg0_offset
)
;
MOZ_ASSERT
(
(
try_mask_nelms
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
>
=
try_nregs
)
;
MOZ_ASSERT
(
(
try_run_size
>
>
gPageSize2Pow
)
<
=
UINT8_MAX
)
;
mRunSizePages
=
static_cast
<
uint8_t
>
(
try_run_size
>
>
gPageSize2Pow
)
;
mRunNumRegions
=
try_nregs
;
mRunNumRegionsMask
=
try_mask_nelms
;
mRunFirstRegionOffset
=
try_reg0_offset
;
mSizeDivisor
=
FastDivisor
<
uint16_t
>
(
aSizeClass
.
Size
(
)
try_run_size
)
;
}
void
*
arena_t
:
:
MallocSmall
(
size_t
aSize
bool
aZero
)
{
void
*
ret
;
arena_bin_t
*
bin
;
arena_run_t
*
run
;
SizeClass
sizeClass
(
aSize
)
;
aSize
=
sizeClass
.
Size
(
)
;
switch
(
sizeClass
.
Type
(
)
)
{
case
SizeClass
:
:
Tiny
:
bin
=
&
mBins
[
FloorLog2
(
aSize
/
kMinTinyClass
)
]
;
break
;
case
SizeClass
:
:
Quantum
:
bin
=
&
mBins
[
kNumTinyClasses
+
(
aSize
/
kQuantum
)
-
(
kMinQuantumClass
/
kQuantum
)
]
;
break
;
case
SizeClass
:
:
QuantumWide
:
bin
=
&
mBins
[
kNumTinyClasses
+
kNumQuantumClasses
+
(
aSize
/
kQuantumWide
)
-
(
kMinQuantumWideClass
/
kQuantumWide
)
]
;
break
;
case
SizeClass
:
:
SubPage
:
bin
=
&
mBins
[
kNumTinyClasses
+
kNumQuantumClasses
+
kNumQuantumWideClasses
+
(
FloorLog2
(
aSize
)
-
LOG2
(
kMinSubPageClass
)
)
]
;
break
;
default
:
MOZ_MAKE_COMPILER_ASSUME_IS_UNREACHABLE
(
"
Unexpected
size
class
type
"
)
;
}
MOZ_DIAGNOSTIC_ASSERT
(
aSize
=
=
bin
-
>
mSizeClass
)
;
{
MaybeMutexAutoLock
lock
(
mLock
)
;
if
(
MOZ_UNLIKELY
(
mRandomizeSmallAllocations
&
&
mPRNG
=
=
nullptr
&
&
!
mIsPRNGInitializing
)
)
{
mIsPRNGInitializing
=
true
;
mozilla
:
:
non_crypto
:
:
XorShift128PlusRNG
*
prng
;
{
mLock
.
Unlock
(
)
;
mozilla
:
:
Maybe
<
uint64_t
>
prngState1
=
mozilla
:
:
RandomUint64
(
)
;
mozilla
:
:
Maybe
<
uint64_t
>
prngState2
=
mozilla
:
:
RandomUint64
(
)
;
void
*
backing
=
base_alloc
(
sizeof
(
mozilla
:
:
non_crypto
:
:
XorShift128PlusRNG
)
)
;
prng
=
new
(
backing
)
mozilla
:
:
non_crypto
:
:
XorShift128PlusRNG
(
prngState1
.
valueOr
(
0
)
prngState2
.
valueOr
(
0
)
)
;
mLock
.
Lock
(
)
;
}
mPRNG
=
prng
;
mIsPRNGInitializing
=
false
;
}
MOZ_ASSERT
(
!
mRandomizeSmallAllocations
|
|
mPRNG
)
;
run
=
bin
-
>
mCurrentRun
;
if
(
MOZ_UNLIKELY
(
!
run
|
|
run
-
>
mNumFree
=
=
0
)
)
{
run
=
bin
-
>
mCurrentRun
=
GetNonFullBinRun
(
bin
)
;
}
if
(
MOZ_UNLIKELY
(
!
run
)
)
{
return
nullptr
;
}
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
mMagic
=
=
ARENA_RUN_MAGIC
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
mNumFree
>
0
)
;
ret
=
ArenaRunRegAlloc
(
run
bin
)
;
MOZ_DIAGNOSTIC_ASSERT
(
ret
)
;
run
-
>
mNumFree
-
-
;
if
(
!
ret
)
{
return
nullptr
;
}
mStats
.
allocated_small
+
=
aSize
;
}
if
(
!
aZero
)
{
ApplyZeroOrJunk
(
ret
aSize
)
;
}
else
{
memset
(
ret
0
aSize
)
;
}
return
ret
;
}
void
*
arena_t
:
:
MallocLarge
(
size_t
aSize
bool
aZero
)
{
void
*
ret
;
aSize
=
PAGE_CEILING
(
aSize
)
;
{
MaybeMutexAutoLock
lock
(
mLock
)
;
ret
=
AllocRun
(
aSize
true
aZero
)
;
if
(
!
ret
)
{
return
nullptr
;
}
mStats
.
allocated_large
+
=
aSize
;
}
if
(
!
aZero
)
{
ApplyZeroOrJunk
(
ret
aSize
)
;
}
return
ret
;
}
void
*
arena_t
:
:
Malloc
(
size_t
aSize
bool
aZero
)
{
MOZ_DIAGNOSTIC_ASSERT
(
mMagic
=
=
ARENA_MAGIC
)
;
MOZ_ASSERT
(
aSize
!
=
0
)
;
if
(
aSize
<
=
gMaxBinClass
)
{
return
MallocSmall
(
aSize
aZero
)
;
}
if
(
aSize
<
=
gMaxLargeClass
)
{
return
MallocLarge
(
aSize
aZero
)
;
}
return
MallocHuge
(
aSize
aZero
)
;
}
void
*
arena_t
:
:
PallocLarge
(
size_t
aAlignment
size_t
aSize
size_t
aAllocSize
)
{
void
*
ret
;
size_t
offset
;
arena_chunk_t
*
chunk
;
MOZ_ASSERT
(
(
aSize
&
gPageSizeMask
)
=
=
0
)
;
MOZ_ASSERT
(
(
aAlignment
&
gPageSizeMask
)
=
=
0
)
;
{
MaybeMutexAutoLock
lock
(
mLock
)
;
ret
=
AllocRun
(
aAllocSize
true
false
)
;
if
(
!
ret
)
{
return
nullptr
;
}
chunk
=
GetChunkForPtr
(
ret
)
;
offset
=
uintptr_t
(
ret
)
&
(
aAlignment
-
1
)
;
MOZ_ASSERT
(
(
offset
&
gPageSizeMask
)
=
=
0
)
;
MOZ_ASSERT
(
offset
<
aAllocSize
)
;
if
(
offset
=
=
0
)
{
TrimRunTail
(
chunk
(
arena_run_t
*
)
ret
aAllocSize
aSize
false
)
;
}
else
{
size_t
leadsize
trailsize
;
leadsize
=
aAlignment
-
offset
;
if
(
leadsize
>
0
)
{
TrimRunHead
(
chunk
(
arena_run_t
*
)
ret
aAllocSize
aAllocSize
-
leadsize
)
;
ret
=
(
void
*
)
(
uintptr_t
(
ret
)
+
leadsize
)
;
}
trailsize
=
aAllocSize
-
leadsize
-
aSize
;
if
(
trailsize
!
=
0
)
{
MOZ_ASSERT
(
trailsize
<
aAllocSize
)
;
TrimRunTail
(
chunk
(
arena_run_t
*
)
ret
aSize
+
trailsize
aSize
false
)
;
}
}
mStats
.
allocated_large
+
=
aSize
;
}
ApplyZeroOrJunk
(
ret
aSize
)
;
return
ret
;
}
void
*
arena_t
:
:
Palloc
(
size_t
aAlignment
size_t
aSize
)
{
void
*
ret
;
size_t
ceil_size
;
ceil_size
=
ALIGNMENT_CEILING
(
aSize
aAlignment
)
;
if
(
ceil_size
<
aSize
)
{
return
nullptr
;
}
if
(
ceil_size
<
=
gPageSize
|
|
(
aAlignment
<
=
gPageSize
&
&
ceil_size
<
=
gMaxLargeClass
)
)
{
ret
=
Malloc
(
ceil_size
false
)
;
}
else
{
size_t
run_size
;
aAlignment
=
PAGE_CEILING
(
aAlignment
)
;
ceil_size
=
PAGE_CEILING
(
aSize
)
;
if
(
ceil_size
<
aSize
|
|
ceil_size
+
aAlignment
<
ceil_size
)
{
return
nullptr
;
}
if
(
ceil_size
>
=
aAlignment
)
{
run_size
=
ceil_size
+
aAlignment
-
gPageSize
;
}
else
{
run_size
=
(
aAlignment
<
<
1
)
-
gPageSize
;
}
if
(
run_size
<
=
gMaxLargeClass
)
{
ret
=
PallocLarge
(
aAlignment
ceil_size
run_size
)
;
}
else
if
(
aAlignment
<
=
kChunkSize
)
{
ret
=
MallocHuge
(
ceil_size
false
)
;
}
else
{
ret
=
PallocHuge
(
ceil_size
aAlignment
false
)
;
}
}
MOZ_ASSERT
(
(
uintptr_t
(
ret
)
&
(
aAlignment
-
1
)
)
=
=
0
)
;
return
ret
;
}
class
AllocInfo
{
public
:
template
<
bool
Validate
=
false
>
static
inline
AllocInfo
Get
(
const
void
*
aPtr
)
{
if
(
Validate
&
&
!
malloc_initialized
)
{
return
AllocInfo
(
)
;
}
auto
chunk
=
GetChunkForPtr
(
aPtr
)
;
if
(
Validate
)
{
if
(
!
chunk
|
|
!
gChunkRTree
.
Get
(
chunk
)
)
{
return
AllocInfo
(
)
;
}
}
if
(
chunk
!
=
aPtr
)
{
MOZ_DIAGNOSTIC_ASSERT
(
chunk
-
>
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
size_t
pageind
=
(
(
(
uintptr_t
)
aPtr
-
(
uintptr_t
)
chunk
)
>
>
gPageSize2Pow
)
;
return
GetInChunk
(
aPtr
chunk
pageind
)
;
}
extent_node_t
key
;
key
.
mAddr
=
chunk
;
MutexAutoLock
lock
(
huge_mtx
)
;
extent_node_t
*
node
=
huge
.
Search
(
&
key
)
;
if
(
Validate
&
&
!
node
)
{
return
AllocInfo
(
)
;
}
return
AllocInfo
(
node
-
>
mSize
node
)
;
}
static
inline
AllocInfo
GetInChunk
(
const
void
*
aPtr
arena_chunk_t
*
aChunk
size_t
pageind
)
{
size_t
mapbits
=
aChunk
-
>
map
[
pageind
]
.
bits
;
MOZ_DIAGNOSTIC_ASSERT
(
(
mapbits
&
CHUNK_MAP_ALLOCATED
)
!
=
0
)
;
size_t
size
;
if
(
(
mapbits
&
CHUNK_MAP_LARGE
)
=
=
0
)
{
arena_run_t
*
run
=
(
arena_run_t
*
)
(
mapbits
&
~
gPageSizeMask
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
mMagic
=
=
ARENA_RUN_MAGIC
)
;
size
=
run
-
>
mBin
-
>
mSizeClass
;
}
else
{
size
=
mapbits
&
~
gPageSizeMask
;
MOZ_DIAGNOSTIC_ASSERT
(
size
!
=
0
)
;
}
return
AllocInfo
(
size
aChunk
)
;
}
static
inline
AllocInfo
GetValidated
(
const
void
*
aPtr
)
{
return
Get
<
true
>
(
aPtr
)
;
}
AllocInfo
(
)
:
mSize
(
0
)
mChunk
(
nullptr
)
{
}
explicit
AllocInfo
(
size_t
aSize
arena_chunk_t
*
aChunk
)
:
mSize
(
aSize
)
mChunk
(
aChunk
)
{
MOZ_ASSERT
(
mSize
<
=
gMaxLargeClass
)
;
}
explicit
AllocInfo
(
size_t
aSize
extent_node_t
*
aNode
)
:
mSize
(
aSize
)
mNode
(
aNode
)
{
MOZ_ASSERT
(
mSize
>
gMaxLargeClass
)
;
}
size_t
Size
(
)
{
return
mSize
;
}
arena_t
*
Arena
(
)
{
if
(
mSize
<
=
gMaxLargeClass
)
{
return
mChunk
-
>
arena
;
}
MOZ_RELEASE_ASSERT
(
mNode
-
>
mArenaId
=
=
mNode
-
>
mArena
-
>
mId
)
;
return
mNode
-
>
mArena
;
}
bool
IsValid
(
)
const
{
return
!
!
mSize
;
}
private
:
size_t
mSize
;
union
{
arena_chunk_t
*
mChunk
;
extent_node_t
*
mNode
;
}
;
}
;
inline
void
MozJemalloc
:
:
jemalloc_ptr_info
(
const
void
*
aPtr
jemalloc_ptr_info_t
*
aInfo
)
{
arena_chunk_t
*
chunk
=
GetChunkForPtr
(
aPtr
)
;
if
(
!
chunk
|
|
!
malloc_initialized
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
return
;
}
extent_node_t
*
node
;
extent_node_t
key
;
{
MutexAutoLock
lock
(
huge_mtx
)
;
key
.
mAddr
=
const_cast
<
void
*
>
(
aPtr
)
;
node
=
reinterpret_cast
<
RedBlackTree
<
extent_node_t
ExtentTreeBoundsTrait
>
*
>
(
&
huge
)
-
>
Search
(
&
key
)
;
if
(
node
)
{
*
aInfo
=
{
TagLiveAlloc
node
-
>
mAddr
node
-
>
mSize
node
-
>
mArena
-
>
mId
}
;
return
;
}
}
if
(
!
gChunkRTree
.
Get
(
chunk
)
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
return
;
}
MOZ_DIAGNOSTIC_ASSERT
(
chunk
-
>
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
size_t
pageind
=
(
(
(
uintptr_t
)
aPtr
-
(
uintptr_t
)
chunk
)
>
>
gPageSize2Pow
)
;
if
(
pageind
<
gChunkHeaderNumPages
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
return
;
}
size_t
mapbits
=
chunk
-
>
map
[
pageind
]
.
bits
;
if
(
!
(
mapbits
&
CHUNK_MAP_ALLOCATED
)
)
{
void
*
pageaddr
=
(
void
*
)
(
uintptr_t
(
aPtr
)
&
~
gPageSizeMask
)
;
*
aInfo
=
{
TagFreedPage
pageaddr
gPageSize
chunk
-
>
arena
-
>
mId
}
;
return
;
}
if
(
mapbits
&
CHUNK_MAP_LARGE
)
{
size_t
size
;
while
(
true
)
{
size
=
mapbits
&
~
gPageSizeMask
;
if
(
size
!
=
0
)
{
break
;
}
pageind
-
-
;
MOZ_DIAGNOSTIC_ASSERT
(
pageind
>
=
gChunkHeaderNumPages
)
;
if
(
pageind
<
gChunkHeaderNumPages
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
return
;
}
mapbits
=
chunk
-
>
map
[
pageind
]
.
bits
;
MOZ_DIAGNOSTIC_ASSERT
(
mapbits
&
CHUNK_MAP_LARGE
)
;
if
(
!
(
mapbits
&
CHUNK_MAP_LARGE
)
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
return
;
}
}
void
*
addr
=
(
(
char
*
)
chunk
)
+
(
pageind
<
<
gPageSize2Pow
)
;
*
aInfo
=
{
TagLiveAlloc
addr
size
chunk
-
>
arena
-
>
mId
}
;
return
;
}
auto
run
=
(
arena_run_t
*
)
(
mapbits
&
~
gPageSizeMask
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
mMagic
=
=
ARENA_RUN_MAGIC
)
;
size_t
size
=
run
-
>
mBin
-
>
mSizeClass
;
uintptr_t
reg0_addr
=
(
uintptr_t
)
run
+
run
-
>
mBin
-
>
mRunFirstRegionOffset
;
if
(
aPtr
<
(
void
*
)
reg0_addr
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
return
;
}
unsigned
regind
=
(
(
uintptr_t
)
aPtr
-
reg0_addr
)
/
size
;
void
*
addr
=
(
void
*
)
(
reg0_addr
+
regind
*
size
)
;
unsigned
elm
=
regind
>
>
(
LOG2
(
sizeof
(
int
)
)
+
3
)
;
unsigned
bit
=
regind
-
(
elm
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
;
PtrInfoTag
tag
=
(
(
run
-
>
mRegionsMask
[
elm
]
&
(
1U
<
<
bit
)
)
)
?
TagFreedAlloc
:
TagLiveAlloc
;
*
aInfo
=
{
tag
addr
size
chunk
-
>
arena
-
>
mId
}
;
}
namespace
Debug
{
MOZ_NEVER_INLINE
jemalloc_ptr_info_t
*
jemalloc_ptr_info
(
const
void
*
aPtr
)
{
static
jemalloc_ptr_info_t
info
;
MozJemalloc
:
:
jemalloc_ptr_info
(
aPtr
&
info
)
;
return
&
info
;
}
}
arena_chunk_t
*
arena_t
:
:
DallocSmall
(
arena_chunk_t
*
aChunk
void
*
aPtr
arena_chunk_map_t
*
aMapElm
)
{
arena_run_t
*
run
;
arena_bin_t
*
bin
;
size_t
size
;
run
=
(
arena_run_t
*
)
(
aMapElm
-
>
bits
&
~
gPageSizeMask
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
mMagic
=
=
ARENA_RUN_MAGIC
)
;
bin
=
run
-
>
mBin
;
size
=
bin
-
>
mSizeClass
;
MOZ_DIAGNOSTIC_ASSERT
(
uintptr_t
(
aPtr
)
>
=
uintptr_t
(
run
)
+
bin
-
>
mRunFirstRegionOffset
)
;
arena_run_reg_dalloc
(
run
bin
aPtr
size
)
;
run
-
>
mNumFree
+
+
;
arena_chunk_t
*
dealloc_chunk
=
nullptr
;
if
(
run
-
>
mNumFree
=
=
bin
-
>
mRunNumRegions
)
{
if
(
run
=
=
bin
-
>
mCurrentRun
)
{
bin
-
>
mCurrentRun
=
nullptr
;
}
else
if
(
bin
-
>
mRunNumRegions
!
=
1
)
{
size_t
run_pageind
=
(
uintptr_t
(
run
)
-
uintptr_t
(
aChunk
)
)
>
>
gPageSize2Pow
;
arena_chunk_map_t
*
run_mapelm
=
&
aChunk
-
>
map
[
run_pageind
]
;
MOZ_DIAGNOSTIC_ASSERT
(
bin
-
>
mNonFullRuns
.
Search
(
run_mapelm
)
=
=
run_mapelm
)
;
bin
-
>
mNonFullRuns
.
Remove
(
run_mapelm
)
;
}
#
if
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
run
-
>
mMagic
=
0
;
#
endif
dealloc_chunk
=
DallocRun
(
run
true
)
;
bin
-
>
mNumRuns
-
-
;
}
else
if
(
run
-
>
mNumFree
=
=
1
&
&
run
!
=
bin
-
>
mCurrentRun
)
{
if
(
!
bin
-
>
mCurrentRun
)
{
bin
-
>
mCurrentRun
=
run
;
}
else
if
(
uintptr_t
(
run
)
<
uintptr_t
(
bin
-
>
mCurrentRun
)
)
{
if
(
bin
-
>
mCurrentRun
-
>
mNumFree
>
0
)
{
arena_chunk_t
*
runcur_chunk
=
GetChunkForPtr
(
bin
-
>
mCurrentRun
)
;
size_t
runcur_pageind
=
(
uintptr_t
(
bin
-
>
mCurrentRun
)
-
uintptr_t
(
runcur_chunk
)
)
>
>
gPageSize2Pow
;
arena_chunk_map_t
*
runcur_mapelm
=
&
runcur_chunk
-
>
map
[
runcur_pageind
]
;
MOZ_DIAGNOSTIC_ASSERT
(
!
bin
-
>
mNonFullRuns
.
Search
(
runcur_mapelm
)
)
;
bin
-
>
mNonFullRuns
.
Insert
(
runcur_mapelm
)
;
}
bin
-
>
mCurrentRun
=
run
;
}
else
{
size_t
run_pageind
=
(
uintptr_t
(
run
)
-
uintptr_t
(
aChunk
)
)
>
>
gPageSize2Pow
;
arena_chunk_map_t
*
run_mapelm
=
&
aChunk
-
>
map
[
run_pageind
]
;
MOZ_DIAGNOSTIC_ASSERT
(
bin
-
>
mNonFullRuns
.
Search
(
run_mapelm
)
=
=
nullptr
)
;
bin
-
>
mNonFullRuns
.
Insert
(
run_mapelm
)
;
}
}
mStats
.
allocated_small
-
=
size
;
return
dealloc_chunk
;
}
arena_chunk_t
*
arena_t
:
:
DallocLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
)
{
MOZ_DIAGNOSTIC_ASSERT
(
(
uintptr_t
(
aPtr
)
&
gPageSizeMask
)
=
=
0
)
;
size_t
pageind
=
(
uintptr_t
(
aPtr
)
-
uintptr_t
(
aChunk
)
)
>
>
gPageSize2Pow
;
size_t
size
=
aChunk
-
>
map
[
pageind
]
.
bits
&
~
gPageSizeMask
;
mStats
.
allocated_large
-
=
size
;
return
DallocRun
(
(
arena_run_t
*
)
aPtr
true
)
;
}
static
inline
void
arena_dalloc
(
void
*
aPtr
size_t
aOffset
arena_t
*
aArena
)
{
MOZ_ASSERT
(
aPtr
)
;
MOZ_ASSERT
(
aOffset
!
=
0
)
;
MOZ_ASSERT
(
GetChunkOffsetForPtr
(
aPtr
)
=
=
aOffset
)
;
auto
chunk
=
(
arena_chunk_t
*
)
(
(
uintptr_t
)
aPtr
-
aOffset
)
;
auto
arena
=
chunk
-
>
arena
;
MOZ_ASSERT
(
arena
)
;
MOZ_DIAGNOSTIC_ASSERT
(
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
MOZ_RELEASE_ASSERT
(
!
aArena
|
|
arena
=
=
aArena
)
;
size_t
pageind
=
aOffset
>
>
gPageSize2Pow
;
if
(
opt_poison
)
{
AllocInfo
info
=
AllocInfo
:
:
GetInChunk
(
aPtr
chunk
pageind
)
;
MOZ_ASSERT
(
info
.
IsValid
(
)
)
;
MaybePoison
(
aPtr
info
.
Size
(
)
)
;
}
arena_chunk_t
*
chunk_dealloc_delay
=
nullptr
;
{
MaybeMutexAutoLock
lock
(
arena
-
>
mLock
)
;
arena_chunk_map_t
*
mapelm
=
&
chunk
-
>
map
[
pageind
]
;
MOZ_RELEASE_ASSERT
(
(
mapelm
-
>
bits
&
(
CHUNK_MAP_FRESH_MADVISED_OR_DECOMMITTED
|
CHUNK_MAP_ZEROED
)
)
=
=
0
"
Freeing
in
a
page
with
bad
bits
.
"
)
;
MOZ_RELEASE_ASSERT
(
(
mapelm
-
>
bits
&
CHUNK_MAP_ALLOCATED
)
!
=
0
"
Double
-
free
?
"
)
;
if
(
(
mapelm
-
>
bits
&
CHUNK_MAP_LARGE
)
=
=
0
)
{
chunk_dealloc_delay
=
arena
-
>
DallocSmall
(
chunk
aPtr
mapelm
)
;
}
else
{
chunk_dealloc_delay
=
arena
-
>
DallocLarge
(
chunk
aPtr
)
;
}
}
if
(
chunk_dealloc_delay
)
{
chunk_dealloc
(
(
void
*
)
chunk_dealloc_delay
kChunkSize
ARENA_CHUNK
)
;
}
}
static
inline
void
idalloc
(
void
*
ptr
arena_t
*
aArena
)
{
size_t
offset
;
MOZ_ASSERT
(
ptr
)
;
offset
=
GetChunkOffsetForPtr
(
ptr
)
;
if
(
offset
!
=
0
)
{
arena_dalloc
(
ptr
offset
aArena
)
;
}
else
{
huge_dalloc
(
ptr
aArena
)
;
}
}
void
arena_t
:
:
RallocShrinkLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
{
MOZ_ASSERT
(
aSize
<
aOldSize
)
;
MaybeMutexAutoLock
lock
(
mLock
)
;
TrimRunTail
(
aChunk
(
arena_run_t
*
)
aPtr
aOldSize
aSize
true
)
;
mStats
.
allocated_large
-
=
aOldSize
-
aSize
;
}
bool
arena_t
:
:
RallocGrowLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
{
size_t
pageind
=
(
uintptr_t
(
aPtr
)
-
uintptr_t
(
aChunk
)
)
>
>
gPageSize2Pow
;
size_t
npages
=
aOldSize
>
>
gPageSize2Pow
;
MaybeMutexAutoLock
lock
(
mLock
)
;
MOZ_DIAGNOSTIC_ASSERT
(
aOldSize
=
=
(
aChunk
-
>
map
[
pageind
]
.
bits
&
~
gPageSizeMask
)
)
;
MOZ_ASSERT
(
aSize
>
aOldSize
)
;
if
(
pageind
+
npages
<
gChunkNumPages
-
1
&
&
(
aChunk
-
>
map
[
pageind
+
npages
]
.
bits
&
CHUNK_MAP_ALLOCATED
)
=
=
0
&
&
(
aChunk
-
>
map
[
pageind
+
npages
]
.
bits
&
~
gPageSizeMask
)
>
=
aSize
-
aOldSize
)
{
if
(
!
SplitRun
(
(
arena_run_t
*
)
(
uintptr_t
(
aChunk
)
+
(
(
pageind
+
npages
)
<
<
gPageSize2Pow
)
)
aSize
-
aOldSize
true
false
)
)
{
return
false
;
}
aChunk
-
>
map
[
pageind
]
.
bits
=
aSize
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
aChunk
-
>
map
[
pageind
+
npages
]
.
bits
=
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
mStats
.
allocated_large
+
=
aSize
-
aOldSize
;
return
true
;
}
return
false
;
}
void
*
arena_t
:
:
RallocSmallOrLarge
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
{
void
*
ret
;
size_t
copysize
;
SizeClass
sizeClass
(
aSize
)
;
if
(
aOldSize
<
=
gMaxLargeClass
&
&
sizeClass
.
Size
(
)
=
=
aOldSize
)
{
if
(
aSize
<
aOldSize
)
{
MaybePoison
(
(
void
*
)
(
uintptr_t
(
aPtr
)
+
aSize
)
aOldSize
-
aSize
)
;
}
return
aPtr
;
}
if
(
sizeClass
.
Type
(
)
=
=
SizeClass
:
:
Large
&
&
aOldSize
>
gMaxBinClass
&
&
aOldSize
<
=
gMaxLargeClass
)
{
arena_chunk_t
*
chunk
=
GetChunkForPtr
(
aPtr
)
;
if
(
sizeClass
.
Size
(
)
<
aOldSize
)
{
MaybePoison
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
aSize
)
aOldSize
-
aSize
)
;
RallocShrinkLarge
(
chunk
aPtr
sizeClass
.
Size
(
)
aOldSize
)
;
return
aPtr
;
}
if
(
RallocGrowLarge
(
chunk
aPtr
sizeClass
.
Size
(
)
aOldSize
)
)
{
ApplyZeroOrJunk
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
aOldSize
)
aSize
-
aOldSize
)
;
return
aPtr
;
}
}
ret
=
(
mIsPrivate
?
this
:
choose_arena
(
aSize
)
)
-
>
Malloc
(
aSize
false
)
;
if
(
!
ret
)
{
return
nullptr
;
}
copysize
=
(
aSize
<
aOldSize
)
?
aSize
:
aOldSize
;
#
ifdef
VM_COPY_MIN
if
(
copysize
>
=
VM_COPY_MIN
)
{
pages_copy
(
ret
aPtr
copysize
)
;
}
else
#
endif
{
memcpy
(
ret
aPtr
copysize
)
;
}
idalloc
(
aPtr
this
)
;
return
ret
;
}
void
*
arena_t
:
:
Ralloc
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
{
MOZ_DIAGNOSTIC_ASSERT
(
mMagic
=
=
ARENA_MAGIC
)
;
MOZ_ASSERT
(
aPtr
)
;
MOZ_ASSERT
(
aSize
!
=
0
)
;
return
(
aSize
<
=
gMaxLargeClass
)
?
RallocSmallOrLarge
(
aPtr
aSize
aOldSize
)
:
RallocHuge
(
aPtr
aSize
aOldSize
)
;
}
void
*
arena_t
:
:
operator
new
(
size_t
aCount
const
fallible_t
&
)
noexcept
{
MOZ_ASSERT
(
aCount
=
=
sizeof
(
arena_t
)
)
;
return
TypedBaseAlloc
<
arena_t
>
:
:
alloc
(
)
;
}
void
arena_t
:
:
operator
delete
(
void
*
aPtr
)
{
TypedBaseAlloc
<
arena_t
>
:
:
dealloc
(
(
arena_t
*
)
aPtr
)
;
}
arena_t
:
:
arena_t
(
arena_params_t
*
aParams
bool
aIsPrivate
)
{
unsigned
i
;
memset
(
&
mLink
0
sizeof
(
mLink
)
)
;
memset
(
&
mStats
0
sizeof
(
arena_stats_t
)
)
;
mId
=
0
;
mChunksDirty
.
Init
(
)
;
#
ifdef
MALLOC_DOUBLE_PURGE
new
(
&
mChunksMAdvised
)
DoublyLinkedList
<
arena_chunk_t
>
(
)
;
#
endif
mSpare
=
nullptr
;
mRandomizeSmallAllocations
=
opt_randomize_small
;
MaybeMutex
:
:
DoLock
doLock
=
MaybeMutex
:
:
MUST_LOCK
;
if
(
aParams
)
{
uint32_t
randFlags
=
aParams
-
>
mFlags
&
ARENA_FLAG_RANDOMIZE_SMALL_MASK
;
switch
(
randFlags
)
{
case
ARENA_FLAG_RANDOMIZE_SMALL_ENABLED
:
mRandomizeSmallAllocations
=
true
;
break
;
case
ARENA_FLAG_RANDOMIZE_SMALL_DISABLED
:
mRandomizeSmallAllocations
=
false
;
break
;
case
ARENA_FLAG_RANDOMIZE_SMALL_DEFAULT
:
default
:
break
;
}
uint32_t
threadFlags
=
aParams
-
>
mFlags
&
ARENA_FLAG_THREAD_MASK
;
if
(
threadFlags
=
=
ARENA_FLAG_THREAD_MAIN_THREAD_ONLY
)
{
MOZ_ASSERT
(
gArenas
.
IsOnMainThread
(
)
)
;
MOZ_ASSERT
(
aIsPrivate
)
;
doLock
=
MaybeMutex
:
:
AVOID_LOCK_UNSAFE
;
}
mMaxDirtyIncreaseOverride
=
aParams
-
>
mMaxDirtyIncreaseOverride
;
mMaxDirtyDecreaseOverride
=
aParams
-
>
mMaxDirtyDecreaseOverride
;
}
else
{
mMaxDirtyIncreaseOverride
=
0
;
mMaxDirtyDecreaseOverride
=
0
;
}
MOZ_RELEASE_ASSERT
(
mLock
.
Init
(
doLock
)
)
;
mPRNG
=
nullptr
;
mIsPRNGInitializing
=
false
;
mIsPrivate
=
aIsPrivate
;
mNumDirty
=
0
;
mNumFresh
=
0
;
mNumMAdvised
=
0
;
mMaxDirty
=
(
aParams
&
&
aParams
-
>
mMaxDirty
)
?
aParams
-
>
mMaxDirty
:
(
opt_dirty_max
/
8
)
;
mRunsAvail
.
Init
(
)
;
SizeClass
sizeClass
(
1
)
;
for
(
i
=
0
;
;
i
+
+
)
{
arena_bin_t
&
bin
=
mBins
[
i
]
;
bin
.
Init
(
sizeClass
)
;
if
(
sizeClass
.
Size
(
)
=
=
gMaxBinClass
)
{
break
;
}
sizeClass
=
sizeClass
.
Next
(
)
;
}
MOZ_ASSERT
(
i
=
=
NUM_SMALL_CLASSES
-
1
)
;
#
if
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
mMagic
=
ARENA_MAGIC
;
#
endif
}
arena_t
:
:
~
arena_t
(
)
{
size_t
i
;
MaybeMutexAutoLock
lock
(
mLock
)
;
MOZ_RELEASE_ASSERT
(
!
mLink
.
Left
(
)
&
&
!
mLink
.
Right
(
)
"
Arena
is
still
registered
"
)
;
MOZ_RELEASE_ASSERT
(
!
mStats
.
allocated_small
&
&
!
mStats
.
allocated_large
"
Arena
is
not
empty
"
)
;
if
(
mSpare
)
{
chunk_dealloc
(
mSpare
kChunkSize
ARENA_CHUNK
)
;
}
for
(
i
=
0
;
i
<
NUM_SMALL_CLASSES
;
i
+
+
)
{
MOZ_RELEASE_ASSERT
(
!
mBins
[
i
]
.
mNonFullRuns
.
First
(
)
"
Bin
is
not
empty
"
)
;
}
#
ifdef
MOZ_DEBUG
{
MutexAutoLock
lock
(
huge_mtx
)
;
for
(
auto
node
:
huge
.
iter
(
)
)
{
MOZ_RELEASE_ASSERT
(
node
-
>
mArenaId
!
=
mId
"
Arena
has
huge
allocations
"
)
;
}
}
#
endif
mId
=
0
;
}
arena_t
*
ArenaCollection
:
:
CreateArena
(
bool
aIsPrivate
arena_params_t
*
aParams
)
{
arena_t
*
ret
=
new
(
fallible
)
arena_t
(
aParams
aIsPrivate
)
;
if
(
!
ret
)
{
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Error
initializing
arena
\
n
"
)
;
return
mDefaultArena
;
}
MutexAutoLock
lock
(
mLock
)
;
if
(
!
aIsPrivate
)
{
ret
-
>
mId
=
mLastPublicArenaId
+
+
;
mArenas
.
Insert
(
ret
)
;
return
ret
;
}
Tree
&
tree
=
(
ret
-
>
IsMainThreadOnly
(
)
)
?
mMainThreadArenas
:
mPrivateArenas
;
arena_id_t
arena_id
;
do
{
arena_id
=
MakeRandArenaId
(
ret
-
>
IsMainThreadOnly
(
)
)
;
}
while
(
GetByIdInternal
(
tree
arena_id
)
)
;
ret
-
>
mId
=
arena_id
;
tree
.
Insert
(
ret
)
;
return
ret
;
}
arena_id_t
ArenaCollection
:
:
MakeRandArenaId
(
bool
aIsMainThreadOnly
)
const
{
uint64_t
rand
;
do
{
mozilla
:
:
Maybe
<
uint64_t
>
maybeRandomId
=
mozilla
:
:
RandomUint64
(
)
;
MOZ_RELEASE_ASSERT
(
maybeRandomId
.
isSome
(
)
)
;
rand
=
maybeRandomId
.
value
(
)
;
if
(
aIsMainThreadOnly
)
{
rand
=
rand
|
MAIN_THREAD_ARENA_BIT
;
}
else
{
rand
=
rand
&
~
MAIN_THREAD_ARENA_BIT
;
}
}
while
(
rand
=
=
0
)
;
return
arena_id_t
(
rand
)
;
}
void
*
arena_t
:
:
MallocHuge
(
size_t
aSize
bool
aZero
)
{
return
PallocHuge
(
aSize
kChunkSize
aZero
)
;
}
void
*
arena_t
:
:
PallocHuge
(
size_t
aSize
size_t
aAlignment
bool
aZero
)
{
void
*
ret
;
size_t
csize
;
size_t
psize
;
extent_node_t
*
node
;
csize
=
CHUNK_CEILING
(
aSize
+
gPageSize
)
;
if
(
csize
<
aSize
)
{
return
nullptr
;
}
node
=
ExtentAlloc
:
:
alloc
(
)
;
if
(
!
node
)
{
return
nullptr
;
}
ret
=
chunk_alloc
(
csize
aAlignment
false
)
;
if
(
!
ret
)
{
ExtentAlloc
:
:
dealloc
(
node
)
;
return
nullptr
;
}
psize
=
PAGE_CEILING
(
aSize
)
;
#
ifdef
MOZ_DEBUG
if
(
aZero
)
{
chunk_assert_zero
(
ret
psize
)
;
}
#
endif
node
-
>
mAddr
=
ret
;
node
-
>
mSize
=
psize
;
node
-
>
mArena
=
this
;
node
-
>
mArenaId
=
mId
;
{
MutexAutoLock
lock
(
huge_mtx
)
;
huge
.
Insert
(
node
)
;
huge_allocated
+
=
psize
;
huge_mapped
+
=
csize
;
}
pages_decommit
(
(
void
*
)
(
(
uintptr_t
)
ret
+
psize
)
csize
-
psize
)
;
if
(
!
aZero
)
{
ApplyZeroOrJunk
(
ret
psize
)
;
}
return
ret
;
}
void
*
arena_t
:
:
RallocHuge
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
{
void
*
ret
;
size_t
copysize
;
if
(
aOldSize
>
gMaxLargeClass
&
&
CHUNK_CEILING
(
aSize
+
gPageSize
)
=
=
CHUNK_CEILING
(
aOldSize
+
gPageSize
)
)
{
size_t
psize
=
PAGE_CEILING
(
aSize
)
;
if
(
aSize
<
aOldSize
)
{
MaybePoison
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
aSize
)
aOldSize
-
aSize
)
;
}
if
(
psize
<
aOldSize
)
{
extent_node_t
key
;
pages_decommit
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
psize
)
aOldSize
-
psize
)
;
MutexAutoLock
lock
(
huge_mtx
)
;
key
.
mAddr
=
const_cast
<
void
*
>
(
aPtr
)
;
extent_node_t
*
node
=
huge
.
Search
(
&
key
)
;
MOZ_ASSERT
(
node
)
;
MOZ_ASSERT
(
node
-
>
mSize
=
=
aOldSize
)
;
MOZ_RELEASE_ASSERT
(
node
-
>
mArena
=
=
this
)
;
huge_allocated
-
=
aOldSize
-
psize
;
node
-
>
mSize
=
psize
;
}
else
if
(
psize
>
aOldSize
)
{
if
(
!
pages_commit
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
aOldSize
)
psize
-
aOldSize
)
)
{
return
nullptr
;
}
extent_node_t
key
;
MutexAutoLock
lock
(
huge_mtx
)
;
key
.
mAddr
=
const_cast
<
void
*
>
(
aPtr
)
;
extent_node_t
*
node
=
huge
.
Search
(
&
key
)
;
MOZ_ASSERT
(
node
)
;
MOZ_ASSERT
(
node
-
>
mSize
=
=
aOldSize
)
;
MOZ_RELEASE_ASSERT
(
node
-
>
mArena
=
=
this
)
;
huge_allocated
+
=
psize
-
aOldSize
;
node
-
>
mSize
=
psize
;
}
if
(
aSize
>
aOldSize
)
{
ApplyZeroOrJunk
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
aOldSize
)
aSize
-
aOldSize
)
;
}
return
aPtr
;
}
ret
=
(
mIsPrivate
?
this
:
choose_arena
(
aSize
)
)
-
>
MallocHuge
(
aSize
false
)
;
if
(
!
ret
)
{
return
nullptr
;
}
copysize
=
(
aSize
<
aOldSize
)
?
aSize
:
aOldSize
;
#
ifdef
VM_COPY_MIN
if
(
copysize
>
=
VM_COPY_MIN
)
{
pages_copy
(
ret
aPtr
copysize
)
;
}
else
#
endif
{
memcpy
(
ret
aPtr
copysize
)
;
}
idalloc
(
aPtr
this
)
;
return
ret
;
}
static
void
huge_dalloc
(
void
*
aPtr
arena_t
*
aArena
)
{
extent_node_t
*
node
;
size_t
mapped
=
0
;
{
extent_node_t
key
;
MutexAutoLock
lock
(
huge_mtx
)
;
key
.
mAddr
=
aPtr
;
node
=
huge
.
Search
(
&
key
)
;
MOZ_RELEASE_ASSERT
(
node
"
Double
-
free
?
"
)
;
MOZ_ASSERT
(
node
-
>
mAddr
=
=
aPtr
)
;
MOZ_RELEASE_ASSERT
(
!
aArena
|
|
node
-
>
mArena
=
=
aArena
)
;
MOZ_RELEASE_ASSERT
(
node
-
>
mArenaId
=
=
node
-
>
mArena
-
>
mId
)
;
huge
.
Remove
(
node
)
;
mapped
=
CHUNK_CEILING
(
node
-
>
mSize
+
gPageSize
)
;
huge_allocated
-
=
node
-
>
mSize
;
huge_mapped
-
=
mapped
;
}
chunk_dealloc
(
node
-
>
mAddr
mapped
HUGE_CHUNK
)
;
ExtentAlloc
:
:
dealloc
(
node
)
;
}
size_t
GetKernelPageSize
(
)
{
static
size_t
kernel_page_size
=
(
[
]
(
)
{
#
ifdef
XP_WIN
SYSTEM_INFO
info
;
GetSystemInfo
(
&
info
)
;
return
info
.
dwPageSize
;
#
else
long
result
=
sysconf
(
_SC_PAGESIZE
)
;
MOZ_ASSERT
(
result
!
=
-
1
)
;
return
result
;
#
endif
}
)
(
)
;
return
kernel_page_size
;
}
static
bool
malloc_init_hard
(
)
{
unsigned
i
;
const
char
*
opts
;
AutoLock
<
StaticMutex
>
lock
(
gInitLock
)
;
if
(
malloc_initialized
)
{
return
true
;
}
if
(
!
thread_arena
.
init
(
)
)
{
return
true
;
}
const
size_t
page_size
=
GetKernelPageSize
(
)
;
MOZ_ASSERT
(
IsPowerOfTwo
(
page_size
)
)
;
#
ifdef
MALLOC_STATIC_PAGESIZE
if
(
gPageSize
%
page_size
)
{
_malloc_message
(
_getprogname
(
)
"
Compile
-
time
page
size
does
not
divide
the
runtime
one
.
\
n
"
)
;
MOZ_CRASH
(
)
;
}
#
else
gRealPageSize
=
gPageSize
=
page_size
;
#
endif
if
(
(
opts
=
getenv
(
"
MALLOC_OPTIONS
"
)
)
)
{
for
(
i
=
0
;
opts
[
i
]
!
=
'
\
0
'
;
i
+
+
)
{
unsigned
prefix_arg
=
0
;
while
(
opts
[
i
]
>
=
'
0
'
&
&
opts
[
i
]
<
=
'
9
'
)
{
prefix_arg
*
=
10
;
prefix_arg
+
=
opts
[
i
]
-
'
0
'
;
i
+
+
;
}
switch
(
opts
[
i
]
)
{
case
'
f
'
:
opt_dirty_max
>
>
=
prefix_arg
?
prefix_arg
:
1
;
break
;
case
'
F
'
:
prefix_arg
=
prefix_arg
?
prefix_arg
:
1
;
if
(
opt_dirty_max
=
=
0
)
{
opt_dirty_max
=
1
;
prefix_arg
-
-
;
}
opt_dirty_max
<
<
=
prefix_arg
;
if
(
opt_dirty_max
=
=
0
)
{
opt_dirty_max
=
size_t
(
1
)
<
<
(
sizeof
(
size_t
)
*
CHAR_BIT
-
1
)
;
}
break
;
#
ifdef
MALLOC_RUNTIME_CONFIG
case
'
j
'
:
opt_junk
=
false
;
break
;
case
'
J
'
:
opt_junk
=
true
;
break
;
case
'
q
'
:
opt_poison
=
NONE
;
break
;
case
'
Q
'
:
if
(
opts
[
i
+
1
]
=
=
'
Q
'
)
{
i
+
+
;
opt_poison
=
ALL
;
}
else
{
opt_poison
=
SOME
;
opt_poison_size
=
kCacheLineSize
*
prefix_arg
;
}
break
;
case
'
z
'
:
opt_zero
=
false
;
break
;
case
'
Z
'
:
opt_zero
=
true
;
break
;
#
ifndef
MALLOC_STATIC_PAGESIZE
case
'
P
'
:
MOZ_ASSERT
(
gPageSize
>
=
4_KiB
)
;
MOZ_ASSERT
(
gPageSize
<
=
64_KiB
)
;
prefix_arg
=
prefix_arg
?
prefix_arg
:
1
;
gPageSize
<
<
=
prefix_arg
;
if
(
gPageSize
<
4_KiB
|
|
gPageSize
>
64_KiB
)
{
gPageSize
=
64_KiB
;
}
break
;
#
endif
#
endif
case
'
r
'
:
opt_randomize_small
=
false
;
break
;
case
'
R
'
:
opt_randomize_small
=
true
;
break
;
default
:
{
char
cbuf
[
2
]
;
cbuf
[
0
]
=
opts
[
i
]
;
cbuf
[
1
]
=
'
\
0
'
;
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Unsupported
character
"
"
in
malloc
options
:
'
"
cbuf
"
'
\
n
"
)
;
}
}
}
}
#
ifndef
MALLOC_STATIC_PAGESIZE
DefineGlobals
(
)
;
#
endif
gRecycledSize
=
0
;
chunks_mtx
.
Init
(
)
;
MOZ_PUSH_IGNORE_THREAD_SAFETY
gChunksBySize
.
Init
(
)
;
gChunksByAddress
.
Init
(
)
;
MOZ_POP_THREAD_SAFETY
huge_mtx
.
Init
(
)
;
MOZ_PUSH_IGNORE_THREAD_SAFETY
huge
.
Init
(
)
;
huge_allocated
=
0
;
huge_mapped
=
0
;
MOZ_POP_THREAD_SAFETY
base_mtx
.
Init
(
)
;
MOZ_PUSH_IGNORE_THREAD_SAFETY
base_mapped
=
0
;
base_committed
=
0
;
MOZ_POP_THREAD_SAFETY
if
(
!
gArenas
.
Init
(
)
)
{
return
false
;
}
thread_arena
.
set
(
gArenas
.
GetDefault
(
)
)
;
if
(
!
gChunkRTree
.
Init
(
)
)
{
return
false
;
}
malloc_initialized
=
true
;
Debug
:
:
jemalloc_ptr_info
(
nullptr
)
;
#
if
!
defined
(
XP_WIN
)
&
&
!
defined
(
XP_DARWIN
)
pthread_atfork
(
_malloc_prefork
_malloc_postfork_parent
_malloc_postfork_child
)
;
#
endif
return
true
;
}
struct
BaseAllocator
{
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
inline
return_type
name
(
__VA_ARGS__
)
;
#
define
MALLOC_FUNCS
MALLOC_FUNCS_MALLOC_BASE
#
include
"
malloc_decls
.
h
"
explicit
BaseAllocator
(
arena_t
*
aArena
)
:
mArena
(
aArena
)
{
}
private
:
arena_t
*
mArena
;
}
;
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
inline
return_type
MozJemalloc
:
:
name
(
\
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
{
\
BaseAllocator
allocator
(
nullptr
)
;
\
return
allocator
.
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
define
MALLOC_FUNCS
MALLOC_FUNCS_MALLOC_BASE
#
include
"
malloc_decls
.
h
"
inline
void
*
BaseAllocator
:
:
malloc
(
size_t
aSize
)
{
void
*
ret
;
arena_t
*
arena
;
if
(
!
malloc_init
(
)
)
{
ret
=
nullptr
;
goto
RETURN
;
}
if
(
aSize
=
=
0
)
{
aSize
=
1
;
}
MOZ_DIAGNOSTIC_ASSERT_IF
(
mArena
(
size_t
)
mArena
>
=
gPageSize
)
;
arena
=
mArena
?
mArena
:
choose_arena
(
aSize
)
;
ret
=
arena
-
>
Malloc
(
aSize
false
)
;
RETURN
:
if
(
!
ret
)
{
errno
=
ENOMEM
;
}
return
ret
;
}
inline
void
*
BaseAllocator
:
:
memalign
(
size_t
aAlignment
size_t
aSize
)
{
MOZ_ASSERT
(
(
(
aAlignment
-
1
)
&
aAlignment
)
=
=
0
)
;
if
(
!
malloc_init
(
)
)
{
return
nullptr
;
}
if
(
aSize
=
=
0
)
{
aSize
=
1
;
}
aAlignment
=
aAlignment
<
sizeof
(
void
*
)
?
sizeof
(
void
*
)
:
aAlignment
;
arena_t
*
arena
=
mArena
?
mArena
:
choose_arena
(
aSize
)
;
return
arena
-
>
Palloc
(
aAlignment
aSize
)
;
}
inline
void
*
BaseAllocator
:
:
calloc
(
size_t
aNum
size_t
aSize
)
{
void
*
ret
;
if
(
malloc_init
(
)
)
{
CheckedInt
<
size_t
>
checkedSize
=
CheckedInt
<
size_t
>
(
aNum
)
*
aSize
;
if
(
checkedSize
.
isValid
(
)
)
{
size_t
allocSize
=
checkedSize
.
value
(
)
;
if
(
allocSize
=
=
0
)
{
allocSize
=
1
;
}
arena_t
*
arena
=
mArena
?
mArena
:
choose_arena
(
allocSize
)
;
ret
=
arena
-
>
Malloc
(
allocSize
true
)
;
}
else
{
ret
=
nullptr
;
}
}
else
{
ret
=
nullptr
;
}
if
(
!
ret
)
{
errno
=
ENOMEM
;
}
return
ret
;
}
inline
void
*
BaseAllocator
:
:
realloc
(
void
*
aPtr
size_t
aSize
)
{
void
*
ret
;
if
(
aSize
=
=
0
)
{
aSize
=
1
;
}
if
(
aPtr
)
{
MOZ_RELEASE_ASSERT
(
malloc_initialized
)
;
auto
info
=
AllocInfo
:
:
Get
(
aPtr
)
;
auto
arena
=
info
.
Arena
(
)
;
MOZ_RELEASE_ASSERT
(
!
mArena
|
|
arena
=
=
mArena
)
;
ret
=
arena
-
>
Ralloc
(
aPtr
aSize
info
.
Size
(
)
)
;
}
else
{
if
(
!
malloc_init
(
)
)
{
ret
=
nullptr
;
}
else
{
arena_t
*
arena
=
mArena
?
mArena
:
choose_arena
(
aSize
)
;
ret
=
arena
-
>
Malloc
(
aSize
false
)
;
}
}
if
(
!
ret
)
{
errno
=
ENOMEM
;
}
return
ret
;
}
inline
void
BaseAllocator
:
:
free
(
void
*
aPtr
)
{
size_t
offset
;
offset
=
GetChunkOffsetForPtr
(
aPtr
)
;
if
(
offset
!
=
0
)
{
MOZ_RELEASE_ASSERT
(
malloc_initialized
)
;
arena_dalloc
(
aPtr
offset
mArena
)
;
}
else
if
(
aPtr
)
{
MOZ_RELEASE_ASSERT
(
malloc_initialized
)
;
huge_dalloc
(
aPtr
mArena
)
;
}
}
inline
int
MozJemalloc
:
:
posix_memalign
(
void
*
*
aMemPtr
size_t
aAlignment
size_t
aSize
)
{
return
AlignedAllocator
<
memalign
>
:
:
posix_memalign
(
aMemPtr
aAlignment
aSize
)
;
}
inline
void
*
MozJemalloc
:
:
aligned_alloc
(
size_t
aAlignment
size_t
aSize
)
{
return
AlignedAllocator
<
memalign
>
:
:
aligned_alloc
(
aAlignment
aSize
)
;
}
inline
void
*
MozJemalloc
:
:
valloc
(
size_t
aSize
)
{
return
AlignedAllocator
<
memalign
>
:
:
valloc
(
aSize
)
;
}
inline
size_t
MozJemalloc
:
:
malloc_good_size
(
size_t
aSize
)
{
if
(
aSize
<
=
gMaxLargeClass
)
{
aSize
=
SizeClass
(
aSize
)
.
Size
(
)
;
}
else
{
aSize
=
PAGE_CEILING
(
aSize
)
;
}
return
aSize
;
}
inline
size_t
MozJemalloc
:
:
malloc_usable_size
(
usable_ptr_t
aPtr
)
{
return
AllocInfo
:
:
GetValidated
(
aPtr
)
.
Size
(
)
;
}
inline
void
MozJemalloc
:
:
jemalloc_stats_internal
(
jemalloc_stats_t
*
aStats
jemalloc_bin_stats_t
*
aBinStats
)
{
size_t
non_arena_mapped
chunk_header_size
;
if
(
!
aStats
)
{
return
;
}
if
(
!
malloc_init
(
)
)
{
memset
(
aStats
0
sizeof
(
*
aStats
)
)
;
return
;
}
if
(
aBinStats
)
{
memset
(
aBinStats
0
sizeof
(
jemalloc_bin_stats_t
)
*
NUM_SMALL_CLASSES
)
;
}
aStats
-
>
opt_junk
=
opt_junk
;
aStats
-
>
opt_zero
=
opt_zero
;
aStats
-
>
quantum
=
kQuantum
;
aStats
-
>
quantum_max
=
kMaxQuantumClass
;
aStats
-
>
quantum_wide
=
kQuantumWide
;
aStats
-
>
quantum_wide_max
=
kMaxQuantumWideClass
;
aStats
-
>
subpage_max
=
gMaxSubPageClass
;
aStats
-
>
large_max
=
gMaxLargeClass
;
aStats
-
>
chunksize
=
kChunkSize
;
aStats
-
>
page_size
=
gPageSize
;
aStats
-
>
dirty_max
=
opt_dirty_max
;
aStats
-
>
narenas
=
0
;
aStats
-
>
mapped
=
0
;
aStats
-
>
allocated
=
0
;
aStats
-
>
waste
=
0
;
aStats
-
>
pages_dirty
=
0
;
aStats
-
>
pages_fresh
=
0
;
aStats
-
>
pages_madvised
=
0
;
aStats
-
>
bookkeeping
=
0
;
aStats
-
>
bin_unused
=
0
;
non_arena_mapped
=
0
;
{
MutexAutoLock
lock
(
huge_mtx
)
;
non_arena_mapped
+
=
huge_mapped
;
aStats
-
>
allocated
+
=
huge_allocated
;
MOZ_ASSERT
(
huge_mapped
>
=
huge_allocated
)
;
}
{
MutexAutoLock
lock
(
base_mtx
)
;
non_arena_mapped
+
=
base_mapped
;
aStats
-
>
bookkeeping
+
=
base_committed
;
MOZ_ASSERT
(
base_mapped
>
=
base_committed
)
;
}
gArenas
.
mLock
.
Lock
(
)
;
MOZ_ASSERT
(
gArenas
.
IsOnMainThreadWeak
(
)
)
;
for
(
auto
arena
:
gArenas
.
iter
(
)
)
{
MOZ_ASSERT
(
arena
-
>
mLock
.
SafeOnThisThread
(
)
)
;
size_t
arena_mapped
arena_allocated
arena_committed
arena_dirty
arena_fresh
arena_madvised
j
arena_unused
arena_headers
;
arena_headers
=
0
;
arena_unused
=
0
;
{
MaybeMutexAutoLock
lock
(
arena
-
>
mLock
)
;
arena_mapped
=
arena
-
>
mStats
.
mapped
;
arena_committed
=
arena
-
>
mStats
.
committed
<
<
gPageSize2Pow
;
arena_allocated
=
arena
-
>
mStats
.
allocated_small
+
arena
-
>
mStats
.
allocated_large
;
arena_dirty
=
arena
-
>
mNumDirty
<
<
gPageSize2Pow
;
arena_fresh
=
arena
-
>
mNumFresh
<
<
gPageSize2Pow
;
arena_madvised
=
arena
-
>
mNumMAdvised
<
<
gPageSize2Pow
;
for
(
j
=
0
;
j
<
NUM_SMALL_CLASSES
;
j
+
+
)
{
arena_bin_t
*
bin
=
&
arena
-
>
mBins
[
j
]
;
size_t
bin_unused
=
0
;
size_t
num_non_full_runs
=
0
;
for
(
auto
mapelm
:
bin
-
>
mNonFullRuns
.
iter
(
)
)
{
arena_run_t
*
run
=
(
arena_run_t
*
)
(
mapelm
-
>
bits
&
~
gPageSizeMask
)
;
bin_unused
+
=
run
-
>
mNumFree
*
bin
-
>
mSizeClass
;
num_non_full_runs
+
+
;
}
if
(
bin
-
>
mCurrentRun
)
{
bin_unused
+
=
bin
-
>
mCurrentRun
-
>
mNumFree
*
bin
-
>
mSizeClass
;
num_non_full_runs
+
+
;
}
arena_unused
+
=
bin_unused
;
arena_headers
+
=
bin
-
>
mNumRuns
*
bin
-
>
mRunFirstRegionOffset
;
if
(
aBinStats
)
{
aBinStats
[
j
]
.
size
=
bin
-
>
mSizeClass
;
aBinStats
[
j
]
.
num_non_full_runs
+
=
num_non_full_runs
;
aBinStats
[
j
]
.
num_runs
+
=
bin
-
>
mNumRuns
;
aBinStats
[
j
]
.
bytes_unused
+
=
bin_unused
;
size_t
bytes_per_run
=
static_cast
<
size_t
>
(
bin
-
>
mRunSizePages
)
<
<
gPageSize2Pow
;
aBinStats
[
j
]
.
bytes_total
+
=
bin
-
>
mNumRuns
*
(
bytes_per_run
-
bin
-
>
mRunFirstRegionOffset
)
;
aBinStats
[
j
]
.
bytes_per_run
=
bytes_per_run
;
}
}
}
MOZ_ASSERT
(
arena_mapped
>
=
arena_committed
)
;
MOZ_ASSERT
(
arena_committed
>
=
arena_allocated
+
arena_dirty
)
;
aStats
-
>
mapped
+
=
arena_mapped
;
aStats
-
>
allocated
+
=
arena_allocated
;
aStats
-
>
pages_dirty
+
=
arena_dirty
;
aStats
-
>
pages_fresh
+
=
arena_fresh
;
aStats
-
>
pages_madvised
+
=
arena_madvised
;
MOZ_ASSERT
(
arena_committed
>
=
(
arena_allocated
+
arena_dirty
+
arena_unused
+
arena_headers
)
)
;
aStats
-
>
waste
+
=
arena_committed
-
arena_allocated
-
arena_dirty
-
arena_unused
-
arena_headers
;
aStats
-
>
bin_unused
+
=
arena_unused
;
aStats
-
>
bookkeeping
+
=
arena_headers
;
aStats
-
>
narenas
+
+
;
}
gArenas
.
mLock
.
Unlock
(
)
;
chunk_header_size
=
(
(
aStats
-
>
mapped
/
aStats
-
>
chunksize
)
*
(
gChunkHeaderNumPages
-
1
)
)
<
<
gPageSize2Pow
;
aStats
-
>
mapped
+
=
non_arena_mapped
;
aStats
-
>
bookkeeping
+
=
chunk_header_size
;
aStats
-
>
waste
-
=
chunk_header_size
;
MOZ_ASSERT
(
aStats
-
>
mapped
>
=
aStats
-
>
allocated
+
aStats
-
>
waste
+
aStats
-
>
pages_dirty
+
aStats
-
>
bookkeeping
)
;
}
inline
size_t
MozJemalloc
:
:
jemalloc_stats_num_bins
(
)
{
return
NUM_SMALL_CLASSES
;
}
inline
void
MozJemalloc
:
:
jemalloc_set_main_thread
(
)
{
MOZ_ASSERT
(
malloc_initialized
)
;
gArenas
.
SetMainThread
(
)
;
}
#
ifdef
MALLOC_DOUBLE_PURGE
static
size_t
hard_purge_chunk
(
arena_chunk_t
*
aChunk
)
{
size_t
total_npages
=
0
;
for
(
size_t
i
=
gChunkHeaderNumPages
;
i
<
gChunkNumPages
;
i
+
+
)
{
size_t
npages
;
for
(
npages
=
0
;
aChunk
-
>
map
[
i
+
npages
]
.
bits
&
CHUNK_MAP_MADVISED
&
&
i
+
npages
<
gChunkNumPages
;
npages
+
+
)
{
MOZ_DIAGNOSTIC_ASSERT
(
!
(
aChunk
-
>
map
[
i
+
npages
]
.
bits
&
(
CHUNK_MAP_FRESH
|
CHUNK_MAP_DECOMMITTED
)
)
)
;
aChunk
-
>
map
[
i
+
npages
]
.
bits
^
=
(
CHUNK_MAP_MADVISED
|
CHUNK_MAP_FRESH
)
;
}
if
(
npages
>
0
)
{
pages_decommit
(
(
(
char
*
)
aChunk
)
+
(
i
<
<
gPageSize2Pow
)
npages
<
<
gPageSize2Pow
)
;
Unused
<
<
pages_commit
(
(
(
char
*
)
aChunk
)
+
(
i
<
<
gPageSize2Pow
)
npages
<
<
gPageSize2Pow
)
;
}
total_npages
+
=
npages
;
i
+
=
npages
;
}
return
total_npages
;
}
void
arena_t
:
:
HardPurge
(
)
{
MaybeMutexAutoLock
lock
(
mLock
)
;
while
(
!
mChunksMAdvised
.
isEmpty
(
)
)
{
arena_chunk_t
*
chunk
=
mChunksMAdvised
.
popFront
(
)
;
size_t
npages
=
hard_purge_chunk
(
chunk
)
;
mNumMAdvised
-
=
npages
;
mNumFresh
+
=
npages
;
}
}
inline
void
MozJemalloc
:
:
jemalloc_purge_freed_pages
(
)
{
if
(
malloc_initialized
)
{
MutexAutoLock
lock
(
gArenas
.
mLock
)
;
MOZ_ASSERT
(
gArenas
.
IsOnMainThreadWeak
(
)
)
;
for
(
auto
arena
:
gArenas
.
iter
(
)
)
{
arena
-
>
HardPurge
(
)
;
}
}
}
#
else
inline
void
MozJemalloc
:
:
jemalloc_purge_freed_pages
(
)
{
}
#
endif
inline
void
MozJemalloc
:
:
jemalloc_free_dirty_pages
(
void
)
{
if
(
malloc_initialized
)
{
MutexAutoLock
lock
(
gArenas
.
mLock
)
;
MOZ_ASSERT
(
gArenas
.
IsOnMainThreadWeak
(
)
)
;
for
(
auto
arena
:
gArenas
.
iter
(
)
)
{
MaybeMutexAutoLock
arena_lock
(
arena
-
>
mLock
)
;
arena
-
>
Purge
(
1
)
;
}
}
}
inline
arena_t
*
ArenaCollection
:
:
GetByIdInternal
(
Tree
&
aTree
arena_id_t
aArenaId
)
{
mozilla
:
:
AlignedStorage2
<
arena_t
>
key
;
key
.
addr
(
)
-
>
mId
=
aArenaId
;
return
aTree
.
Search
(
key
.
addr
(
)
)
;
}
inline
arena_t
*
ArenaCollection
:
:
GetById
(
arena_id_t
aArenaId
bool
aIsPrivate
)
{
if
(
!
malloc_initialized
)
{
return
nullptr
;
}
Tree
*
tree
=
nullptr
;
if
(
aIsPrivate
)
{
if
(
ArenaIdIsMainThreadOnly
(
aArenaId
)
)
{
arena_t
*
result
=
GetByIdInternal
(
mMainThreadArenas
aArenaId
)
;
MOZ_RELEASE_ASSERT
(
result
)
;
return
result
;
}
tree
=
&
mPrivateArenas
;
}
else
{
tree
=
&
mArenas
;
}
MutexAutoLock
lock
(
mLock
)
;
arena_t
*
result
=
GetByIdInternal
(
*
tree
aArenaId
)
;
MOZ_RELEASE_ASSERT
(
result
)
;
return
result
;
}
inline
arena_id_t
MozJemalloc
:
:
moz_create_arena_with_params
(
arena_params_t
*
aParams
)
{
if
(
malloc_init
(
)
)
{
arena_t
*
arena
=
gArenas
.
CreateArena
(
true
aParams
)
;
return
arena
-
>
mId
;
}
return
0
;
}
inline
void
MozJemalloc
:
:
moz_dispose_arena
(
arena_id_t
aArenaId
)
{
arena_t
*
arena
=
gArenas
.
GetById
(
aArenaId
true
)
;
MOZ_RELEASE_ASSERT
(
arena
)
;
gArenas
.
DisposeArena
(
arena
)
;
}
inline
void
MozJemalloc
:
:
moz_set_max_dirty_page_modifier
(
int32_t
aModifier
)
{
gArenas
.
SetDefaultMaxDirtyPageModifier
(
aModifier
)
;
}
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
inline
return_type
MozJemalloc
:
:
moz_arena_
#
#
name
(
\
arena_id_t
aArenaId
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
{
\
BaseAllocator
allocator
(
\
gArenas
.
GetById
(
aArenaId
/
*
IsPrivate
=
*
/
true
)
)
;
\
return
allocator
.
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
define
MALLOC_FUNCS
MALLOC_FUNCS_MALLOC_BASE
#
include
"
malloc_decls
.
h
"
#
ifndef
XP_WIN
static
pthread_t
gForkingThread
;
#
ifdef
XP_DARWIN
static
pid_t
gForkingProcess
;
#
endif
FORK_HOOK
void
_malloc_prefork
(
void
)
MOZ_NO_THREAD_SAFETY_ANALYSIS
{
gArenas
.
mLock
.
Lock
(
)
;
gForkingThread
=
pthread_self
(
)
;
#
ifdef
XP_DARWIN
gForkingProcess
=
getpid
(
)
;
#
endif
for
(
auto
arena
:
gArenas
.
iter
(
)
)
{
if
(
arena
-
>
mLock
.
LockIsEnabled
(
)
)
{
arena
-
>
mLock
.
Lock
(
)
;
}
}
base_mtx
.
Lock
(
)
;
huge_mtx
.
Lock
(
)
;
}
FORK_HOOK
void
_malloc_postfork_parent
(
void
)
MOZ_NO_THREAD_SAFETY_ANALYSIS
{
huge_mtx
.
Unlock
(
)
;
base_mtx
.
Unlock
(
)
;
for
(
auto
arena
:
gArenas
.
iter
(
)
)
{
if
(
arena
-
>
mLock
.
LockIsEnabled
(
)
)
{
arena
-
>
mLock
.
Unlock
(
)
;
}
}
gArenas
.
mLock
.
Unlock
(
)
;
}
FORK_HOOK
void
_malloc_postfork_child
(
void
)
{
gArenas
.
ResetMainThread
(
)
;
huge_mtx
.
Init
(
)
;
base_mtx
.
Init
(
)
;
for
(
auto
arena
:
gArenas
.
iter
(
)
)
{
arena
-
>
mLock
.
Reinit
(
gForkingThread
)
;
}
gArenas
.
mLock
.
Init
(
)
;
}
#
ifdef
XP_DARWIN
FORK_HOOK
void
_malloc_postfork
(
void
)
{
bool
is_in_parent
=
getpid
(
)
=
=
gForkingProcess
;
gForkingProcess
=
0
;
if
(
is_in_parent
)
{
_malloc_postfork_parent
(
)
;
}
else
{
_malloc_postfork_child
(
)
;
}
}
#
endif
#
endif
#
ifdef
MOZ_REPLACE_MALLOC
#
ifdef
XP_DARWIN
#
define
MOZ_REPLACE_WEAK
__attribute__
(
(
weak_import
)
)
#
elif
defined
(
XP_WIN
)
|
|
defined
(
ANDROID
)
#
define
MOZ_DYNAMIC_REPLACE_INIT
#
define
replace_init
replace_init_decl
#
elif
defined
(
__GNUC__
)
#
define
MOZ_REPLACE_WEAK
__attribute__
(
(
weak
)
)
#
endif
#
include
"
replace_malloc
.
h
"
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
CanonicalMalloc
:
:
name
static
const
malloc_table_t
gDefaultMallocTable
=
{
#
include
"
malloc_decls
.
h
"
}
;
static
malloc_table_t
gOriginalMallocTable
=
{
#
include
"
malloc_decls
.
h
"
}
;
static
malloc_table_t
gDynamicMallocTable
=
{
#
include
"
malloc_decls
.
h
"
}
;
static
Atomic
<
malloc_table_t
const
*
mozilla
:
:
MemoryOrdering
:
:
Relaxed
>
gMallocTablePtr
;
#
ifdef
MOZ_DYNAMIC_REPLACE_INIT
#
undef
replace_init
typedef
decltype
(
replace_init_decl
)
replace_init_impl_t
;
static
replace_init_impl_t
*
replace_init
=
nullptr
;
#
endif
#
ifdef
XP_WIN
typedef
HMODULE
replace_malloc_handle_t
;
static
replace_malloc_handle_t
replace_malloc_handle
(
)
{
wchar_t
replace_malloc_lib
[
1024
]
;
if
(
GetEnvironmentVariableW
(
L
"
MOZ_REPLACE_MALLOC_LIB
"
replace_malloc_lib
ArrayLength
(
replace_malloc_lib
)
)
>
0
)
{
return
LoadLibraryW
(
replace_malloc_lib
)
;
}
return
nullptr
;
}
#
define
REPLACE_MALLOC_GET_INIT_FUNC
(
handle
)
\
(
replace_init_impl_t
*
)
GetProcAddress
(
handle
"
replace_init
"
)
#
elif
defined
(
ANDROID
)
#
include
<
dlfcn
.
h
>
typedef
void
*
replace_malloc_handle_t
;
static
replace_malloc_handle_t
replace_malloc_handle
(
)
{
const
char
*
replace_malloc_lib
=
getenv
(
"
MOZ_REPLACE_MALLOC_LIB
"
)
;
if
(
replace_malloc_lib
&
&
*
replace_malloc_lib
)
{
return
dlopen
(
replace_malloc_lib
RTLD_LAZY
)
;
}
return
nullptr
;
}
#
define
REPLACE_MALLOC_GET_INIT_FUNC
(
handle
)
\
(
replace_init_impl_t
*
)
dlsym
(
handle
"
replace_init
"
)
#
endif
static
void
replace_malloc_init_funcs
(
malloc_table_t
*
)
;
#
ifdef
MOZ_REPLACE_MALLOC_STATIC
extern
"
C
"
void
logalloc_init
(
malloc_table_t
*
ReplaceMallocBridge
*
*
)
;
extern
"
C
"
void
dmd_init
(
malloc_table_t
*
ReplaceMallocBridge
*
*
)
;
#
endif
void
phc_init
(
malloc_table_t
*
ReplaceMallocBridge
*
*
)
;
bool
Equals
(
const
malloc_table_t
&
aTable1
const
malloc_table_t
&
aTable2
)
{
return
memcmp
(
&
aTable1
&
aTable2
sizeof
(
malloc_table_t
)
)
=
=
0
;
}
static
ReplaceMallocBridge
*
gReplaceMallocBridge
=
nullptr
;
static
void
init
(
)
{
malloc_table_t
tempTable
=
gDefaultMallocTable
;
#
ifdef
MOZ_DYNAMIC_REPLACE_INIT
replace_malloc_handle_t
handle
=
replace_malloc_handle
(
)
;
if
(
handle
)
{
replace_init
=
REPLACE_MALLOC_GET_INIT_FUNC
(
handle
)
;
}
#
endif
gMallocTablePtr
=
&
gDefaultMallocTable
;
if
(
replace_init
)
{
replace_init
(
&
tempTable
&
gReplaceMallocBridge
)
;
}
#
ifdef
MOZ_REPLACE_MALLOC_STATIC
if
(
Equals
(
tempTable
gDefaultMallocTable
)
)
{
logalloc_init
(
&
tempTable
&
gReplaceMallocBridge
)
;
}
#
ifdef
MOZ_DMD
if
(
Equals
(
tempTable
gDefaultMallocTable
)
)
{
dmd_init
(
&
tempTable
&
gReplaceMallocBridge
)
;
}
#
endif
#
endif
if
(
!
Equals
(
tempTable
gDefaultMallocTable
)
)
{
replace_malloc_init_funcs
(
&
tempTable
)
;
}
gOriginalMallocTable
=
tempTable
;
gMallocTablePtr
=
&
gOriginalMallocTable
;
}
MOZ_JEMALLOC_API
void
jemalloc_replace_dynamic
(
jemalloc_init_func
replace_init_func
)
{
if
(
replace_init_func
)
{
malloc_table_t
tempTable
=
gOriginalMallocTable
;
(
*
replace_init_func
)
(
&
tempTable
&
gReplaceMallocBridge
)
;
if
(
!
Equals
(
tempTable
gOriginalMallocTable
)
)
{
replace_malloc_init_funcs
(
&
tempTable
)
;
gMallocTablePtr
=
&
gOriginalMallocTable
;
gDynamicMallocTable
=
tempTable
;
gMallocTablePtr
=
&
gDynamicMallocTable
;
}
}
else
{
gMallocTablePtr
=
&
gOriginalMallocTable
;
}
}
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
inline
return_type
ReplaceMalloc
:
:
name
(
\
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
{
\
if
(
MOZ_UNLIKELY
(
!
gMallocTablePtr
)
)
{
\
init
(
)
;
\
}
\
return
(
*
gMallocTablePtr
)
.
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
include
"
malloc_decls
.
h
"
MOZ_JEMALLOC_API
struct
ReplaceMallocBridge
*
get_bridge
(
void
)
{
if
(
MOZ_UNLIKELY
(
!
gMallocTablePtr
)
)
{
init
(
)
;
}
return
gReplaceMallocBridge
;
}
static
void
replace_malloc_init_funcs
(
malloc_table_t
*
table
)
{
if
(
table
-
>
posix_memalign
=
=
CanonicalMalloc
:
:
posix_memalign
&
&
table
-
>
memalign
!
=
CanonicalMalloc
:
:
memalign
)
{
table
-
>
posix_memalign
=
AlignedAllocator
<
ReplaceMalloc
:
:
memalign
>
:
:
posix_memalign
;
}
if
(
table
-
>
aligned_alloc
=
=
CanonicalMalloc
:
:
aligned_alloc
&
&
table
-
>
memalign
!
=
CanonicalMalloc
:
:
memalign
)
{
table
-
>
aligned_alloc
=
AlignedAllocator
<
ReplaceMalloc
:
:
memalign
>
:
:
aligned_alloc
;
}
if
(
table
-
>
valloc
=
=
CanonicalMalloc
:
:
valloc
&
&
table
-
>
memalign
!
=
CanonicalMalloc
:
:
memalign
)
{
table
-
>
valloc
=
AlignedAllocator
<
ReplaceMalloc
:
:
memalign
>
:
:
valloc
;
}
if
(
table
-
>
moz_create_arena_with_params
=
=
CanonicalMalloc
:
:
moz_create_arena_with_params
&
&
table
-
>
malloc
!
=
CanonicalMalloc
:
:
malloc
)
{
#
define
MALLOC_DECL
(
name
.
.
.
)
\
table
-
>
name
=
DummyArenaAllocator
<
ReplaceMalloc
>
:
:
name
;
#
define
MALLOC_FUNCS
MALLOC_FUNCS_ARENA_BASE
#
include
"
malloc_decls
.
h
"
}
if
(
table
-
>
moz_arena_malloc
=
=
CanonicalMalloc
:
:
moz_arena_malloc
&
&
table
-
>
malloc
!
=
CanonicalMalloc
:
:
malloc
)
{
#
define
MALLOC_DECL
(
name
.
.
.
)
\
table
-
>
name
=
DummyArenaAllocator
<
ReplaceMalloc
>
:
:
name
;
#
define
MALLOC_FUNCS
MALLOC_FUNCS_ARENA_ALLOC
#
include
"
malloc_decls
.
h
"
}
}
#
endif
#
define
GENERIC_MALLOC_DECL2_MINGW
(
name
name_impl
return_type
.
.
.
)
\
return_type
name
(
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
\
__attribute__
(
(
alias
(
MOZ_STRINGIFY
(
name_impl
)
)
)
)
;
#
define
GENERIC_MALLOC_DECL2
(
attributes
name
name_impl
return_type
.
.
.
)
\
return_type
name_impl
(
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
attributes
{
\
return
DefaultMalloc
:
:
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
ifndef
__MINGW32__
#
define
GENERIC_MALLOC_DECL
(
attributes
name
return_type
.
.
.
)
\
GENERIC_MALLOC_DECL2
(
attributes
name
name
#
#
_impl
return_type
\
#
#
__VA_ARGS__
)
#
else
#
define
GENERIC_MALLOC_DECL
(
attributes
name
return_type
.
.
.
)
\
GENERIC_MALLOC_DECL2
(
attributes
name
name
#
#
_impl
return_type
\
#
#
__VA_ARGS__
)
\
GENERIC_MALLOC_DECL2_MINGW
(
name
name
#
#
_impl
return_type
#
#
__VA_ARGS__
)
#
endif
#
define
NOTHROW_MALLOC_DECL
(
.
.
.
)
\
MOZ_MEMORY_API
MACRO_CALL
(
GENERIC_MALLOC_DECL
(
noexcept
(
true
)
__VA_ARGS__
)
)
#
define
MALLOC_DECL
(
.
.
.
)
\
MOZ_MEMORY_API
MACRO_CALL
(
GENERIC_MALLOC_DECL
(
__VA_ARGS__
)
)
#
define
MALLOC_FUNCS
MALLOC_FUNCS_MALLOC
#
include
"
malloc_decls
.
h
"
#
undef
GENERIC_MALLOC_DECL
#
define
GENERIC_MALLOC_DECL
(
attributes
name
return_type
.
.
.
)
\
GENERIC_MALLOC_DECL2
(
attributes
name
name
return_type
#
#
__VA_ARGS__
)
#
define
MALLOC_DECL
(
.
.
.
)
\
MOZ_JEMALLOC_API
MACRO_CALL
(
GENERIC_MALLOC_DECL
(
__VA_ARGS__
)
)
#
define
MALLOC_FUNCS
(
MALLOC_FUNCS_JEMALLOC
|
MALLOC_FUNCS_ARENA
)
#
include
"
malloc_decls
.
h
"
#
ifdef
HAVE_DLFCN_H
#
include
<
dlfcn
.
h
>
#
endif
#
if
defined
(
__GLIBC__
)
&
&
!
defined
(
__UCLIBC__
)
extern
"
C
"
{
MOZ_EXPORT
void
(
*
__free_hook
)
(
void
*
)
=
free_impl
;
MOZ_EXPORT
void
*
(
*
__malloc_hook
)
(
size_t
)
=
malloc_impl
;
MOZ_EXPORT
void
*
(
*
__realloc_hook
)
(
void
*
size_t
)
=
realloc_impl
;
MOZ_EXPORT
void
*
(
*
__memalign_hook
)
(
size_t
size_t
)
=
memalign_impl
;
}
#
elif
defined
(
RTLD_DEEPBIND
)
#
error
\
"
Interposing
malloc
is
unsafe
on
this
system
without
libc
malloc
hooks
.
"
#
endif
#
ifdef
XP_WIN
MOZ_EXPORT
void
*
_recalloc
(
void
*
aPtr
size_t
aCount
size_t
aSize
)
{
size_t
oldsize
=
aPtr
?
AllocInfo
:
:
Get
(
aPtr
)
.
Size
(
)
:
0
;
CheckedInt
<
size_t
>
checkedSize
=
CheckedInt
<
size_t
>
(
aCount
)
*
aSize
;
if
(
!
checkedSize
.
isValid
(
)
)
{
return
nullptr
;
}
size_t
newsize
=
checkedSize
.
value
(
)
;
aPtr
=
DefaultMalloc
:
:
realloc
(
aPtr
newsize
)
;
if
(
aPtr
&
&
oldsize
<
newsize
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
oldsize
)
0
newsize
-
oldsize
)
;
}
return
aPtr
;
}
MOZ_EXPORT
void
*
_expand
(
void
*
aPtr
size_t
newsize
)
{
if
(
AllocInfo
:
:
Get
(
aPtr
)
.
Size
(
)
>
=
newsize
)
{
return
aPtr
;
}
return
nullptr
;
}
MOZ_EXPORT
size_t
_msize
(
void
*
aPtr
)
{
return
DefaultMalloc
:
:
malloc_usable_size
(
aPtr
)
;
}
#
endif
#
ifdef
MOZ_PHC
#
include
"
PHC
.
cpp
"
#
endif
