#
include
"
mozmemory_wrap
.
h
"
#
include
"
mozjemalloc
.
h
"
#
include
"
mozilla
/
Sprintf
.
h
"
#
include
"
mozilla
/
Likely
.
h
"
#
include
"
mozilla
/
DoublyLinkedList
.
h
"
#
include
"
mozilla
/
GuardObjects
.
h
"
#
include
"
mozilla
/
UniquePtr
.
h
"
#
ifdef
XP_DARWIN
#
define
MALLOC_DOUBLE_PURGE
#
endif
#
include
<
sys
/
types
.
h
>
#
include
<
errno
.
h
>
#
include
<
stdlib
.
h
>
#
include
<
limits
.
h
>
#
include
<
stdarg
.
h
>
#
include
<
stdio
.
h
>
#
include
<
string
.
h
>
#
include
<
algorithm
>
#
ifdef
XP_WIN
#
define
_CRT_SPINCOUNT
5000
#
include
<
io
.
h
>
#
include
<
windows
.
h
>
#
include
<
intrin
.
h
>
#
define
SIZE_T_MAX
SIZE_MAX
#
define
STDERR_FILENO
2
#
pragma
intrinsic
(
_BitScanForward
)
static
__forceinline
int
ffs
(
int
x
)
{
unsigned
long
i
;
if
(
_BitScanForward
(
&
i
x
)
!
=
0
)
return
(
i
+
1
)
;
return
(
0
)
;
}
static
char
mozillaMallocOptionsBuf
[
64
]
;
#
define
getenv
xgetenv
static
char
*
getenv
(
const
char
*
name
)
{
if
(
GetEnvironmentVariableA
(
name
(
LPSTR
)
&
mozillaMallocOptionsBuf
sizeof
(
mozillaMallocOptionsBuf
)
)
>
0
)
return
(
mozillaMallocOptionsBuf
)
;
return
nullptr
;
}
#
if
defined
(
_WIN64
)
typedef
long
long
ssize_t
;
#
else
typedef
long
ssize_t
;
#
endif
#
define
MALLOC_DECOMMIT
#
endif
#
ifndef
XP_WIN
#
ifndef
XP_SOLARIS
#
include
<
sys
/
cdefs
.
h
>
#
endif
#
include
<
sys
/
mman
.
h
>
#
ifndef
MADV_FREE
#
define
MADV_FREE
MADV_DONTNEED
#
endif
#
ifndef
MAP_NOSYNC
#
define
MAP_NOSYNC
0
#
endif
#
include
<
sys
/
param
.
h
>
#
include
<
sys
/
time
.
h
>
#
include
<
sys
/
types
.
h
>
#
if
!
defined
(
XP_SOLARIS
)
&
&
!
defined
(
ANDROID
)
#
include
<
sys
/
sysctl
.
h
>
#
endif
#
include
<
sys
/
uio
.
h
>
#
include
<
errno
.
h
>
#
include
<
limits
.
h
>
#
ifndef
SIZE_T_MAX
#
define
SIZE_T_MAX
SIZE_MAX
#
endif
#
include
<
pthread
.
h
>
#
include
<
sched
.
h
>
#
include
<
stdarg
.
h
>
#
include
<
stdio
.
h
>
#
include
<
stdbool
.
h
>
#
include
<
stdint
.
h
>
#
include
<
stdlib
.
h
>
#
include
<
string
.
h
>
#
ifndef
XP_DARWIN
#
include
<
strings
.
h
>
#
endif
#
include
<
unistd
.
h
>
#
ifdef
XP_DARWIN
#
include
<
libkern
/
OSAtomic
.
h
>
#
include
<
mach
/
mach_error
.
h
>
#
include
<
mach
/
mach_init
.
h
>
#
include
<
mach
/
vm_map
.
h
>
#
include
<
malloc
/
malloc
.
h
>
#
endif
#
endif
#
include
"
mozilla
/
ThreadLocal
.
h
"
#
include
"
mozjemalloc_types
.
h
"
#
if
(
defined
(
XP_LINUX
)
&
&
!
defined
(
__alpha__
)
)
|
|
\
(
defined
(
__FreeBSD_kernel__
)
&
&
defined
(
__GLIBC__
)
)
#
include
<
sys
/
syscall
.
h
>
#
if
defined
(
SYS_mmap
)
|
|
defined
(
SYS_mmap2
)
static
inline
void
*
_mmap
(
void
*
addr
size_t
length
int
prot
int
flags
int
fd
off_t
offset
)
{
#
ifdef
__s390__
struct
{
void
*
addr
;
size_t
length
;
long
prot
;
long
flags
;
long
fd
;
off_t
offset
;
}
args
=
{
addr
length
prot
flags
fd
offset
}
;
return
(
void
*
)
syscall
(
SYS_mmap
&
args
)
;
#
else
#
if
defined
(
ANDROID
)
&
&
defined
(
__aarch64__
)
&
&
defined
(
SYS_mmap2
)
#
undef
SYS_mmap2
#
endif
#
ifdef
SYS_mmap2
return
(
void
*
)
syscall
(
SYS_mmap2
addr
length
prot
flags
fd
offset
>
>
12
)
;
#
else
return
(
void
*
)
syscall
(
SYS_mmap
addr
length
prot
flags
fd
offset
)
;
#
endif
#
endif
}
#
define
mmap
_mmap
#
define
munmap
(
a
l
)
syscall
(
SYS_munmap
a
l
)
#
endif
#
endif
#
define
STRERROR_BUF
64
#
define
QUANTUM_2POW_MIN
4
#
if
defined
(
_WIN64
)
|
|
defined
(
__LP64__
)
#
define
SIZEOF_PTR_2POW
3
#
else
#
define
SIZEOF_PTR_2POW
2
#
endif
#
define
SIZEOF_PTR
(
1U
<
<
SIZEOF_PTR_2POW
)
#
include
"
rb
.
h
"
#
ifndef
SIZEOF_INT_2POW
#
define
SIZEOF_INT_2POW
2
#
endif
#
define
CHUNK_2POW_DEFAULT
20
#
define
DIRTY_MAX_DEFAULT
(
1U
<
<
8
)
#
define
CACHELINE_2POW
6
#
define
CACHELINE
(
(
size_t
)
(
1U
<
<
CACHELINE_2POW
)
)
#
ifdef
XP_WIN
#
define
TINY_MIN_2POW
(
sizeof
(
void
*
)
=
=
8
?
4
:
3
)
#
else
#
define
TINY_MIN_2POW
(
sizeof
(
void
*
)
=
=
8
?
3
:
2
)
#
endif
#
define
SMALL_MAX_2POW_DEFAULT
9
#
define
SMALL_MAX_DEFAULT
(
1U
<
<
SMALL_MAX_2POW_DEFAULT
)
#
define
RUN_BFP
12
#
define
RUN_MAX_OVRHD
0x0000003dU
#
define
RUN_MAX_OVRHD_RELAX
0x00001800U
#
ifndef
MOZ_DEBUG
#
if
!
defined
(
__ia64__
)
&
&
!
defined
(
__sparc__
)
&
&
!
defined
(
__mips__
)
&
&
!
defined
(
__aarch64__
)
#
define
MALLOC_STATIC_PAGESIZE
1
#
endif
#
endif
#
define
QUANTUM_DEFAULT
(
size_t
(
1
)
<
<
QUANTUM_2POW_MIN
)
static
const
size_t
quantum
=
QUANTUM_DEFAULT
;
static
const
size_t
quantum_mask
=
QUANTUM_DEFAULT
-
1
;
static
const
size_t
small_min
=
(
QUANTUM_DEFAULT
>
>
1
)
+
1
;
static
const
size_t
small_max
=
size_t
(
SMALL_MAX_DEFAULT
)
;
static
const
unsigned
ntbins
=
unsigned
(
QUANTUM_2POW_MIN
-
TINY_MIN_2POW
)
;
static
const
unsigned
nqbins
=
unsigned
(
SMALL_MAX_DEFAULT
>
>
QUANTUM_2POW_MIN
)
;
#
ifdef
MALLOC_STATIC_PAGESIZE
#
if
(
defined
(
SOLARIS
)
|
|
defined
(
__FreeBSD__
)
)
&
&
\
(
defined
(
__sparc
)
|
|
defined
(
__sparcv9
)
|
|
defined
(
__ia64
)
)
#
define
pagesize_2pow
(
size_t
(
13
)
)
#
elif
defined
(
__powerpc64__
)
#
define
pagesize_2pow
(
size_t
(
16
)
)
#
else
#
define
pagesize_2pow
(
size_t
(
12
)
)
#
endif
#
define
pagesize
(
size_t
(
1
)
<
<
pagesize_2pow
)
#
define
pagesize_mask
(
pagesize
-
1
)
static
const
size_t
bin_maxclass
=
pagesize
>
>
1
;
static
const
unsigned
nsbins
=
unsigned
(
pagesize_2pow
-
SMALL_MAX_2POW_DEFAULT
-
1
)
;
#
else
static
size_t
pagesize
;
static
size_t
pagesize_mask
;
static
size_t
pagesize_2pow
;
static
size_t
bin_maxclass
;
static
unsigned
nsbins
;
#
endif
#
define
calculate_arena_header_size
(
)
\
(
sizeof
(
arena_chunk_t
)
+
sizeof
(
arena_chunk_map_t
)
*
(
chunk_npages
-
1
)
)
#
define
calculate_arena_header_pages
(
)
\
(
(
calculate_arena_header_size
(
)
>
>
pagesize_2pow
)
+
\
(
(
calculate_arena_header_size
(
)
&
pagesize_mask
)
?
1
:
0
)
)
#
define
calculate_arena_maxclass
(
)
\
(
chunksize
-
(
arena_chunk_header_npages
<
<
pagesize_2pow
)
)
#
define
CHUNKSIZE_DEFAULT
(
(
size_t
)
1
<
<
CHUNK_2POW_DEFAULT
)
static
const
size_t
chunksize
=
CHUNKSIZE_DEFAULT
;
static
const
size_t
chunksize_mask
=
CHUNKSIZE_DEFAULT
-
1
;
#
ifdef
MALLOC_STATIC_PAGESIZE
static
const
size_t
chunk_npages
=
CHUNKSIZE_DEFAULT
>
>
pagesize_2pow
;
#
define
arena_chunk_header_npages
calculate_arena_header_pages
(
)
#
define
arena_maxclass
calculate_arena_maxclass
(
)
#
else
static
size_t
chunk_npages
;
static
size_t
arena_chunk_header_npages
;
static
size_t
arena_maxclass
;
#
endif
#
define
CHUNK_RECYCLE_LIMIT
128
static
const
size_t
recycle_limit
=
CHUNK_RECYCLE_LIMIT
*
CHUNKSIZE_DEFAULT
;
static
size_t
recycled_size
;
#
if
defined
(
MALLOC_DECOMMIT
)
&
&
defined
(
MALLOC_DOUBLE_PURGE
)
#
error
MALLOC_DECOMMIT
and
MALLOC_DOUBLE_PURGE
are
mutually
exclusive
.
#
endif
struct
Mutex
{
#
if
defined
(
XP_WIN
)
CRITICAL_SECTION
mMutex
;
#
elif
defined
(
XP_DARWIN
)
OSSpinLock
mMutex
;
#
else
pthread_mutex_t
mMutex
;
#
endif
inline
bool
Init
(
)
;
inline
void
Lock
(
)
;
inline
void
Unlock
(
)
;
}
;
struct
MOZ_RAII
MutexAutoLock
{
explicit
MutexAutoLock
(
Mutex
&
aMutex
MOZ_GUARD_OBJECT_NOTIFIER_PARAM
)
:
mMutex
(
aMutex
)
{
MOZ_GUARD_OBJECT_NOTIFIER_INIT
;
mMutex
.
Lock
(
)
;
}
~
MutexAutoLock
(
)
{
mMutex
.
Unlock
(
)
;
}
private
:
MOZ_DECL_USE_GUARD_OBJECT_NOTIFIER
;
Mutex
&
mMutex
;
}
;
static
bool
malloc_initialized
=
false
;
#
if
defined
(
XP_WIN
)
#
elif
defined
(
XP_DARWIN
)
static
Mutex
gInitLock
=
{
OS_SPINLOCK_INIT
}
;
#
elif
defined
(
XP_LINUX
)
&
&
!
defined
(
ANDROID
)
static
Mutex
gInitLock
=
{
PTHREAD_ADAPTIVE_MUTEX_INITIALIZER_NP
}
;
#
else
static
Mutex
gInitLock
=
{
PTHREAD_MUTEX_INITIALIZER
}
;
#
endif
struct
malloc_bin_stats_t
{
unsigned
long
curruns
;
}
;
struct
arena_stats_t
{
size_t
mapped
;
size_t
committed
;
size_t
allocated_small
;
size_t
allocated_large
;
}
;
enum
ChunkType
{
UNKNOWN_CHUNK
ZEROED_CHUNK
ARENA_CHUNK
HUGE_CHUNK
RECYCLED_CHUNK
}
;
struct
extent_node_t
{
RedBlackTreeNode
<
extent_node_t
>
link_szad
;
RedBlackTreeNode
<
extent_node_t
>
link_ad
;
void
*
addr
;
size_t
size
;
ChunkType
chunk_type
;
}
;
template
<
typename
T
>
int
CompareAddr
(
T
*
aAddr1
T
*
aAddr2
)
{
uintptr_t
addr1
=
reinterpret_cast
<
uintptr_t
>
(
aAddr1
)
;
uintptr_t
addr2
=
reinterpret_cast
<
uintptr_t
>
(
aAddr2
)
;
return
(
addr1
>
addr2
)
-
(
addr1
<
addr2
)
;
}
struct
ExtentTreeSzTrait
{
static
RedBlackTreeNode
<
extent_node_t
>
&
GetTreeNode
(
extent_node_t
*
aThis
)
{
return
aThis
-
>
link_szad
;
}
static
inline
int
Compare
(
extent_node_t
*
aNode
extent_node_t
*
aOther
)
{
int
ret
=
(
aNode
-
>
size
>
aOther
-
>
size
)
-
(
aNode
-
>
size
<
aOther
-
>
size
)
;
return
ret
?
ret
:
CompareAddr
(
aNode
-
>
addr
aOther
-
>
addr
)
;
}
}
;
struct
ExtentTreeTrait
{
static
RedBlackTreeNode
<
extent_node_t
>
&
GetTreeNode
(
extent_node_t
*
aThis
)
{
return
aThis
-
>
link_ad
;
}
static
inline
int
Compare
(
extent_node_t
*
aNode
extent_node_t
*
aOther
)
{
return
CompareAddr
(
aNode
-
>
addr
aOther
-
>
addr
)
;
}
}
;
struct
ExtentTreeBoundsTrait
:
public
ExtentTreeTrait
{
static
inline
int
Compare
(
extent_node_t
*
aKey
extent_node_t
*
aNode
)
{
uintptr_t
key_addr
=
reinterpret_cast
<
uintptr_t
>
(
aKey
-
>
addr
)
;
uintptr_t
node_addr
=
reinterpret_cast
<
uintptr_t
>
(
aNode
-
>
addr
)
;
size_t
node_size
=
aNode
-
>
size
;
if
(
node_addr
<
=
key_addr
&
&
key_addr
<
node_addr
+
node_size
)
{
return
0
;
}
return
(
key_addr
>
node_addr
)
-
(
key_addr
<
node_addr
)
;
}
}
;
template
<
size_t
Bits
>
class
AddressRadixTree
{
#
if
(
SIZEOF_PTR
=
=
4
)
static
const
size_t
kNodeSize2Pow
=
14
;
#
else
static
const
size_t
kNodeSize2Pow
=
CACHELINE_2POW
;
#
endif
static
const
size_t
kBitsPerLevel
=
kNodeSize2Pow
-
SIZEOF_PTR_2POW
;
static
const
size_t
kBitsAtLevel1
=
(
Bits
%
kBitsPerLevel
)
?
Bits
%
kBitsPerLevel
:
kBitsPerLevel
;
static
const
size_t
kHeight
=
(
Bits
+
kBitsPerLevel
-
1
)
/
kBitsPerLevel
;
static_assert
(
kBitsAtLevel1
+
(
kHeight
-
1
)
*
kBitsPerLevel
=
=
Bits
"
AddressRadixTree
parameters
don
'
t
work
out
"
)
;
Mutex
mLock
;
void
*
*
mRoot
;
public
:
bool
Init
(
)
;
inline
void
*
Get
(
void
*
aAddr
)
;
inline
bool
Set
(
void
*
aAddr
void
*
aValue
)
;
inline
bool
Unset
(
void
*
aAddr
)
{
return
Set
(
aAddr
nullptr
)
;
}
private
:
inline
void
*
*
GetSlot
(
void
*
aAddr
bool
aCreate
=
false
)
;
}
;
struct
arena_t
;
struct
arena_bin_t
;
struct
arena_chunk_map_t
{
RedBlackTreeNode
<
arena_chunk_map_t
>
link
;
size_t
bits
;
#
define
CHUNK_MAP_MADVISED
(
(
size_t
)
0x40U
)
#
define
CHUNK_MAP_DECOMMITTED
(
(
size_t
)
0x20U
)
#
define
CHUNK_MAP_MADVISED_OR_DECOMMITTED
(
CHUNK_MAP_MADVISED
|
CHUNK_MAP_DECOMMITTED
)
#
define
CHUNK_MAP_KEY
(
(
size_t
)
0x10U
)
#
define
CHUNK_MAP_DIRTY
(
(
size_t
)
0x08U
)
#
define
CHUNK_MAP_ZEROED
(
(
size_t
)
0x04U
)
#
define
CHUNK_MAP_LARGE
(
(
size_t
)
0x02U
)
#
define
CHUNK_MAP_ALLOCATED
(
(
size_t
)
0x01U
)
}
;
struct
ArenaChunkMapLink
{
static
RedBlackTreeNode
<
arena_chunk_map_t
>
&
GetTreeNode
(
arena_chunk_map_t
*
aThis
)
{
return
aThis
-
>
link
;
}
}
;
struct
ArenaRunTreeTrait
:
public
ArenaChunkMapLink
{
static
inline
int
Compare
(
arena_chunk_map_t
*
aNode
arena_chunk_map_t
*
aOther
)
{
MOZ_ASSERT
(
aNode
)
;
MOZ_ASSERT
(
aOther
)
;
return
CompareAddr
(
aNode
aOther
)
;
}
}
;
struct
ArenaAvailTreeTrait
:
public
ArenaChunkMapLink
{
static
inline
int
Compare
(
arena_chunk_map_t
*
aNode
arena_chunk_map_t
*
aOther
)
{
size_t
size1
=
aNode
-
>
bits
&
~
pagesize_mask
;
size_t
size2
=
aOther
-
>
bits
&
~
pagesize_mask
;
int
ret
=
(
size1
>
size2
)
-
(
size1
<
size2
)
;
return
ret
?
ret
:
CompareAddr
(
(
aNode
-
>
bits
&
CHUNK_MAP_KEY
)
?
nullptr
:
aNode
aOther
)
;
}
}
;
struct
arena_chunk_t
{
arena_t
*
arena
;
RedBlackTreeNode
<
arena_chunk_t
>
link_dirty
;
#
ifdef
MALLOC_DOUBLE_PURGE
mozilla
:
:
DoublyLinkedListElement
<
arena_chunk_t
>
chunks_madvised_elem
;
#
endif
size_t
ndirty
;
arena_chunk_map_t
map
[
1
]
;
}
;
struct
ArenaDirtyChunkTrait
{
static
RedBlackTreeNode
<
arena_chunk_t
>
&
GetTreeNode
(
arena_chunk_t
*
aThis
)
{
return
aThis
-
>
link_dirty
;
}
static
inline
int
Compare
(
arena_chunk_t
*
aNode
arena_chunk_t
*
aOther
)
{
MOZ_ASSERT
(
aNode
)
;
MOZ_ASSERT
(
aOther
)
;
return
CompareAddr
(
aNode
aOther
)
;
}
}
;
#
ifdef
MALLOC_DOUBLE_PURGE
namespace
mozilla
{
template
<
>
struct
GetDoublyLinkedListElement
<
arena_chunk_t
>
{
static
DoublyLinkedListElement
<
arena_chunk_t
>
&
Get
(
arena_chunk_t
*
aThis
)
{
return
aThis
-
>
chunks_madvised_elem
;
}
}
;
}
#
endif
struct
arena_run_t
{
#
if
defined
(
MOZ_DEBUG
)
|
|
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
uint32_t
magic
;
#
define
ARENA_RUN_MAGIC
0x384adf93
#
endif
arena_bin_t
*
bin
;
unsigned
regs_minelm
;
unsigned
nfree
;
unsigned
regs_mask
[
1
]
;
}
;
struct
arena_bin_t
{
arena_run_t
*
runcur
;
RedBlackTree
<
arena_chunk_map_t
ArenaRunTreeTrait
>
runs
;
size_t
reg_size
;
size_t
run_size
;
uint32_t
nregs
;
uint32_t
regs_mask_nelms
;
uint32_t
reg0_offset
;
malloc_bin_stats_t
stats
;
}
;
struct
arena_t
{
#
if
defined
(
MOZ_DEBUG
)
|
|
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
uint32_t
mMagic
;
#
define
ARENA_MAGIC
0x947d3d24
#
endif
arena_id_t
mId
;
RedBlackTreeNode
<
arena_t
>
mLink
;
Mutex
mLock
;
arena_stats_t
mStats
;
private
:
RedBlackTree
<
arena_chunk_t
ArenaDirtyChunkTrait
>
mChunksDirty
;
#
ifdef
MALLOC_DOUBLE_PURGE
mozilla
:
:
DoublyLinkedList
<
arena_chunk_t
>
mChunksMAdvised
;
#
endif
arena_chunk_t
*
mSpare
;
public
:
size_t
mNumDirty
;
size_t
mMaxDirty
;
private
:
RedBlackTree
<
arena_chunk_map_t
ArenaAvailTreeTrait
>
mRunsAvail
;
public
:
arena_bin_t
mBins
[
1
]
;
bool
Init
(
)
;
static
inline
arena_t
*
GetById
(
arena_id_t
aArenaId
)
;
private
:
void
InitChunk
(
arena_chunk_t
*
aChunk
bool
aZeroed
)
;
void
DeallocChunk
(
arena_chunk_t
*
aChunk
)
;
arena_run_t
*
AllocRun
(
arena_bin_t
*
aBin
size_t
aSize
bool
aLarge
bool
aZero
)
;
void
DallocRun
(
arena_run_t
*
aRun
bool
aDirty
)
;
void
SplitRun
(
arena_run_t
*
aRun
size_t
aSize
bool
aLarge
bool
aZero
)
;
void
TrimRunHead
(
arena_chunk_t
*
aChunk
arena_run_t
*
aRun
size_t
aOldSize
size_t
aNewSize
)
;
void
TrimRunTail
(
arena_chunk_t
*
aChunk
arena_run_t
*
aRun
size_t
aOldSize
size_t
aNewSize
bool
dirty
)
;
inline
void
*
MallocBinEasy
(
arena_bin_t
*
aBin
arena_run_t
*
aRun
)
;
void
*
MallocBinHard
(
arena_bin_t
*
aBin
)
;
arena_run_t
*
GetNonFullBinRun
(
arena_bin_t
*
aBin
)
;
inline
void
*
MallocSmall
(
size_t
aSize
bool
aZero
)
;
void
*
MallocLarge
(
size_t
aSize
bool
aZero
)
;
public
:
inline
void
*
Malloc
(
size_t
aSize
bool
aZero
)
;
void
*
Palloc
(
size_t
aAlignment
size_t
aSize
size_t
aAllocSize
)
;
inline
void
DallocSmall
(
arena_chunk_t
*
aChunk
void
*
aPtr
arena_chunk_map_t
*
aMapElm
)
;
void
DallocLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
)
;
void
RallocShrinkLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
;
bool
RallocGrowLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
;
void
Purge
(
bool
aAll
)
;
void
HardPurge
(
)
;
}
;
struct
ArenaTreeTrait
{
static
RedBlackTreeNode
<
arena_t
>
&
GetTreeNode
(
arena_t
*
aThis
)
{
return
aThis
-
>
mLink
;
}
static
inline
int
Compare
(
arena_t
*
aNode
arena_t
*
aOther
)
{
MOZ_ASSERT
(
aNode
)
;
MOZ_ASSERT
(
aOther
)
;
return
(
aNode
-
>
mId
>
aOther
-
>
mId
)
-
(
aNode
-
>
mId
<
aOther
-
>
mId
)
;
}
}
;
static
AddressRadixTree
<
(
SIZEOF_PTR
<
<
3
)
-
CHUNK_2POW_DEFAULT
>
gChunkRTree
;
static
Mutex
chunks_mtx
;
static
RedBlackTree
<
extent_node_t
ExtentTreeSzTrait
>
gChunksBySize
;
static
RedBlackTree
<
extent_node_t
ExtentTreeTrait
>
gChunksByAddress
;
static
Mutex
huge_mtx
;
static
RedBlackTree
<
extent_node_t
ExtentTreeTrait
>
huge
;
static
size_t
huge_allocated
;
static
size_t
huge_mapped
;
static
void
*
base_pages
;
static
void
*
base_next_addr
;
static
void
*
base_next_decommitted
;
static
void
*
base_past_addr
;
static
extent_node_t
*
base_nodes
;
static
Mutex
base_mtx
;
static
size_t
base_mapped
;
static
size_t
base_committed
;
static
RedBlackTree
<
arena_t
ArenaTreeTrait
>
gArenaTree
;
static
unsigned
narenas
;
static
Mutex
arenas_lock
;
#
if
!
defined
(
XP_DARWIN
)
static
MOZ_THREAD_LOCAL
(
arena_t
*
)
thread_arena
;
#
else
static
mozilla
:
:
detail
:
:
ThreadLocal
<
arena_t
*
mozilla
:
:
detail
:
:
ThreadLocalKeyStorage
>
thread_arena
;
#
endif
static
arena_t
*
gMainArena
;
const
uint8_t
kAllocJunk
=
0xe4
;
const
uint8_t
kAllocPoison
=
0xe5
;
#
ifdef
MOZ_DEBUG
static
bool
opt_junk
=
true
;
static
bool
opt_zero
=
false
;
#
else
static
const
bool
opt_junk
=
false
;
static
const
bool
opt_zero
=
false
;
#
endif
static
size_t
opt_dirty_max
=
DIRTY_MAX_DEFAULT
;
static
void
*
chunk_alloc
(
size_t
aSize
size_t
aAlignment
bool
aBase
bool
*
aZeroed
=
nullptr
)
;
static
void
chunk_dealloc
(
void
*
aChunk
size_t
aSize
ChunkType
aType
)
;
static
void
chunk_ensure_zero
(
void
*
aPtr
size_t
aSize
bool
aZeroed
)
;
static
arena_t
*
arenas_extend
(
)
;
static
void
*
huge_malloc
(
size_t
size
bool
zero
)
;
static
void
*
huge_palloc
(
size_t
aSize
size_t
aAlignment
bool
aZero
)
;
static
void
*
huge_ralloc
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
;
static
void
huge_dalloc
(
void
*
aPtr
)
;
#
ifdef
XP_WIN
extern
"
C
"
#
else
static
#
endif
bool
malloc_init_hard
(
void
)
;
#
ifdef
XP_DARWIN
#
define
FORK_HOOK
extern
"
C
"
#
else
#
define
FORK_HOOK
static
#
endif
FORK_HOOK
void
_malloc_prefork
(
void
)
;
FORK_HOOK
void
_malloc_postfork_parent
(
void
)
;
FORK_HOOK
void
_malloc_postfork_child
(
void
)
;
static
inline
size_t
load_acquire_z
(
size_t
*
p
)
{
volatile
size_t
result
=
*
p
;
#
ifdef
XP_WIN
volatile
long
dummy
=
0
;
InterlockedExchange
(
&
dummy
1
)
;
#
else
__sync_synchronize
(
)
;
#
endif
return
result
;
}
static
void
_malloc_message
(
const
char
*
p
)
{
#
if
!
defined
(
XP_WIN
)
#
define
_write
write
#
endif
if
(
_write
(
STDERR_FILENO
p
(
unsigned
int
)
strlen
(
p
)
)
<
0
)
return
;
}
template
<
typename
.
.
.
Args
>
static
void
_malloc_message
(
const
char
*
p
Args
.
.
.
args
)
{
_malloc_message
(
p
)
;
_malloc_message
(
args
.
.
.
)
;
}
#
include
"
mozilla
/
Assertions
.
h
"
#
include
"
mozilla
/
Attributes
.
h
"
#
include
"
mozilla
/
TaggedAnonymousMemory
.
h
"
#
ifdef
ANDROID
extern
"
C
"
MOZ_EXPORT
int
pthread_atfork
(
void
(
*
)
(
void
)
void
(
*
)
(
void
)
void
(
*
)
(
void
)
)
;
#
endif
bool
Mutex
:
:
Init
(
)
{
#
if
defined
(
XP_WIN
)
if
(
!
InitializeCriticalSectionAndSpinCount
(
&
mMutex
_CRT_SPINCOUNT
)
)
{
return
false
;
}
#
elif
defined
(
XP_DARWIN
)
mMutex
=
OS_SPINLOCK_INIT
;
#
elif
defined
(
XP_LINUX
)
&
&
!
defined
(
ANDROID
)
pthread_mutexattr_t
attr
;
if
(
pthread_mutexattr_init
(
&
attr
)
!
=
0
)
{
return
false
;
}
pthread_mutexattr_settype
(
&
attr
PTHREAD_MUTEX_ADAPTIVE_NP
)
;
if
(
pthread_mutex_init
(
&
mMutex
&
attr
)
!
=
0
)
{
pthread_mutexattr_destroy
(
&
attr
)
;
return
false
;
}
pthread_mutexattr_destroy
(
&
attr
)
;
#
else
if
(
pthread_mutex_init
(
&
mMutex
nullptr
)
!
=
0
)
{
return
false
;
}
#
endif
return
true
;
}
void
Mutex
:
:
Lock
(
)
{
#
if
defined
(
XP_WIN
)
EnterCriticalSection
(
&
mMutex
)
;
#
elif
defined
(
XP_DARWIN
)
OSSpinLockLock
(
&
mMutex
)
;
#
else
pthread_mutex_lock
(
&
mMutex
)
;
#
endif
}
void
Mutex
:
:
Unlock
(
)
{
#
if
defined
(
XP_WIN
)
LeaveCriticalSection
(
&
mMutex
)
;
#
elif
defined
(
XP_DARWIN
)
OSSpinLockUnlock
(
&
mMutex
)
;
#
else
pthread_mutex_unlock
(
&
mMutex
)
;
#
endif
}
#
define
CHUNK_ADDR2BASE
(
a
)
\
(
(
void
*
)
(
(
uintptr_t
)
(
a
)
&
~
chunksize_mask
)
)
#
define
CHUNK_ADDR2OFFSET
(
a
)
\
(
(
size_t
)
(
(
uintptr_t
)
(
a
)
&
chunksize_mask
)
)
#
define
CHUNK_CEILING
(
s
)
\
(
(
(
s
)
+
chunksize_mask
)
&
~
chunksize_mask
)
#
define
CACHELINE_CEILING
(
s
)
\
(
(
(
s
)
+
(
CACHELINE
-
1
)
)
&
~
(
CACHELINE
-
1
)
)
#
define
QUANTUM_CEILING
(
a
)
\
(
(
(
a
)
+
quantum_mask
)
&
~
quantum_mask
)
#
define
PAGE_CEILING
(
s
)
\
(
(
(
s
)
+
pagesize_mask
)
&
~
pagesize_mask
)
static
inline
size_t
pow2_ceil
(
size_t
x
)
{
x
-
-
;
x
|
=
x
>
>
1
;
x
|
=
x
>
>
2
;
x
|
=
x
>
>
4
;
x
|
=
x
>
>
8
;
x
|
=
x
>
>
16
;
#
if
(
SIZEOF_PTR
=
=
8
)
x
|
=
x
>
>
32
;
#
endif
x
+
+
;
return
(
x
)
;
}
static
inline
const
char
*
_getprogname
(
void
)
{
return
(
"
<
jemalloc
>
"
)
;
}
static
inline
void
pages_decommit
(
void
*
addr
size_t
size
)
{
#
ifdef
XP_WIN
size_t
pages_size
=
std
:
:
min
(
size
chunksize
-
CHUNK_ADDR2OFFSET
(
(
uintptr_t
)
addr
)
)
;
while
(
size
>
0
)
{
if
(
!
VirtualFree
(
addr
pages_size
MEM_DECOMMIT
)
)
MOZ_CRASH
(
)
;
addr
=
(
void
*
)
(
(
uintptr_t
)
addr
+
pages_size
)
;
size
-
=
pages_size
;
pages_size
=
std
:
:
min
(
size
chunksize
)
;
}
#
else
if
(
mmap
(
addr
size
PROT_NONE
MAP_FIXED
|
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
=
=
MAP_FAILED
)
MOZ_CRASH
(
)
;
MozTagAnonymousMemory
(
addr
size
"
jemalloc
-
decommitted
"
)
;
#
endif
}
static
inline
void
pages_commit
(
void
*
addr
size_t
size
)
{
#
ifdef
XP_WIN
size_t
pages_size
=
std
:
:
min
(
size
chunksize
-
CHUNK_ADDR2OFFSET
(
(
uintptr_t
)
addr
)
)
;
while
(
size
>
0
)
{
if
(
!
VirtualAlloc
(
addr
pages_size
MEM_COMMIT
PAGE_READWRITE
)
)
MOZ_CRASH
(
)
;
addr
=
(
void
*
)
(
(
uintptr_t
)
addr
+
pages_size
)
;
size
-
=
pages_size
;
pages_size
=
std
:
:
min
(
size
chunksize
)
;
}
#
else
if
(
mmap
(
addr
size
PROT_READ
|
PROT_WRITE
MAP_FIXED
|
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
=
=
MAP_FAILED
)
MOZ_CRASH
(
)
;
MozTagAnonymousMemory
(
addr
size
"
jemalloc
"
)
;
#
endif
}
static
bool
base_pages_alloc
(
size_t
minsize
)
{
size_t
csize
;
size_t
pminsize
;
MOZ_ASSERT
(
minsize
!
=
0
)
;
csize
=
CHUNK_CEILING
(
minsize
)
;
base_pages
=
chunk_alloc
(
csize
chunksize
true
)
;
if
(
!
base_pages
)
return
(
true
)
;
base_next_addr
=
base_pages
;
base_past_addr
=
(
void
*
)
(
(
uintptr_t
)
base_pages
+
csize
)
;
pminsize
=
PAGE_CEILING
(
minsize
)
;
base_next_decommitted
=
(
void
*
)
(
(
uintptr_t
)
base_pages
+
pminsize
)
;
#
if
defined
(
MALLOC_DECOMMIT
)
if
(
pminsize
<
csize
)
pages_decommit
(
base_next_decommitted
csize
-
pminsize
)
;
#
endif
base_mapped
+
=
csize
;
base_committed
+
=
pminsize
;
return
(
false
)
;
}
static
void
*
base_alloc
(
size_t
aSize
)
{
void
*
ret
;
size_t
csize
;
csize
=
CACHELINE_CEILING
(
aSize
)
;
MutexAutoLock
lock
(
base_mtx
)
;
if
(
(
uintptr_t
)
base_next_addr
+
csize
>
(
uintptr_t
)
base_past_addr
)
{
if
(
base_pages_alloc
(
csize
)
)
{
return
nullptr
;
}
}
ret
=
base_next_addr
;
base_next_addr
=
(
void
*
)
(
(
uintptr_t
)
base_next_addr
+
csize
)
;
if
(
(
uintptr_t
)
base_next_addr
>
(
uintptr_t
)
base_next_decommitted
)
{
void
*
pbase_next_addr
=
(
void
*
)
(
PAGE_CEILING
(
(
uintptr_t
)
base_next_addr
)
)
;
#
ifdef
MALLOC_DECOMMIT
pages_commit
(
base_next_decommitted
(
uintptr_t
)
pbase_next_addr
-
(
uintptr_t
)
base_next_decommitted
)
;
#
endif
base_next_decommitted
=
pbase_next_addr
;
base_committed
+
=
(
uintptr_t
)
pbase_next_addr
-
(
uintptr_t
)
base_next_decommitted
;
}
return
ret
;
}
static
void
*
base_calloc
(
size_t
number
size_t
size
)
{
void
*
ret
;
ret
=
base_alloc
(
number
*
size
)
;
memset
(
ret
0
number
*
size
)
;
return
(
ret
)
;
}
static
extent_node_t
*
base_node_alloc
(
void
)
{
extent_node_t
*
ret
;
base_mtx
.
Lock
(
)
;
if
(
base_nodes
)
{
ret
=
base_nodes
;
base_nodes
=
*
(
extent_node_t
*
*
)
ret
;
base_mtx
.
Unlock
(
)
;
}
else
{
base_mtx
.
Unlock
(
)
;
ret
=
(
extent_node_t
*
)
base_alloc
(
sizeof
(
extent_node_t
)
)
;
}
return
(
ret
)
;
}
static
void
base_node_dealloc
(
extent_node_t
*
aNode
)
{
MutexAutoLock
lock
(
base_mtx
)
;
*
(
extent_node_t
*
*
)
aNode
=
base_nodes
;
base_nodes
=
aNode
;
}
struct
BaseNodeFreePolicy
{
void
operator
(
)
(
extent_node_t
*
aPtr
)
{
base_node_dealloc
(
aPtr
)
;
}
}
;
using
UniqueBaseNode
=
mozilla
:
:
UniquePtr
<
extent_node_t
BaseNodeFreePolicy
>
;
#
ifdef
XP_WIN
static
void
*
pages_map
(
void
*
addr
size_t
size
)
{
void
*
ret
=
nullptr
;
ret
=
VirtualAlloc
(
addr
size
MEM_COMMIT
|
MEM_RESERVE
PAGE_READWRITE
)
;
return
(
ret
)
;
}
static
void
pages_unmap
(
void
*
addr
size_t
size
)
{
if
(
VirtualFree
(
addr
0
MEM_RELEASE
)
=
=
0
)
{
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Error
in
VirtualFree
(
)
\
n
"
)
;
}
}
#
else
static
void
*
pages_map
(
void
*
addr
size_t
size
)
{
void
*
ret
;
#
if
defined
(
__ia64__
)
|
|
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
)
bool
check_placement
=
true
;
if
(
!
addr
)
{
addr
=
(
void
*
)
0x0000070000000000
;
check_placement
=
false
;
}
#
endif
#
if
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
const
uintptr_t
start
=
0x0000070000000000ULL
;
const
uintptr_t
end
=
0x0000800000000000ULL
;
uintptr_t
hint
;
void
*
region
=
MAP_FAILED
;
for
(
hint
=
start
;
region
=
=
MAP_FAILED
&
&
hint
+
size
<
=
end
;
hint
+
=
chunksize
)
{
region
=
mmap
(
(
void
*
)
hint
size
PROT_READ
|
PROT_WRITE
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
;
if
(
region
!
=
MAP_FAILED
)
{
if
(
(
(
size_t
)
region
+
(
size
-
1
)
)
&
0xffff800000000000
)
{
if
(
munmap
(
region
size
)
)
{
MOZ_ASSERT
(
errno
=
=
ENOMEM
)
;
}
region
=
MAP_FAILED
;
}
}
}
ret
=
region
;
#
else
ret
=
mmap
(
addr
size
PROT_READ
|
PROT_WRITE
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
;
MOZ_ASSERT
(
ret
)
;
#
endif
if
(
ret
=
=
MAP_FAILED
)
{
ret
=
nullptr
;
}
#
if
defined
(
__ia64__
)
|
|
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
)
else
if
(
(
long
long
)
ret
&
0xffff800000000000
)
{
munmap
(
ret
size
)
;
ret
=
nullptr
;
}
else
if
(
check_placement
&
&
ret
!
=
addr
)
{
#
else
else
if
(
addr
&
&
ret
!
=
addr
)
{
#
endif
if
(
munmap
(
ret
size
)
=
=
-
1
)
{
char
buf
[
STRERROR_BUF
]
;
if
(
strerror_r
(
errno
buf
sizeof
(
buf
)
)
=
=
0
)
{
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Error
in
munmap
(
)
:
"
buf
"
\
n
"
)
;
}
}
ret
=
nullptr
;
}
if
(
ret
)
{
MozTagAnonymousMemory
(
ret
size
"
jemalloc
"
)
;
}
#
if
defined
(
__ia64__
)
|
|
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
)
MOZ_ASSERT
(
!
ret
|
|
(
!
check_placement
&
&
ret
)
|
|
(
check_placement
&
&
ret
=
=
addr
)
)
;
#
else
MOZ_ASSERT
(
!
ret
|
|
(
!
addr
&
&
ret
!
=
addr
)
|
|
(
addr
&
&
ret
=
=
addr
)
)
;
#
endif
return
(
ret
)
;
}
static
void
pages_unmap
(
void
*
addr
size_t
size
)
{
if
(
munmap
(
addr
size
)
=
=
-
1
)
{
char
buf
[
STRERROR_BUF
]
;
if
(
strerror_r
(
errno
buf
sizeof
(
buf
)
)
=
=
0
)
{
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Error
in
munmap
(
)
:
"
buf
"
\
n
"
)
;
}
}
}
#
endif
#
ifdef
XP_DARWIN
#
define
VM_COPY_MIN
(
pagesize
<
<
5
)
static
inline
void
pages_copy
(
void
*
dest
const
void
*
src
size_t
n
)
{
MOZ_ASSERT
(
(
void
*
)
(
(
uintptr_t
)
dest
&
~
pagesize_mask
)
=
=
dest
)
;
MOZ_ASSERT
(
n
>
=
VM_COPY_MIN
)
;
MOZ_ASSERT
(
(
void
*
)
(
(
uintptr_t
)
src
&
~
pagesize_mask
)
=
=
src
)
;
vm_copy
(
mach_task_self
(
)
(
vm_address_t
)
src
(
vm_size_t
)
n
(
vm_address_t
)
dest
)
;
}
#
endif
template
<
size_t
Bits
>
bool
AddressRadixTree
<
Bits
>
:
:
Init
(
)
{
mLock
.
Init
(
)
;
mRoot
=
(
void
*
*
)
base_calloc
(
1
<
<
kBitsAtLevel1
sizeof
(
void
*
)
)
;
return
mRoot
;
}
template
<
size_t
Bits
>
void
*
*
AddressRadixTree
<
Bits
>
:
:
GetSlot
(
void
*
aKey
bool
aCreate
)
{
uintptr_t
key
=
reinterpret_cast
<
uintptr_t
>
(
aKey
)
;
uintptr_t
subkey
;
unsigned
i
lshift
height
bits
;
void
*
*
node
;
void
*
*
child
;
for
(
i
=
lshift
=
0
height
=
kHeight
node
=
mRoot
;
i
<
height
-
1
;
i
+
+
lshift
+
=
bits
node
=
child
)
{
bits
=
i
?
kBitsPerLevel
:
kBitsAtLevel1
;
subkey
=
(
key
<
<
lshift
)
>
>
(
(
SIZEOF_PTR
<
<
3
)
-
bits
)
;
child
=
(
void
*
*
)
node
[
subkey
]
;
if
(
!
child
&
&
aCreate
)
{
child
=
(
void
*
*
)
base_calloc
(
1
<
<
kBitsPerLevel
sizeof
(
void
*
)
)
;
if
(
child
)
{
node
[
subkey
]
=
child
;
}
}
if
(
!
child
)
{
return
nullptr
;
}
}
bits
=
i
?
kBitsPerLevel
:
kBitsAtLevel1
;
subkey
=
(
key
<
<
lshift
)
>
>
(
(
SIZEOF_PTR
<
<
3
)
-
bits
)
;
return
&
node
[
subkey
]
;
}
template
<
size_t
Bits
>
void
*
AddressRadixTree
<
Bits
>
:
:
Get
(
void
*
aKey
)
{
void
*
ret
=
nullptr
;
void
*
*
slot
=
GetSlot
(
aKey
)
;
if
(
slot
)
{
ret
=
*
slot
;
}
#
ifdef
MOZ_DEBUG
MutexAutoLock
lock
(
mLock
)
;
if
(
!
slot
)
{
slot
=
GetSlot
(
aKey
)
;
}
if
(
slot
)
{
MOZ_ASSERT
(
ret
=
=
*
slot
)
;
}
else
{
MOZ_ASSERT
(
ret
=
=
nullptr
)
;
}
#
endif
return
ret
;
}
template
<
size_t
Bits
>
bool
AddressRadixTree
<
Bits
>
:
:
Set
(
void
*
aKey
void
*
aValue
)
{
MutexAutoLock
lock
(
mLock
)
;
void
*
*
slot
=
GetSlot
(
aKey
true
)
;
if
(
slot
)
{
*
slot
=
aValue
;
}
return
slot
;
}
#
define
ALIGNMENT_ADDR2OFFSET
(
a
alignment
)
\
(
(
size_t
)
(
(
uintptr_t
)
(
a
)
&
(
alignment
-
1
)
)
)
#
define
ALIGNMENT_CEILING
(
s
alignment
)
\
(
(
(
s
)
+
(
alignment
-
1
)
)
&
(
~
(
alignment
-
1
)
)
)
static
void
*
pages_trim
(
void
*
addr
size_t
alloc_size
size_t
leadsize
size_t
size
)
{
void
*
ret
=
(
void
*
)
(
(
uintptr_t
)
addr
+
leadsize
)
;
MOZ_ASSERT
(
alloc_size
>
=
leadsize
+
size
)
;
#
ifdef
XP_WIN
{
void
*
new_addr
;
pages_unmap
(
addr
alloc_size
)
;
new_addr
=
pages_map
(
ret
size
)
;
if
(
new_addr
=
=
ret
)
return
(
ret
)
;
if
(
new_addr
)
pages_unmap
(
new_addr
size
)
;
return
nullptr
;
}
#
else
{
size_t
trailsize
=
alloc_size
-
leadsize
-
size
;
if
(
leadsize
!
=
0
)
pages_unmap
(
addr
leadsize
)
;
if
(
trailsize
!
=
0
)
pages_unmap
(
(
void
*
)
(
(
uintptr_t
)
ret
+
size
)
trailsize
)
;
return
(
ret
)
;
}
#
endif
}
static
void
*
chunk_alloc_mmap_slow
(
size_t
size
size_t
alignment
)
{
void
*
ret
*
pages
;
size_t
alloc_size
leadsize
;
alloc_size
=
size
+
alignment
-
pagesize
;
if
(
alloc_size
<
size
)
return
nullptr
;
do
{
pages
=
pages_map
(
nullptr
alloc_size
)
;
if
(
!
pages
)
return
nullptr
;
leadsize
=
ALIGNMENT_CEILING
(
(
uintptr_t
)
pages
alignment
)
-
(
uintptr_t
)
pages
;
ret
=
pages_trim
(
pages
alloc_size
leadsize
size
)
;
}
while
(
!
ret
)
;
MOZ_ASSERT
(
ret
)
;
return
(
ret
)
;
}
static
void
*
chunk_alloc_mmap
(
size_t
size
size_t
alignment
)
{
void
*
ret
;
size_t
offset
;
ret
=
pages_map
(
nullptr
size
)
;
if
(
!
ret
)
return
nullptr
;
offset
=
ALIGNMENT_ADDR2OFFSET
(
ret
alignment
)
;
if
(
offset
!
=
0
)
{
pages_unmap
(
ret
size
)
;
return
(
chunk_alloc_mmap_slow
(
size
alignment
)
)
;
}
MOZ_ASSERT
(
ret
)
;
return
(
ret
)
;
}
static
bool
pages_purge
(
void
*
addr
size_t
length
bool
force_zero
)
{
#
ifdef
MALLOC_DECOMMIT
pages_decommit
(
addr
length
)
;
return
true
;
#
else
#
ifndef
XP_LINUX
if
(
force_zero
)
memset
(
addr
0
length
)
;
#
endif
#
ifdef
XP_WIN
size_t
pages_size
=
std
:
:
min
(
length
chunksize
-
CHUNK_ADDR2OFFSET
(
(
uintptr_t
)
addr
)
)
;
while
(
length
>
0
)
{
VirtualAlloc
(
addr
pages_size
MEM_RESET
PAGE_READWRITE
)
;
addr
=
(
void
*
)
(
(
uintptr_t
)
addr
+
pages_size
)
;
length
-
=
pages_size
;
pages_size
=
std
:
:
min
(
length
chunksize
)
;
}
return
force_zero
;
#
else
#
ifdef
XP_LINUX
#
define
JEMALLOC_MADV_PURGE
MADV_DONTNEED
#
define
JEMALLOC_MADV_ZEROS
true
#
else
#
define
JEMALLOC_MADV_PURGE
MADV_FREE
#
define
JEMALLOC_MADV_ZEROS
force_zero
#
endif
int
err
=
madvise
(
addr
length
JEMALLOC_MADV_PURGE
)
;
return
JEMALLOC_MADV_ZEROS
&
&
err
=
=
0
;
#
undef
JEMALLOC_MADV_PURGE
#
undef
JEMALLOC_MADV_ZEROS
#
endif
#
endif
}
static
void
*
chunk_recycle
(
size_t
aSize
size_t
aAlignment
bool
*
aZeroed
)
{
extent_node_t
key
;
size_t
alloc_size
=
aSize
+
aAlignment
-
chunksize
;
if
(
alloc_size
<
aSize
)
{
return
nullptr
;
}
key
.
addr
=
nullptr
;
key
.
size
=
alloc_size
;
chunks_mtx
.
Lock
(
)
;
extent_node_t
*
node
=
gChunksBySize
.
SearchOrNext
(
&
key
)
;
if
(
!
node
)
{
chunks_mtx
.
Unlock
(
)
;
return
nullptr
;
}
size_t
leadsize
=
ALIGNMENT_CEILING
(
(
uintptr_t
)
node
-
>
addr
aAlignment
)
-
(
uintptr_t
)
node
-
>
addr
;
MOZ_ASSERT
(
node
-
>
size
>
=
leadsize
+
aSize
)
;
size_t
trailsize
=
node
-
>
size
-
leadsize
-
aSize
;
void
*
ret
=
(
void
*
)
(
(
uintptr_t
)
node
-
>
addr
+
leadsize
)
;
ChunkType
chunk_type
=
node
-
>
chunk_type
;
if
(
aZeroed
)
{
*
aZeroed
=
(
chunk_type
=
=
ZEROED_CHUNK
)
;
}
gChunksBySize
.
Remove
(
node
)
;
gChunksByAddress
.
Remove
(
node
)
;
if
(
leadsize
!
=
0
)
{
node
-
>
size
=
leadsize
;
gChunksBySize
.
Insert
(
node
)
;
gChunksByAddress
.
Insert
(
node
)
;
node
=
nullptr
;
}
if
(
trailsize
!
=
0
)
{
if
(
!
node
)
{
chunks_mtx
.
Unlock
(
)
;
node
=
base_node_alloc
(
)
;
if
(
!
node
)
{
chunk_dealloc
(
ret
aSize
chunk_type
)
;
return
nullptr
;
}
chunks_mtx
.
Lock
(
)
;
}
node
-
>
addr
=
(
void
*
)
(
(
uintptr_t
)
(
ret
)
+
aSize
)
;
node
-
>
size
=
trailsize
;
node
-
>
chunk_type
=
chunk_type
;
gChunksBySize
.
Insert
(
node
)
;
gChunksByAddress
.
Insert
(
node
)
;
node
=
nullptr
;
}
recycled_size
-
=
aSize
;
chunks_mtx
.
Unlock
(
)
;
if
(
node
)
{
base_node_dealloc
(
node
)
;
}
#
ifdef
MALLOC_DECOMMIT
pages_commit
(
ret
aSize
)
;
if
(
aZeroed
)
{
*
aZeroed
=
true
;
}
#
endif
return
ret
;
}
#
ifdef
XP_WIN
#
define
CAN_RECYCLE
(
size
)
(
size
=
=
chunksize
)
#
else
#
define
CAN_RECYCLE
(
size
)
true
#
endif
static
void
*
chunk_alloc
(
size_t
aSize
size_t
aAlignment
bool
aBase
bool
*
aZeroed
)
{
void
*
ret
=
nullptr
;
MOZ_ASSERT
(
aSize
!
=
0
)
;
MOZ_ASSERT
(
(
aSize
&
chunksize_mask
)
=
=
0
)
;
MOZ_ASSERT
(
aAlignment
!
=
0
)
;
MOZ_ASSERT
(
(
aAlignment
&
chunksize_mask
)
=
=
0
)
;
if
(
CAN_RECYCLE
(
aSize
)
&
&
!
aBase
)
{
ret
=
chunk_recycle
(
aSize
aAlignment
aZeroed
)
;
}
if
(
!
ret
)
{
ret
=
chunk_alloc_mmap
(
aSize
aAlignment
)
;
if
(
aZeroed
)
{
*
aZeroed
=
true
;
}
}
if
(
ret
&
&
!
aBase
)
{
if
(
!
gChunkRTree
.
Set
(
ret
ret
)
)
{
chunk_dealloc
(
ret
aSize
UNKNOWN_CHUNK
)
;
return
nullptr
;
}
}
MOZ_ASSERT
(
CHUNK_ADDR2BASE
(
ret
)
=
=
ret
)
;
return
ret
;
}
static
void
chunk_ensure_zero
(
void
*
aPtr
size_t
aSize
bool
aZeroed
)
{
if
(
aZeroed
=
=
false
)
{
memset
(
aPtr
0
aSize
)
;
}
#
ifdef
MOZ_DEBUG
else
{
size_t
i
;
size_t
*
p
=
(
size_t
*
)
(
uintptr_t
)
aPtr
;
for
(
i
=
0
;
i
<
aSize
/
sizeof
(
size_t
)
;
i
+
+
)
{
MOZ_ASSERT
(
p
[
i
]
=
=
0
)
;
}
}
#
endif
}
static
void
chunk_record
(
void
*
aChunk
size_t
aSize
ChunkType
aType
)
{
extent_node_t
key
;
if
(
aType
!
=
ZEROED_CHUNK
)
{
if
(
pages_purge
(
aChunk
aSize
aType
=
=
HUGE_CHUNK
)
)
{
aType
=
ZEROED_CHUNK
;
}
}
UniqueBaseNode
xnode
(
base_node_alloc
(
)
)
;
UniqueBaseNode
xprev
;
MutexAutoLock
lock
(
chunks_mtx
)
;
key
.
addr
=
(
void
*
)
(
(
uintptr_t
)
aChunk
+
aSize
)
;
extent_node_t
*
node
=
gChunksByAddress
.
SearchOrNext
(
&
key
)
;
if
(
node
&
&
node
-
>
addr
=
=
key
.
addr
)
{
gChunksBySize
.
Remove
(
node
)
;
node
-
>
addr
=
aChunk
;
node
-
>
size
+
=
aSize
;
if
(
node
-
>
chunk_type
!
=
aType
)
{
node
-
>
chunk_type
=
RECYCLED_CHUNK
;
}
gChunksBySize
.
Insert
(
node
)
;
}
else
{
if
(
!
xnode
)
{
return
;
}
node
=
xnode
.
release
(
)
;
node
-
>
addr
=
aChunk
;
node
-
>
size
=
aSize
;
node
-
>
chunk_type
=
aType
;
gChunksByAddress
.
Insert
(
node
)
;
gChunksBySize
.
Insert
(
node
)
;
}
extent_node_t
*
prev
=
gChunksByAddress
.
Prev
(
node
)
;
if
(
prev
&
&
(
void
*
)
(
(
uintptr_t
)
prev
-
>
addr
+
prev
-
>
size
)
=
=
aChunk
)
{
gChunksBySize
.
Remove
(
prev
)
;
gChunksByAddress
.
Remove
(
prev
)
;
gChunksBySize
.
Remove
(
node
)
;
node
-
>
addr
=
prev
-
>
addr
;
node
-
>
size
+
=
prev
-
>
size
;
if
(
node
-
>
chunk_type
!
=
prev
-
>
chunk_type
)
{
node
-
>
chunk_type
=
RECYCLED_CHUNK
;
}
gChunksBySize
.
Insert
(
node
)
;
xprev
.
reset
(
prev
)
;
}
recycled_size
+
=
aSize
;
}
static
void
chunk_dealloc
(
void
*
aChunk
size_t
aSize
ChunkType
aType
)
{
MOZ_ASSERT
(
aChunk
)
;
MOZ_ASSERT
(
CHUNK_ADDR2BASE
(
aChunk
)
=
=
aChunk
)
;
MOZ_ASSERT
(
aSize
!
=
0
)
;
MOZ_ASSERT
(
(
aSize
&
chunksize_mask
)
=
=
0
)
;
gChunkRTree
.
Unset
(
aChunk
)
;
if
(
CAN_RECYCLE
(
aSize
)
)
{
size_t
recycled_so_far
=
load_acquire_z
(
&
recycled_size
)
;
if
(
recycled_so_far
<
recycle_limit
)
{
size_t
recycle_remaining
=
recycle_limit
-
recycled_so_far
;
size_t
to_recycle
;
if
(
aSize
>
recycle_remaining
)
{
to_recycle
=
recycle_remaining
;
pages_trim
(
aChunk
aSize
0
to_recycle
)
;
}
else
{
to_recycle
=
aSize
;
}
chunk_record
(
aChunk
to_recycle
aType
)
;
return
;
}
}
pages_unmap
(
aChunk
aSize
)
;
}
#
undef
CAN_RECYCLE
static
inline
arena_t
*
thread_local_arena
(
bool
enabled
)
{
arena_t
*
arena
;
if
(
enabled
)
{
arena
=
arenas_extend
(
)
;
}
else
{
arena
=
gMainArena
;
}
thread_arena
.
set
(
arena
)
;
return
arena
;
}
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_thread_local_arena
(
bool
aEnabled
)
{
thread_local_arena
(
aEnabled
)
;
}
static
inline
arena_t
*
choose_arena
(
size_t
size
)
{
arena_t
*
ret
=
nullptr
;
if
(
size
<
=
small_max
)
{
ret
=
thread_arena
.
get
(
)
;
}
if
(
!
ret
)
{
ret
=
thread_local_arena
(
false
)
;
}
MOZ_DIAGNOSTIC_ASSERT
(
ret
)
;
return
(
ret
)
;
}
static
inline
void
*
arena_run_reg_alloc
(
arena_run_t
*
run
arena_bin_t
*
bin
)
{
void
*
ret
;
unsigned
i
mask
bit
regind
;
MOZ_ASSERT
(
run
-
>
magic
=
=
ARENA_RUN_MAGIC
)
;
MOZ_ASSERT
(
run
-
>
regs_minelm
<
bin
-
>
regs_mask_nelms
)
;
i
=
run
-
>
regs_minelm
;
mask
=
run
-
>
regs_mask
[
i
]
;
if
(
mask
!
=
0
)
{
bit
=
ffs
(
(
int
)
mask
)
-
1
;
regind
=
(
(
i
<
<
(
SIZEOF_INT_2POW
+
3
)
)
+
bit
)
;
MOZ_ASSERT
(
regind
<
bin
-
>
nregs
)
;
ret
=
(
void
*
)
(
(
(
uintptr_t
)
run
)
+
bin
-
>
reg0_offset
+
(
bin
-
>
reg_size
*
regind
)
)
;
mask
^
=
(
1U
<
<
bit
)
;
run
-
>
regs_mask
[
i
]
=
mask
;
return
(
ret
)
;
}
for
(
i
+
+
;
i
<
bin
-
>
regs_mask_nelms
;
i
+
+
)
{
mask
=
run
-
>
regs_mask
[
i
]
;
if
(
mask
!
=
0
)
{
bit
=
ffs
(
(
int
)
mask
)
-
1
;
regind
=
(
(
i
<
<
(
SIZEOF_INT_2POW
+
3
)
)
+
bit
)
;
MOZ_ASSERT
(
regind
<
bin
-
>
nregs
)
;
ret
=
(
void
*
)
(
(
(
uintptr_t
)
run
)
+
bin
-
>
reg0_offset
+
(
bin
-
>
reg_size
*
regind
)
)
;
mask
^
=
(
1U
<
<
bit
)
;
run
-
>
regs_mask
[
i
]
=
mask
;
run
-
>
regs_minelm
=
i
;
return
(
ret
)
;
}
}
MOZ_DIAGNOSTIC_ASSERT
(
0
)
;
return
nullptr
;
}
static
inline
void
arena_run_reg_dalloc
(
arena_run_t
*
run
arena_bin_t
*
bin
void
*
ptr
size_t
size
)
{
#
define
SIZE_INV_SHIFT
21
#
define
SIZE_INV
(
s
)
(
(
(
1U
<
<
SIZE_INV_SHIFT
)
/
(
s
<
<
QUANTUM_2POW_MIN
)
)
+
1
)
static
const
unsigned
size_invs
[
]
=
{
SIZE_INV
(
3
)
SIZE_INV
(
4
)
SIZE_INV
(
5
)
SIZE_INV
(
6
)
SIZE_INV
(
7
)
SIZE_INV
(
8
)
SIZE_INV
(
9
)
SIZE_INV
(
10
)
SIZE_INV
(
11
)
SIZE_INV
(
12
)
SIZE_INV
(
13
)
SIZE_INV
(
14
)
SIZE_INV
(
15
)
SIZE_INV
(
16
)
SIZE_INV
(
17
)
SIZE_INV
(
18
)
SIZE_INV
(
19
)
SIZE_INV
(
20
)
SIZE_INV
(
21
)
SIZE_INV
(
22
)
SIZE_INV
(
23
)
SIZE_INV
(
24
)
SIZE_INV
(
25
)
SIZE_INV
(
26
)
SIZE_INV
(
27
)
SIZE_INV
(
28
)
SIZE_INV
(
29
)
SIZE_INV
(
30
)
SIZE_INV
(
31
)
#
if
(
QUANTUM_2POW_MIN
<
4
)
SIZE_INV
(
32
)
SIZE_INV
(
33
)
SIZE_INV
(
34
)
SIZE_INV
(
35
)
SIZE_INV
(
36
)
SIZE_INV
(
37
)
SIZE_INV
(
38
)
SIZE_INV
(
39
)
SIZE_INV
(
40
)
SIZE_INV
(
41
)
SIZE_INV
(
42
)
SIZE_INV
(
43
)
SIZE_INV
(
44
)
SIZE_INV
(
45
)
SIZE_INV
(
46
)
SIZE_INV
(
47
)
SIZE_INV
(
48
)
SIZE_INV
(
49
)
SIZE_INV
(
50
)
SIZE_INV
(
51
)
SIZE_INV
(
52
)
SIZE_INV
(
53
)
SIZE_INV
(
54
)
SIZE_INV
(
55
)
SIZE_INV
(
56
)
SIZE_INV
(
57
)
SIZE_INV
(
58
)
SIZE_INV
(
59
)
SIZE_INV
(
60
)
SIZE_INV
(
61
)
SIZE_INV
(
62
)
SIZE_INV
(
63
)
#
endif
}
;
unsigned
diff
regind
elm
bit
;
MOZ_ASSERT
(
run
-
>
magic
=
=
ARENA_RUN_MAGIC
)
;
MOZ_ASSERT
(
(
(
sizeof
(
size_invs
)
)
/
sizeof
(
unsigned
)
)
+
3
>
=
(
SMALL_MAX_DEFAULT
>
>
QUANTUM_2POW_MIN
)
)
;
diff
=
(
unsigned
)
(
(
uintptr_t
)
ptr
-
(
uintptr_t
)
run
-
bin
-
>
reg0_offset
)
;
if
(
(
size
&
(
size
-
1
)
)
=
=
0
)
{
static
const
unsigned
char
log2_table
[
]
=
{
0
1
0
2
0
0
0
3
0
0
0
0
0
0
0
4
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
5
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
6
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
7
}
;
if
(
size
<
=
128
)
regind
=
(
diff
>
>
log2_table
[
size
-
1
]
)
;
else
if
(
size
<
=
32768
)
regind
=
diff
>
>
(
8
+
log2_table
[
(
size
>
>
8
)
-
1
]
)
;
else
{
regind
=
diff
/
size
;
}
}
else
if
(
size
<
=
(
(
sizeof
(
size_invs
)
/
sizeof
(
unsigned
)
)
<
<
QUANTUM_2POW_MIN
)
+
2
)
{
regind
=
size_invs
[
(
size
>
>
QUANTUM_2POW_MIN
)
-
3
]
*
diff
;
regind
>
>
=
SIZE_INV_SHIFT
;
}
else
{
regind
=
diff
/
size
;
}
;
MOZ_DIAGNOSTIC_ASSERT
(
diff
=
=
regind
*
size
)
;
MOZ_DIAGNOSTIC_ASSERT
(
regind
<
bin
-
>
nregs
)
;
elm
=
regind
>
>
(
SIZEOF_INT_2POW
+
3
)
;
if
(
elm
<
run
-
>
regs_minelm
)
run
-
>
regs_minelm
=
elm
;
bit
=
regind
-
(
elm
<
<
(
SIZEOF_INT_2POW
+
3
)
)
;
MOZ_DIAGNOSTIC_ASSERT
(
(
run
-
>
regs_mask
[
elm
]
&
(
1U
<
<
bit
)
)
=
=
0
)
;
run
-
>
regs_mask
[
elm
]
|
=
(
1U
<
<
bit
)
;
#
undef
SIZE_INV
#
undef
SIZE_INV_SHIFT
}
void
arena_t
:
:
SplitRun
(
arena_run_t
*
aRun
size_t
aSize
bool
aLarge
bool
aZero
)
{
arena_chunk_t
*
chunk
;
size_t
old_ndirty
run_ind
total_pages
need_pages
rem_pages
i
;
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
aRun
)
;
old_ndirty
=
chunk
-
>
ndirty
;
run_ind
=
(
unsigned
)
(
(
uintptr_t
(
aRun
)
-
uintptr_t
(
chunk
)
)
>
>
pagesize_2pow
)
;
total_pages
=
(
chunk
-
>
map
[
run_ind
]
.
bits
&
~
pagesize_mask
)
>
>
pagesize_2pow
;
need_pages
=
(
aSize
>
>
pagesize_2pow
)
;
MOZ_ASSERT
(
need_pages
>
0
)
;
MOZ_ASSERT
(
need_pages
<
=
total_pages
)
;
rem_pages
=
total_pages
-
need_pages
;
mRunsAvail
.
Remove
(
&
chunk
-
>
map
[
run_ind
]
)
;
if
(
rem_pages
>
0
)
{
chunk
-
>
map
[
run_ind
+
need_pages
]
.
bits
=
(
rem_pages
<
<
pagesize_2pow
)
|
(
chunk
-
>
map
[
run_ind
+
need_pages
]
.
bits
&
pagesize_mask
)
;
chunk
-
>
map
[
run_ind
+
total_pages
-
1
]
.
bits
=
(
rem_pages
<
<
pagesize_2pow
)
|
(
chunk
-
>
map
[
run_ind
+
total_pages
-
1
]
.
bits
&
pagesize_mask
)
;
mRunsAvail
.
Insert
(
&
chunk
-
>
map
[
run_ind
+
need_pages
]
)
;
}
for
(
i
=
0
;
i
<
need_pages
;
i
+
+
)
{
if
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_MADVISED_OR_DECOMMITTED
)
{
size_t
j
;
for
(
j
=
0
;
i
+
j
<
need_pages
&
&
(
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
CHUNK_MAP_MADVISED_OR_DECOMMITTED
)
;
j
+
+
)
{
MOZ_ASSERT
(
!
(
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
CHUNK_MAP_DECOMMITTED
&
&
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
CHUNK_MAP_MADVISED
)
)
;
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
=
~
CHUNK_MAP_MADVISED_OR_DECOMMITTED
;
}
#
ifdef
MALLOC_DECOMMIT
pages_commit
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
(
run_ind
+
i
)
<
<
pagesize_2pow
)
)
j
<
<
pagesize_2pow
)
;
#
endif
mStats
.
committed
+
=
j
;
}
#
ifdef
MALLOC_DECOMMIT
else
#
endif
if
(
aZero
)
{
if
(
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_ZEROED
)
=
=
0
)
{
memset
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
(
run_ind
+
i
)
<
<
pagesize_2pow
)
)
0
pagesize
)
;
}
}
if
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_DIRTY
)
{
chunk
-
>
ndirty
-
-
;
mNumDirty
-
-
;
}
if
(
aLarge
)
{
chunk
-
>
map
[
run_ind
+
i
]
.
bits
=
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
}
else
{
chunk
-
>
map
[
run_ind
+
i
]
.
bits
=
size_t
(
aRun
)
|
CHUNK_MAP_ALLOCATED
;
}
}
if
(
aLarge
)
{
chunk
-
>
map
[
run_ind
]
.
bits
|
=
aSize
;
}
if
(
chunk
-
>
ndirty
=
=
0
&
&
old_ndirty
>
0
)
{
mChunksDirty
.
Remove
(
chunk
)
;
}
}
void
arena_t
:
:
InitChunk
(
arena_chunk_t
*
aChunk
bool
aZeroed
)
{
size_t
i
;
size_t
flags
=
aZeroed
?
CHUNK_MAP_DECOMMITTED
|
CHUNK_MAP_ZEROED
:
CHUNK_MAP_MADVISED
;
mStats
.
mapped
+
=
chunksize
;
aChunk
-
>
arena
=
this
;
aChunk
-
>
ndirty
=
0
;
#
ifdef
MALLOC_DECOMMIT
arena_run_t
*
run
=
(
arena_run_t
*
)
(
uintptr_t
(
aChunk
)
+
(
arena_chunk_header_npages
<
<
pagesize_2pow
)
)
;
#
endif
for
(
i
=
0
;
i
<
arena_chunk_header_npages
;
i
+
+
)
{
aChunk
-
>
map
[
i
]
.
bits
=
0
;
}
aChunk
-
>
map
[
i
]
.
bits
=
arena_maxclass
|
flags
;
for
(
i
+
+
;
i
<
chunk_npages
-
1
;
i
+
+
)
{
aChunk
-
>
map
[
i
]
.
bits
=
flags
;
}
aChunk
-
>
map
[
chunk_npages
-
1
]
.
bits
=
arena_maxclass
|
flags
;
#
ifdef
MALLOC_DECOMMIT
pages_decommit
(
run
arena_maxclass
)
;
#
endif
mStats
.
committed
+
=
arena_chunk_header_npages
;
mRunsAvail
.
Insert
(
&
aChunk
-
>
map
[
arena_chunk_header_npages
]
)
;
#
ifdef
MALLOC_DOUBLE_PURGE
new
(
&
aChunk
-
>
chunks_madvised_elem
)
mozilla
:
:
DoublyLinkedListElement
<
arena_chunk_t
>
(
)
;
#
endif
}
void
arena_t
:
:
DeallocChunk
(
arena_chunk_t
*
aChunk
)
{
if
(
mSpare
)
{
if
(
mSpare
-
>
ndirty
>
0
)
{
aChunk
-
>
arena
-
>
mChunksDirty
.
Remove
(
mSpare
)
;
mNumDirty
-
=
mSpare
-
>
ndirty
;
mStats
.
committed
-
=
mSpare
-
>
ndirty
;
}
#
ifdef
MALLOC_DOUBLE_PURGE
if
(
mChunksMAdvised
.
ElementProbablyInList
(
mSpare
)
)
{
mChunksMAdvised
.
remove
(
mSpare
)
;
}
#
endif
chunk_dealloc
(
(
void
*
)
mSpare
chunksize
ARENA_CHUNK
)
;
mStats
.
mapped
-
=
chunksize
;
mStats
.
committed
-
=
arena_chunk_header_npages
;
}
mRunsAvail
.
Remove
(
&
aChunk
-
>
map
[
arena_chunk_header_npages
]
)
;
mSpare
=
aChunk
;
}
arena_run_t
*
arena_t
:
:
AllocRun
(
arena_bin_t
*
aBin
size_t
aSize
bool
aLarge
bool
aZero
)
{
arena_run_t
*
run
;
arena_chunk_map_t
*
mapelm
;
arena_chunk_map_t
key
;
MOZ_ASSERT
(
aSize
<
=
arena_maxclass
)
;
MOZ_ASSERT
(
(
aSize
&
pagesize_mask
)
=
=
0
)
;
key
.
bits
=
aSize
|
CHUNK_MAP_KEY
;
mapelm
=
mRunsAvail
.
SearchOrNext
(
&
key
)
;
if
(
mapelm
)
{
arena_chunk_t
*
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
mapelm
)
;
size_t
pageind
=
(
uintptr_t
(
mapelm
)
-
uintptr_t
(
chunk
-
>
map
)
)
/
sizeof
(
arena_chunk_map_t
)
;
run
=
(
arena_run_t
*
)
(
uintptr_t
(
chunk
)
+
(
pageind
<
<
pagesize_2pow
)
)
;
SplitRun
(
run
aSize
aLarge
aZero
)
;
return
run
;
}
if
(
mSpare
)
{
arena_chunk_t
*
chunk
=
mSpare
;
mSpare
=
nullptr
;
run
=
(
arena_run_t
*
)
(
uintptr_t
(
chunk
)
+
(
arena_chunk_header_npages
<
<
pagesize_2pow
)
)
;
mRunsAvail
.
Insert
(
&
chunk
-
>
map
[
arena_chunk_header_npages
]
)
;
SplitRun
(
run
aSize
aLarge
aZero
)
;
return
run
;
}
{
bool
zeroed
;
arena_chunk_t
*
chunk
=
(
arena_chunk_t
*
)
chunk_alloc
(
chunksize
chunksize
false
&
zeroed
)
;
if
(
!
chunk
)
{
return
nullptr
;
}
InitChunk
(
chunk
zeroed
)
;
run
=
(
arena_run_t
*
)
(
uintptr_t
(
chunk
)
+
(
arena_chunk_header_npages
<
<
pagesize_2pow
)
)
;
}
SplitRun
(
run
aSize
aLarge
aZero
)
;
return
run
;
}
void
arena_t
:
:
Purge
(
bool
aAll
)
{
arena_chunk_t
*
chunk
;
size_t
i
npages
;
size_t
dirty_max
=
aAll
?
1
:
mMaxDirty
;
#
ifdef
MOZ_DEBUG
size_t
ndirty
=
0
;
for
(
auto
chunk
:
mChunksDirty
.
iter
(
)
)
{
ndirty
+
=
chunk
-
>
ndirty
;
}
MOZ_ASSERT
(
ndirty
=
=
mNumDirty
)
;
#
endif
MOZ_DIAGNOSTIC_ASSERT
(
aAll
|
|
(
mNumDirty
>
mMaxDirty
)
)
;
while
(
mNumDirty
>
(
dirty_max
>
>
1
)
)
{
#
ifdef
MALLOC_DOUBLE_PURGE
bool
madvised
=
false
;
#
endif
chunk
=
mChunksDirty
.
Last
(
)
;
MOZ_DIAGNOSTIC_ASSERT
(
chunk
)
;
for
(
i
=
chunk_npages
-
1
;
chunk
-
>
ndirty
>
0
;
i
-
-
)
{
MOZ_DIAGNOSTIC_ASSERT
(
i
>
=
arena_chunk_header_npages
)
;
if
(
chunk
-
>
map
[
i
]
.
bits
&
CHUNK_MAP_DIRTY
)
{
#
ifdef
MALLOC_DECOMMIT
const
size_t
free_operation
=
CHUNK_MAP_DECOMMITTED
;
#
else
const
size_t
free_operation
=
CHUNK_MAP_MADVISED
;
#
endif
MOZ_ASSERT
(
(
chunk
-
>
map
[
i
]
.
bits
&
CHUNK_MAP_MADVISED_OR_DECOMMITTED
)
=
=
0
)
;
chunk
-
>
map
[
i
]
.
bits
^
=
free_operation
|
CHUNK_MAP_DIRTY
;
for
(
npages
=
1
;
i
>
arena_chunk_header_npages
&
&
(
chunk
-
>
map
[
i
-
1
]
.
bits
&
CHUNK_MAP_DIRTY
)
;
npages
+
+
)
{
i
-
-
;
MOZ_ASSERT
(
(
chunk
-
>
map
[
i
]
.
bits
&
CHUNK_MAP_MADVISED_OR_DECOMMITTED
)
=
=
0
)
;
chunk
-
>
map
[
i
]
.
bits
^
=
free_operation
|
CHUNK_MAP_DIRTY
;
}
chunk
-
>
ndirty
-
=
npages
;
mNumDirty
-
=
npages
;
#
ifdef
MALLOC_DECOMMIT
pages_decommit
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
i
<
<
pagesize_2pow
)
)
(
npages
<
<
pagesize_2pow
)
)
;
#
endif
mStats
.
committed
-
=
npages
;
#
ifndef
MALLOC_DECOMMIT
madvise
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
i
<
<
pagesize_2pow
)
)
(
npages
<
<
pagesize_2pow
)
MADV_FREE
)
;
#
ifdef
MALLOC_DOUBLE_PURGE
madvised
=
true
;
#
endif
#
endif
if
(
mNumDirty
<
=
(
dirty_max
>
>
1
)
)
{
break
;
}
}
}
if
(
chunk
-
>
ndirty
=
=
0
)
{
mChunksDirty
.
Remove
(
chunk
)
;
}
#
ifdef
MALLOC_DOUBLE_PURGE
if
(
madvised
)
{
if
(
mChunksMAdvised
.
ElementProbablyInList
(
chunk
)
)
{
mChunksMAdvised
.
remove
(
chunk
)
;
}
mChunksMAdvised
.
pushFront
(
chunk
)
;
}
#
endif
}
}
void
arena_t
:
:
DallocRun
(
arena_run_t
*
aRun
bool
aDirty
)
{
arena_chunk_t
*
chunk
;
size_t
size
run_ind
run_pages
;
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
aRun
)
;
run_ind
=
(
size_t
)
(
(
uintptr_t
(
aRun
)
-
uintptr_t
(
chunk
)
)
>
>
pagesize_2pow
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run_ind
>
=
arena_chunk_header_npages
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run_ind
<
chunk_npages
)
;
if
(
(
chunk
-
>
map
[
run_ind
]
.
bits
&
CHUNK_MAP_LARGE
)
!
=
0
)
size
=
chunk
-
>
map
[
run_ind
]
.
bits
&
~
pagesize_mask
;
else
size
=
aRun
-
>
bin
-
>
run_size
;
run_pages
=
(
size
>
>
pagesize_2pow
)
;
if
(
aDirty
)
{
size_t
i
;
for
(
i
=
0
;
i
<
run_pages
;
i
+
+
)
{
MOZ_DIAGNOSTIC_ASSERT
(
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_DIRTY
)
=
=
0
)
;
chunk
-
>
map
[
run_ind
+
i
]
.
bits
=
CHUNK_MAP_DIRTY
;
}
if
(
chunk
-
>
ndirty
=
=
0
)
{
mChunksDirty
.
Insert
(
chunk
)
;
}
chunk
-
>
ndirty
+
=
run_pages
;
mNumDirty
+
=
run_pages
;
}
else
{
size_t
i
;
for
(
i
=
0
;
i
<
run_pages
;
i
+
+
)
{
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
=
~
(
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
)
;
}
}
chunk
-
>
map
[
run_ind
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
]
.
bits
&
pagesize_mask
)
;
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
pagesize_mask
)
;
if
(
run_ind
+
run_pages
<
chunk_npages
&
&
(
chunk
-
>
map
[
run_ind
+
run_pages
]
.
bits
&
CHUNK_MAP_ALLOCATED
)
=
=
0
)
{
size_t
nrun_size
=
chunk
-
>
map
[
run_ind
+
run_pages
]
.
bits
&
~
pagesize_mask
;
mRunsAvail
.
Remove
(
&
chunk
-
>
map
[
run_ind
+
run_pages
]
)
;
size
+
=
nrun_size
;
run_pages
=
size
>
>
pagesize_2pow
;
MOZ_DIAGNOSTIC_ASSERT
(
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
~
pagesize_mask
)
=
=
nrun_size
)
;
chunk
-
>
map
[
run_ind
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
]
.
bits
&
pagesize_mask
)
;
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
pagesize_mask
)
;
}
if
(
run_ind
>
arena_chunk_header_npages
&
&
(
chunk
-
>
map
[
run_ind
-
1
]
.
bits
&
CHUNK_MAP_ALLOCATED
)
=
=
0
)
{
size_t
prun_size
=
chunk
-
>
map
[
run_ind
-
1
]
.
bits
&
~
pagesize_mask
;
run_ind
-
=
prun_size
>
>
pagesize_2pow
;
mRunsAvail
.
Remove
(
&
chunk
-
>
map
[
run_ind
]
)
;
size
+
=
prun_size
;
run_pages
=
size
>
>
pagesize_2pow
;
MOZ_DIAGNOSTIC_ASSERT
(
(
chunk
-
>
map
[
run_ind
]
.
bits
&
~
pagesize_mask
)
=
=
prun_size
)
;
chunk
-
>
map
[
run_ind
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
]
.
bits
&
pagesize_mask
)
;
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
pagesize_mask
)
;
}
mRunsAvail
.
Insert
(
&
chunk
-
>
map
[
run_ind
]
)
;
if
(
(
chunk
-
>
map
[
arena_chunk_header_npages
]
.
bits
&
(
~
pagesize_mask
|
CHUNK_MAP_ALLOCATED
)
)
=
=
arena_maxclass
)
{
DeallocChunk
(
chunk
)
;
}
if
(
mNumDirty
>
mMaxDirty
)
{
Purge
(
false
)
;
}
}
void
arena_t
:
:
TrimRunHead
(
arena_chunk_t
*
aChunk
arena_run_t
*
aRun
size_t
aOldSize
size_t
aNewSize
)
{
size_t
pageind
=
(
uintptr_t
(
aRun
)
-
uintptr_t
(
aChunk
)
)
>
>
pagesize_2pow
;
size_t
head_npages
=
(
aOldSize
-
aNewSize
)
>
>
pagesize_2pow
;
MOZ_ASSERT
(
aOldSize
>
aNewSize
)
;
aChunk
-
>
map
[
pageind
]
.
bits
=
(
aOldSize
-
aNewSize
)
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
aChunk
-
>
map
[
pageind
+
head_npages
]
.
bits
=
aNewSize
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
DallocRun
(
aRun
false
)
;
}
void
arena_t
:
:
TrimRunTail
(
arena_chunk_t
*
aChunk
arena_run_t
*
aRun
size_t
aOldSize
size_t
aNewSize
bool
aDirty
)
{
size_t
pageind
=
(
uintptr_t
(
aRun
)
-
uintptr_t
(
aChunk
)
)
>
>
pagesize_2pow
;
size_t
npages
=
aNewSize
>
>
pagesize_2pow
;
MOZ_ASSERT
(
aOldSize
>
aNewSize
)
;
aChunk
-
>
map
[
pageind
]
.
bits
=
aNewSize
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
aChunk
-
>
map
[
pageind
+
npages
]
.
bits
=
(
aOldSize
-
aNewSize
)
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
DallocRun
(
(
arena_run_t
*
)
(
uintptr_t
(
aRun
)
+
aNewSize
)
aDirty
)
;
}
arena_run_t
*
arena_t
:
:
GetNonFullBinRun
(
arena_bin_t
*
aBin
)
{
arena_chunk_map_t
*
mapelm
;
arena_run_t
*
run
;
unsigned
i
remainder
;
mapelm
=
aBin
-
>
runs
.
First
(
)
;
if
(
mapelm
)
{
aBin
-
>
runs
.
Remove
(
mapelm
)
;
run
=
(
arena_run_t
*
)
(
mapelm
-
>
bits
&
~
pagesize_mask
)
;
return
run
;
}
run
=
AllocRun
(
aBin
aBin
-
>
run_size
false
false
)
;
if
(
!
run
)
return
nullptr
;
if
(
run
=
=
aBin
-
>
runcur
)
{
return
run
;
}
run
-
>
bin
=
aBin
;
for
(
i
=
0
;
i
<
aBin
-
>
regs_mask_nelms
-
1
;
i
+
+
)
{
run
-
>
regs_mask
[
i
]
=
UINT_MAX
;
}
remainder
=
aBin
-
>
nregs
&
(
(
1U
<
<
(
SIZEOF_INT_2POW
+
3
)
)
-
1
)
;
if
(
remainder
=
=
0
)
{
run
-
>
regs_mask
[
i
]
=
UINT_MAX
;
}
else
{
run
-
>
regs_mask
[
i
]
=
(
UINT_MAX
>
>
(
(
1U
<
<
(
SIZEOF_INT_2POW
+
3
)
)
-
remainder
)
)
;
}
run
-
>
regs_minelm
=
0
;
run
-
>
nfree
=
aBin
-
>
nregs
;
#
if
defined
(
MOZ_DEBUG
)
|
|
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
run
-
>
magic
=
ARENA_RUN_MAGIC
;
#
endif
aBin
-
>
stats
.
curruns
+
+
;
return
run
;
}
void
*
arena_t
:
:
MallocBinEasy
(
arena_bin_t
*
aBin
arena_run_t
*
aRun
)
{
void
*
ret
;
MOZ_DIAGNOSTIC_ASSERT
(
aRun
-
>
magic
=
=
ARENA_RUN_MAGIC
)
;
MOZ_DIAGNOSTIC_ASSERT
(
aRun
-
>
nfree
>
0
)
;
ret
=
arena_run_reg_alloc
(
aRun
aBin
)
;
MOZ_DIAGNOSTIC_ASSERT
(
ret
)
;
aRun
-
>
nfree
-
-
;
return
ret
;
}
void
*
arena_t
:
:
MallocBinHard
(
arena_bin_t
*
aBin
)
{
aBin
-
>
runcur
=
GetNonFullBinRun
(
aBin
)
;
if
(
!
aBin
-
>
runcur
)
{
return
nullptr
;
}
MOZ_DIAGNOSTIC_ASSERT
(
aBin
-
>
runcur
-
>
magic
=
=
ARENA_RUN_MAGIC
)
;
MOZ_DIAGNOSTIC_ASSERT
(
aBin
-
>
runcur
-
>
nfree
>
0
)
;
return
MallocBinEasy
(
aBin
aBin
-
>
runcur
)
;
}
static
size_t
arena_bin_run_size_calc
(
arena_bin_t
*
bin
size_t
min_run_size
)
{
size_t
try_run_size
good_run_size
;
unsigned
good_nregs
good_mask_nelms
good_reg0_offset
;
unsigned
try_nregs
try_mask_nelms
try_reg0_offset
;
MOZ_ASSERT
(
min_run_size
>
=
pagesize
)
;
MOZ_ASSERT
(
min_run_size
<
=
arena_maxclass
)
;
try_run_size
=
min_run_size
;
try_nregs
=
(
(
try_run_size
-
sizeof
(
arena_run_t
)
)
/
bin
-
>
reg_size
)
+
1
;
do
{
try_nregs
-
-
;
try_mask_nelms
=
(
try_nregs
>
>
(
SIZEOF_INT_2POW
+
3
)
)
+
(
(
try_nregs
&
(
(
1U
<
<
(
SIZEOF_INT_2POW
+
3
)
)
-
1
)
)
?
1
:
0
)
;
try_reg0_offset
=
try_run_size
-
(
try_nregs
*
bin
-
>
reg_size
)
;
}
while
(
sizeof
(
arena_run_t
)
+
(
sizeof
(
unsigned
)
*
(
try_mask_nelms
-
1
)
)
>
try_reg0_offset
)
;
do
{
good_run_size
=
try_run_size
;
good_nregs
=
try_nregs
;
good_mask_nelms
=
try_mask_nelms
;
good_reg0_offset
=
try_reg0_offset
;
try_run_size
+
=
pagesize
;
try_nregs
=
(
(
try_run_size
-
sizeof
(
arena_run_t
)
)
/
bin
-
>
reg_size
)
+
1
;
do
{
try_nregs
-
-
;
try_mask_nelms
=
(
try_nregs
>
>
(
SIZEOF_INT_2POW
+
3
)
)
+
(
(
try_nregs
&
(
(
1U
<
<
(
SIZEOF_INT_2POW
+
3
)
)
-
1
)
)
?
1
:
0
)
;
try_reg0_offset
=
try_run_size
-
(
try_nregs
*
bin
-
>
reg_size
)
;
}
while
(
sizeof
(
arena_run_t
)
+
(
sizeof
(
unsigned
)
*
(
try_mask_nelms
-
1
)
)
>
try_reg0_offset
)
;
}
while
(
try_run_size
<
=
arena_maxclass
&
&
RUN_MAX_OVRHD
*
(
bin
-
>
reg_size
<
<
3
)
>
RUN_MAX_OVRHD_RELAX
&
&
(
try_reg0_offset
<
<
RUN_BFP
)
>
RUN_MAX_OVRHD
*
try_run_size
)
;
MOZ_ASSERT
(
sizeof
(
arena_run_t
)
+
(
sizeof
(
unsigned
)
*
(
good_mask_nelms
-
1
)
)
<
=
good_reg0_offset
)
;
MOZ_ASSERT
(
(
good_mask_nelms
<
<
(
SIZEOF_INT_2POW
+
3
)
)
>
=
good_nregs
)
;
bin
-
>
run_size
=
good_run_size
;
bin
-
>
nregs
=
good_nregs
;
bin
-
>
regs_mask_nelms
=
good_mask_nelms
;
bin
-
>
reg0_offset
=
good_reg0_offset
;
return
(
good_run_size
)
;
}
void
*
arena_t
:
:
MallocSmall
(
size_t
aSize
bool
aZero
)
{
void
*
ret
;
arena_bin_t
*
bin
;
arena_run_t
*
run
;
if
(
aSize
<
small_min
)
{
aSize
=
pow2_ceil
(
aSize
)
;
bin
=
&
mBins
[
ffs
(
(
int
)
(
aSize
>
>
(
TINY_MIN_2POW
+
1
)
)
)
]
;
if
(
aSize
<
(
1U
<
<
TINY_MIN_2POW
)
)
{
aSize
=
1U
<
<
TINY_MIN_2POW
;
}
}
else
if
(
aSize
<
=
small_max
)
{
aSize
=
QUANTUM_CEILING
(
aSize
)
;
bin
=
&
mBins
[
ntbins
+
(
aSize
>
>
QUANTUM_2POW_MIN
)
-
1
]
;
}
else
{
aSize
=
pow2_ceil
(
aSize
)
;
bin
=
&
mBins
[
ntbins
+
nqbins
+
(
ffs
(
(
int
)
(
aSize
>
>
SMALL_MAX_2POW_DEFAULT
)
)
-
2
)
]
;
}
MOZ_DIAGNOSTIC_ASSERT
(
aSize
=
=
bin
-
>
reg_size
)
;
{
MutexAutoLock
lock
(
mLock
)
;
if
(
(
run
=
bin
-
>
runcur
)
&
&
run
-
>
nfree
>
0
)
{
ret
=
MallocBinEasy
(
bin
run
)
;
}
else
{
ret
=
MallocBinHard
(
bin
)
;
}
if
(
!
ret
)
{
return
nullptr
;
}
mStats
.
allocated_small
+
=
aSize
;
}
if
(
aZero
=
=
false
)
{
if
(
opt_junk
)
{
memset
(
ret
kAllocJunk
aSize
)
;
}
else
if
(
opt_zero
)
{
memset
(
ret
0
aSize
)
;
}
}
else
memset
(
ret
0
aSize
)
;
return
ret
;
}
void
*
arena_t
:
:
MallocLarge
(
size_t
aSize
bool
aZero
)
{
void
*
ret
;
aSize
=
PAGE_CEILING
(
aSize
)
;
{
MutexAutoLock
lock
(
mLock
)
;
ret
=
AllocRun
(
nullptr
aSize
true
aZero
)
;
if
(
!
ret
)
{
return
nullptr
;
}
mStats
.
allocated_large
+
=
aSize
;
}
if
(
aZero
=
=
false
)
{
if
(
opt_junk
)
{
memset
(
ret
kAllocJunk
aSize
)
;
}
else
if
(
opt_zero
)
{
memset
(
ret
0
aSize
)
;
}
}
return
(
ret
)
;
}
void
*
arena_t
:
:
Malloc
(
size_t
aSize
bool
aZero
)
{
MOZ_DIAGNOSTIC_ASSERT
(
mMagic
=
=
ARENA_MAGIC
)
;
MOZ_ASSERT
(
aSize
!
=
0
)
;
MOZ_ASSERT
(
QUANTUM_CEILING
(
aSize
)
<
=
arena_maxclass
)
;
return
(
aSize
<
=
bin_maxclass
)
?
MallocSmall
(
aSize
aZero
)
:
MallocLarge
(
aSize
aZero
)
;
}
static
inline
void
*
imalloc
(
size_t
aSize
bool
aZero
arena_t
*
aArena
)
{
MOZ_ASSERT
(
aSize
!
=
0
)
;
if
(
aSize
<
=
arena_maxclass
)
{
aArena
=
aArena
?
aArena
:
choose_arena
(
aSize
)
;
return
aArena
-
>
Malloc
(
aSize
aZero
)
;
}
return
huge_malloc
(
aSize
aZero
)
;
}
void
*
arena_t
:
:
Palloc
(
size_t
aAlignment
size_t
aSize
size_t
aAllocSize
)
{
void
*
ret
;
size_t
offset
;
arena_chunk_t
*
chunk
;
MOZ_ASSERT
(
(
aSize
&
pagesize_mask
)
=
=
0
)
;
MOZ_ASSERT
(
(
aAlignment
&
pagesize_mask
)
=
=
0
)
;
{
MutexAutoLock
lock
(
mLock
)
;
ret
=
AllocRun
(
nullptr
aAllocSize
true
false
)
;
if
(
!
ret
)
{
return
nullptr
;
}
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
ret
)
;
offset
=
uintptr_t
(
ret
)
&
(
aAlignment
-
1
)
;
MOZ_ASSERT
(
(
offset
&
pagesize_mask
)
=
=
0
)
;
MOZ_ASSERT
(
offset
<
aAllocSize
)
;
if
(
offset
=
=
0
)
{
TrimRunTail
(
chunk
(
arena_run_t
*
)
ret
aAllocSize
aSize
false
)
;
}
else
{
size_t
leadsize
trailsize
;
leadsize
=
aAlignment
-
offset
;
if
(
leadsize
>
0
)
{
TrimRunHead
(
chunk
(
arena_run_t
*
)
ret
aAllocSize
aAllocSize
-
leadsize
)
;
ret
=
(
void
*
)
(
uintptr_t
(
ret
)
+
leadsize
)
;
}
trailsize
=
aAllocSize
-
leadsize
-
aSize
;
if
(
trailsize
!
=
0
)
{
MOZ_ASSERT
(
trailsize
<
aAllocSize
)
;
TrimRunTail
(
chunk
(
arena_run_t
*
)
ret
aSize
+
trailsize
aSize
false
)
;
}
}
mStats
.
allocated_large
+
=
aSize
;
}
if
(
opt_junk
)
{
memset
(
ret
kAllocJunk
aSize
)
;
}
else
if
(
opt_zero
)
{
memset
(
ret
0
aSize
)
;
}
return
ret
;
}
static
inline
void
*
ipalloc
(
size_t
aAlignment
size_t
aSize
arena_t
*
aArena
)
{
void
*
ret
;
size_t
ceil_size
;
ceil_size
=
ALIGNMENT_CEILING
(
aSize
aAlignment
)
;
if
(
ceil_size
<
aSize
)
{
return
nullptr
;
}
if
(
ceil_size
<
=
pagesize
|
|
(
aAlignment
<
=
pagesize
&
&
ceil_size
<
=
arena_maxclass
)
)
{
aArena
=
aArena
?
aArena
:
choose_arena
(
aSize
)
;
ret
=
aArena
-
>
Malloc
(
ceil_size
false
)
;
}
else
{
size_t
run_size
;
aAlignment
=
PAGE_CEILING
(
aAlignment
)
;
ceil_size
=
PAGE_CEILING
(
aSize
)
;
if
(
ceil_size
<
aSize
|
|
ceil_size
+
aAlignment
<
ceil_size
)
{
return
nullptr
;
}
if
(
ceil_size
>
=
aAlignment
)
{
run_size
=
ceil_size
+
aAlignment
-
pagesize
;
}
else
{
run_size
=
(
aAlignment
<
<
1
)
-
pagesize
;
}
if
(
run_size
<
=
arena_maxclass
)
{
aArena
=
aArena
?
aArena
:
choose_arena
(
aSize
)
;
ret
=
aArena
-
>
Palloc
(
aAlignment
ceil_size
run_size
)
;
}
else
if
(
aAlignment
<
=
chunksize
)
ret
=
huge_malloc
(
ceil_size
false
)
;
else
ret
=
huge_palloc
(
ceil_size
aAlignment
false
)
;
}
MOZ_ASSERT
(
(
uintptr_t
(
ret
)
&
(
aAlignment
-
1
)
)
=
=
0
)
;
return
ret
;
}
static
size_t
arena_salloc
(
const
void
*
ptr
)
{
size_t
ret
;
arena_chunk_t
*
chunk
;
size_t
pageind
mapbits
;
MOZ_ASSERT
(
ptr
)
;
MOZ_ASSERT
(
CHUNK_ADDR2BASE
(
ptr
)
!
=
ptr
)
;
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
ptr
)
;
pageind
=
(
(
(
uintptr_t
)
ptr
-
(
uintptr_t
)
chunk
)
>
>
pagesize_2pow
)
;
mapbits
=
chunk
-
>
map
[
pageind
]
.
bits
;
MOZ_DIAGNOSTIC_ASSERT
(
(
mapbits
&
CHUNK_MAP_ALLOCATED
)
!
=
0
)
;
if
(
(
mapbits
&
CHUNK_MAP_LARGE
)
=
=
0
)
{
arena_run_t
*
run
=
(
arena_run_t
*
)
(
mapbits
&
~
pagesize_mask
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
magic
=
=
ARENA_RUN_MAGIC
)
;
ret
=
run
-
>
bin
-
>
reg_size
;
}
else
{
ret
=
mapbits
&
~
pagesize_mask
;
MOZ_DIAGNOSTIC_ASSERT
(
ret
!
=
0
)
;
}
return
(
ret
)
;
}
static
inline
size_t
isalloc_validate
(
const
void
*
aPtr
)
{
if
(
malloc_initialized
=
=
false
)
{
return
0
;
}
auto
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
aPtr
)
;
if
(
!
chunk
)
{
return
0
;
}
if
(
!
gChunkRTree
.
Get
(
chunk
)
)
{
return
0
;
}
if
(
chunk
!
=
aPtr
)
{
MOZ_DIAGNOSTIC_ASSERT
(
chunk
-
>
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
return
arena_salloc
(
aPtr
)
;
}
extent_node_t
key
;
key
.
addr
=
(
void
*
)
chunk
;
MutexAutoLock
lock
(
huge_mtx
)
;
extent_node_t
*
node
=
huge
.
Search
(
&
key
)
;
if
(
node
)
{
return
node
-
>
size
;
}
return
0
;
}
static
inline
size_t
isalloc
(
const
void
*
aPtr
)
{
MOZ_ASSERT
(
aPtr
)
;
auto
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
aPtr
)
;
if
(
chunk
!
=
aPtr
)
{
MOZ_ASSERT
(
chunk
-
>
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
return
arena_salloc
(
aPtr
)
;
}
extent_node_t
key
;
MutexAutoLock
lock
(
huge_mtx
)
;
key
.
addr
=
const_cast
<
void
*
>
(
aPtr
)
;
extent_node_t
*
node
=
huge
.
Search
(
&
key
)
;
MOZ_DIAGNOSTIC_ASSERT
(
node
)
;
return
node
-
>
size
;
}
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_ptr_info
(
const
void
*
aPtr
jemalloc_ptr_info_t
*
aInfo
)
{
arena_chunk_t
*
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
aPtr
)
;
if
(
!
chunk
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
}
;
return
;
}
extent_node_t
*
node
;
extent_node_t
key
;
{
MutexAutoLock
lock
(
huge_mtx
)
;
key
.
addr
=
const_cast
<
void
*
>
(
aPtr
)
;
node
=
reinterpret_cast
<
RedBlackTree
<
extent_node_t
ExtentTreeBoundsTrait
>
*
>
(
&
huge
)
-
>
Search
(
&
key
)
;
if
(
node
)
{
*
aInfo
=
{
TagLiveHuge
node
-
>
addr
node
-
>
size
}
;
return
;
}
}
if
(
!
gChunkRTree
.
Get
(
chunk
)
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
}
;
return
;
}
MOZ_DIAGNOSTIC_ASSERT
(
chunk
-
>
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
size_t
pageind
=
(
(
(
uintptr_t
)
aPtr
-
(
uintptr_t
)
chunk
)
>
>
pagesize_2pow
)
;
if
(
pageind
<
arena_chunk_header_npages
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
}
;
return
;
}
size_t
mapbits
=
chunk
-
>
map
[
pageind
]
.
bits
;
if
(
!
(
mapbits
&
CHUNK_MAP_ALLOCATED
)
)
{
PtrInfoTag
tag
=
TagFreedPageDirty
;
if
(
mapbits
&
CHUNK_MAP_DIRTY
)
tag
=
TagFreedPageDirty
;
else
if
(
mapbits
&
CHUNK_MAP_DECOMMITTED
)
tag
=
TagFreedPageDecommitted
;
else
if
(
mapbits
&
CHUNK_MAP_MADVISED
)
tag
=
TagFreedPageMadvised
;
else
if
(
mapbits
&
CHUNK_MAP_ZEROED
)
tag
=
TagFreedPageZeroed
;
else
MOZ_CRASH
(
)
;
void
*
pageaddr
=
(
void
*
)
(
uintptr_t
(
aPtr
)
&
~
pagesize_mask
)
;
*
aInfo
=
{
tag
pageaddr
pagesize
}
;
return
;
}
if
(
mapbits
&
CHUNK_MAP_LARGE
)
{
size_t
size
;
while
(
true
)
{
size
=
mapbits
&
~
pagesize_mask
;
if
(
size
!
=
0
)
{
break
;
}
pageind
-
-
;
MOZ_DIAGNOSTIC_ASSERT
(
pageind
>
=
arena_chunk_header_npages
)
;
if
(
pageind
<
arena_chunk_header_npages
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
}
;
return
;
}
mapbits
=
chunk
-
>
map
[
pageind
]
.
bits
;
MOZ_DIAGNOSTIC_ASSERT
(
mapbits
&
CHUNK_MAP_LARGE
)
;
if
(
!
(
mapbits
&
CHUNK_MAP_LARGE
)
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
}
;
return
;
}
}
void
*
addr
=
(
(
char
*
)
chunk
)
+
(
pageind
<
<
pagesize_2pow
)
;
*
aInfo
=
{
TagLiveLarge
addr
size
}
;
return
;
}
auto
run
=
(
arena_run_t
*
)
(
mapbits
&
~
pagesize_mask
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
magic
=
=
ARENA_RUN_MAGIC
)
;
size_t
size
=
run
-
>
bin
-
>
reg_size
;
uintptr_t
reg0_addr
=
(
uintptr_t
)
run
+
run
-
>
bin
-
>
reg0_offset
;
if
(
aPtr
<
(
void
*
)
reg0_addr
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
}
;
return
;
}
unsigned
regind
=
(
(
uintptr_t
)
aPtr
-
reg0_addr
)
/
size
;
void
*
addr
=
(
void
*
)
(
reg0_addr
+
regind
*
size
)
;
unsigned
elm
=
regind
>
>
(
SIZEOF_INT_2POW
+
3
)
;
unsigned
bit
=
regind
-
(
elm
<
<
(
SIZEOF_INT_2POW
+
3
)
)
;
PtrInfoTag
tag
=
(
(
run
-
>
regs_mask
[
elm
]
&
(
1U
<
<
bit
)
)
)
?
TagFreedSmall
:
TagLiveSmall
;
*
aInfo
=
{
tag
addr
size
}
;
}
namespace
Debug
{
MOZ_NEVER_INLINE
jemalloc_ptr_info_t
*
jemalloc_ptr_info
(
const
void
*
aPtr
)
{
static
jemalloc_ptr_info_t
info
;
MozJemalloc
:
:
jemalloc_ptr_info
(
aPtr
&
info
)
;
return
&
info
;
}
}
void
arena_t
:
:
DallocSmall
(
arena_chunk_t
*
aChunk
void
*
aPtr
arena_chunk_map_t
*
aMapElm
)
{
arena_run_t
*
run
;
arena_bin_t
*
bin
;
size_t
size
;
run
=
(
arena_run_t
*
)
(
aMapElm
-
>
bits
&
~
pagesize_mask
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
magic
=
=
ARENA_RUN_MAGIC
)
;
bin
=
run
-
>
bin
;
size
=
bin
-
>
reg_size
;
MOZ_DIAGNOSTIC_ASSERT
(
uintptr_t
(
aPtr
)
>
=
uintptr_t
(
run
)
+
bin
-
>
reg0_offset
)
;
MOZ_DIAGNOSTIC_ASSERT
(
(
uintptr_t
(
aPtr
)
-
(
uintptr_t
(
run
)
+
bin
-
>
reg0_offset
)
)
%
size
=
=
0
)
;
memset
(
aPtr
kAllocPoison
size
)
;
arena_run_reg_dalloc
(
run
bin
aPtr
size
)
;
run
-
>
nfree
+
+
;
if
(
run
-
>
nfree
=
=
bin
-
>
nregs
)
{
if
(
run
=
=
bin
-
>
runcur
)
{
bin
-
>
runcur
=
nullptr
;
}
else
if
(
bin
-
>
nregs
!
=
1
)
{
size_t
run_pageind
=
(
uintptr_t
(
run
)
-
uintptr_t
(
aChunk
)
)
>
>
pagesize_2pow
;
arena_chunk_map_t
*
run_mapelm
=
&
aChunk
-
>
map
[
run_pageind
]
;
MOZ_DIAGNOSTIC_ASSERT
(
bin
-
>
runs
.
Search
(
run_mapelm
)
=
=
run_mapelm
)
;
bin
-
>
runs
.
Remove
(
run_mapelm
)
;
}
#
if
defined
(
MOZ_DEBUG
)
|
|
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
run
-
>
magic
=
0
;
#
endif
DallocRun
(
run
true
)
;
bin
-
>
stats
.
curruns
-
-
;
}
else
if
(
run
-
>
nfree
=
=
1
&
&
run
!
=
bin
-
>
runcur
)
{
if
(
!
bin
-
>
runcur
)
{
bin
-
>
runcur
=
run
;
}
else
if
(
uintptr_t
(
run
)
<
uintptr_t
(
bin
-
>
runcur
)
)
{
if
(
bin
-
>
runcur
-
>
nfree
>
0
)
{
arena_chunk_t
*
runcur_chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
bin
-
>
runcur
)
;
size_t
runcur_pageind
=
(
uintptr_t
(
bin
-
>
runcur
)
-
uintptr_t
(
runcur_chunk
)
)
>
>
pagesize_2pow
;
arena_chunk_map_t
*
runcur_mapelm
=
&
runcur_chunk
-
>
map
[
runcur_pageind
]
;
MOZ_DIAGNOSTIC_ASSERT
(
!
bin
-
>
runs
.
Search
(
runcur_mapelm
)
)
;
bin
-
>
runs
.
Insert
(
runcur_mapelm
)
;
}
bin
-
>
runcur
=
run
;
}
else
{
size_t
run_pageind
=
(
uintptr_t
(
run
)
-
uintptr_t
(
aChunk
)
)
>
>
pagesize_2pow
;
arena_chunk_map_t
*
run_mapelm
=
&
aChunk
-
>
map
[
run_pageind
]
;
MOZ_DIAGNOSTIC_ASSERT
(
bin
-
>
runs
.
Search
(
run_mapelm
)
=
=
nullptr
)
;
bin
-
>
runs
.
Insert
(
run_mapelm
)
;
}
}
mStats
.
allocated_small
-
=
size
;
}
void
arena_t
:
:
DallocLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
)
{
MOZ_DIAGNOSTIC_ASSERT
(
(
uintptr_t
(
aPtr
)
&
pagesize_mask
)
=
=
0
)
;
size_t
pageind
=
(
uintptr_t
(
aPtr
)
-
uintptr_t
(
aChunk
)
)
>
>
pagesize_2pow
;
size_t
size
=
aChunk
-
>
map
[
pageind
]
.
bits
&
~
pagesize_mask
;
memset
(
aPtr
kAllocPoison
size
)
;
mStats
.
allocated_large
-
=
size
;
DallocRun
(
(
arena_run_t
*
)
aPtr
true
)
;
}
static
inline
void
arena_dalloc
(
void
*
aPtr
size_t
aOffset
)
{
MOZ_ASSERT
(
aPtr
)
;
MOZ_ASSERT
(
aOffset
!
=
0
)
;
MOZ_ASSERT
(
CHUNK_ADDR2OFFSET
(
aPtr
)
=
=
aOffset
)
;
auto
chunk
=
(
arena_chunk_t
*
)
(
(
uintptr_t
)
aPtr
-
aOffset
)
;
auto
arena
=
chunk
-
>
arena
;
MOZ_ASSERT
(
arena
)
;
MOZ_DIAGNOSTIC_ASSERT
(
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
MutexAutoLock
lock
(
arena
-
>
mLock
)
;
size_t
pageind
=
aOffset
>
>
pagesize_2pow
;
arena_chunk_map_t
*
mapelm
=
&
chunk
-
>
map
[
pageind
]
;
MOZ_DIAGNOSTIC_ASSERT
(
(
mapelm
-
>
bits
&
CHUNK_MAP_ALLOCATED
)
!
=
0
)
;
if
(
(
mapelm
-
>
bits
&
CHUNK_MAP_LARGE
)
=
=
0
)
{
arena
-
>
DallocSmall
(
chunk
aPtr
mapelm
)
;
}
else
{
arena
-
>
DallocLarge
(
chunk
aPtr
)
;
}
}
static
inline
void
idalloc
(
void
*
ptr
)
{
size_t
offset
;
MOZ_ASSERT
(
ptr
)
;
offset
=
CHUNK_ADDR2OFFSET
(
ptr
)
;
if
(
offset
!
=
0
)
arena_dalloc
(
ptr
offset
)
;
else
huge_dalloc
(
ptr
)
;
}
void
arena_t
:
:
RallocShrinkLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
{
MOZ_ASSERT
(
aSize
<
aOldSize
)
;
MutexAutoLock
lock
(
mLock
)
;
TrimRunTail
(
aChunk
(
arena_run_t
*
)
aPtr
aOldSize
aSize
true
)
;
mStats
.
allocated_large
-
=
aOldSize
-
aSize
;
}
bool
arena_t
:
:
RallocGrowLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
{
size_t
pageind
=
(
uintptr_t
(
aPtr
)
-
uintptr_t
(
aChunk
)
)
>
>
pagesize_2pow
;
size_t
npages
=
aOldSize
>
>
pagesize_2pow
;
MutexAutoLock
lock
(
mLock
)
;
MOZ_DIAGNOSTIC_ASSERT
(
aOldSize
=
=
(
aChunk
-
>
map
[
pageind
]
.
bits
&
~
pagesize_mask
)
)
;
MOZ_ASSERT
(
aSize
>
aOldSize
)
;
if
(
pageind
+
npages
<
chunk_npages
&
&
(
aChunk
-
>
map
[
pageind
+
npages
]
.
bits
&
CHUNK_MAP_ALLOCATED
)
=
=
0
&
&
(
aChunk
-
>
map
[
pageind
+
npages
]
.
bits
&
~
pagesize_mask
)
>
=
aSize
-
aOldSize
)
{
SplitRun
(
(
arena_run_t
*
)
(
uintptr_t
(
aChunk
)
+
(
(
pageind
+
npages
)
<
<
pagesize_2pow
)
)
aSize
-
aOldSize
true
false
)
;
aChunk
-
>
map
[
pageind
]
.
bits
=
aSize
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
aChunk
-
>
map
[
pageind
+
npages
]
.
bits
=
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
mStats
.
allocated_large
+
=
aSize
-
aOldSize
;
return
false
;
}
return
true
;
}
static
bool
arena_ralloc_large
(
void
*
ptr
size_t
size
size_t
oldsize
)
{
size_t
psize
;
psize
=
PAGE_CEILING
(
size
)
;
if
(
psize
=
=
oldsize
)
{
if
(
size
<
oldsize
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
ptr
+
size
)
kAllocPoison
oldsize
-
size
)
;
}
return
(
false
)
;
}
else
{
arena_chunk_t
*
chunk
;
arena_t
*
arena
;
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
ptr
)
;
arena
=
chunk
-
>
arena
;
MOZ_DIAGNOSTIC_ASSERT
(
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
if
(
psize
<
oldsize
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
ptr
+
size
)
kAllocPoison
oldsize
-
size
)
;
arena
-
>
RallocShrinkLarge
(
chunk
ptr
psize
oldsize
)
;
return
(
false
)
;
}
else
{
bool
ret
=
arena
-
>
RallocGrowLarge
(
chunk
ptr
psize
oldsize
)
;
if
(
ret
=
=
false
&
&
opt_zero
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
ptr
+
oldsize
)
0
size
-
oldsize
)
;
}
return
(
ret
)
;
}
}
}
static
void
*
arena_ralloc
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
arena_t
*
aArena
)
{
void
*
ret
;
size_t
copysize
;
if
(
aSize
<
small_min
)
{
if
(
aOldSize
<
small_min
&
&
ffs
(
(
int
)
(
pow2_ceil
(
aSize
)
>
>
(
TINY_MIN_2POW
+
1
)
)
)
=
=
ffs
(
(
int
)
(
pow2_ceil
(
aOldSize
)
>
>
(
TINY_MIN_2POW
+
1
)
)
)
)
{
goto
IN_PLACE
;
}
}
else
if
(
aSize
<
=
small_max
)
{
if
(
aOldSize
>
=
small_min
&
&
aOldSize
<
=
small_max
&
&
(
QUANTUM_CEILING
(
aSize
)
>
>
QUANTUM_2POW_MIN
)
=
=
(
QUANTUM_CEILING
(
aOldSize
)
>
>
QUANTUM_2POW_MIN
)
)
{
goto
IN_PLACE
;
}
}
else
if
(
aSize
<
=
bin_maxclass
)
{
if
(
aOldSize
>
small_max
&
&
aOldSize
<
=
bin_maxclass
&
&
pow2_ceil
(
aSize
)
=
=
pow2_ceil
(
aOldSize
)
)
{
goto
IN_PLACE
;
}
}
else
if
(
aOldSize
>
bin_maxclass
&
&
aOldSize
<
=
arena_maxclass
)
{
MOZ_ASSERT
(
aSize
>
bin_maxclass
)
;
if
(
arena_ralloc_large
(
aPtr
aSize
aOldSize
)
=
=
false
)
{
return
aPtr
;
}
}
aArena
=
aArena
?
aArena
:
choose_arena
(
aSize
)
;
ret
=
aArena
-
>
Malloc
(
aSize
false
)
;
if
(
!
ret
)
{
return
nullptr
;
}
copysize
=
(
aSize
<
aOldSize
)
?
aSize
:
aOldSize
;
#
ifdef
VM_COPY_MIN
if
(
copysize
>
=
VM_COPY_MIN
)
{
pages_copy
(
ret
aPtr
copysize
)
;
}
else
#
endif
{
memcpy
(
ret
aPtr
copysize
)
;
}
idalloc
(
aPtr
)
;
return
ret
;
IN_PLACE
:
if
(
aSize
<
aOldSize
)
{
memset
(
(
void
*
)
(
uintptr_t
(
aPtr
)
+
aSize
)
kAllocPoison
aOldSize
-
aSize
)
;
}
else
if
(
opt_zero
&
&
aSize
>
aOldSize
)
{
memset
(
(
void
*
)
(
uintptr_t
(
aPtr
)
+
aOldSize
)
0
aSize
-
aOldSize
)
;
}
return
aPtr
;
}
static
inline
void
*
iralloc
(
void
*
aPtr
size_t
aSize
arena_t
*
aArena
)
{
size_t
oldsize
;
MOZ_ASSERT
(
aPtr
)
;
MOZ_ASSERT
(
aSize
!
=
0
)
;
oldsize
=
isalloc
(
aPtr
)
;
return
(
aSize
<
=
arena_maxclass
)
?
arena_ralloc
(
aPtr
aSize
oldsize
aArena
)
:
huge_ralloc
(
aPtr
aSize
oldsize
)
;
}
bool
arena_t
:
:
Init
(
)
{
unsigned
i
;
arena_bin_t
*
bin
;
size_t
prev_run_size
;
if
(
!
mLock
.
Init
(
)
)
{
return
false
;
}
memset
(
&
mLink
0
sizeof
(
mLink
)
)
;
memset
(
&
mStats
0
sizeof
(
arena_stats_t
)
)
;
mChunksDirty
.
Init
(
)
;
#
ifdef
MALLOC_DOUBLE_PURGE
new
(
&
mChunksMAdvised
)
mozilla
:
:
DoublyLinkedList
<
arena_chunk_t
>
(
)
;
#
endif
mSpare
=
nullptr
;
mNumDirty
=
0
;
mMaxDirty
=
opt_dirty_max
>
>
3
;
mRunsAvail
.
Init
(
)
;
prev_run_size
=
pagesize
;
for
(
i
=
0
;
i
<
ntbins
;
i
+
+
)
{
bin
=
&
mBins
[
i
]
;
bin
-
>
runcur
=
nullptr
;
bin
-
>
runs
.
Init
(
)
;
bin
-
>
reg_size
=
(
1ULL
<
<
(
TINY_MIN_2POW
+
i
)
)
;
prev_run_size
=
arena_bin_run_size_calc
(
bin
prev_run_size
)
;
memset
(
&
bin
-
>
stats
0
sizeof
(
malloc_bin_stats_t
)
)
;
}
for
(
;
i
<
ntbins
+
nqbins
;
i
+
+
)
{
bin
=
&
mBins
[
i
]
;
bin
-
>
runcur
=
nullptr
;
bin
-
>
runs
.
Init
(
)
;
bin
-
>
reg_size
=
quantum
*
(
i
-
ntbins
+
1
)
;
prev_run_size
=
arena_bin_run_size_calc
(
bin
prev_run_size
)
;
memset
(
&
bin
-
>
stats
0
sizeof
(
malloc_bin_stats_t
)
)
;
}
for
(
;
i
<
ntbins
+
nqbins
+
nsbins
;
i
+
+
)
{
bin
=
&
mBins
[
i
]
;
bin
-
>
runcur
=
nullptr
;
bin
-
>
runs
.
Init
(
)
;
bin
-
>
reg_size
=
(
small_max
<
<
(
i
-
(
ntbins
+
nqbins
)
+
1
)
)
;
prev_run_size
=
arena_bin_run_size_calc
(
bin
prev_run_size
)
;
memset
(
&
bin
-
>
stats
0
sizeof
(
malloc_bin_stats_t
)
)
;
}
#
if
defined
(
MOZ_DEBUG
)
|
|
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
mMagic
=
ARENA_MAGIC
;
#
endif
return
true
;
}
static
inline
arena_t
*
arenas_fallback
(
)
{
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Error
initializing
arena
\
n
"
)
;
return
gMainArena
;
}
static
arena_t
*
arenas_extend
(
)
{
arena_t
*
ret
;
ret
=
(
arena_t
*
)
base_alloc
(
sizeof
(
arena_t
)
+
(
sizeof
(
arena_bin_t
)
*
(
ntbins
+
nqbins
+
nsbins
-
1
)
)
)
;
if
(
!
ret
|
|
!
ret
-
>
Init
(
)
)
{
return
arenas_fallback
(
)
;
}
MutexAutoLock
lock
(
arenas_lock
)
;
ret
-
>
mId
=
narenas
+
+
;
gArenaTree
.
Insert
(
ret
)
;
return
ret
;
}
static
void
*
huge_malloc
(
size_t
size
bool
zero
)
{
return
huge_palloc
(
size
chunksize
zero
)
;
}
static
void
*
huge_palloc
(
size_t
aSize
size_t
aAlignment
bool
aZero
)
{
void
*
ret
;
size_t
csize
;
size_t
psize
;
extent_node_t
*
node
;
bool
zeroed
;
csize
=
CHUNK_CEILING
(
aSize
)
;
if
(
csize
=
=
0
)
{
return
nullptr
;
}
node
=
base_node_alloc
(
)
;
if
(
!
node
)
{
return
nullptr
;
}
ret
=
chunk_alloc
(
csize
aAlignment
false
&
zeroed
)
;
if
(
!
ret
)
{
base_node_dealloc
(
node
)
;
return
nullptr
;
}
if
(
aZero
)
{
chunk_ensure_zero
(
ret
csize
zeroed
)
;
}
node
-
>
addr
=
ret
;
psize
=
PAGE_CEILING
(
aSize
)
;
node
-
>
size
=
psize
;
{
MutexAutoLock
lock
(
huge_mtx
)
;
huge
.
Insert
(
node
)
;
huge_allocated
+
=
psize
;
huge_mapped
+
=
csize
;
}
#
ifdef
MALLOC_DECOMMIT
if
(
csize
-
psize
>
0
)
pages_decommit
(
(
void
*
)
(
(
uintptr_t
)
ret
+
psize
)
csize
-
psize
)
;
#
endif
if
(
aZero
=
=
false
)
{
if
(
opt_junk
)
{
#
ifdef
MALLOC_DECOMMIT
memset
(
ret
kAllocJunk
psize
)
;
#
else
memset
(
ret
kAllocJunk
csize
)
;
#
endif
}
else
if
(
opt_zero
)
{
#
ifdef
MALLOC_DECOMMIT
memset
(
ret
0
psize
)
;
#
else
memset
(
ret
0
csize
)
;
#
endif
}
}
return
ret
;
}
static
void
*
huge_ralloc
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
{
void
*
ret
;
size_t
copysize
;
if
(
aOldSize
>
arena_maxclass
&
&
CHUNK_CEILING
(
aSize
)
=
=
CHUNK_CEILING
(
aOldSize
)
)
{
size_t
psize
=
PAGE_CEILING
(
aSize
)
;
if
(
aSize
<
aOldSize
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
aSize
)
kAllocPoison
aOldSize
-
aSize
)
;
}
#
ifdef
MALLOC_DECOMMIT
if
(
psize
<
aOldSize
)
{
extent_node_t
key
;
pages_decommit
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
psize
)
aOldSize
-
psize
)
;
MutexAutoLock
lock
(
huge_mtx
)
;
key
.
addr
=
const_cast
<
void
*
>
(
aPtr
)
;
extent_node_t
*
node
=
huge
.
Search
(
&
key
)
;
MOZ_ASSERT
(
node
)
;
MOZ_ASSERT
(
node
-
>
size
=
=
aOldSize
)
;
huge_allocated
-
=
aOldSize
-
psize
;
node
-
>
size
=
psize
;
}
else
if
(
psize
>
aOldSize
)
{
pages_commit
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
aOldSize
)
psize
-
aOldSize
)
;
}
#
endif
if
(
psize
>
aOldSize
)
{
extent_node_t
key
;
MutexAutoLock
lock
(
huge_mtx
)
;
key
.
addr
=
const_cast
<
void
*
>
(
aPtr
)
;
extent_node_t
*
node
=
huge
.
Search
(
&
key
)
;
MOZ_ASSERT
(
node
)
;
MOZ_ASSERT
(
node
-
>
size
=
=
aOldSize
)
;
huge_allocated
+
=
psize
-
aOldSize
;
node
-
>
size
=
psize
;
}
if
(
opt_zero
&
&
aSize
>
aOldSize
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
aOldSize
)
0
aSize
-
aOldSize
)
;
}
return
aPtr
;
}
ret
=
huge_malloc
(
aSize
false
)
;
if
(
!
ret
)
{
return
nullptr
;
}
copysize
=
(
aSize
<
aOldSize
)
?
aSize
:
aOldSize
;
#
ifdef
VM_COPY_MIN
if
(
copysize
>
=
VM_COPY_MIN
)
{
pages_copy
(
ret
aPtr
copysize
)
;
}
else
#
endif
{
memcpy
(
ret
aPtr
copysize
)
;
}
idalloc
(
aPtr
)
;
return
ret
;
}
static
void
huge_dalloc
(
void
*
aPtr
)
{
extent_node_t
*
node
;
{
extent_node_t
key
;
MutexAutoLock
lock
(
huge_mtx
)
;
key
.
addr
=
aPtr
;
node
=
huge
.
Search
(
&
key
)
;
MOZ_ASSERT
(
node
)
;
MOZ_ASSERT
(
node
-
>
addr
=
=
aPtr
)
;
huge
.
Remove
(
node
)
;
huge_allocated
-
=
node
-
>
size
;
huge_mapped
-
=
CHUNK_CEILING
(
node
-
>
size
)
;
}
chunk_dealloc
(
node
-
>
addr
CHUNK_CEILING
(
node
-
>
size
)
HUGE_CHUNK
)
;
base_node_dealloc
(
node
)
;
}
#
if
defined
(
XP_WIN
)
#
define
malloc_init
(
)
false
#
else
static
inline
bool
malloc_init
(
void
)
{
if
(
malloc_initialized
=
=
false
)
return
(
malloc_init_hard
(
)
)
;
return
(
false
)
;
}
#
endif
static
size_t
GetKernelPageSize
(
)
{
static
size_t
kernel_page_size
=
(
[
]
(
)
{
#
ifdef
XP_WIN
SYSTEM_INFO
info
;
GetSystemInfo
(
&
info
)
;
return
info
.
dwPageSize
;
#
else
long
result
=
sysconf
(
_SC_PAGESIZE
)
;
MOZ_ASSERT
(
result
!
=
-
1
)
;
return
result
;
#
endif
}
)
(
)
;
return
kernel_page_size
;
}
#
if
!
defined
(
XP_WIN
)
static
#
endif
bool
malloc_init_hard
(
void
)
{
unsigned
i
;
const
char
*
opts
;
long
result
;
#
ifndef
XP_WIN
MutexAutoLock
lock
(
gInitLock
)
;
#
endif
if
(
malloc_initialized
)
{
return
false
;
}
if
(
!
thread_arena
.
init
(
)
)
{
return
false
;
}
result
=
GetKernelPageSize
(
)
;
MOZ_ASSERT
(
(
(
result
-
1
)
&
result
)
=
=
0
)
;
#
ifdef
MALLOC_STATIC_PAGESIZE
if
(
pagesize
%
(
size_t
)
result
)
{
_malloc_message
(
_getprogname
(
)
"
Compile
-
time
page
size
does
not
divide
the
runtime
one
.
\
n
"
)
;
MOZ_CRASH
(
)
;
}
#
else
pagesize
=
(
size_t
)
result
;
pagesize_mask
=
(
size_t
)
result
-
1
;
pagesize_2pow
=
ffs
(
(
int
)
result
)
-
1
;
#
endif
if
(
(
opts
=
getenv
(
"
MALLOC_OPTIONS
"
)
)
)
{
for
(
i
=
0
;
opts
[
i
]
!
=
'
\
0
'
;
i
+
+
)
{
unsigned
j
nreps
;
bool
nseen
;
for
(
nreps
=
0
nseen
=
false
;
;
i
+
+
nseen
=
true
)
{
switch
(
opts
[
i
]
)
{
case
'
0
'
:
case
'
1
'
:
case
'
2
'
:
case
'
3
'
:
case
'
4
'
:
case
'
5
'
:
case
'
6
'
:
case
'
7
'
:
case
'
8
'
:
case
'
9
'
:
nreps
*
=
10
;
nreps
+
=
opts
[
i
]
-
'
0
'
;
break
;
default
:
goto
MALLOC_OUT
;
}
}
MALLOC_OUT
:
if
(
nseen
=
=
false
)
nreps
=
1
;
for
(
j
=
0
;
j
<
nreps
;
j
+
+
)
{
switch
(
opts
[
i
]
)
{
case
'
f
'
:
opt_dirty_max
>
>
=
1
;
break
;
case
'
F
'
:
if
(
opt_dirty_max
=
=
0
)
opt_dirty_max
=
1
;
else
if
(
(
opt_dirty_max
<
<
1
)
!
=
0
)
opt_dirty_max
<
<
=
1
;
break
;
#
ifdef
MOZ_DEBUG
case
'
j
'
:
opt_junk
=
false
;
break
;
case
'
J
'
:
opt_junk
=
true
;
break
;
#
endif
#
ifdef
MOZ_DEBUG
case
'
z
'
:
opt_zero
=
false
;
break
;
case
'
Z
'
:
opt_zero
=
true
;
break
;
#
endif
default
:
{
char
cbuf
[
2
]
;
cbuf
[
0
]
=
opts
[
i
]
;
cbuf
[
1
]
=
'
\
0
'
;
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Unsupported
character
"
"
in
malloc
options
:
'
"
cbuf
"
'
\
n
"
)
;
}
}
}
}
}
#
ifndef
MALLOC_STATIC_PAGESIZE
bin_maxclass
=
(
pagesize
>
>
1
)
;
nsbins
=
pagesize_2pow
-
SMALL_MAX_2POW_DEFAULT
-
1
;
chunk_npages
=
(
chunksize
>
>
pagesize_2pow
)
;
arena_chunk_header_npages
=
calculate_arena_header_pages
(
)
;
arena_maxclass
=
calculate_arena_maxclass
(
)
;
#
endif
recycled_size
=
0
;
MOZ_ASSERT
(
quantum
>
=
sizeof
(
void
*
)
)
;
MOZ_ASSERT
(
quantum
<
=
pagesize
)
;
MOZ_ASSERT
(
chunksize
>
=
pagesize
)
;
MOZ_ASSERT
(
quantum
*
4
<
=
chunksize
)
;
chunks_mtx
.
Init
(
)
;
gChunksBySize
.
Init
(
)
;
gChunksByAddress
.
Init
(
)
;
huge_mtx
.
Init
(
)
;
huge
.
Init
(
)
;
huge_allocated
=
0
;
huge_mapped
=
0
;
base_mapped
=
0
;
base_committed
=
0
;
base_nodes
=
nullptr
;
base_mtx
.
Init
(
)
;
arenas_lock
.
Init
(
)
;
gArenaTree
.
Init
(
)
;
arenas_extend
(
)
;
gMainArena
=
gArenaTree
.
First
(
)
;
if
(
!
gMainArena
)
{
return
true
;
}
gMainArena
-
>
mMaxDirty
=
opt_dirty_max
;
thread_arena
.
set
(
gMainArena
)
;
if
(
!
gChunkRTree
.
Init
(
)
)
{
return
true
;
}
malloc_initialized
=
true
;
Debug
:
:
jemalloc_ptr_info
(
nullptr
)
;
#
if
!
defined
(
XP_WIN
)
&
&
!
defined
(
XP_DARWIN
)
pthread_atfork
(
_malloc_prefork
_malloc_postfork_parent
_malloc_postfork_child
)
;
#
endif
return
false
;
}
struct
BaseAllocator
{
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
inline
return_type
name
(
__VA_ARGS__
)
;
#
define
MALLOC_FUNCS
MALLOC_FUNCS_MALLOC_BASE
#
include
"
malloc_decls
.
h
"
explicit
BaseAllocator
(
arena_t
*
aArena
)
:
mArena
(
aArena
)
{
}
private
:
arena_t
*
mArena
;
}
;
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
template
<
>
inline
return_type
\
MozJemalloc
:
:
name
(
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
\
{
\
BaseAllocator
allocator
(
nullptr
)
;
\
return
allocator
.
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
define
MALLOC_FUNCS
MALLOC_FUNCS_MALLOC_BASE
#
include
"
malloc_decls
.
h
"
inline
void
*
BaseAllocator
:
:
malloc
(
size_t
aSize
)
{
void
*
ret
;
if
(
malloc_init
(
)
)
{
ret
=
nullptr
;
goto
RETURN
;
}
if
(
aSize
=
=
0
)
{
aSize
=
1
;
}
ret
=
imalloc
(
aSize
false
mArena
)
;
RETURN
:
if
(
!
ret
)
{
errno
=
ENOMEM
;
}
return
ret
;
}
inline
void
*
BaseAllocator
:
:
memalign
(
size_t
aAlignment
size_t
aSize
)
{
void
*
ret
;
MOZ_ASSERT
(
(
(
aAlignment
-
1
)
&
aAlignment
)
=
=
0
)
;
if
(
malloc_init
(
)
)
{
return
nullptr
;
}
if
(
aSize
=
=
0
)
{
aSize
=
1
;
}
aAlignment
=
aAlignment
<
sizeof
(
void
*
)
?
sizeof
(
void
*
)
:
aAlignment
;
ret
=
ipalloc
(
aAlignment
aSize
mArena
)
;
return
ret
;
}
inline
void
*
BaseAllocator
:
:
calloc
(
size_t
aNum
size_t
aSize
)
{
void
*
ret
;
size_t
num_size
;
if
(
malloc_init
(
)
)
{
num_size
=
0
;
ret
=
nullptr
;
goto
RETURN
;
}
num_size
=
aNum
*
aSize
;
if
(
num_size
=
=
0
)
{
num_size
=
1
;
}
else
if
(
(
(
aNum
|
aSize
)
&
(
SIZE_T_MAX
<
<
(
sizeof
(
size_t
)
<
<
2
)
)
)
&
&
(
num_size
/
aSize
!
=
aNum
)
)
{
ret
=
nullptr
;
goto
RETURN
;
}
ret
=
imalloc
(
num_size
true
mArena
)
;
RETURN
:
if
(
!
ret
)
{
errno
=
ENOMEM
;
}
return
ret
;
}
inline
void
*
BaseAllocator
:
:
realloc
(
void
*
aPtr
size_t
aSize
)
{
void
*
ret
;
if
(
aSize
=
=
0
)
{
aSize
=
1
;
}
if
(
aPtr
)
{
MOZ_ASSERT
(
malloc_initialized
)
;
ret
=
iralloc
(
aPtr
aSize
mArena
)
;
if
(
!
ret
)
{
errno
=
ENOMEM
;
}
}
else
{
if
(
malloc_init
(
)
)
{
ret
=
nullptr
;
}
else
{
ret
=
imalloc
(
aSize
false
mArena
)
;
}
if
(
!
ret
)
{
errno
=
ENOMEM
;
}
}
return
ret
;
}
inline
void
BaseAllocator
:
:
free
(
void
*
aPtr
)
{
size_t
offset
;
MOZ_ASSERT
(
CHUNK_ADDR2OFFSET
(
nullptr
)
=
=
0
)
;
offset
=
CHUNK_ADDR2OFFSET
(
aPtr
)
;
if
(
offset
!
=
0
)
{
arena_dalloc
(
aPtr
offset
)
;
}
else
if
(
aPtr
)
{
huge_dalloc
(
aPtr
)
;
}
}
template
<
void
*
(
*
memalign
)
(
size_t
size_t
)
>
struct
AlignedAllocator
{
static
inline
int
posix_memalign
(
void
*
*
aMemPtr
size_t
aAlignment
size_t
aSize
)
{
void
*
result
;
if
(
(
(
aAlignment
-
1
)
&
aAlignment
)
!
=
0
|
|
aAlignment
<
sizeof
(
void
*
)
)
{
return
EINVAL
;
}
result
=
memalign
(
aAlignment
aSize
)
;
if
(
!
result
)
{
return
ENOMEM
;
}
*
aMemPtr
=
result
;
return
0
;
}
static
inline
void
*
aligned_alloc
(
size_t
aAlignment
size_t
aSize
)
{
if
(
aSize
%
aAlignment
)
{
return
nullptr
;
}
return
memalign
(
aAlignment
aSize
)
;
}
static
inline
void
*
valloc
(
size_t
aSize
)
{
return
memalign
(
GetKernelPageSize
(
)
aSize
)
;
}
}
;
template
<
>
inline
int
MozJemalloc
:
:
posix_memalign
(
void
*
*
aMemPtr
size_t
aAlignment
size_t
aSize
)
{
return
AlignedAllocator
<
memalign
>
:
:
posix_memalign
(
aMemPtr
aAlignment
aSize
)
;
}
template
<
>
inline
void
*
MozJemalloc
:
:
aligned_alloc
(
size_t
aAlignment
size_t
aSize
)
{
return
AlignedAllocator
<
memalign
>
:
:
aligned_alloc
(
aAlignment
aSize
)
;
}
template
<
>
inline
void
*
MozJemalloc
:
:
valloc
(
size_t
aSize
)
{
return
AlignedAllocator
<
memalign
>
:
:
valloc
(
aSize
)
;
}
template
<
>
inline
size_t
MozJemalloc
:
:
malloc_good_size
(
size_t
aSize
)
{
if
(
aSize
<
small_min
)
{
aSize
=
pow2_ceil
(
aSize
)
;
if
(
aSize
<
(
1U
<
<
TINY_MIN_2POW
)
)
aSize
=
(
1U
<
<
TINY_MIN_2POW
)
;
}
else
if
(
aSize
<
=
small_max
)
{
aSize
=
QUANTUM_CEILING
(
aSize
)
;
}
else
if
(
aSize
<
=
bin_maxclass
)
{
aSize
=
pow2_ceil
(
aSize
)
;
}
else
if
(
aSize
<
=
arena_maxclass
)
{
aSize
=
PAGE_CEILING
(
aSize
)
;
}
else
{
aSize
=
PAGE_CEILING
(
aSize
)
;
}
return
aSize
;
}
template
<
>
inline
size_t
MozJemalloc
:
:
malloc_usable_size
(
usable_ptr_t
aPtr
)
{
return
isalloc_validate
(
aPtr
)
;
}
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_stats
(
jemalloc_stats_t
*
aStats
)
{
size_t
non_arena_mapped
chunk_header_size
;
MOZ_ASSERT
(
aStats
)
;
aStats
-
>
opt_junk
=
opt_junk
;
aStats
-
>
opt_zero
=
opt_zero
;
aStats
-
>
narenas
=
narenas
;
aStats
-
>
quantum
=
quantum
;
aStats
-
>
small_max
=
small_max
;
aStats
-
>
large_max
=
arena_maxclass
;
aStats
-
>
chunksize
=
chunksize
;
aStats
-
>
page_size
=
pagesize
;
aStats
-
>
dirty_max
=
opt_dirty_max
;
aStats
-
>
mapped
=
0
;
aStats
-
>
allocated
=
0
;
aStats
-
>
waste
=
0
;
aStats
-
>
page_cache
=
0
;
aStats
-
>
bookkeeping
=
0
;
aStats
-
>
bin_unused
=
0
;
non_arena_mapped
=
0
;
{
MutexAutoLock
lock
(
huge_mtx
)
;
non_arena_mapped
+
=
huge_mapped
;
aStats
-
>
allocated
+
=
huge_allocated
;
MOZ_ASSERT
(
huge_mapped
>
=
huge_allocated
)
;
}
{
MutexAutoLock
lock
(
base_mtx
)
;
non_arena_mapped
+
=
base_mapped
;
aStats
-
>
bookkeeping
+
=
base_committed
;
MOZ_ASSERT
(
base_mapped
>
=
base_committed
)
;
}
arenas_lock
.
Lock
(
)
;
for
(
auto
arena
:
gArenaTree
.
iter
(
)
)
{
size_t
arena_mapped
arena_allocated
arena_committed
arena_dirty
j
arena_unused
arena_headers
;
arena_run_t
*
run
;
if
(
!
arena
)
{
continue
;
}
arena_headers
=
0
;
arena_unused
=
0
;
{
MutexAutoLock
lock
(
arena
-
>
mLock
)
;
arena_mapped
=
arena
-
>
mStats
.
mapped
;
arena_committed
=
arena
-
>
mStats
.
committed
<
<
pagesize_2pow
;
arena_allocated
=
arena
-
>
mStats
.
allocated_small
+
arena
-
>
mStats
.
allocated_large
;
arena_dirty
=
arena
-
>
mNumDirty
<
<
pagesize_2pow
;
for
(
j
=
0
;
j
<
ntbins
+
nqbins
+
nsbins
;
j
+
+
)
{
arena_bin_t
*
bin
=
&
arena
-
>
mBins
[
j
]
;
size_t
bin_unused
=
0
;
for
(
auto
mapelm
:
bin
-
>
runs
.
iter
(
)
)
{
run
=
(
arena_run_t
*
)
(
mapelm
-
>
bits
&
~
pagesize_mask
)
;
bin_unused
+
=
run
-
>
nfree
*
bin
-
>
reg_size
;
}
if
(
bin
-
>
runcur
)
{
bin_unused
+
=
bin
-
>
runcur
-
>
nfree
*
bin
-
>
reg_size
;
}
arena_unused
+
=
bin_unused
;
arena_headers
+
=
bin
-
>
stats
.
curruns
*
bin
-
>
reg0_offset
;
}
}
MOZ_ASSERT
(
arena_mapped
>
=
arena_committed
)
;
MOZ_ASSERT
(
arena_committed
>
=
arena_allocated
+
arena_dirty
)
;
aStats
-
>
mapped
+
=
arena_mapped
;
aStats
-
>
allocated
+
=
arena_allocated
;
aStats
-
>
page_cache
+
=
arena_dirty
;
aStats
-
>
waste
+
=
arena_committed
-
arena_allocated
-
arena_dirty
-
arena_unused
-
arena_headers
;
aStats
-
>
bin_unused
+
=
arena_unused
;
aStats
-
>
bookkeeping
+
=
arena_headers
;
}
arenas_lock
.
Unlock
(
)
;
chunk_header_size
=
(
(
aStats
-
>
mapped
/
aStats
-
>
chunksize
)
*
arena_chunk_header_npages
)
<
<
pagesize_2pow
;
aStats
-
>
mapped
+
=
non_arena_mapped
;
aStats
-
>
bookkeeping
+
=
chunk_header_size
;
aStats
-
>
waste
-
=
chunk_header_size
;
MOZ_ASSERT
(
aStats
-
>
mapped
>
=
aStats
-
>
allocated
+
aStats
-
>
waste
+
aStats
-
>
page_cache
+
aStats
-
>
bookkeeping
)
;
}
#
ifdef
MALLOC_DOUBLE_PURGE
static
void
hard_purge_chunk
(
arena_chunk_t
*
chunk
)
{
size_t
i
;
for
(
i
=
arena_chunk_header_npages
;
i
<
chunk_npages
;
i
+
+
)
{
size_t
npages
;
for
(
npages
=
0
;
chunk
-
>
map
[
i
+
npages
]
.
bits
&
CHUNK_MAP_MADVISED
&
&
i
+
npages
<
chunk_npages
;
npages
+
+
)
{
MOZ_DIAGNOSTIC_ASSERT
(
!
(
chunk
-
>
map
[
i
+
npages
]
.
bits
&
CHUNK_MAP_DECOMMITTED
)
)
;
chunk
-
>
map
[
i
+
npages
]
.
bits
^
=
CHUNK_MAP_MADVISED_OR_DECOMMITTED
;
}
if
(
npages
>
0
)
{
pages_decommit
(
(
(
char
*
)
chunk
)
+
(
i
<
<
pagesize_2pow
)
npages
<
<
pagesize_2pow
)
;
pages_commit
(
(
(
char
*
)
chunk
)
+
(
i
<
<
pagesize_2pow
)
npages
<
<
pagesize_2pow
)
;
}
i
+
=
npages
;
}
}
void
arena_t
:
:
HardPurge
(
)
{
MutexAutoLock
lock
(
mLock
)
;
while
(
!
mChunksMAdvised
.
isEmpty
(
)
)
{
arena_chunk_t
*
chunk
=
mChunksMAdvised
.
popFront
(
)
;
hard_purge_chunk
(
chunk
)
;
}
}
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_purge_freed_pages
(
)
{
MutexAutoLock
lock
(
arenas_lock
)
;
for
(
auto
arena
:
gArenaTree
.
iter
(
)
)
{
arena
-
>
HardPurge
(
)
;
}
}
#
else
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_purge_freed_pages
(
)
{
}
#
endif
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_free_dirty_pages
(
void
)
{
MutexAutoLock
lock
(
arenas_lock
)
;
for
(
auto
arena
:
gArenaTree
.
iter
(
)
)
{
MutexAutoLock
arena_lock
(
arena
-
>
mLock
)
;
arena
-
>
Purge
(
true
)
;
}
}
inline
arena_t
*
arena_t
:
:
GetById
(
arena_id_t
aArenaId
)
{
arena_t
key
;
key
.
mId
=
aArenaId
;
MutexAutoLock
lock
(
arenas_lock
)
;
arena_t
*
result
=
gArenaTree
.
Search
(
&
key
)
;
MOZ_RELEASE_ASSERT
(
result
)
;
return
result
;
}
#
ifdef
NIGHTLY_BUILD
template
<
>
inline
arena_id_t
MozJemalloc
:
:
moz_create_arena
(
)
{
arena_t
*
arena
=
arenas_extend
(
)
;
return
arena
-
>
mId
;
}
template
<
>
inline
void
MozJemalloc
:
:
moz_dispose_arena
(
arena_id_t
aArenaId
)
{
arena_t
*
arena
=
arena_t
:
:
GetById
(
aArenaId
)
;
MutexAutoLock
lock
(
arenas_lock
)
;
gArenaTree
.
Remove
(
arena
)
;
}
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
template
<
>
inline
return_type
\
MozJemalloc
:
:
moz_arena_
#
#
name
(
arena_id_t
aArenaId
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
\
{
\
BaseAllocator
allocator
(
arena_t
:
:
GetById
(
aArenaId
)
)
;
\
return
allocator
.
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
define
MALLOC_FUNCS
MALLOC_FUNCS_MALLOC_BASE
#
include
"
malloc_decls
.
h
"
#
else
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
template
<
>
inline
return_type
\
MozJemalloc
:
:
name
(
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
\
{
\
return
DummyArenaAllocator
<
MozJemalloc
>
:
:
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
define
MALLOC_FUNCS
MALLOC_FUNCS_ARENA
#
include
"
malloc_decls
.
h
"
#
endif
#
ifndef
XP_DARWIN
static
#
endif
void
_malloc_prefork
(
void
)
{
arenas_lock
.
Lock
(
)
;
for
(
auto
arena
:
gArenaTree
.
iter
(
)
)
{
arena
-
>
mLock
.
Lock
(
)
;
}
base_mtx
.
Lock
(
)
;
huge_mtx
.
Lock
(
)
;
}
#
ifndef
XP_DARWIN
static
#
endif
void
_malloc_postfork_parent
(
void
)
{
huge_mtx
.
Unlock
(
)
;
base_mtx
.
Unlock
(
)
;
for
(
auto
arena
:
gArenaTree
.
iter
(
)
)
{
arena
-
>
mLock
.
Unlock
(
)
;
}
arenas_lock
.
Unlock
(
)
;
}
#
ifndef
XP_DARWIN
static
#
endif
void
_malloc_postfork_child
(
void
)
{
huge_mtx
.
Init
(
)
;
base_mtx
.
Init
(
)
;
for
(
auto
arena
:
gArenaTree
.
iter
(
)
)
{
arena
-
>
mLock
.
Init
(
)
;
}
arenas_lock
.
Init
(
)
;
}
#
ifdef
MOZ_REPLACE_MALLOC
#
ifdef
XP_DARWIN
#
define
MOZ_REPLACE_WEAK
__attribute__
(
(
weak_import
)
)
#
elif
defined
(
XP_WIN
)
|
|
defined
(
MOZ_WIDGET_ANDROID
)
#
define
MOZ_NO_REPLACE_FUNC_DECL
#
elif
defined
(
__GNUC__
)
#
define
MOZ_REPLACE_WEAK
__attribute__
(
(
weak
)
)
#
endif
#
include
"
replace_malloc
.
h
"
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
MozJemalloc
:
:
name
static
const
malloc_table_t
malloc_table
=
{
#
include
"
malloc_decls
.
h
"
}
;
static
malloc_table_t
replace_malloc_table
;
#
ifdef
MOZ_NO_REPLACE_FUNC_DECL
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
typedef
return_type
(
name
#
#
_impl_t
)
(
__VA_ARGS__
)
;
\
name
#
#
_impl_t
*
replace_
#
#
name
=
nullptr
;
#
define
MALLOC_FUNCS
(
MALLOC_FUNCS_INIT
|
MALLOC_FUNCS_BRIDGE
)
#
include
"
malloc_decls
.
h
"
#
endif
#
ifdef
XP_WIN
typedef
HMODULE
replace_malloc_handle_t
;
static
replace_malloc_handle_t
replace_malloc_handle
(
)
{
char
replace_malloc_lib
[
1024
]
;
if
(
GetEnvironmentVariableA
(
"
MOZ_REPLACE_MALLOC_LIB
"
(
LPSTR
)
&
replace_malloc_lib
sizeof
(
replace_malloc_lib
)
)
>
0
)
{
return
LoadLibraryA
(
replace_malloc_lib
)
;
}
return
nullptr
;
}
#
define
REPLACE_MALLOC_GET_FUNC
(
handle
name
)
\
(
name
#
#
_impl_t
*
)
GetProcAddress
(
handle
"
replace_
"
#
name
)
#
elif
defined
(
ANDROID
)
#
include
<
dlfcn
.
h
>
typedef
void
*
replace_malloc_handle_t
;
static
replace_malloc_handle_t
replace_malloc_handle
(
)
{
const
char
*
replace_malloc_lib
=
getenv
(
"
MOZ_REPLACE_MALLOC_LIB
"
)
;
if
(
replace_malloc_lib
&
&
*
replace_malloc_lib
)
{
return
dlopen
(
replace_malloc_lib
RTLD_LAZY
)
;
}
return
nullptr
;
}
#
define
REPLACE_MALLOC_GET_FUNC
(
handle
name
)
\
(
name
#
#
_impl_t
*
)
dlsym
(
handle
"
replace_
"
#
name
)
#
else
typedef
bool
replace_malloc_handle_t
;
static
replace_malloc_handle_t
replace_malloc_handle
(
)
{
return
true
;
}
#
define
REPLACE_MALLOC_GET_FUNC
(
handle
name
)
\
replace_
#
#
name
#
endif
static
void
replace_malloc_init_funcs
(
)
;
static
int
replace_malloc_initialized
=
0
;
static
void
init
(
)
{
replace_malloc_init_funcs
(
)
;
replace_malloc_initialized
=
1
;
if
(
replace_init
)
{
replace_init
(
&
malloc_table
)
;
}
}
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
template
<
>
inline
return_type
\
ReplaceMalloc
:
:
name
(
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
\
{
\
if
(
MOZ_UNLIKELY
(
!
replace_malloc_initialized
)
)
{
\
init
(
)
;
\
}
\
return
replace_malloc_table
.
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
include
"
malloc_decls
.
h
"
MOZ_JEMALLOC_API
struct
ReplaceMallocBridge
*
get_bridge
(
void
)
{
if
(
MOZ_UNLIKELY
(
!
replace_malloc_initialized
)
)
init
(
)
;
if
(
MOZ_LIKELY
(
!
replace_get_bridge
)
)
return
nullptr
;
return
replace_get_bridge
(
)
;
}
static
void
replace_malloc_init_funcs
(
)
{
replace_malloc_handle_t
handle
=
replace_malloc_handle
(
)
;
if
(
handle
)
{
#
ifdef
MOZ_NO_REPLACE_FUNC_DECL
#
define
MALLOC_DECL
(
name
.
.
.
)
\
replace_
#
#
name
=
REPLACE_MALLOC_GET_FUNC
(
handle
name
)
;
#
define
MALLOC_FUNCS
(
MALLOC_FUNCS_INIT
|
MALLOC_FUNCS_BRIDGE
)
#
include
"
malloc_decls
.
h
"
#
endif
#
define
MALLOC_DECL
(
name
.
.
.
)
\
replace_malloc_table
.
name
=
REPLACE_MALLOC_GET_FUNC
(
handle
name
)
;
#
include
"
malloc_decls
.
h
"
}
if
(
!
replace_malloc_table
.
posix_memalign
&
&
replace_malloc_table
.
memalign
)
{
replace_malloc_table
.
posix_memalign
=
AlignedAllocator
<
ReplaceMalloc
:
:
memalign
>
:
:
posix_memalign
;
}
if
(
!
replace_malloc_table
.
aligned_alloc
&
&
replace_malloc_table
.
memalign
)
{
replace_malloc_table
.
aligned_alloc
=
AlignedAllocator
<
ReplaceMalloc
:
:
memalign
>
:
:
aligned_alloc
;
}
if
(
!
replace_malloc_table
.
valloc
&
&
replace_malloc_table
.
memalign
)
{
replace_malloc_table
.
valloc
=
AlignedAllocator
<
ReplaceMalloc
:
:
memalign
>
:
:
valloc
;
}
if
(
!
replace_malloc_table
.
moz_create_arena
&
&
replace_malloc_table
.
malloc
)
{
#
define
MALLOC_DECL
(
name
.
.
.
)
\
replace_malloc_table
.
name
=
DummyArenaAllocator
<
ReplaceMalloc
>
:
:
name
;
#
define
MALLOC_FUNCS
MALLOC_FUNCS_ARENA
#
include
"
malloc_decls
.
h
"
}
#
define
MALLOC_DECL
(
name
.
.
.
)
\
if
(
!
replace_malloc_table
.
name
)
{
\
replace_malloc_table
.
name
=
MozJemalloc
:
:
name
;
\
}
#
include
"
malloc_decls
.
h
"
}
#
endif
#
define
GENERIC_MALLOC_DECL2
(
name
name_impl
return_type
.
.
.
)
\
return_type
name_impl
(
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
\
{
\
return
DefaultMalloc
:
:
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
define
GENERIC_MALLOC_DECL
(
name
return_type
.
.
.
)
\
GENERIC_MALLOC_DECL2
(
name
name
#
#
_impl
return_type
#
#
__VA_ARGS__
)
#
define
MALLOC_DECL
(
.
.
.
)
MOZ_MEMORY_API
MACRO_CALL
(
GENERIC_MALLOC_DECL
(
__VA_ARGS__
)
)
#
define
MALLOC_FUNCS
MALLOC_FUNCS_MALLOC
#
include
"
malloc_decls
.
h
"
#
undef
GENERIC_MALLOC_DECL
#
define
GENERIC_MALLOC_DECL
(
name
return_type
.
.
.
)
\
GENERIC_MALLOC_DECL2
(
name
name
return_type
#
#
__VA_ARGS__
)
#
define
MALLOC_DECL
(
.
.
.
)
MOZ_JEMALLOC_API
MACRO_CALL
(
GENERIC_MALLOC_DECL
(
__VA_ARGS__
)
)
#
define
MALLOC_FUNCS
(
MALLOC_FUNCS_JEMALLOC
|
MALLOC_FUNCS_ARENA
)
#
include
"
malloc_decls
.
h
"
#
ifdef
HAVE_DLOPEN
#
include
<
dlfcn
.
h
>
#
endif
#
if
defined
(
__GLIBC__
)
&
&
!
defined
(
__UCLIBC__
)
extern
"
C
"
{
MOZ_EXPORT
void
(
*
__free_hook
)
(
void
*
)
=
free_impl
;
MOZ_EXPORT
void
*
(
*
__malloc_hook
)
(
size_t
)
=
malloc_impl
;
MOZ_EXPORT
void
*
(
*
__realloc_hook
)
(
void
*
size_t
)
=
realloc_impl
;
MOZ_EXPORT
void
*
(
*
__memalign_hook
)
(
size_t
size_t
)
=
memalign_impl
;
}
#
elif
defined
(
RTLD_DEEPBIND
)
#
error
"
Interposing
malloc
is
unsafe
on
this
system
without
libc
malloc
hooks
.
"
#
endif
#
ifdef
XP_WIN
void
*
_recalloc
(
void
*
aPtr
size_t
aCount
size_t
aSize
)
{
size_t
oldsize
=
aPtr
?
isalloc
(
aPtr
)
:
0
;
size_t
newsize
=
aCount
*
aSize
;
aPtr
=
DefaultMalloc
:
:
realloc
(
aPtr
newsize
)
;
if
(
aPtr
&
&
oldsize
<
newsize
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
oldsize
)
0
newsize
-
oldsize
)
;
}
return
aPtr
;
}
void
*
_expand
(
void
*
aPtr
size_t
newsize
)
{
if
(
isalloc
(
aPtr
)
>
=
newsize
)
{
return
aPtr
;
}
return
nullptr
;
}
size_t
_msize
(
void
*
aPtr
)
{
return
DefaultMalloc
:
:
malloc_usable_size
(
aPtr
)
;
}
BOOL
APIENTRY
DllMain
(
HINSTANCE
hModule
DWORD
reason
LPVOID
lpReserved
)
{
switch
(
reason
)
{
case
DLL_PROCESS_ATTACH
:
DisableThreadLibraryCalls
(
hModule
)
;
malloc_init_hard
(
)
;
break
;
case
DLL_PROCESS_DETACH
:
break
;
}
return
TRUE
;
}
#
endif
