#
include
"
mozmemory_wrap
.
h
"
#
include
"
mozjemalloc
.
h
"
#
include
"
mozjemalloc_types
.
h
"
#
include
<
cstring
>
#
include
<
cerrno
>
#
ifdef
XP_WIN
#
include
<
io
.
h
>
#
include
<
windows
.
h
>
#
else
#
include
<
sys
/
mman
.
h
>
#
include
<
unistd
.
h
>
#
endif
#
ifdef
XP_DARWIN
#
include
<
libkern
/
OSAtomic
.
h
>
#
include
<
mach
/
mach_init
.
h
>
#
include
<
mach
/
vm_map
.
h
>
#
endif
#
include
"
mozilla
/
Atomics
.
h
"
#
include
"
mozilla
/
Alignment
.
h
"
#
include
"
mozilla
/
ArrayUtils
.
h
"
#
include
"
mozilla
/
Assertions
.
h
"
#
include
"
mozilla
/
CheckedInt
.
h
"
#
include
"
mozilla
/
DoublyLinkedList
.
h
"
#
include
"
mozilla
/
HelperMacros
.
h
"
#
include
"
mozilla
/
Likely
.
h
"
#
include
"
mozilla
/
MathAlgorithms
.
h
"
#
include
"
mozilla
/
RandomNum
.
h
"
#
include
"
mozilla
/
Sprintf
.
h
"
#
include
"
mozilla
/
TaggedAnonymousMemory
.
h
"
#
include
"
mozilla
/
ThreadLocal
.
h
"
#
include
"
mozilla
/
UniquePtr
.
h
"
#
include
"
mozilla
/
Unused
.
h
"
#
include
"
mozilla
/
XorShift128PlusRNG
.
h
"
#
include
"
mozilla
/
fallible
.
h
"
#
include
"
rb
.
h
"
#
include
"
Mutex
.
h
"
#
include
"
Utils
.
h
"
using
namespace
mozilla
;
#
ifdef
XP_DARWIN
#
define
MALLOC_DOUBLE_PURGE
#
endif
#
ifdef
XP_WIN
#
define
MALLOC_DECOMMIT
#
endif
#
ifndef
MOZ_DEBUG
#
if
!
defined
(
__ia64__
)
&
&
!
defined
(
__sparc__
)
&
&
!
defined
(
__mips__
)
&
&
\
!
defined
(
__aarch64__
)
&
&
!
defined
(
__powerpc__
)
&
&
!
defined
(
XP_MACOSX
)
&
&
\
!
defined
(
__loongarch__
)
#
define
MALLOC_STATIC_PAGESIZE
1
#
endif
#
endif
#
ifdef
XP_WIN
#
define
STDERR_FILENO
2
static
char
mozillaMallocOptionsBuf
[
64
]
;
#
define
getenv
xgetenv
static
char
*
getenv
(
const
char
*
name
)
{
if
(
GetEnvironmentVariableA
(
name
mozillaMallocOptionsBuf
sizeof
(
mozillaMallocOptionsBuf
)
)
>
0
)
{
return
mozillaMallocOptionsBuf
;
}
return
nullptr
;
}
#
endif
#
ifndef
XP_WIN
#
if
defined
(
XP_LINUX
)
&
&
defined
(
MADV_FREE
)
#
undef
MADV_FREE
#
endif
#
ifndef
MADV_FREE
#
define
MADV_FREE
MADV_DONTNEED
#
endif
#
endif
#
if
(
defined
(
XP_LINUX
)
&
&
!
defined
(
__alpha__
)
)
|
|
\
(
defined
(
__FreeBSD_kernel__
)
&
&
defined
(
__GLIBC__
)
)
#
include
<
sys
/
syscall
.
h
>
#
if
defined
(
SYS_mmap
)
|
|
defined
(
SYS_mmap2
)
static
inline
void
*
_mmap
(
void
*
addr
size_t
length
int
prot
int
flags
int
fd
off_t
offset
)
{
#
ifdef
__s390__
struct
{
void
*
addr
;
size_t
length
;
long
prot
;
long
flags
;
long
fd
;
off_t
offset
;
}
args
=
{
addr
length
prot
flags
fd
offset
}
;
return
(
void
*
)
syscall
(
SYS_mmap
&
args
)
;
#
else
#
if
defined
(
ANDROID
)
&
&
defined
(
__aarch64__
)
&
&
defined
(
SYS_mmap2
)
#
undef
SYS_mmap2
#
endif
#
ifdef
SYS_mmap2
return
(
void
*
)
syscall
(
SYS_mmap2
addr
length
prot
flags
fd
offset
>
>
12
)
;
#
else
return
(
void
*
)
syscall
(
SYS_mmap
addr
length
prot
flags
fd
offset
)
;
#
endif
#
endif
}
#
define
mmap
_mmap
#
define
munmap
(
a
l
)
syscall
(
SYS_munmap
a
l
)
#
endif
#
endif
struct
arena_t
;
struct
arena_chunk_map_t
{
RedBlackTreeNode
<
arena_chunk_map_t
>
link
;
size_t
bits
;
#
define
CHUNK_MAP_MADVISED
(
(
size_t
)
0x40U
)
#
define
CHUNK_MAP_DECOMMITTED
(
(
size_t
)
0x20U
)
#
define
CHUNK_MAP_MADVISED_OR_DECOMMITTED
\
(
CHUNK_MAP_MADVISED
|
CHUNK_MAP_DECOMMITTED
)
#
define
CHUNK_MAP_KEY
(
(
size_t
)
0x10U
)
#
define
CHUNK_MAP_DIRTY
(
(
size_t
)
0x08U
)
#
define
CHUNK_MAP_ZEROED
(
(
size_t
)
0x04U
)
#
define
CHUNK_MAP_LARGE
(
(
size_t
)
0x02U
)
#
define
CHUNK_MAP_ALLOCATED
(
(
size_t
)
0x01U
)
}
;
struct
arena_chunk_t
{
arena_t
*
arena
;
RedBlackTreeNode
<
arena_chunk_t
>
link_dirty
;
#
ifdef
MALLOC_DOUBLE_PURGE
DoublyLinkedListElement
<
arena_chunk_t
>
chunks_madvised_elem
;
#
endif
size_t
ndirty
;
arena_chunk_map_t
map
[
1
]
;
}
;
static
const
size_t
kCacheLineSize
=
64
;
#
ifdef
XP_WIN
static
const
size_t
kMinTinyClass
=
sizeof
(
void
*
)
*
2
;
#
else
static
const
size_t
kMinTinyClass
=
sizeof
(
void
*
)
;
#
endif
static
const
size_t
kMaxTinyClass
=
8
;
static
const
size_t
kMinQuantumClass
=
kMaxTinyClass
*
2
;
static
const
size_t
kMinQuantumWideClass
=
512
;
static
const
size_t
kMinSubPageClass
=
4_KiB
;
static
const
size_t
kQuantum
=
16
;
static
const
size_t
kQuantumMask
=
kQuantum
-
1
;
static
const
size_t
kQuantumWide
=
256
;
static
const
size_t
kQuantumWideMask
=
kQuantumWide
-
1
;
static
const
size_t
kMaxQuantumClass
=
kMinQuantumWideClass
-
kQuantum
;
static
const
size_t
kMaxQuantumWideClass
=
kMinSubPageClass
-
kQuantumWide
;
static_assert
(
mozilla
:
:
IsPowerOfTwo
(
kQuantum
)
"
kQuantum
is
not
a
power
of
two
"
)
;
static_assert
(
mozilla
:
:
IsPowerOfTwo
(
kQuantumWide
)
"
kQuantumWide
is
not
a
power
of
two
"
)
;
static_assert
(
kMaxQuantumClass
%
kQuantum
=
=
0
"
kMaxQuantumClass
is
not
a
multiple
of
kQuantum
"
)
;
static_assert
(
kMaxQuantumWideClass
%
kQuantumWide
=
=
0
"
kMaxQuantumWideClass
is
not
a
multiple
of
kQuantumWide
"
)
;
static_assert
(
kQuantum
<
kQuantumWide
"
kQuantum
must
be
smaller
than
kQuantumWide
"
)
;
static_assert
(
mozilla
:
:
IsPowerOfTwo
(
kMinSubPageClass
)
"
kMinSubPageClass
is
not
a
power
of
two
"
)
;
static
const
size_t
kNumTinyClasses
=
LOG2
(
kMaxTinyClass
)
-
LOG2
(
kMinTinyClass
)
+
1
;
static
const
size_t
kNumQuantumClasses
=
(
kMaxQuantumClass
+
kQuantum
-
kMinQuantumClass
)
/
kQuantum
;
static
const
size_t
kNumQuantumWideClasses
=
(
kMaxQuantumWideClass
+
kQuantumWide
-
kMinQuantumWideClass
)
/
kQuantumWide
;
static
const
size_t
kChunkSize
=
1_MiB
;
static
const
size_t
kChunkSizeMask
=
kChunkSize
-
1
;
#
ifdef
MALLOC_STATIC_PAGESIZE
#
if
defined
(
__powerpc64__
)
static
const
size_t
gPageSize
=
64_KiB
;
#
elif
defined
(
__loongarch64
)
static
const
size_t
gPageSize
=
16_KiB
;
#
else
static
const
size_t
gPageSize
=
4_KiB
;
#
endif
static
const
size_t
gRealPageSize
=
gPageSize
;
#
else
static
size_t
gRealPageSize
;
static
size_t
gPageSize
;
#
endif
#
ifdef
MALLOC_STATIC_PAGESIZE
#
define
DECLARE_GLOBAL
(
type
name
)
#
define
DEFINE_GLOBALS
#
define
END_GLOBALS
#
define
DEFINE_GLOBAL
(
type
)
static
const
type
#
define
GLOBAL_LOG2
LOG2
#
define
GLOBAL_ASSERT_HELPER1
(
x
)
static_assert
(
x
#
x
)
#
define
GLOBAL_ASSERT_HELPER2
(
x
y
)
static_assert
(
x
y
)
#
define
GLOBAL_ASSERT
(
.
.
.
)
\
MACRO_CALL
(
\
MOZ_PASTE_PREFIX_AND_ARG_COUNT
(
GLOBAL_ASSERT_HELPER
__VA_ARGS__
)
\
(
__VA_ARGS__
)
)
#
define
GLOBAL_CONSTEXPR
constexpr
#
else
#
define
DECLARE_GLOBAL
(
type
name
)
static
type
name
;
#
define
DEFINE_GLOBALS
static
void
DefineGlobals
(
)
{
#
define
END_GLOBALS
}
#
define
DEFINE_GLOBAL
(
type
)
#
define
GLOBAL_LOG2
FloorLog2
#
define
GLOBAL_ASSERT
MOZ_RELEASE_ASSERT
#
define
GLOBAL_CONSTEXPR
#
endif
DECLARE_GLOBAL
(
size_t
gMaxSubPageClass
)
DECLARE_GLOBAL
(
uint8_t
gNumSubPageClasses
)
DECLARE_GLOBAL
(
uint8_t
gPageSize2Pow
)
DECLARE_GLOBAL
(
size_t
gPageSizeMask
)
DECLARE_GLOBAL
(
size_t
gChunkNumPages
)
DECLARE_GLOBAL
(
size_t
gChunkHeaderNumPages
)
DECLARE_GLOBAL
(
size_t
gMaxLargeClass
)
DEFINE_GLOBALS
DEFINE_GLOBAL
(
size_t
)
gMaxSubPageClass
=
gPageSize
/
2
>
=
kMinSubPageClass
?
gPageSize
/
2
:
0
;
#
define
gMaxBinClass
\
(
gMaxSubPageClass
?
gMaxSubPageClass
:
kMaxQuantumWideClass
)
DEFINE_GLOBAL
(
uint8_t
)
gNumSubPageClasses
=
[
]
(
)
GLOBAL_CONSTEXPR
-
>
uint8_t
{
if
GLOBAL_CONSTEXPR
(
gMaxSubPageClass
!
=
0
)
{
return
FloorLog2
(
gMaxSubPageClass
)
-
LOG2
(
kMinSubPageClass
)
+
1
;
}
return
0
;
}
(
)
;
DEFINE_GLOBAL
(
uint8_t
)
gPageSize2Pow
=
GLOBAL_LOG2
(
gPageSize
)
;
DEFINE_GLOBAL
(
size_t
)
gPageSizeMask
=
gPageSize
-
1
;
DEFINE_GLOBAL
(
size_t
)
gChunkNumPages
=
kChunkSize
>
>
gPageSize2Pow
;
DEFINE_GLOBAL
(
size_t
)
gChunkHeaderNumPages
=
1
+
(
(
(
sizeof
(
arena_chunk_t
)
+
sizeof
(
arena_chunk_map_t
)
*
(
gChunkNumPages
-
1
)
+
gPageSizeMask
)
&
~
gPageSizeMask
)
>
>
gPageSize2Pow
)
;
DEFINE_GLOBAL
(
size_t
)
gMaxLargeClass
=
kChunkSize
-
gPageSize
-
(
gChunkHeaderNumPages
<
<
gPageSize2Pow
)
;
GLOBAL_ASSERT
(
1ULL
<
<
gPageSize2Pow
=
=
gPageSize
"
Page
size
is
not
a
power
of
two
"
)
;
GLOBAL_ASSERT
(
kQuantum
>
=
sizeof
(
void
*
)
)
;
GLOBAL_ASSERT
(
kQuantum
<
=
kQuantumWide
)
;
GLOBAL_ASSERT
(
!
kNumQuantumWideClasses
|
|
kQuantumWide
<
=
(
kMinSubPageClass
-
kMaxQuantumClass
)
)
;
GLOBAL_ASSERT
(
kQuantumWide
<
=
kMaxQuantumClass
)
;
GLOBAL_ASSERT
(
gMaxSubPageClass
>
=
kMinSubPageClass
|
|
gMaxSubPageClass
=
=
0
)
;
GLOBAL_ASSERT
(
gMaxLargeClass
>
=
gMaxSubPageClass
)
;
GLOBAL_ASSERT
(
kChunkSize
>
=
gPageSize
)
;
GLOBAL_ASSERT
(
kQuantum
*
4
<
=
kChunkSize
)
;
END_GLOBALS
static
const
size_t
gRecycleLimit
=
128_MiB
;
static
Atomic
<
size_t
ReleaseAcquire
>
gRecycledSize
;
#
define
DIRTY_MAX_DEFAULT
(
1U
<
<
8
)
static
size_t
opt_dirty_max
=
DIRTY_MAX_DEFAULT
;
#
define
CHUNK_CEILING
(
s
)
(
(
(
s
)
+
kChunkSizeMask
)
&
~
kChunkSizeMask
)
#
define
CACHELINE_CEILING
(
s
)
\
(
(
(
s
)
+
(
kCacheLineSize
-
1
)
)
&
~
(
kCacheLineSize
-
1
)
)
#
define
QUANTUM_CEILING
(
a
)
(
(
(
a
)
+
(
kQuantumMask
)
)
&
~
(
kQuantumMask
)
)
#
define
QUANTUM_WIDE_CEILING
(
a
)
\
(
(
(
a
)
+
(
kQuantumWideMask
)
)
&
~
(
kQuantumWideMask
)
)
#
define
SUBPAGE_CEILING
(
a
)
(
RoundUpPow2
(
a
)
)
#
define
PAGE_CEILING
(
s
)
(
(
(
s
)
+
gPageSizeMask
)
&
~
gPageSizeMask
)
#
define
NUM_SMALL_CLASSES
\
(
kNumTinyClasses
+
kNumQuantumClasses
+
kNumQuantumWideClasses
+
\
gNumSubPageClasses
)
#
if
defined
(
MALLOC_DECOMMIT
)
&
&
defined
(
MALLOC_DOUBLE_PURGE
)
#
error
MALLOC_DECOMMIT
and
MALLOC_DOUBLE_PURGE
are
mutually
exclusive
.
#
endif
static
void
*
base_alloc
(
size_t
aSize
)
;
#
if
defined
(
_MSC_VER
)
&
&
!
defined
(
__clang__
)
static
bool
malloc_initialized
;
#
else
static
Atomic
<
bool
SequentiallyConsistent
>
malloc_initialized
;
#
endif
static
StaticMutex
gInitLock
MOZ_UNANNOTATED
=
{
STATIC_MUTEX_INIT
}
;
struct
arena_stats_t
{
size_t
mapped
;
size_t
committed
;
size_t
allocated_small
;
size_t
allocated_large
;
}
;
enum
ChunkType
{
UNKNOWN_CHUNK
ZEROED_CHUNK
ARENA_CHUNK
HUGE_CHUNK
RECYCLED_CHUNK
}
;
struct
extent_node_t
{
union
{
RedBlackTreeNode
<
extent_node_t
>
mLinkBySize
;
arena_id_t
mArenaId
;
}
;
RedBlackTreeNode
<
extent_node_t
>
mLinkByAddr
;
void
*
mAddr
;
size_t
mSize
;
union
{
ChunkType
mChunkType
;
arena_t
*
mArena
;
}
;
}
;
struct
ExtentTreeSzTrait
{
static
RedBlackTreeNode
<
extent_node_t
>
&
GetTreeNode
(
extent_node_t
*
aThis
)
{
return
aThis
-
>
mLinkBySize
;
}
static
inline
Order
Compare
(
extent_node_t
*
aNode
extent_node_t
*
aOther
)
{
Order
ret
=
CompareInt
(
aNode
-
>
mSize
aOther
-
>
mSize
)
;
return
(
ret
!
=
Order
:
:
eEqual
)
?
ret
:
CompareAddr
(
aNode
-
>
mAddr
aOther
-
>
mAddr
)
;
}
}
;
struct
ExtentTreeTrait
{
static
RedBlackTreeNode
<
extent_node_t
>
&
GetTreeNode
(
extent_node_t
*
aThis
)
{
return
aThis
-
>
mLinkByAddr
;
}
static
inline
Order
Compare
(
extent_node_t
*
aNode
extent_node_t
*
aOther
)
{
return
CompareAddr
(
aNode
-
>
mAddr
aOther
-
>
mAddr
)
;
}
}
;
struct
ExtentTreeBoundsTrait
:
public
ExtentTreeTrait
{
static
inline
Order
Compare
(
extent_node_t
*
aKey
extent_node_t
*
aNode
)
{
uintptr_t
key_addr
=
reinterpret_cast
<
uintptr_t
>
(
aKey
-
>
mAddr
)
;
uintptr_t
node_addr
=
reinterpret_cast
<
uintptr_t
>
(
aNode
-
>
mAddr
)
;
size_t
node_size
=
aNode
-
>
mSize
;
if
(
node_addr
<
=
key_addr
&
&
key_addr
<
node_addr
+
node_size
)
{
return
Order
:
:
eEqual
;
}
return
CompareAddr
(
aKey
-
>
mAddr
aNode
-
>
mAddr
)
;
}
}
;
class
SizeClass
{
public
:
enum
ClassType
{
Tiny
Quantum
QuantumWide
SubPage
Large
}
;
explicit
inline
SizeClass
(
size_t
aSize
)
{
if
(
aSize
<
=
kMaxTinyClass
)
{
mType
=
Tiny
;
mSize
=
std
:
:
max
(
RoundUpPow2
(
aSize
)
kMinTinyClass
)
;
}
else
if
(
aSize
<
=
kMaxQuantumClass
)
{
mType
=
Quantum
;
mSize
=
QUANTUM_CEILING
(
aSize
)
;
}
else
if
(
aSize
<
=
kMaxQuantumWideClass
)
{
mType
=
QuantumWide
;
mSize
=
QUANTUM_WIDE_CEILING
(
aSize
)
;
}
else
if
(
aSize
<
=
gMaxSubPageClass
)
{
mType
=
SubPage
;
mSize
=
SUBPAGE_CEILING
(
aSize
)
;
}
else
if
(
aSize
<
=
gMaxLargeClass
)
{
mType
=
Large
;
mSize
=
PAGE_CEILING
(
aSize
)
;
}
else
{
MOZ_MAKE_COMPILER_ASSUME_IS_UNREACHABLE
(
"
Invalid
size
"
)
;
}
}
SizeClass
&
operator
=
(
const
SizeClass
&
aOther
)
=
default
;
bool
operator
=
=
(
const
SizeClass
&
aOther
)
{
return
aOther
.
mSize
=
=
mSize
;
}
size_t
Size
(
)
{
return
mSize
;
}
ClassType
Type
(
)
{
return
mType
;
}
SizeClass
Next
(
)
{
return
SizeClass
(
mSize
+
1
)
;
}
private
:
ClassType
mType
;
size_t
mSize
;
}
;
template
<
size_t
Bits
>
class
AddressRadixTree
{
#
ifdef
HAVE_64BIT_BUILD
static
const
size_t
kNodeSize
=
kCacheLineSize
;
#
else
static
const
size_t
kNodeSize
=
16_KiB
;
#
endif
static
const
size_t
kBitsPerLevel
=
LOG2
(
kNodeSize
)
-
LOG2
(
sizeof
(
void
*
)
)
;
static
const
size_t
kBitsAtLevel1
=
(
Bits
%
kBitsPerLevel
)
?
Bits
%
kBitsPerLevel
:
kBitsPerLevel
;
static
const
size_t
kHeight
=
(
Bits
+
kBitsPerLevel
-
1
)
/
kBitsPerLevel
;
static_assert
(
kBitsAtLevel1
+
(
kHeight
-
1
)
*
kBitsPerLevel
=
=
Bits
"
AddressRadixTree
parameters
don
'
t
work
out
"
)
;
Mutex
mLock
MOZ_UNANNOTATED
;
void
*
*
mRoot
;
public
:
bool
Init
(
)
;
inline
void
*
Get
(
void
*
aAddr
)
;
inline
bool
Set
(
void
*
aAddr
void
*
aValue
)
;
inline
bool
Unset
(
void
*
aAddr
)
{
return
Set
(
aAddr
nullptr
)
;
}
private
:
inline
void
*
*
GetSlot
(
void
*
aAddr
bool
aCreate
=
false
)
;
}
;
struct
arena_bin_t
;
struct
ArenaChunkMapLink
{
static
RedBlackTreeNode
<
arena_chunk_map_t
>
&
GetTreeNode
(
arena_chunk_map_t
*
aThis
)
{
return
aThis
-
>
link
;
}
}
;
struct
ArenaRunTreeTrait
:
public
ArenaChunkMapLink
{
static
inline
Order
Compare
(
arena_chunk_map_t
*
aNode
arena_chunk_map_t
*
aOther
)
{
MOZ_ASSERT
(
aNode
)
;
MOZ_ASSERT
(
aOther
)
;
return
CompareAddr
(
aNode
aOther
)
;
}
}
;
struct
ArenaAvailTreeTrait
:
public
ArenaChunkMapLink
{
static
inline
Order
Compare
(
arena_chunk_map_t
*
aNode
arena_chunk_map_t
*
aOther
)
{
size_t
size1
=
aNode
-
>
bits
&
~
gPageSizeMask
;
size_t
size2
=
aOther
-
>
bits
&
~
gPageSizeMask
;
Order
ret
=
CompareInt
(
size1
size2
)
;
return
(
ret
!
=
Order
:
:
eEqual
)
?
ret
:
CompareAddr
(
(
aNode
-
>
bits
&
CHUNK_MAP_KEY
)
?
nullptr
:
aNode
aOther
)
;
}
}
;
struct
ArenaDirtyChunkTrait
{
static
RedBlackTreeNode
<
arena_chunk_t
>
&
GetTreeNode
(
arena_chunk_t
*
aThis
)
{
return
aThis
-
>
link_dirty
;
}
static
inline
Order
Compare
(
arena_chunk_t
*
aNode
arena_chunk_t
*
aOther
)
{
MOZ_ASSERT
(
aNode
)
;
MOZ_ASSERT
(
aOther
)
;
return
CompareAddr
(
aNode
aOther
)
;
}
}
;
#
ifdef
MALLOC_DOUBLE_PURGE
namespace
mozilla
{
template
<
>
struct
GetDoublyLinkedListElement
<
arena_chunk_t
>
{
static
DoublyLinkedListElement
<
arena_chunk_t
>
&
Get
(
arena_chunk_t
*
aThis
)
{
return
aThis
-
>
chunks_madvised_elem
;
}
}
;
}
#
endif
struct
arena_run_t
{
#
if
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
uint32_t
mMagic
;
#
define
ARENA_RUN_MAGIC
0x384adf93
unsigned
mNumFree
;
#
endif
arena_bin_t
*
mBin
;
unsigned
mRegionsMinElement
;
#
if
!
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
unsigned
mNumFree
;
#
endif
unsigned
mRegionsMask
[
1
]
;
}
;
struct
arena_bin_t
{
arena_run_t
*
mCurrentRun
;
RedBlackTree
<
arena_chunk_map_t
ArenaRunTreeTrait
>
mNonFullRuns
;
size_t
mSizeClass
;
size_t
mRunSize
;
uint32_t
mRunNumRegions
;
uint32_t
mRunNumRegionsMask
;
uint32_t
mRunFirstRegionOffset
;
unsigned
long
mNumRuns
;
static
constexpr
double
kRunOverhead
=
1
.
6_percent
;
static
constexpr
double
kRunRelaxedOverhead
=
2
.
4_percent
;
inline
void
Init
(
SizeClass
aSizeClass
)
;
}
;
struct
arena_t
{
#
if
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
uint32_t
mMagic
;
#
define
ARENA_MAGIC
0x947d3d24
#
endif
RedBlackTreeNode
<
arena_t
>
mLink
;
arena_id_t
mId
;
Mutex
mLock
MOZ_UNANNOTATED
;
arena_stats_t
mStats
;
private
:
RedBlackTree
<
arena_chunk_t
ArenaDirtyChunkTrait
>
mChunksDirty
;
#
ifdef
MALLOC_DOUBLE_PURGE
DoublyLinkedList
<
arena_chunk_t
>
mChunksMAdvised
;
#
endif
arena_chunk_t
*
mSpare
;
bool
mRandomizeSmallAllocations
;
bool
mIsPrivate
;
mozilla
:
:
non_crypto
:
:
XorShift128PlusRNG
*
mPRNG
;
public
:
size_t
mNumDirty
;
size_t
mMaxDirty
;
private
:
RedBlackTree
<
arena_chunk_map_t
ArenaAvailTreeTrait
>
mRunsAvail
;
public
:
arena_bin_t
mBins
[
1
]
;
explicit
arena_t
(
arena_params_t
*
aParams
bool
aIsPrivate
)
;
~
arena_t
(
)
;
private
:
void
InitChunk
(
arena_chunk_t
*
aChunk
bool
aZeroed
)
;
void
DeallocChunk
(
arena_chunk_t
*
aChunk
)
;
arena_run_t
*
AllocRun
(
size_t
aSize
bool
aLarge
bool
aZero
)
;
void
DallocRun
(
arena_run_t
*
aRun
bool
aDirty
)
;
[
[
nodiscard
]
]
bool
SplitRun
(
arena_run_t
*
aRun
size_t
aSize
bool
aLarge
bool
aZero
)
;
void
TrimRunHead
(
arena_chunk_t
*
aChunk
arena_run_t
*
aRun
size_t
aOldSize
size_t
aNewSize
)
;
void
TrimRunTail
(
arena_chunk_t
*
aChunk
arena_run_t
*
aRun
size_t
aOldSize
size_t
aNewSize
bool
dirty
)
;
arena_run_t
*
GetNonFullBinRun
(
arena_bin_t
*
aBin
)
;
inline
uint8_t
FindFreeBitInMask
(
uint32_t
aMask
uint32_t
&
aRng
)
;
inline
void
*
ArenaRunRegAlloc
(
arena_run_t
*
aRun
arena_bin_t
*
aBin
)
;
inline
void
*
MallocSmall
(
size_t
aSize
bool
aZero
)
;
void
*
MallocLarge
(
size_t
aSize
bool
aZero
)
;
void
*
MallocHuge
(
size_t
aSize
bool
aZero
)
;
void
*
PallocLarge
(
size_t
aAlignment
size_t
aSize
size_t
aAllocSize
)
;
void
*
PallocHuge
(
size_t
aSize
size_t
aAlignment
bool
aZero
)
;
void
RallocShrinkLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
;
bool
RallocGrowLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
;
void
*
RallocSmallOrLarge
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
;
void
*
RallocHuge
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
;
public
:
inline
void
*
Malloc
(
size_t
aSize
bool
aZero
)
;
void
*
Palloc
(
size_t
aAlignment
size_t
aSize
)
;
inline
void
DallocSmall
(
arena_chunk_t
*
aChunk
void
*
aPtr
arena_chunk_map_t
*
aMapElm
)
;
void
DallocLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
)
;
void
*
Ralloc
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
;
void
Purge
(
bool
aAll
)
;
void
HardPurge
(
)
;
void
*
operator
new
(
size_t
aCount
)
=
delete
;
void
*
operator
new
(
size_t
aCount
const
fallible_t
&
)
noexcept
;
void
operator
delete
(
void
*
)
;
}
;
struct
ArenaTreeTrait
{
static
RedBlackTreeNode
<
arena_t
>
&
GetTreeNode
(
arena_t
*
aThis
)
{
return
aThis
-
>
mLink
;
}
static
inline
Order
Compare
(
arena_t
*
aNode
arena_t
*
aOther
)
{
MOZ_ASSERT
(
aNode
)
;
MOZ_ASSERT
(
aOther
)
;
return
CompareInt
(
aNode
-
>
mId
aOther
-
>
mId
)
;
}
}
;
class
ArenaCollection
{
public
:
bool
Init
(
)
{
mArenas
.
Init
(
)
;
mPrivateArenas
.
Init
(
)
;
arena_params_t
params
;
params
.
mMaxDirty
=
opt_dirty_max
;
mDefaultArena
=
mLock
.
Init
(
)
?
CreateArena
(
false
&
params
)
:
nullptr
;
return
bool
(
mDefaultArena
)
;
}
inline
arena_t
*
GetById
(
arena_id_t
aArenaId
bool
aIsPrivate
)
;
arena_t
*
CreateArena
(
bool
aIsPrivate
arena_params_t
*
aParams
)
;
void
DisposeArena
(
arena_t
*
aArena
)
{
MutexAutoLock
lock
(
mLock
)
;
MOZ_RELEASE_ASSERT
(
mPrivateArenas
.
Search
(
aArena
)
"
Can
only
dispose
of
private
arenas
"
)
;
mPrivateArenas
.
Remove
(
aArena
)
;
delete
aArena
;
}
using
Tree
=
RedBlackTree
<
arena_t
ArenaTreeTrait
>
;
struct
Iterator
:
Tree
:
:
Iterator
{
explicit
Iterator
(
Tree
*
aTree
Tree
*
aSecondTree
)
:
Tree
:
:
Iterator
(
aTree
)
mNextTree
(
aSecondTree
)
{
}
Item
<
Iterator
>
begin
(
)
{
return
Item
<
Iterator
>
(
this
*
Tree
:
:
Iterator
:
:
begin
(
)
)
;
}
Item
<
Iterator
>
end
(
)
{
return
Item
<
Iterator
>
(
this
nullptr
)
;
}
arena_t
*
Next
(
)
{
arena_t
*
result
=
Tree
:
:
Iterator
:
:
Next
(
)
;
if
(
!
result
&
&
mNextTree
)
{
new
(
this
)
Iterator
(
mNextTree
nullptr
)
;
result
=
*
Tree
:
:
Iterator
:
:
begin
(
)
;
}
return
result
;
}
private
:
Tree
*
mNextTree
;
}
;
Iterator
iter
(
)
{
return
Iterator
(
&
mArenas
&
mPrivateArenas
)
;
}
inline
arena_t
*
GetDefault
(
)
{
return
mDefaultArena
;
}
Mutex
mLock
MOZ_UNANNOTATED
;
private
:
inline
arena_t
*
GetByIdInternal
(
arena_id_t
aArenaId
bool
aIsPrivate
)
;
arena_t
*
mDefaultArena
;
arena_id_t
mLastPublicArenaId
;
Tree
mArenas
;
Tree
mPrivateArenas
;
}
;
static
ArenaCollection
gArenas
;
static
AddressRadixTree
<
(
sizeof
(
void
*
)
<
<
3
)
-
LOG2
(
kChunkSize
)
>
gChunkRTree
;
static
Mutex
chunks_mtx
MOZ_UNANNOTATED
;
static
RedBlackTree
<
extent_node_t
ExtentTreeSzTrait
>
gChunksBySize
;
static
RedBlackTree
<
extent_node_t
ExtentTreeTrait
>
gChunksByAddress
;
static
Mutex
huge_mtx
MOZ_UNANNOTATED
;
static
RedBlackTree
<
extent_node_t
ExtentTreeTrait
>
huge
;
static
size_t
huge_allocated
;
static
size_t
huge_mapped
;
static
void
*
base_pages
;
static
void
*
base_next_addr
;
static
void
*
base_next_decommitted
;
static
void
*
base_past_addr
;
static
Mutex
base_mtx
MOZ_UNANNOTATED
;
static
size_t
base_mapped
;
static
size_t
base_committed
;
#
if
!
defined
(
XP_DARWIN
)
static
MOZ_THREAD_LOCAL
(
arena_t
*
)
thread_arena
;
#
else
static
detail
:
:
ThreadLocal
<
arena_t
*
detail
:
:
ThreadLocalKeyStorage
>
thread_arena
;
#
endif
const
uint8_t
kAllocJunk
=
0xe4
;
const
uint8_t
kAllocPoison
=
0xe5
;
#
ifdef
MOZ_DEBUG
static
bool
opt_junk
=
true
;
static
bool
opt_zero
=
false
;
#
else
static
const
bool
opt_junk
=
false
;
static
const
bool
opt_zero
=
false
;
#
endif
static
bool
opt_randomize_small
=
true
;
static
void
*
chunk_alloc
(
size_t
aSize
size_t
aAlignment
bool
aBase
bool
*
aZeroed
=
nullptr
)
;
static
void
chunk_dealloc
(
void
*
aChunk
size_t
aSize
ChunkType
aType
)
;
static
void
chunk_ensure_zero
(
void
*
aPtr
size_t
aSize
bool
aZeroed
)
;
static
void
huge_dalloc
(
void
*
aPtr
arena_t
*
aArena
)
;
static
bool
malloc_init_hard
(
)
;
#
ifndef
XP_WIN
#
ifdef
XP_DARWIN
#
define
FORK_HOOK
extern
"
C
"
#
else
#
define
FORK_HOOK
static
#
endif
FORK_HOOK
void
_malloc_prefork
(
void
)
;
FORK_HOOK
void
_malloc_postfork_parent
(
void
)
;
FORK_HOOK
void
_malloc_postfork_child
(
void
)
;
#
endif
static
inline
bool
malloc_init
(
)
{
if
(
malloc_initialized
=
=
false
)
{
return
malloc_init_hard
(
)
;
}
return
true
;
}
static
void
_malloc_message
(
const
char
*
p
)
{
#
if
!
defined
(
XP_WIN
)
#
define
_write
write
#
endif
if
(
_write
(
STDERR_FILENO
p
(
unsigned
int
)
strlen
(
p
)
)
<
0
)
{
return
;
}
}
template
<
typename
.
.
.
Args
>
static
void
_malloc_message
(
const
char
*
p
Args
.
.
.
args
)
{
_malloc_message
(
p
)
;
_malloc_message
(
args
.
.
.
)
;
}
#
ifdef
ANDROID
extern
"
C
"
MOZ_EXPORT
int
pthread_atfork
(
void
(
*
)
(
void
)
void
(
*
)
(
void
)
void
(
*
)
(
void
)
)
;
#
endif
static
inline
arena_chunk_t
*
GetChunkForPtr
(
const
void
*
aPtr
)
{
return
(
arena_chunk_t
*
)
(
uintptr_t
(
aPtr
)
&
~
kChunkSizeMask
)
;
}
static
inline
size_t
GetChunkOffsetForPtr
(
const
void
*
aPtr
)
{
return
(
size_t
)
(
uintptr_t
(
aPtr
)
&
kChunkSizeMask
)
;
}
static
inline
const
char
*
_getprogname
(
void
)
{
return
"
<
jemalloc
>
"
;
}
static
inline
void
ApplyZeroOrJunk
(
void
*
aPtr
size_t
aSize
)
{
if
(
opt_junk
)
{
memset
(
aPtr
kAllocJunk
aSize
)
;
}
else
if
(
opt_zero
)
{
memset
(
aPtr
0
aSize
)
;
}
}
static
inline
void
pages_decommit
(
void
*
aAddr
size_t
aSize
)
{
#
ifdef
XP_WIN
size_t
pages_size
=
std
:
:
min
(
aSize
kChunkSize
-
GetChunkOffsetForPtr
(
aAddr
)
)
;
while
(
aSize
>
0
)
{
if
(
!
VirtualFree
(
aAddr
pages_size
MEM_DECOMMIT
)
)
{
MOZ_CRASH
(
)
;
}
aAddr
=
(
void
*
)
(
(
uintptr_t
)
aAddr
+
pages_size
)
;
aSize
-
=
pages_size
;
pages_size
=
std
:
:
min
(
aSize
kChunkSize
)
;
}
#
else
if
(
mmap
(
aAddr
aSize
PROT_NONE
MAP_FIXED
|
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
=
=
MAP_FAILED
)
{
const
char
out_of_mappings
[
]
=
"
[
unhandlable
oom
]
Failed
to
mmap
likely
no
more
mappings
"
"
available
"
__FILE__
"
:
"
MOZ_STRINGIFY
(
__LINE__
)
;
if
(
errno
=
=
ENOMEM
)
{
#
ifndef
ANDROID
fputs
(
out_of_mappings
stderr
)
;
fflush
(
stderr
)
;
#
endif
MOZ_CRASH_ANNOTATE
(
out_of_mappings
)
;
}
MOZ_REALLY_CRASH
(
__LINE__
)
;
}
MozTagAnonymousMemory
(
aAddr
aSize
"
jemalloc
-
decommitted
"
)
;
#
endif
}
[
[
nodiscard
]
]
static
inline
bool
pages_commit
(
void
*
aAddr
size_t
aSize
)
{
#
ifdef
XP_WIN
size_t
pages_size
=
std
:
:
min
(
aSize
kChunkSize
-
GetChunkOffsetForPtr
(
aAddr
)
)
;
while
(
aSize
>
0
)
{
if
(
!
VirtualAlloc
(
aAddr
pages_size
MEM_COMMIT
PAGE_READWRITE
)
)
{
return
false
;
}
aAddr
=
(
void
*
)
(
(
uintptr_t
)
aAddr
+
pages_size
)
;
aSize
-
=
pages_size
;
pages_size
=
std
:
:
min
(
aSize
kChunkSize
)
;
}
#
else
if
(
mmap
(
aAddr
aSize
PROT_READ
|
PROT_WRITE
MAP_FIXED
|
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
=
=
MAP_FAILED
)
{
return
false
;
}
MozTagAnonymousMemory
(
aAddr
aSize
"
jemalloc
"
)
;
#
endif
return
true
;
}
static
bool
base_pages_alloc
(
size_t
minsize
)
{
size_t
csize
;
size_t
pminsize
;
MOZ_ASSERT
(
minsize
!
=
0
)
;
csize
=
CHUNK_CEILING
(
minsize
)
;
base_pages
=
chunk_alloc
(
csize
kChunkSize
true
)
;
if
(
!
base_pages
)
{
return
true
;
}
base_next_addr
=
base_pages
;
base_past_addr
=
(
void
*
)
(
(
uintptr_t
)
base_pages
+
csize
)
;
pminsize
=
PAGE_CEILING
(
minsize
)
;
base_next_decommitted
=
(
void
*
)
(
(
uintptr_t
)
base_pages
+
pminsize
)
;
if
(
pminsize
<
csize
)
{
pages_decommit
(
base_next_decommitted
csize
-
pminsize
)
;
}
base_mapped
+
=
csize
;
base_committed
+
=
pminsize
;
return
false
;
}
static
void
*
base_alloc
(
size_t
aSize
)
{
void
*
ret
;
size_t
csize
;
csize
=
CACHELINE_CEILING
(
aSize
)
;
MutexAutoLock
lock
(
base_mtx
)
;
if
(
(
uintptr_t
)
base_next_addr
+
csize
>
(
uintptr_t
)
base_past_addr
)
{
if
(
base_pages_alloc
(
csize
)
)
{
return
nullptr
;
}
}
ret
=
base_next_addr
;
base_next_addr
=
(
void
*
)
(
(
uintptr_t
)
base_next_addr
+
csize
)
;
if
(
(
uintptr_t
)
base_next_addr
>
(
uintptr_t
)
base_next_decommitted
)
{
void
*
pbase_next_addr
=
(
void
*
)
(
PAGE_CEILING
(
(
uintptr_t
)
base_next_addr
)
)
;
if
(
!
pages_commit
(
base_next_decommitted
(
uintptr_t
)
pbase_next_addr
-
(
uintptr_t
)
base_next_decommitted
)
)
{
return
nullptr
;
}
base_committed
+
=
(
uintptr_t
)
pbase_next_addr
-
(
uintptr_t
)
base_next_decommitted
;
base_next_decommitted
=
pbase_next_addr
;
}
return
ret
;
}
static
void
*
base_calloc
(
size_t
aNumber
size_t
aSize
)
{
void
*
ret
=
base_alloc
(
aNumber
*
aSize
)
;
if
(
ret
)
{
memset
(
ret
0
aNumber
*
aSize
)
;
}
return
ret
;
}
template
<
typename
T
>
struct
TypedBaseAlloc
{
static
T
*
sFirstFree
;
static
size_t
size_of
(
)
{
return
sizeof
(
T
)
;
}
static
T
*
alloc
(
)
{
T
*
ret
;
base_mtx
.
Lock
(
)
;
if
(
sFirstFree
)
{
ret
=
sFirstFree
;
sFirstFree
=
*
(
T
*
*
)
ret
;
base_mtx
.
Unlock
(
)
;
}
else
{
base_mtx
.
Unlock
(
)
;
ret
=
(
T
*
)
base_alloc
(
size_of
(
)
)
;
}
return
ret
;
}
static
void
dealloc
(
T
*
aNode
)
{
MutexAutoLock
lock
(
base_mtx
)
;
*
(
T
*
*
)
aNode
=
sFirstFree
;
sFirstFree
=
aNode
;
}
}
;
using
ExtentAlloc
=
TypedBaseAlloc
<
extent_node_t
>
;
template
<
>
extent_node_t
*
ExtentAlloc
:
:
sFirstFree
=
nullptr
;
template
<
>
arena_t
*
TypedBaseAlloc
<
arena_t
>
:
:
sFirstFree
=
nullptr
;
template
<
>
size_t
TypedBaseAlloc
<
arena_t
>
:
:
size_of
(
)
{
return
sizeof
(
arena_t
)
+
(
sizeof
(
arena_bin_t
)
*
(
NUM_SMALL_CLASSES
-
1
)
)
;
}
template
<
typename
T
>
struct
BaseAllocFreePolicy
{
void
operator
(
)
(
T
*
aPtr
)
{
TypedBaseAlloc
<
T
>
:
:
dealloc
(
aPtr
)
;
}
}
;
using
UniqueBaseNode
=
UniquePtr
<
extent_node_t
BaseAllocFreePolicy
<
extent_node_t
>
>
;
#
ifdef
XP_WIN
static
void
*
pages_map
(
void
*
aAddr
size_t
aSize
)
{
void
*
ret
=
nullptr
;
ret
=
VirtualAlloc
(
aAddr
aSize
MEM_COMMIT
|
MEM_RESERVE
PAGE_READWRITE
)
;
return
ret
;
}
static
void
pages_unmap
(
void
*
aAddr
size_t
aSize
)
{
if
(
VirtualFree
(
aAddr
0
MEM_RELEASE
)
=
=
0
)
{
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Error
in
VirtualFree
(
)
\
n
"
)
;
}
}
#
else
static
void
pages_unmap
(
void
*
aAddr
size_t
aSize
)
{
if
(
munmap
(
aAddr
aSize
)
=
=
-
1
)
{
char
buf
[
64
]
;
if
(
strerror_r
(
errno
buf
sizeof
(
buf
)
)
=
=
0
)
{
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Error
in
munmap
(
)
:
"
buf
"
\
n
"
)
;
}
}
}
static
void
*
pages_map
(
void
*
aAddr
size_t
aSize
)
{
void
*
ret
;
#
if
defined
(
__ia64__
)
|
|
\
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
)
bool
check_placement
=
true
;
if
(
!
aAddr
)
{
aAddr
=
(
void
*
)
0x0000070000000000
;
check_placement
=
false
;
}
#
endif
#
if
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
const
uintptr_t
start
=
0x0000070000000000ULL
;
const
uintptr_t
end
=
0x0000800000000000ULL
;
uintptr_t
hint
;
void
*
region
=
MAP_FAILED
;
for
(
hint
=
start
;
region
=
=
MAP_FAILED
&
&
hint
+
aSize
<
=
end
;
hint
+
=
kChunkSize
)
{
region
=
mmap
(
(
void
*
)
hint
aSize
PROT_READ
|
PROT_WRITE
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
;
if
(
region
!
=
MAP_FAILED
)
{
if
(
(
(
size_t
)
region
+
(
aSize
-
1
)
)
&
0xffff800000000000
)
{
if
(
munmap
(
region
aSize
)
)
{
MOZ_ASSERT
(
errno
=
=
ENOMEM
)
;
}
region
=
MAP_FAILED
;
}
}
}
ret
=
region
;
#
else
ret
=
mmap
(
aAddr
aSize
PROT_READ
|
PROT_WRITE
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
;
MOZ_ASSERT
(
ret
)
;
#
endif
if
(
ret
=
=
MAP_FAILED
)
{
ret
=
nullptr
;
}
#
if
defined
(
__ia64__
)
|
|
\
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
)
else
if
(
(
long
long
)
ret
&
0xffff800000000000
)
{
munmap
(
ret
aSize
)
;
ret
=
nullptr
;
}
else
if
(
check_placement
&
&
ret
!
=
aAddr
)
{
#
else
else
if
(
aAddr
&
&
ret
!
=
aAddr
)
{
#
endif
pages_unmap
(
ret
aSize
)
;
ret
=
nullptr
;
}
if
(
ret
)
{
MozTagAnonymousMemory
(
ret
aSize
"
jemalloc
"
)
;
}
#
if
defined
(
__ia64__
)
|
|
\
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
)
MOZ_ASSERT
(
!
ret
|
|
(
!
check_placement
&
&
ret
)
|
|
(
check_placement
&
&
ret
=
=
aAddr
)
)
;
#
else
MOZ_ASSERT
(
!
ret
|
|
(
!
aAddr
&
&
ret
!
=
aAddr
)
|
|
(
aAddr
&
&
ret
=
=
aAddr
)
)
;
#
endif
return
ret
;
}
#
endif
#
ifdef
XP_DARWIN
#
define
VM_COPY_MIN
kChunkSize
static
inline
void
pages_copy
(
void
*
dest
const
void
*
src
size_t
n
)
{
MOZ_ASSERT
(
(
void
*
)
(
(
uintptr_t
)
dest
&
~
gPageSizeMask
)
=
=
dest
)
;
MOZ_ASSERT
(
n
>
=
VM_COPY_MIN
)
;
MOZ_ASSERT
(
(
void
*
)
(
(
uintptr_t
)
src
&
~
gPageSizeMask
)
=
=
src
)
;
kern_return_t
r
=
vm_copy
(
mach_task_self
(
)
(
vm_address_t
)
src
(
vm_size_t
)
n
(
vm_address_t
)
dest
)
;
if
(
r
!
=
KERN_SUCCESS
)
{
MOZ_CRASH
(
"
vm_copy
(
)
failed
"
)
;
}
}
#
endif
template
<
size_t
Bits
>
bool
AddressRadixTree
<
Bits
>
:
:
Init
(
)
{
mLock
.
Init
(
)
;
mRoot
=
(
void
*
*
)
base_calloc
(
1
<
<
kBitsAtLevel1
sizeof
(
void
*
)
)
;
return
mRoot
;
}
template
<
size_t
Bits
>
void
*
*
AddressRadixTree
<
Bits
>
:
:
GetSlot
(
void
*
aKey
bool
aCreate
)
{
uintptr_t
key
=
reinterpret_cast
<
uintptr_t
>
(
aKey
)
;
uintptr_t
subkey
;
unsigned
i
lshift
height
bits
;
void
*
*
node
;
void
*
*
child
;
for
(
i
=
lshift
=
0
height
=
kHeight
node
=
mRoot
;
i
<
height
-
1
;
i
+
+
lshift
+
=
bits
node
=
child
)
{
bits
=
i
?
kBitsPerLevel
:
kBitsAtLevel1
;
subkey
=
(
key
<
<
lshift
)
>
>
(
(
sizeof
(
void
*
)
<
<
3
)
-
bits
)
;
child
=
(
void
*
*
)
node
[
subkey
]
;
if
(
!
child
&
&
aCreate
)
{
child
=
(
void
*
*
)
base_calloc
(
1
<
<
kBitsPerLevel
sizeof
(
void
*
)
)
;
if
(
child
)
{
node
[
subkey
]
=
child
;
}
}
if
(
!
child
)
{
return
nullptr
;
}
}
bits
=
i
?
kBitsPerLevel
:
kBitsAtLevel1
;
subkey
=
(
key
<
<
lshift
)
>
>
(
(
sizeof
(
void
*
)
<
<
3
)
-
bits
)
;
return
&
node
[
subkey
]
;
}
template
<
size_t
Bits
>
void
*
AddressRadixTree
<
Bits
>
:
:
Get
(
void
*
aKey
)
{
void
*
ret
=
nullptr
;
void
*
*
slot
=
GetSlot
(
aKey
)
;
if
(
slot
)
{
ret
=
*
slot
;
}
#
ifdef
MOZ_DEBUG
MutexAutoLock
lock
(
mLock
)
;
if
(
!
slot
)
{
slot
=
GetSlot
(
aKey
)
;
}
if
(
slot
)
{
MOZ_ASSERT
(
ret
=
=
*
slot
)
;
}
else
{
MOZ_ASSERT
(
ret
=
=
nullptr
)
;
}
#
endif
return
ret
;
}
template
<
size_t
Bits
>
bool
AddressRadixTree
<
Bits
>
:
:
Set
(
void
*
aKey
void
*
aValue
)
{
MutexAutoLock
lock
(
mLock
)
;
void
*
*
slot
=
GetSlot
(
aKey
true
)
;
if
(
slot
)
{
*
slot
=
aValue
;
}
return
slot
;
}
#
define
ALIGNMENT_ADDR2OFFSET
(
a
alignment
)
\
(
(
size_t
)
(
(
uintptr_t
)
(
a
)
&
(
(
alignment
)
-
1
)
)
)
#
define
ALIGNMENT_CEILING
(
s
alignment
)
\
(
(
(
s
)
+
(
(
alignment
)
-
1
)
)
&
(
~
(
(
alignment
)
-
1
)
)
)
static
void
*
pages_trim
(
void
*
addr
size_t
alloc_size
size_t
leadsize
size_t
size
)
{
void
*
ret
=
(
void
*
)
(
(
uintptr_t
)
addr
+
leadsize
)
;
MOZ_ASSERT
(
alloc_size
>
=
leadsize
+
size
)
;
#
ifdef
XP_WIN
{
void
*
new_addr
;
pages_unmap
(
addr
alloc_size
)
;
new_addr
=
pages_map
(
ret
size
)
;
if
(
new_addr
=
=
ret
)
{
return
ret
;
}
if
(
new_addr
)
{
pages_unmap
(
new_addr
size
)
;
}
return
nullptr
;
}
#
else
{
size_t
trailsize
=
alloc_size
-
leadsize
-
size
;
if
(
leadsize
!
=
0
)
{
pages_unmap
(
addr
leadsize
)
;
}
if
(
trailsize
!
=
0
)
{
pages_unmap
(
(
void
*
)
(
(
uintptr_t
)
ret
+
size
)
trailsize
)
;
}
return
ret
;
}
#
endif
}
static
void
*
chunk_alloc_mmap_slow
(
size_t
size
size_t
alignment
)
{
void
*
ret
*
pages
;
size_t
alloc_size
leadsize
;
alloc_size
=
size
+
alignment
-
gRealPageSize
;
if
(
alloc_size
<
size
)
{
return
nullptr
;
}
do
{
pages
=
pages_map
(
nullptr
alloc_size
)
;
if
(
!
pages
)
{
return
nullptr
;
}
leadsize
=
ALIGNMENT_CEILING
(
(
uintptr_t
)
pages
alignment
)
-
(
uintptr_t
)
pages
;
ret
=
pages_trim
(
pages
alloc_size
leadsize
size
)
;
}
while
(
!
ret
)
;
MOZ_ASSERT
(
ret
)
;
return
ret
;
}
static
void
*
chunk_alloc_mmap
(
size_t
size
size_t
alignment
)
{
void
*
ret
;
size_t
offset
;
ret
=
pages_map
(
nullptr
size
)
;
if
(
!
ret
)
{
return
nullptr
;
}
offset
=
ALIGNMENT_ADDR2OFFSET
(
ret
alignment
)
;
if
(
offset
!
=
0
)
{
pages_unmap
(
ret
size
)
;
return
chunk_alloc_mmap_slow
(
size
alignment
)
;
}
MOZ_ASSERT
(
ret
)
;
return
ret
;
}
static
bool
pages_purge
(
void
*
addr
size_t
length
bool
force_zero
)
{
pages_decommit
(
addr
length
)
;
return
true
;
}
static
void
*
chunk_recycle
(
size_t
aSize
size_t
aAlignment
bool
*
aZeroed
)
{
extent_node_t
key
;
size_t
alloc_size
=
aSize
+
aAlignment
-
kChunkSize
;
if
(
alloc_size
<
aSize
)
{
return
nullptr
;
}
key
.
mAddr
=
nullptr
;
key
.
mSize
=
alloc_size
;
chunks_mtx
.
Lock
(
)
;
extent_node_t
*
node
=
gChunksBySize
.
SearchOrNext
(
&
key
)
;
if
(
!
node
)
{
chunks_mtx
.
Unlock
(
)
;
return
nullptr
;
}
size_t
leadsize
=
ALIGNMENT_CEILING
(
(
uintptr_t
)
node
-
>
mAddr
aAlignment
)
-
(
uintptr_t
)
node
-
>
mAddr
;
MOZ_ASSERT
(
node
-
>
mSize
>
=
leadsize
+
aSize
)
;
size_t
trailsize
=
node
-
>
mSize
-
leadsize
-
aSize
;
void
*
ret
=
(
void
*
)
(
(
uintptr_t
)
node
-
>
mAddr
+
leadsize
)
;
ChunkType
chunk_type
=
node
-
>
mChunkType
;
if
(
aZeroed
)
{
*
aZeroed
=
(
chunk_type
=
=
ZEROED_CHUNK
)
;
}
gChunksBySize
.
Remove
(
node
)
;
gChunksByAddress
.
Remove
(
node
)
;
if
(
leadsize
!
=
0
)
{
node
-
>
mSize
=
leadsize
;
gChunksBySize
.
Insert
(
node
)
;
gChunksByAddress
.
Insert
(
node
)
;
node
=
nullptr
;
}
if
(
trailsize
!
=
0
)
{
if
(
!
node
)
{
chunks_mtx
.
Unlock
(
)
;
node
=
ExtentAlloc
:
:
alloc
(
)
;
if
(
!
node
)
{
chunk_dealloc
(
ret
aSize
chunk_type
)
;
return
nullptr
;
}
chunks_mtx
.
Lock
(
)
;
}
node
-
>
mAddr
=
(
void
*
)
(
(
uintptr_t
)
(
ret
)
+
aSize
)
;
node
-
>
mSize
=
trailsize
;
node
-
>
mChunkType
=
chunk_type
;
gChunksBySize
.
Insert
(
node
)
;
gChunksByAddress
.
Insert
(
node
)
;
node
=
nullptr
;
}
gRecycledSize
-
=
aSize
;
chunks_mtx
.
Unlock
(
)
;
if
(
node
)
{
ExtentAlloc
:
:
dealloc
(
node
)
;
}
if
(
!
pages_commit
(
ret
aSize
)
)
{
return
nullptr
;
}
if
(
aZeroed
)
{
*
aZeroed
=
true
;
}
return
ret
;
}
#
ifdef
XP_WIN
#
define
CAN_RECYCLE
(
size
)
(
(
size
)
=
=
kChunkSize
)
#
else
#
define
CAN_RECYCLE
(
size
)
true
#
endif
static
void
*
chunk_alloc
(
size_t
aSize
size_t
aAlignment
bool
aBase
bool
*
aZeroed
)
{
void
*
ret
=
nullptr
;
MOZ_ASSERT
(
aSize
!
=
0
)
;
MOZ_ASSERT
(
(
aSize
&
kChunkSizeMask
)
=
=
0
)
;
MOZ_ASSERT
(
aAlignment
!
=
0
)
;
MOZ_ASSERT
(
(
aAlignment
&
kChunkSizeMask
)
=
=
0
)
;
if
(
CAN_RECYCLE
(
aSize
)
&
&
!
aBase
)
{
ret
=
chunk_recycle
(
aSize
aAlignment
aZeroed
)
;
}
if
(
!
ret
)
{
ret
=
chunk_alloc_mmap
(
aSize
aAlignment
)
;
if
(
aZeroed
)
{
*
aZeroed
=
true
;
}
}
if
(
ret
&
&
!
aBase
)
{
if
(
!
gChunkRTree
.
Set
(
ret
ret
)
)
{
chunk_dealloc
(
ret
aSize
UNKNOWN_CHUNK
)
;
return
nullptr
;
}
}
MOZ_ASSERT
(
GetChunkOffsetForPtr
(
ret
)
=
=
0
)
;
return
ret
;
}
static
void
chunk_ensure_zero
(
void
*
aPtr
size_t
aSize
bool
aZeroed
)
{
if
(
aZeroed
=
=
false
)
{
memset
(
aPtr
0
aSize
)
;
}
#
ifdef
MOZ_DEBUG
else
{
size_t
i
;
size_t
*
p
=
(
size_t
*
)
(
uintptr_t
)
aPtr
;
for
(
i
=
0
;
i
<
aSize
/
sizeof
(
size_t
)
;
i
+
+
)
{
MOZ_ASSERT
(
p
[
i
]
=
=
0
)
;
}
}
#
endif
}
static
void
chunk_record
(
void
*
aChunk
size_t
aSize
ChunkType
aType
)
{
extent_node_t
key
;
if
(
aType
!
=
ZEROED_CHUNK
)
{
if
(
pages_purge
(
aChunk
aSize
aType
=
=
HUGE_CHUNK
)
)
{
aType
=
ZEROED_CHUNK
;
}
}
UniqueBaseNode
xnode
(
ExtentAlloc
:
:
alloc
(
)
)
;
UniqueBaseNode
xprev
;
MutexAutoLock
lock
(
chunks_mtx
)
;
key
.
mAddr
=
(
void
*
)
(
(
uintptr_t
)
aChunk
+
aSize
)
;
extent_node_t
*
node
=
gChunksByAddress
.
SearchOrNext
(
&
key
)
;
if
(
node
&
&
node
-
>
mAddr
=
=
key
.
mAddr
)
{
gChunksBySize
.
Remove
(
node
)
;
node
-
>
mAddr
=
aChunk
;
node
-
>
mSize
+
=
aSize
;
if
(
node
-
>
mChunkType
!
=
aType
)
{
node
-
>
mChunkType
=
RECYCLED_CHUNK
;
}
gChunksBySize
.
Insert
(
node
)
;
}
else
{
if
(
!
xnode
)
{
return
;
}
node
=
xnode
.
release
(
)
;
node
-
>
mAddr
=
aChunk
;
node
-
>
mSize
=
aSize
;
node
-
>
mChunkType
=
aType
;
gChunksByAddress
.
Insert
(
node
)
;
gChunksBySize
.
Insert
(
node
)
;
}
extent_node_t
*
prev
=
gChunksByAddress
.
Prev
(
node
)
;
if
(
prev
&
&
(
void
*
)
(
(
uintptr_t
)
prev
-
>
mAddr
+
prev
-
>
mSize
)
=
=
aChunk
)
{
gChunksBySize
.
Remove
(
prev
)
;
gChunksByAddress
.
Remove
(
prev
)
;
gChunksBySize
.
Remove
(
node
)
;
node
-
>
mAddr
=
prev
-
>
mAddr
;
node
-
>
mSize
+
=
prev
-
>
mSize
;
if
(
node
-
>
mChunkType
!
=
prev
-
>
mChunkType
)
{
node
-
>
mChunkType
=
RECYCLED_CHUNK
;
}
gChunksBySize
.
Insert
(
node
)
;
xprev
.
reset
(
prev
)
;
}
gRecycledSize
+
=
aSize
;
}
static
void
chunk_dealloc
(
void
*
aChunk
size_t
aSize
ChunkType
aType
)
{
MOZ_ASSERT
(
aChunk
)
;
MOZ_ASSERT
(
GetChunkOffsetForPtr
(
aChunk
)
=
=
0
)
;
MOZ_ASSERT
(
aSize
!
=
0
)
;
MOZ_ASSERT
(
(
aSize
&
kChunkSizeMask
)
=
=
0
)
;
gChunkRTree
.
Unset
(
aChunk
)
;
if
(
CAN_RECYCLE
(
aSize
)
)
{
size_t
recycled_so_far
=
gRecycledSize
;
if
(
recycled_so_far
<
gRecycleLimit
)
{
size_t
recycle_remaining
=
gRecycleLimit
-
recycled_so_far
;
size_t
to_recycle
;
if
(
aSize
>
recycle_remaining
)
{
to_recycle
=
recycle_remaining
;
pages_trim
(
aChunk
aSize
0
to_recycle
)
;
}
else
{
to_recycle
=
aSize
;
}
chunk_record
(
aChunk
to_recycle
aType
)
;
return
;
}
}
pages_unmap
(
aChunk
aSize
)
;
}
#
undef
CAN_RECYCLE
static
inline
arena_t
*
thread_local_arena
(
bool
enabled
)
{
arena_t
*
arena
;
if
(
enabled
)
{
arena
=
gArenas
.
CreateArena
(
false
nullptr
)
;
}
else
{
arena
=
gArenas
.
GetDefault
(
)
;
}
thread_arena
.
set
(
arena
)
;
return
arena
;
}
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_thread_local_arena
(
bool
aEnabled
)
{
if
(
malloc_init
(
)
)
{
thread_local_arena
(
aEnabled
)
;
}
}
static
inline
arena_t
*
choose_arena
(
size_t
size
)
{
arena_t
*
ret
=
nullptr
;
if
(
size
>
kMaxQuantumClass
)
{
ret
=
gArenas
.
GetDefault
(
)
;
}
else
{
ret
=
thread_arena
.
get
(
)
;
if
(
!
ret
)
{
ret
=
thread_local_arena
(
false
)
;
}
}
MOZ_DIAGNOSTIC_ASSERT
(
ret
)
;
return
ret
;
}
inline
uint8_t
arena_t
:
:
FindFreeBitInMask
(
uint32_t
aMask
uint32_t
&
aRng
)
{
if
(
mPRNG
!
=
nullptr
)
{
if
(
aRng
=
=
UINT_MAX
)
{
aRng
=
mPRNG
-
>
next
(
)
%
32
;
}
uint8_t
bitIndex
;
aMask
=
aRng
?
RotateRight
(
aMask
aRng
)
:
aMask
;
bitIndex
=
CountTrailingZeroes32
(
aMask
)
;
return
(
bitIndex
+
aRng
)
%
32
;
}
return
CountTrailingZeroes32
(
aMask
)
;
}
inline
void
*
arena_t
:
:
ArenaRunRegAlloc
(
arena_run_t
*
aRun
arena_bin_t
*
aBin
)
{
void
*
ret
;
unsigned
i
mask
bit
regind
;
uint32_t
rndPos
=
UINT_MAX
;
MOZ_DIAGNOSTIC_ASSERT
(
aRun
-
>
mMagic
=
=
ARENA_RUN_MAGIC
)
;
MOZ_ASSERT
(
aRun
-
>
mRegionsMinElement
<
aBin
-
>
mRunNumRegionsMask
)
;
i
=
aRun
-
>
mRegionsMinElement
;
mask
=
aRun
-
>
mRegionsMask
[
i
]
;
if
(
mask
!
=
0
)
{
bit
=
FindFreeBitInMask
(
mask
rndPos
)
;
regind
=
(
(
i
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
+
bit
)
;
MOZ_ASSERT
(
regind
<
aBin
-
>
mRunNumRegions
)
;
ret
=
(
void
*
)
(
(
(
uintptr_t
)
aRun
)
+
aBin
-
>
mRunFirstRegionOffset
+
(
aBin
-
>
mSizeClass
*
regind
)
)
;
mask
^
=
(
1U
<
<
bit
)
;
aRun
-
>
mRegionsMask
[
i
]
=
mask
;
return
ret
;
}
for
(
i
+
+
;
i
<
aBin
-
>
mRunNumRegionsMask
;
i
+
+
)
{
mask
=
aRun
-
>
mRegionsMask
[
i
]
;
if
(
mask
!
=
0
)
{
bit
=
FindFreeBitInMask
(
mask
rndPos
)
;
regind
=
(
(
i
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
+
bit
)
;
MOZ_ASSERT
(
regind
<
aBin
-
>
mRunNumRegions
)
;
ret
=
(
void
*
)
(
(
(
uintptr_t
)
aRun
)
+
aBin
-
>
mRunFirstRegionOffset
+
(
aBin
-
>
mSizeClass
*
regind
)
)
;
mask
^
=
(
1U
<
<
bit
)
;
aRun
-
>
mRegionsMask
[
i
]
=
mask
;
aRun
-
>
mRegionsMinElement
=
i
;
return
ret
;
}
}
MOZ_DIAGNOSTIC_ASSERT
(
0
)
;
return
nullptr
;
}
template
<
unsigned
Q
unsigned
Max
>
struct
FastDivide
{
static_assert
(
IsPowerOfTwo
(
Q
)
"
q
must
be
a
power
-
of
-
two
"
)
;
static
const
unsigned
min_divisor
=
Q
*
3
;
static
const
unsigned
max_divisor
=
mozilla
:
:
IsPowerOfTwo
(
Max
)
?
Max
-
Q
:
Max
;
static
const
unsigned
num_divisors
=
(
max_divisor
-
min_divisor
)
/
Q
+
1
;
static
const
unsigned
inv_shift
=
21
;
static
constexpr
unsigned
inv
(
unsigned
s
)
{
return
(
(
1U
<
<
inv_shift
)
/
(
s
*
Q
)
)
+
1
;
}
static
unsigned
divide
(
size_t
num
unsigned
div
)
{
static
const
unsigned
size_invs
[
]
=
{
inv
(
3
)
inv
(
4
)
inv
(
5
)
inv
(
6
)
inv
(
7
)
inv
(
8
)
inv
(
9
)
inv
(
10
)
inv
(
11
)
inv
(
12
)
inv
(
13
)
inv
(
14
)
inv
(
15
)
inv
(
16
)
inv
(
17
)
inv
(
18
)
inv
(
19
)
inv
(
20
)
inv
(
21
)
inv
(
22
)
inv
(
23
)
inv
(
24
)
inv
(
25
)
inv
(
26
)
inv
(
27
)
inv
(
28
)
inv
(
29
)
inv
(
30
)
inv
(
31
)
}
;
static_assert
(
!
(
min_divisor
<
max_divisor
)
|
|
num_divisors
<
=
sizeof
(
size_invs
)
/
sizeof
(
unsigned
)
"
num_divisors
does
not
match
array
size
"
)
;
MOZ_ASSERT
(
div
>
=
min_divisor
)
;
MOZ_ASSERT
(
div
<
=
max_divisor
)
;
MOZ_ASSERT
(
div
%
Q
=
=
0
)
;
const
unsigned
idx
=
div
/
Q
-
3
;
MOZ_ASSERT
(
idx
<
sizeof
(
size_invs
)
/
sizeof
(
unsigned
)
)
;
return
(
num
*
size_invs
[
idx
]
)
>
>
inv_shift
;
}
}
;
static
inline
void
arena_run_reg_dalloc
(
arena_run_t
*
run
arena_bin_t
*
bin
void
*
ptr
size_t
size
)
{
unsigned
diff
regind
elm
bit
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
mMagic
=
=
ARENA_RUN_MAGIC
)
;
diff
=
(
unsigned
)
(
(
uintptr_t
)
ptr
-
(
uintptr_t
)
run
-
bin
-
>
mRunFirstRegionOffset
)
;
if
(
mozilla
:
:
IsPowerOfTwo
(
size
)
)
{
regind
=
diff
>
>
FloorLog2
(
size
)
;
}
else
{
SizeClass
sc
(
size
)
;
switch
(
sc
.
Type
(
)
)
{
case
SizeClass
:
:
Quantum
:
regind
=
FastDivide
<
kQuantum
kMaxQuantumClass
>
:
:
divide
(
diff
size
)
;
break
;
case
SizeClass
:
:
QuantumWide
:
regind
=
FastDivide
<
kQuantumWide
kMaxQuantumWideClass
>
:
:
divide
(
diff
size
)
;
break
;
default
:
regind
=
diff
/
size
;
}
}
MOZ_DIAGNOSTIC_ASSERT
(
diff
=
=
regind
*
size
)
;
MOZ_DIAGNOSTIC_ASSERT
(
regind
<
bin
-
>
mRunNumRegions
)
;
elm
=
regind
>
>
(
LOG2
(
sizeof
(
int
)
)
+
3
)
;
if
(
elm
<
run
-
>
mRegionsMinElement
)
{
run
-
>
mRegionsMinElement
=
elm
;
}
bit
=
regind
-
(
elm
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
;
MOZ_RELEASE_ASSERT
(
(
run
-
>
mRegionsMask
[
elm
]
&
(
1U
<
<
bit
)
)
=
=
0
"
Double
-
free
?
"
)
;
run
-
>
mRegionsMask
[
elm
]
|
=
(
1U
<
<
bit
)
;
}
bool
arena_t
:
:
SplitRun
(
arena_run_t
*
aRun
size_t
aSize
bool
aLarge
bool
aZero
)
{
arena_chunk_t
*
chunk
;
size_t
old_ndirty
run_ind
total_pages
need_pages
rem_pages
i
;
chunk
=
GetChunkForPtr
(
aRun
)
;
old_ndirty
=
chunk
-
>
ndirty
;
run_ind
=
(
unsigned
)
(
(
uintptr_t
(
aRun
)
-
uintptr_t
(
chunk
)
)
>
>
gPageSize2Pow
)
;
total_pages
=
(
chunk
-
>
map
[
run_ind
]
.
bits
&
~
gPageSizeMask
)
>
>
gPageSize2Pow
;
need_pages
=
(
aSize
>
>
gPageSize2Pow
)
;
MOZ_ASSERT
(
need_pages
>
0
)
;
MOZ_ASSERT
(
need_pages
<
=
total_pages
)
;
rem_pages
=
total_pages
-
need_pages
;
for
(
i
=
0
;
i
<
need_pages
;
i
+
+
)
{
if
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_MADVISED_OR_DECOMMITTED
)
{
size_t
j
;
for
(
j
=
0
;
i
+
j
<
need_pages
&
&
(
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
CHUNK_MAP_MADVISED_OR_DECOMMITTED
)
;
j
+
+
)
{
MOZ_ASSERT
(
!
(
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
CHUNK_MAP_DECOMMITTED
&
&
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
CHUNK_MAP_MADVISED
)
)
;
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
=
~
CHUNK_MAP_MADVISED_OR_DECOMMITTED
;
}
#
ifdef
MALLOC_DECOMMIT
bool
committed
=
pages_commit
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
(
run_ind
+
i
)
<
<
gPageSize2Pow
)
)
j
<
<
gPageSize2Pow
)
;
for
(
size_t
k
=
0
;
k
<
j
;
k
+
+
)
{
chunk
-
>
map
[
run_ind
+
i
+
k
]
.
bits
|
=
committed
?
CHUNK_MAP_ZEROED
:
CHUNK_MAP_DECOMMITTED
;
}
if
(
!
committed
)
{
return
false
;
}
#
endif
mStats
.
committed
+
=
j
;
}
}
mRunsAvail
.
Remove
(
&
chunk
-
>
map
[
run_ind
]
)
;
if
(
rem_pages
>
0
)
{
chunk
-
>
map
[
run_ind
+
need_pages
]
.
bits
=
(
rem_pages
<
<
gPageSize2Pow
)
|
(
chunk
-
>
map
[
run_ind
+
need_pages
]
.
bits
&
gPageSizeMask
)
;
chunk
-
>
map
[
run_ind
+
total_pages
-
1
]
.
bits
=
(
rem_pages
<
<
gPageSize2Pow
)
|
(
chunk
-
>
map
[
run_ind
+
total_pages
-
1
]
.
bits
&
gPageSizeMask
)
;
mRunsAvail
.
Insert
(
&
chunk
-
>
map
[
run_ind
+
need_pages
]
)
;
}
for
(
i
=
0
;
i
<
need_pages
;
i
+
+
)
{
if
(
aZero
)
{
if
(
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_ZEROED
)
=
=
0
)
{
memset
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
(
run_ind
+
i
)
<
<
gPageSize2Pow
)
)
0
gPageSize
)
;
}
}
if
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_DIRTY
)
{
chunk
-
>
ndirty
-
-
;
mNumDirty
-
-
;
}
if
(
aLarge
)
{
chunk
-
>
map
[
run_ind
+
i
]
.
bits
=
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
}
else
{
chunk
-
>
map
[
run_ind
+
i
]
.
bits
=
size_t
(
aRun
)
|
CHUNK_MAP_ALLOCATED
;
}
}
if
(
aLarge
)
{
chunk
-
>
map
[
run_ind
]
.
bits
|
=
aSize
;
}
if
(
chunk
-
>
ndirty
=
=
0
&
&
old_ndirty
>
0
)
{
mChunksDirty
.
Remove
(
chunk
)
;
}
return
true
;
}
void
arena_t
:
:
InitChunk
(
arena_chunk_t
*
aChunk
bool
aZeroed
)
{
size_t
i
;
size_t
flags
=
aZeroed
?
CHUNK_MAP_DECOMMITTED
|
CHUNK_MAP_ZEROED
:
CHUNK_MAP_MADVISED
;
mStats
.
mapped
+
=
kChunkSize
;
aChunk
-
>
arena
=
this
;
aChunk
-
>
ndirty
=
0
;
arena_run_t
*
run
=
(
arena_run_t
*
)
(
uintptr_t
(
aChunk
)
+
(
gChunkHeaderNumPages
<
<
gPageSize2Pow
)
)
;
for
(
i
=
0
;
i
<
gChunkHeaderNumPages
-
1
;
i
+
+
)
{
aChunk
-
>
map
[
i
]
.
bits
=
0
;
}
aChunk
-
>
map
[
i
+
+
]
.
bits
=
CHUNK_MAP_DECOMMITTED
;
aChunk
-
>
map
[
i
+
+
]
.
bits
=
gMaxLargeClass
|
flags
;
for
(
;
i
<
gChunkNumPages
-
2
;
i
+
+
)
{
aChunk
-
>
map
[
i
]
.
bits
=
flags
;
}
aChunk
-
>
map
[
gChunkNumPages
-
2
]
.
bits
=
gMaxLargeClass
|
flags
;
aChunk
-
>
map
[
gChunkNumPages
-
1
]
.
bits
=
CHUNK_MAP_DECOMMITTED
;
#
ifdef
MALLOC_DECOMMIT
pages_decommit
(
(
void
*
)
(
uintptr_t
(
run
)
-
gPageSize
)
gMaxLargeClass
+
2
*
gPageSize
)
;
#
else
pages_decommit
(
(
void
*
)
(
uintptr_t
(
run
)
-
gPageSize
)
gPageSize
)
;
pages_decommit
(
(
void
*
)
(
uintptr_t
(
aChunk
)
+
kChunkSize
-
gPageSize
)
gPageSize
)
;
#
endif
mStats
.
committed
+
=
gChunkHeaderNumPages
;
mRunsAvail
.
Insert
(
&
aChunk
-
>
map
[
gChunkHeaderNumPages
]
)
;
#
ifdef
MALLOC_DOUBLE_PURGE
new
(
&
aChunk
-
>
chunks_madvised_elem
)
DoublyLinkedListElement
<
arena_chunk_t
>
(
)
;
#
endif
}
void
arena_t
:
:
DeallocChunk
(
arena_chunk_t
*
aChunk
)
{
if
(
mSpare
)
{
if
(
mSpare
-
>
ndirty
>
0
)
{
aChunk
-
>
arena
-
>
mChunksDirty
.
Remove
(
mSpare
)
;
mNumDirty
-
=
mSpare
-
>
ndirty
;
mStats
.
committed
-
=
mSpare
-
>
ndirty
;
}
#
ifdef
MALLOC_DOUBLE_PURGE
if
(
mChunksMAdvised
.
ElementProbablyInList
(
mSpare
)
)
{
mChunksMAdvised
.
remove
(
mSpare
)
;
}
#
endif
chunk_dealloc
(
(
void
*
)
mSpare
kChunkSize
ARENA_CHUNK
)
;
mStats
.
mapped
-
=
kChunkSize
;
mStats
.
committed
-
=
gChunkHeaderNumPages
;
}
mRunsAvail
.
Remove
(
&
aChunk
-
>
map
[
gChunkHeaderNumPages
]
)
;
mSpare
=
aChunk
;
}
arena_run_t
*
arena_t
:
:
AllocRun
(
size_t
aSize
bool
aLarge
bool
aZero
)
{
arena_run_t
*
run
;
arena_chunk_map_t
*
mapelm
;
arena_chunk_map_t
key
;
MOZ_ASSERT
(
aSize
<
=
gMaxLargeClass
)
;
MOZ_ASSERT
(
(
aSize
&
gPageSizeMask
)
=
=
0
)
;
key
.
bits
=
aSize
|
CHUNK_MAP_KEY
;
mapelm
=
mRunsAvail
.
SearchOrNext
(
&
key
)
;
if
(
mapelm
)
{
arena_chunk_t
*
chunk
=
GetChunkForPtr
(
mapelm
)
;
size_t
pageind
=
(
uintptr_t
(
mapelm
)
-
uintptr_t
(
chunk
-
>
map
)
)
/
sizeof
(
arena_chunk_map_t
)
;
run
=
(
arena_run_t
*
)
(
uintptr_t
(
chunk
)
+
(
pageind
<
<
gPageSize2Pow
)
)
;
}
else
if
(
mSpare
)
{
arena_chunk_t
*
chunk
=
mSpare
;
mSpare
=
nullptr
;
run
=
(
arena_run_t
*
)
(
uintptr_t
(
chunk
)
+
(
gChunkHeaderNumPages
<
<
gPageSize2Pow
)
)
;
mRunsAvail
.
Insert
(
&
chunk
-
>
map
[
gChunkHeaderNumPages
]
)
;
}
else
{
bool
zeroed
;
arena_chunk_t
*
chunk
=
(
arena_chunk_t
*
)
chunk_alloc
(
kChunkSize
kChunkSize
false
&
zeroed
)
;
if
(
!
chunk
)
{
return
nullptr
;
}
InitChunk
(
chunk
zeroed
)
;
run
=
(
arena_run_t
*
)
(
uintptr_t
(
chunk
)
+
(
gChunkHeaderNumPages
<
<
gPageSize2Pow
)
)
;
}
return
SplitRun
(
run
aSize
aLarge
aZero
)
?
run
:
nullptr
;
}
void
arena_t
:
:
Purge
(
bool
aAll
)
{
arena_chunk_t
*
chunk
;
size_t
i
npages
;
size_t
dirty_max
=
aAll
?
1
:
mMaxDirty
;
#
ifdef
MOZ_DEBUG
size_t
ndirty
=
0
;
for
(
auto
chunk
:
mChunksDirty
.
iter
(
)
)
{
ndirty
+
=
chunk
-
>
ndirty
;
}
MOZ_ASSERT
(
ndirty
=
=
mNumDirty
)
;
#
endif
MOZ_DIAGNOSTIC_ASSERT
(
aAll
|
|
(
mNumDirty
>
mMaxDirty
)
)
;
while
(
mNumDirty
>
(
dirty_max
>
>
1
)
)
{
#
ifdef
MALLOC_DOUBLE_PURGE
bool
madvised
=
false
;
#
endif
chunk
=
mChunksDirty
.
Last
(
)
;
MOZ_DIAGNOSTIC_ASSERT
(
chunk
)
;
MOZ_ASSERT
(
(
chunk
-
>
map
[
gChunkNumPages
-
1
]
.
bits
&
CHUNK_MAP_DECOMMITTED
)
!
=
0
)
;
for
(
i
=
gChunkNumPages
-
2
;
chunk
-
>
ndirty
>
0
;
i
-
-
)
{
MOZ_DIAGNOSTIC_ASSERT
(
i
>
=
gChunkHeaderNumPages
)
;
if
(
chunk
-
>
map
[
i
]
.
bits
&
CHUNK_MAP_DIRTY
)
{
#
ifdef
MALLOC_DECOMMIT
const
size_t
free_operation
=
CHUNK_MAP_DECOMMITTED
;
#
else
const
size_t
free_operation
=
CHUNK_MAP_MADVISED
;
#
endif
MOZ_ASSERT
(
(
chunk
-
>
map
[
i
]
.
bits
&
CHUNK_MAP_MADVISED_OR_DECOMMITTED
)
=
=
0
)
;
chunk
-
>
map
[
i
]
.
bits
^
=
free_operation
|
CHUNK_MAP_DIRTY
;
for
(
npages
=
1
;
i
>
gChunkHeaderNumPages
&
&
(
chunk
-
>
map
[
i
-
1
]
.
bits
&
CHUNK_MAP_DIRTY
)
;
npages
+
+
)
{
i
-
-
;
MOZ_ASSERT
(
(
chunk
-
>
map
[
i
]
.
bits
&
CHUNK_MAP_MADVISED_OR_DECOMMITTED
)
=
=
0
)
;
chunk
-
>
map
[
i
]
.
bits
^
=
free_operation
|
CHUNK_MAP_DIRTY
;
}
chunk
-
>
ndirty
-
=
npages
;
mNumDirty
-
=
npages
;
#
ifdef
MALLOC_DECOMMIT
pages_decommit
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
i
<
<
gPageSize2Pow
)
)
(
npages
<
<
gPageSize2Pow
)
)
;
#
endif
mStats
.
committed
-
=
npages
;
#
ifndef
MALLOC_DECOMMIT
#
ifdef
XP_SOLARIS
posix_madvise
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
i
<
<
gPageSize2Pow
)
)
(
npages
<
<
gPageSize2Pow
)
MADV_FREE
)
;
#
else
madvise
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
i
<
<
gPageSize2Pow
)
)
(
npages
<
<
gPageSize2Pow
)
MADV_FREE
)
;
#
endif
#
ifdef
MALLOC_DOUBLE_PURGE
madvised
=
true
;
#
endif
#
endif
if
(
mNumDirty
<
=
(
dirty_max
>
>
1
)
)
{
break
;
}
}
}
if
(
chunk
-
>
ndirty
=
=
0
)
{
mChunksDirty
.
Remove
(
chunk
)
;
}
#
ifdef
MALLOC_DOUBLE_PURGE
if
(
madvised
)
{
if
(
mChunksMAdvised
.
ElementProbablyInList
(
chunk
)
)
{
mChunksMAdvised
.
remove
(
chunk
)
;
}
mChunksMAdvised
.
pushFront
(
chunk
)
;
}
#
endif
}
}
void
arena_t
:
:
DallocRun
(
arena_run_t
*
aRun
bool
aDirty
)
{
arena_chunk_t
*
chunk
;
size_t
size
run_ind
run_pages
;
chunk
=
GetChunkForPtr
(
aRun
)
;
run_ind
=
(
size_t
)
(
(
uintptr_t
(
aRun
)
-
uintptr_t
(
chunk
)
)
>
>
gPageSize2Pow
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run_ind
>
=
gChunkHeaderNumPages
)
;
MOZ_RELEASE_ASSERT
(
run_ind
<
gChunkNumPages
-
1
)
;
if
(
(
chunk
-
>
map
[
run_ind
]
.
bits
&
CHUNK_MAP_LARGE
)
!
=
0
)
{
size
=
chunk
-
>
map
[
run_ind
]
.
bits
&
~
gPageSizeMask
;
}
else
{
size
=
aRun
-
>
mBin
-
>
mRunSize
;
}
run_pages
=
(
size
>
>
gPageSize2Pow
)
;
if
(
aDirty
)
{
size_t
i
;
for
(
i
=
0
;
i
<
run_pages
;
i
+
+
)
{
MOZ_DIAGNOSTIC_ASSERT
(
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_DIRTY
)
=
=
0
)
;
chunk
-
>
map
[
run_ind
+
i
]
.
bits
=
CHUNK_MAP_DIRTY
;
}
if
(
chunk
-
>
ndirty
=
=
0
)
{
mChunksDirty
.
Insert
(
chunk
)
;
}
chunk
-
>
ndirty
+
=
run_pages
;
mNumDirty
+
=
run_pages
;
}
else
{
size_t
i
;
for
(
i
=
0
;
i
<
run_pages
;
i
+
+
)
{
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
=
~
(
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
)
;
}
}
chunk
-
>
map
[
run_ind
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
]
.
bits
&
gPageSizeMask
)
;
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
gPageSizeMask
)
;
if
(
run_ind
+
run_pages
<
gChunkNumPages
-
1
&
&
(
chunk
-
>
map
[
run_ind
+
run_pages
]
.
bits
&
CHUNK_MAP_ALLOCATED
)
=
=
0
)
{
size_t
nrun_size
=
chunk
-
>
map
[
run_ind
+
run_pages
]
.
bits
&
~
gPageSizeMask
;
mRunsAvail
.
Remove
(
&
chunk
-
>
map
[
run_ind
+
run_pages
]
)
;
size
+
=
nrun_size
;
run_pages
=
size
>
>
gPageSize2Pow
;
MOZ_DIAGNOSTIC_ASSERT
(
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
~
gPageSizeMask
)
=
=
nrun_size
)
;
chunk
-
>
map
[
run_ind
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
]
.
bits
&
gPageSizeMask
)
;
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
gPageSizeMask
)
;
}
if
(
run_ind
>
gChunkHeaderNumPages
&
&
(
chunk
-
>
map
[
run_ind
-
1
]
.
bits
&
CHUNK_MAP_ALLOCATED
)
=
=
0
)
{
size_t
prun_size
=
chunk
-
>
map
[
run_ind
-
1
]
.
bits
&
~
gPageSizeMask
;
run_ind
-
=
prun_size
>
>
gPageSize2Pow
;
mRunsAvail
.
Remove
(
&
chunk
-
>
map
[
run_ind
]
)
;
size
+
=
prun_size
;
run_pages
=
size
>
>
gPageSize2Pow
;
MOZ_DIAGNOSTIC_ASSERT
(
(
chunk
-
>
map
[
run_ind
]
.
bits
&
~
gPageSizeMask
)
=
=
prun_size
)
;
chunk
-
>
map
[
run_ind
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
]
.
bits
&
gPageSizeMask
)
;
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
gPageSizeMask
)
;
}
mRunsAvail
.
Insert
(
&
chunk
-
>
map
[
run_ind
]
)
;
if
(
(
chunk
-
>
map
[
gChunkHeaderNumPages
]
.
bits
&
(
~
gPageSizeMask
|
CHUNK_MAP_ALLOCATED
)
)
=
=
gMaxLargeClass
)
{
DeallocChunk
(
chunk
)
;
}
if
(
mNumDirty
>
mMaxDirty
)
{
Purge
(
false
)
;
}
}
void
arena_t
:
:
TrimRunHead
(
arena_chunk_t
*
aChunk
arena_run_t
*
aRun
size_t
aOldSize
size_t
aNewSize
)
{
size_t
pageind
=
(
uintptr_t
(
aRun
)
-
uintptr_t
(
aChunk
)
)
>
>
gPageSize2Pow
;
size_t
head_npages
=
(
aOldSize
-
aNewSize
)
>
>
gPageSize2Pow
;
MOZ_ASSERT
(
aOldSize
>
aNewSize
)
;
aChunk
-
>
map
[
pageind
]
.
bits
=
(
aOldSize
-
aNewSize
)
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
aChunk
-
>
map
[
pageind
+
head_npages
]
.
bits
=
aNewSize
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
DallocRun
(
aRun
false
)
;
}
void
arena_t
:
:
TrimRunTail
(
arena_chunk_t
*
aChunk
arena_run_t
*
aRun
size_t
aOldSize
size_t
aNewSize
bool
aDirty
)
{
size_t
pageind
=
(
uintptr_t
(
aRun
)
-
uintptr_t
(
aChunk
)
)
>
>
gPageSize2Pow
;
size_t
npages
=
aNewSize
>
>
gPageSize2Pow
;
MOZ_ASSERT
(
aOldSize
>
aNewSize
)
;
aChunk
-
>
map
[
pageind
]
.
bits
=
aNewSize
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
aChunk
-
>
map
[
pageind
+
npages
]
.
bits
=
(
aOldSize
-
aNewSize
)
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
DallocRun
(
(
arena_run_t
*
)
(
uintptr_t
(
aRun
)
+
aNewSize
)
aDirty
)
;
}
arena_run_t
*
arena_t
:
:
GetNonFullBinRun
(
arena_bin_t
*
aBin
)
{
arena_chunk_map_t
*
mapelm
;
arena_run_t
*
run
;
unsigned
i
remainder
;
mapelm
=
aBin
-
>
mNonFullRuns
.
First
(
)
;
if
(
mapelm
)
{
aBin
-
>
mNonFullRuns
.
Remove
(
mapelm
)
;
run
=
(
arena_run_t
*
)
(
mapelm
-
>
bits
&
~
gPageSizeMask
)
;
return
run
;
}
run
=
AllocRun
(
aBin
-
>
mRunSize
false
false
)
;
if
(
!
run
)
{
return
nullptr
;
}
if
(
run
=
=
aBin
-
>
mCurrentRun
)
{
return
run
;
}
run
-
>
mBin
=
aBin
;
for
(
i
=
0
;
i
<
aBin
-
>
mRunNumRegionsMask
-
1
;
i
+
+
)
{
run
-
>
mRegionsMask
[
i
]
=
UINT_MAX
;
}
remainder
=
aBin
-
>
mRunNumRegions
&
(
(
1U
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
-
1
)
;
if
(
remainder
=
=
0
)
{
run
-
>
mRegionsMask
[
i
]
=
UINT_MAX
;
}
else
{
run
-
>
mRegionsMask
[
i
]
=
(
UINT_MAX
>
>
(
(
1U
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
-
remainder
)
)
;
}
run
-
>
mRegionsMinElement
=
0
;
run
-
>
mNumFree
=
aBin
-
>
mRunNumRegions
;
#
if
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
run
-
>
mMagic
=
ARENA_RUN_MAGIC
;
#
endif
aBin
-
>
mNumRuns
+
+
;
return
run
;
}
void
arena_bin_t
:
:
Init
(
SizeClass
aSizeClass
)
{
size_t
try_run_size
;
unsigned
try_nregs
try_mask_nelms
try_reg0_offset
;
static
const
size_t
kFixedHeaderSize
=
offsetof
(
arena_run_t
mRegionsMask
)
;
MOZ_ASSERT
(
aSizeClass
.
Size
(
)
<
=
gMaxBinClass
)
;
try_run_size
=
gPageSize
;
mCurrentRun
=
nullptr
;
mNonFullRuns
.
Init
(
)
;
mSizeClass
=
aSizeClass
.
Size
(
)
;
mNumRuns
=
0
;
while
(
true
)
{
try_nregs
=
(
(
try_run_size
-
kFixedHeaderSize
)
/
mSizeClass
)
+
1
;
do
{
try_nregs
-
-
;
try_mask_nelms
=
(
try_nregs
>
>
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
+
(
(
try_nregs
&
(
(
1U
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
-
1
)
)
?
1
:
0
)
;
try_reg0_offset
=
try_run_size
-
(
try_nregs
*
mSizeClass
)
;
}
while
(
kFixedHeaderSize
+
(
sizeof
(
unsigned
)
*
try_mask_nelms
)
>
try_reg0_offset
)
;
if
(
Fraction
(
try_reg0_offset
try_run_size
)
<
=
kRunOverhead
)
{
break
;
}
if
(
try_reg0_offset
>
mSizeClass
)
{
if
(
Fraction
(
try_reg0_offset
try_run_size
)
<
=
kRunRelaxedOverhead
)
{
break
;
}
}
if
(
try_mask_nelms
*
sizeof
(
unsigned
)
>
=
kFixedHeaderSize
)
{
break
;
}
if
(
try_run_size
+
gPageSize
>
gMaxLargeClass
)
{
break
;
}
try_run_size
+
=
gPageSize
;
}
MOZ_ASSERT
(
kFixedHeaderSize
+
(
sizeof
(
unsigned
)
*
try_mask_nelms
)
<
=
try_reg0_offset
)
;
MOZ_ASSERT
(
(
try_mask_nelms
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
>
=
try_nregs
)
;
mRunSize
=
try_run_size
;
mRunNumRegions
=
try_nregs
;
mRunNumRegionsMask
=
try_mask_nelms
;
mRunFirstRegionOffset
=
try_reg0_offset
;
}
void
*
arena_t
:
:
MallocSmall
(
size_t
aSize
bool
aZero
)
{
void
*
ret
;
arena_bin_t
*
bin
;
arena_run_t
*
run
;
SizeClass
sizeClass
(
aSize
)
;
aSize
=
sizeClass
.
Size
(
)
;
switch
(
sizeClass
.
Type
(
)
)
{
case
SizeClass
:
:
Tiny
:
bin
=
&
mBins
[
FloorLog2
(
aSize
/
kMinTinyClass
)
]
;
break
;
case
SizeClass
:
:
Quantum
:
bin
=
&
mBins
[
kNumTinyClasses
+
(
aSize
/
kQuantum
)
-
(
kMinQuantumClass
/
kQuantum
)
]
;
break
;
case
SizeClass
:
:
QuantumWide
:
bin
=
&
mBins
[
kNumTinyClasses
+
kNumQuantumClasses
+
(
aSize
/
kQuantumWide
)
-
(
kMinQuantumWideClass
/
kQuantumWide
)
]
;
break
;
case
SizeClass
:
:
SubPage
:
bin
=
&
mBins
[
kNumTinyClasses
+
kNumQuantumClasses
+
kNumQuantumWideClasses
+
(
FloorLog2
(
aSize
)
-
LOG2
(
kMinSubPageClass
)
)
]
;
break
;
default
:
MOZ_MAKE_COMPILER_ASSUME_IS_UNREACHABLE
(
"
Unexpected
size
class
type
"
)
;
}
MOZ_DIAGNOSTIC_ASSERT
(
aSize
=
=
bin
-
>
mSizeClass
)
;
{
if
(
MOZ_UNLIKELY
(
mRandomizeSmallAllocations
&
&
mPRNG
=
=
nullptr
)
)
{
mRandomizeSmallAllocations
=
false
;
mozilla
:
:
Maybe
<
uint64_t
>
prngState1
=
mozilla
:
:
RandomUint64
(
)
;
mozilla
:
:
Maybe
<
uint64_t
>
prngState2
=
mozilla
:
:
RandomUint64
(
)
;
void
*
backing
=
base_alloc
(
sizeof
(
mozilla
:
:
non_crypto
:
:
XorShift128PlusRNG
)
)
;
mPRNG
=
new
(
backing
)
mozilla
:
:
non_crypto
:
:
XorShift128PlusRNG
(
prngState1
.
valueOr
(
0
)
prngState2
.
valueOr
(
0
)
)
;
mRandomizeSmallAllocations
=
true
;
}
MOZ_ASSERT
(
!
mRandomizeSmallAllocations
|
|
mPRNG
)
;
MutexAutoLock
lock
(
mLock
)
;
run
=
bin
-
>
mCurrentRun
;
if
(
MOZ_UNLIKELY
(
!
run
|
|
run
-
>
mNumFree
=
=
0
)
)
{
run
=
bin
-
>
mCurrentRun
=
GetNonFullBinRun
(
bin
)
;
}
if
(
MOZ_UNLIKELY
(
!
run
)
)
{
return
nullptr
;
}
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
mMagic
=
=
ARENA_RUN_MAGIC
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
mNumFree
>
0
)
;
ret
=
ArenaRunRegAlloc
(
run
bin
)
;
MOZ_DIAGNOSTIC_ASSERT
(
ret
)
;
run
-
>
mNumFree
-
-
;
if
(
!
ret
)
{
return
nullptr
;
}
mStats
.
allocated_small
+
=
aSize
;
}
if
(
!
aZero
)
{
ApplyZeroOrJunk
(
ret
aSize
)
;
}
else
{
memset
(
ret
0
aSize
)
;
}
return
ret
;
}
void
*
arena_t
:
:
MallocLarge
(
size_t
aSize
bool
aZero
)
{
void
*
ret
;
aSize
=
PAGE_CEILING
(
aSize
)
;
{
MutexAutoLock
lock
(
mLock
)
;
ret
=
AllocRun
(
aSize
true
aZero
)
;
if
(
!
ret
)
{
return
nullptr
;
}
mStats
.
allocated_large
+
=
aSize
;
}
if
(
!
aZero
)
{
ApplyZeroOrJunk
(
ret
aSize
)
;
}
return
ret
;
}
void
*
arena_t
:
:
Malloc
(
size_t
aSize
bool
aZero
)
{
MOZ_DIAGNOSTIC_ASSERT
(
mMagic
=
=
ARENA_MAGIC
)
;
MOZ_ASSERT
(
aSize
!
=
0
)
;
if
(
aSize
<
=
gMaxBinClass
)
{
return
MallocSmall
(
aSize
aZero
)
;
}
if
(
aSize
<
=
gMaxLargeClass
)
{
return
MallocLarge
(
aSize
aZero
)
;
}
return
MallocHuge
(
aSize
aZero
)
;
}
void
*
arena_t
:
:
PallocLarge
(
size_t
aAlignment
size_t
aSize
size_t
aAllocSize
)
{
void
*
ret
;
size_t
offset
;
arena_chunk_t
*
chunk
;
MOZ_ASSERT
(
(
aSize
&
gPageSizeMask
)
=
=
0
)
;
MOZ_ASSERT
(
(
aAlignment
&
gPageSizeMask
)
=
=
0
)
;
{
MutexAutoLock
lock
(
mLock
)
;
ret
=
AllocRun
(
aAllocSize
true
false
)
;
if
(
!
ret
)
{
return
nullptr
;
}
chunk
=
GetChunkForPtr
(
ret
)
;
offset
=
uintptr_t
(
ret
)
&
(
aAlignment
-
1
)
;
MOZ_ASSERT
(
(
offset
&
gPageSizeMask
)
=
=
0
)
;
MOZ_ASSERT
(
offset
<
aAllocSize
)
;
if
(
offset
=
=
0
)
{
TrimRunTail
(
chunk
(
arena_run_t
*
)
ret
aAllocSize
aSize
false
)
;
}
else
{
size_t
leadsize
trailsize
;
leadsize
=
aAlignment
-
offset
;
if
(
leadsize
>
0
)
{
TrimRunHead
(
chunk
(
arena_run_t
*
)
ret
aAllocSize
aAllocSize
-
leadsize
)
;
ret
=
(
void
*
)
(
uintptr_t
(
ret
)
+
leadsize
)
;
}
trailsize
=
aAllocSize
-
leadsize
-
aSize
;
if
(
trailsize
!
=
0
)
{
MOZ_ASSERT
(
trailsize
<
aAllocSize
)
;
TrimRunTail
(
chunk
(
arena_run_t
*
)
ret
aSize
+
trailsize
aSize
false
)
;
}
}
mStats
.
allocated_large
+
=
aSize
;
}
ApplyZeroOrJunk
(
ret
aSize
)
;
return
ret
;
}
void
*
arena_t
:
:
Palloc
(
size_t
aAlignment
size_t
aSize
)
{
void
*
ret
;
size_t
ceil_size
;
ceil_size
=
ALIGNMENT_CEILING
(
aSize
aAlignment
)
;
if
(
ceil_size
<
aSize
)
{
return
nullptr
;
}
if
(
ceil_size
<
=
gPageSize
|
|
(
aAlignment
<
=
gPageSize
&
&
ceil_size
<
=
gMaxLargeClass
)
)
{
ret
=
Malloc
(
ceil_size
false
)
;
}
else
{
size_t
run_size
;
aAlignment
=
PAGE_CEILING
(
aAlignment
)
;
ceil_size
=
PAGE_CEILING
(
aSize
)
;
if
(
ceil_size
<
aSize
|
|
ceil_size
+
aAlignment
<
ceil_size
)
{
return
nullptr
;
}
if
(
ceil_size
>
=
aAlignment
)
{
run_size
=
ceil_size
+
aAlignment
-
gPageSize
;
}
else
{
run_size
=
(
aAlignment
<
<
1
)
-
gPageSize
;
}
if
(
run_size
<
=
gMaxLargeClass
)
{
ret
=
PallocLarge
(
aAlignment
ceil_size
run_size
)
;
}
else
if
(
aAlignment
<
=
kChunkSize
)
{
ret
=
MallocHuge
(
ceil_size
false
)
;
}
else
{
ret
=
PallocHuge
(
ceil_size
aAlignment
false
)
;
}
}
MOZ_ASSERT
(
(
uintptr_t
(
ret
)
&
(
aAlignment
-
1
)
)
=
=
0
)
;
return
ret
;
}
class
AllocInfo
{
public
:
template
<
bool
Validate
=
false
>
static
inline
AllocInfo
Get
(
const
void
*
aPtr
)
{
if
(
Validate
&
&
malloc_initialized
=
=
false
)
{
return
AllocInfo
(
)
;
}
auto
chunk
=
GetChunkForPtr
(
aPtr
)
;
if
(
Validate
)
{
if
(
!
chunk
|
|
!
gChunkRTree
.
Get
(
chunk
)
)
{
return
AllocInfo
(
)
;
}
}
if
(
chunk
!
=
aPtr
)
{
MOZ_DIAGNOSTIC_ASSERT
(
chunk
-
>
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
size_t
pageind
=
(
(
(
uintptr_t
)
aPtr
-
(
uintptr_t
)
chunk
)
>
>
gPageSize2Pow
)
;
size_t
mapbits
=
chunk
-
>
map
[
pageind
]
.
bits
;
MOZ_DIAGNOSTIC_ASSERT
(
(
mapbits
&
CHUNK_MAP_ALLOCATED
)
!
=
0
)
;
size_t
size
;
if
(
(
mapbits
&
CHUNK_MAP_LARGE
)
=
=
0
)
{
arena_run_t
*
run
=
(
arena_run_t
*
)
(
mapbits
&
~
gPageSizeMask
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
mMagic
=
=
ARENA_RUN_MAGIC
)
;
size
=
run
-
>
mBin
-
>
mSizeClass
;
}
else
{
size
=
mapbits
&
~
gPageSizeMask
;
MOZ_DIAGNOSTIC_ASSERT
(
size
!
=
0
)
;
}
return
AllocInfo
(
size
chunk
)
;
}
extent_node_t
key
;
key
.
mAddr
=
chunk
;
MutexAutoLock
lock
(
huge_mtx
)
;
extent_node_t
*
node
=
huge
.
Search
(
&
key
)
;
if
(
Validate
&
&
!
node
)
{
return
AllocInfo
(
)
;
}
return
AllocInfo
(
node
-
>
mSize
node
)
;
}
static
inline
AllocInfo
GetValidated
(
const
void
*
aPtr
)
{
return
Get
<
true
>
(
aPtr
)
;
}
AllocInfo
(
)
:
mSize
(
0
)
mChunk
(
nullptr
)
{
}
explicit
AllocInfo
(
size_t
aSize
arena_chunk_t
*
aChunk
)
:
mSize
(
aSize
)
mChunk
(
aChunk
)
{
MOZ_ASSERT
(
mSize
<
=
gMaxLargeClass
)
;
}
explicit
AllocInfo
(
size_t
aSize
extent_node_t
*
aNode
)
:
mSize
(
aSize
)
mNode
(
aNode
)
{
MOZ_ASSERT
(
mSize
>
gMaxLargeClass
)
;
}
size_t
Size
(
)
{
return
mSize
;
}
arena_t
*
Arena
(
)
{
if
(
mSize
<
=
gMaxLargeClass
)
{
return
mChunk
-
>
arena
;
}
MOZ_RELEASE_ASSERT
(
mNode
-
>
mArenaId
=
=
mNode
-
>
mArena
-
>
mId
)
;
return
mNode
-
>
mArena
;
}
private
:
size_t
mSize
;
union
{
arena_chunk_t
*
mChunk
;
extent_node_t
*
mNode
;
}
;
}
;
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_ptr_info
(
const
void
*
aPtr
jemalloc_ptr_info_t
*
aInfo
)
{
arena_chunk_t
*
chunk
=
GetChunkForPtr
(
aPtr
)
;
if
(
!
chunk
|
|
!
malloc_initialized
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
return
;
}
extent_node_t
*
node
;
extent_node_t
key
;
{
MutexAutoLock
lock
(
huge_mtx
)
;
key
.
mAddr
=
const_cast
<
void
*
>
(
aPtr
)
;
node
=
reinterpret_cast
<
RedBlackTree
<
extent_node_t
ExtentTreeBoundsTrait
>
*
>
(
&
huge
)
-
>
Search
(
&
key
)
;
if
(
node
)
{
*
aInfo
=
{
TagLiveAlloc
node
-
>
mAddr
node
-
>
mSize
node
-
>
mArena
-
>
mId
}
;
return
;
}
}
if
(
!
gChunkRTree
.
Get
(
chunk
)
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
return
;
}
MOZ_DIAGNOSTIC_ASSERT
(
chunk
-
>
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
size_t
pageind
=
(
(
(
uintptr_t
)
aPtr
-
(
uintptr_t
)
chunk
)
>
>
gPageSize2Pow
)
;
if
(
pageind
<
gChunkHeaderNumPages
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
return
;
}
size_t
mapbits
=
chunk
-
>
map
[
pageind
]
.
bits
;
if
(
!
(
mapbits
&
CHUNK_MAP_ALLOCATED
)
)
{
void
*
pageaddr
=
(
void
*
)
(
uintptr_t
(
aPtr
)
&
~
gPageSizeMask
)
;
*
aInfo
=
{
TagFreedPage
pageaddr
gPageSize
chunk
-
>
arena
-
>
mId
}
;
return
;
}
if
(
mapbits
&
CHUNK_MAP_LARGE
)
{
size_t
size
;
while
(
true
)
{
size
=
mapbits
&
~
gPageSizeMask
;
if
(
size
!
=
0
)
{
break
;
}
pageind
-
-
;
MOZ_DIAGNOSTIC_ASSERT
(
pageind
>
=
gChunkHeaderNumPages
)
;
if
(
pageind
<
gChunkHeaderNumPages
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
return
;
}
mapbits
=
chunk
-
>
map
[
pageind
]
.
bits
;
MOZ_DIAGNOSTIC_ASSERT
(
mapbits
&
CHUNK_MAP_LARGE
)
;
if
(
!
(
mapbits
&
CHUNK_MAP_LARGE
)
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
return
;
}
}
void
*
addr
=
(
(
char
*
)
chunk
)
+
(
pageind
<
<
gPageSize2Pow
)
;
*
aInfo
=
{
TagLiveAlloc
addr
size
chunk
-
>
arena
-
>
mId
}
;
return
;
}
auto
run
=
(
arena_run_t
*
)
(
mapbits
&
~
gPageSizeMask
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
mMagic
=
=
ARENA_RUN_MAGIC
)
;
size_t
size
=
run
-
>
mBin
-
>
mSizeClass
;
uintptr_t
reg0_addr
=
(
uintptr_t
)
run
+
run
-
>
mBin
-
>
mRunFirstRegionOffset
;
if
(
aPtr
<
(
void
*
)
reg0_addr
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
0
}
;
return
;
}
unsigned
regind
=
(
(
uintptr_t
)
aPtr
-
reg0_addr
)
/
size
;
void
*
addr
=
(
void
*
)
(
reg0_addr
+
regind
*
size
)
;
unsigned
elm
=
regind
>
>
(
LOG2
(
sizeof
(
int
)
)
+
3
)
;
unsigned
bit
=
regind
-
(
elm
<
<
(
LOG2
(
sizeof
(
int
)
)
+
3
)
)
;
PtrInfoTag
tag
=
(
(
run
-
>
mRegionsMask
[
elm
]
&
(
1U
<
<
bit
)
)
)
?
TagFreedAlloc
:
TagLiveAlloc
;
*
aInfo
=
{
tag
addr
size
chunk
-
>
arena
-
>
mId
}
;
}
namespace
Debug
{
MOZ_NEVER_INLINE
jemalloc_ptr_info_t
*
jemalloc_ptr_info
(
const
void
*
aPtr
)
{
static
jemalloc_ptr_info_t
info
;
MozJemalloc
:
:
jemalloc_ptr_info
(
aPtr
&
info
)
;
return
&
info
;
}
}
void
arena_t
:
:
DallocSmall
(
arena_chunk_t
*
aChunk
void
*
aPtr
arena_chunk_map_t
*
aMapElm
)
{
arena_run_t
*
run
;
arena_bin_t
*
bin
;
size_t
size
;
run
=
(
arena_run_t
*
)
(
aMapElm
-
>
bits
&
~
gPageSizeMask
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
mMagic
=
=
ARENA_RUN_MAGIC
)
;
bin
=
run
-
>
mBin
;
size
=
bin
-
>
mSizeClass
;
MOZ_DIAGNOSTIC_ASSERT
(
uintptr_t
(
aPtr
)
>
=
uintptr_t
(
run
)
+
bin
-
>
mRunFirstRegionOffset
)
;
memset
(
aPtr
kAllocPoison
size
)
;
arena_run_reg_dalloc
(
run
bin
aPtr
size
)
;
run
-
>
mNumFree
+
+
;
if
(
run
-
>
mNumFree
=
=
bin
-
>
mRunNumRegions
)
{
if
(
run
=
=
bin
-
>
mCurrentRun
)
{
bin
-
>
mCurrentRun
=
nullptr
;
}
else
if
(
bin
-
>
mRunNumRegions
!
=
1
)
{
size_t
run_pageind
=
(
uintptr_t
(
run
)
-
uintptr_t
(
aChunk
)
)
>
>
gPageSize2Pow
;
arena_chunk_map_t
*
run_mapelm
=
&
aChunk
-
>
map
[
run_pageind
]
;
MOZ_DIAGNOSTIC_ASSERT
(
bin
-
>
mNonFullRuns
.
Search
(
run_mapelm
)
=
=
run_mapelm
)
;
bin
-
>
mNonFullRuns
.
Remove
(
run_mapelm
)
;
}
#
if
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
run
-
>
mMagic
=
0
;
#
endif
DallocRun
(
run
true
)
;
bin
-
>
mNumRuns
-
-
;
}
else
if
(
run
-
>
mNumFree
=
=
1
&
&
run
!
=
bin
-
>
mCurrentRun
)
{
if
(
!
bin
-
>
mCurrentRun
)
{
bin
-
>
mCurrentRun
=
run
;
}
else
if
(
uintptr_t
(
run
)
<
uintptr_t
(
bin
-
>
mCurrentRun
)
)
{
if
(
bin
-
>
mCurrentRun
-
>
mNumFree
>
0
)
{
arena_chunk_t
*
runcur_chunk
=
GetChunkForPtr
(
bin
-
>
mCurrentRun
)
;
size_t
runcur_pageind
=
(
uintptr_t
(
bin
-
>
mCurrentRun
)
-
uintptr_t
(
runcur_chunk
)
)
>
>
gPageSize2Pow
;
arena_chunk_map_t
*
runcur_mapelm
=
&
runcur_chunk
-
>
map
[
runcur_pageind
]
;
MOZ_DIAGNOSTIC_ASSERT
(
!
bin
-
>
mNonFullRuns
.
Search
(
runcur_mapelm
)
)
;
bin
-
>
mNonFullRuns
.
Insert
(
runcur_mapelm
)
;
}
bin
-
>
mCurrentRun
=
run
;
}
else
{
size_t
run_pageind
=
(
uintptr_t
(
run
)
-
uintptr_t
(
aChunk
)
)
>
>
gPageSize2Pow
;
arena_chunk_map_t
*
run_mapelm
=
&
aChunk
-
>
map
[
run_pageind
]
;
MOZ_DIAGNOSTIC_ASSERT
(
bin
-
>
mNonFullRuns
.
Search
(
run_mapelm
)
=
=
nullptr
)
;
bin
-
>
mNonFullRuns
.
Insert
(
run_mapelm
)
;
}
}
mStats
.
allocated_small
-
=
size
;
}
void
arena_t
:
:
DallocLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
)
{
MOZ_DIAGNOSTIC_ASSERT
(
(
uintptr_t
(
aPtr
)
&
gPageSizeMask
)
=
=
0
)
;
size_t
pageind
=
(
uintptr_t
(
aPtr
)
-
uintptr_t
(
aChunk
)
)
>
>
gPageSize2Pow
;
size_t
size
=
aChunk
-
>
map
[
pageind
]
.
bits
&
~
gPageSizeMask
;
memset
(
aPtr
kAllocPoison
size
)
;
mStats
.
allocated_large
-
=
size
;
DallocRun
(
(
arena_run_t
*
)
aPtr
true
)
;
}
static
inline
void
arena_dalloc
(
void
*
aPtr
size_t
aOffset
arena_t
*
aArena
)
{
MOZ_ASSERT
(
aPtr
)
;
MOZ_ASSERT
(
aOffset
!
=
0
)
;
MOZ_ASSERT
(
GetChunkOffsetForPtr
(
aPtr
)
=
=
aOffset
)
;
auto
chunk
=
(
arena_chunk_t
*
)
(
(
uintptr_t
)
aPtr
-
aOffset
)
;
auto
arena
=
chunk
-
>
arena
;
MOZ_ASSERT
(
arena
)
;
MOZ_DIAGNOSTIC_ASSERT
(
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
MOZ_RELEASE_ASSERT
(
!
aArena
|
|
arena
=
=
aArena
)
;
MutexAutoLock
lock
(
arena
-
>
mLock
)
;
size_t
pageind
=
aOffset
>
>
gPageSize2Pow
;
arena_chunk_map_t
*
mapelm
=
&
chunk
-
>
map
[
pageind
]
;
MOZ_RELEASE_ASSERT
(
(
mapelm
-
>
bits
&
CHUNK_MAP_DECOMMITTED
)
=
=
0
"
Freeing
in
decommitted
page
.
"
)
;
MOZ_RELEASE_ASSERT
(
(
mapelm
-
>
bits
&
CHUNK_MAP_ALLOCATED
)
!
=
0
"
Double
-
free
?
"
)
;
if
(
(
mapelm
-
>
bits
&
CHUNK_MAP_LARGE
)
=
=
0
)
{
arena
-
>
DallocSmall
(
chunk
aPtr
mapelm
)
;
}
else
{
arena
-
>
DallocLarge
(
chunk
aPtr
)
;
}
}
static
inline
void
idalloc
(
void
*
ptr
arena_t
*
aArena
)
{
size_t
offset
;
MOZ_ASSERT
(
ptr
)
;
offset
=
GetChunkOffsetForPtr
(
ptr
)
;
if
(
offset
!
=
0
)
{
arena_dalloc
(
ptr
offset
aArena
)
;
}
else
{
huge_dalloc
(
ptr
aArena
)
;
}
}
void
arena_t
:
:
RallocShrinkLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
{
MOZ_ASSERT
(
aSize
<
aOldSize
)
;
MutexAutoLock
lock
(
mLock
)
;
TrimRunTail
(
aChunk
(
arena_run_t
*
)
aPtr
aOldSize
aSize
true
)
;
mStats
.
allocated_large
-
=
aOldSize
-
aSize
;
}
bool
arena_t
:
:
RallocGrowLarge
(
arena_chunk_t
*
aChunk
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
{
size_t
pageind
=
(
uintptr_t
(
aPtr
)
-
uintptr_t
(
aChunk
)
)
>
>
gPageSize2Pow
;
size_t
npages
=
aOldSize
>
>
gPageSize2Pow
;
MutexAutoLock
lock
(
mLock
)
;
MOZ_DIAGNOSTIC_ASSERT
(
aOldSize
=
=
(
aChunk
-
>
map
[
pageind
]
.
bits
&
~
gPageSizeMask
)
)
;
MOZ_ASSERT
(
aSize
>
aOldSize
)
;
if
(
pageind
+
npages
<
gChunkNumPages
-
1
&
&
(
aChunk
-
>
map
[
pageind
+
npages
]
.
bits
&
CHUNK_MAP_ALLOCATED
)
=
=
0
&
&
(
aChunk
-
>
map
[
pageind
+
npages
]
.
bits
&
~
gPageSizeMask
)
>
=
aSize
-
aOldSize
)
{
if
(
!
SplitRun
(
(
arena_run_t
*
)
(
uintptr_t
(
aChunk
)
+
(
(
pageind
+
npages
)
<
<
gPageSize2Pow
)
)
aSize
-
aOldSize
true
false
)
)
{
return
false
;
}
aChunk
-
>
map
[
pageind
]
.
bits
=
aSize
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
aChunk
-
>
map
[
pageind
+
npages
]
.
bits
=
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
mStats
.
allocated_large
+
=
aSize
-
aOldSize
;
return
true
;
}
return
false
;
}
void
*
arena_t
:
:
RallocSmallOrLarge
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
{
void
*
ret
;
size_t
copysize
;
SizeClass
sizeClass
(
aSize
)
;
if
(
aOldSize
<
=
gMaxLargeClass
&
&
sizeClass
.
Size
(
)
=
=
aOldSize
)
{
if
(
aSize
<
aOldSize
)
{
memset
(
(
void
*
)
(
uintptr_t
(
aPtr
)
+
aSize
)
kAllocPoison
aOldSize
-
aSize
)
;
}
return
aPtr
;
}
if
(
sizeClass
.
Type
(
)
=
=
SizeClass
:
:
Large
&
&
aOldSize
>
gMaxBinClass
&
&
aOldSize
<
=
gMaxLargeClass
)
{
arena_chunk_t
*
chunk
=
GetChunkForPtr
(
aPtr
)
;
if
(
sizeClass
.
Size
(
)
<
aOldSize
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
aSize
)
kAllocPoison
aOldSize
-
aSize
)
;
RallocShrinkLarge
(
chunk
aPtr
sizeClass
.
Size
(
)
aOldSize
)
;
return
aPtr
;
}
if
(
RallocGrowLarge
(
chunk
aPtr
sizeClass
.
Size
(
)
aOldSize
)
)
{
ApplyZeroOrJunk
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
aOldSize
)
aSize
-
aOldSize
)
;
return
aPtr
;
}
}
ret
=
(
mIsPrivate
?
this
:
choose_arena
(
aSize
)
)
-
>
Malloc
(
aSize
false
)
;
if
(
!
ret
)
{
return
nullptr
;
}
copysize
=
(
aSize
<
aOldSize
)
?
aSize
:
aOldSize
;
#
ifdef
VM_COPY_MIN
if
(
copysize
>
=
VM_COPY_MIN
)
{
pages_copy
(
ret
aPtr
copysize
)
;
}
else
#
endif
{
memcpy
(
ret
aPtr
copysize
)
;
}
idalloc
(
aPtr
this
)
;
return
ret
;
}
void
*
arena_t
:
:
Ralloc
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
{
MOZ_DIAGNOSTIC_ASSERT
(
mMagic
=
=
ARENA_MAGIC
)
;
MOZ_ASSERT
(
aPtr
)
;
MOZ_ASSERT
(
aSize
!
=
0
)
;
return
(
aSize
<
=
gMaxLargeClass
)
?
RallocSmallOrLarge
(
aPtr
aSize
aOldSize
)
:
RallocHuge
(
aPtr
aSize
aOldSize
)
;
}
void
*
arena_t
:
:
operator
new
(
size_t
aCount
const
fallible_t
&
)
noexcept
{
MOZ_ASSERT
(
aCount
=
=
sizeof
(
arena_t
)
)
;
return
TypedBaseAlloc
<
arena_t
>
:
:
alloc
(
)
;
}
void
arena_t
:
:
operator
delete
(
void
*
aPtr
)
{
TypedBaseAlloc
<
arena_t
>
:
:
dealloc
(
(
arena_t
*
)
aPtr
)
;
}
arena_t
:
:
arena_t
(
arena_params_t
*
aParams
bool
aIsPrivate
)
{
unsigned
i
;
MOZ_RELEASE_ASSERT
(
mLock
.
Init
(
)
)
;
memset
(
&
mLink
0
sizeof
(
mLink
)
)
;
memset
(
&
mStats
0
sizeof
(
arena_stats_t
)
)
;
mId
=
0
;
mChunksDirty
.
Init
(
)
;
#
ifdef
MALLOC_DOUBLE_PURGE
new
(
&
mChunksMAdvised
)
DoublyLinkedList
<
arena_chunk_t
>
(
)
;
#
endif
mSpare
=
nullptr
;
mRandomizeSmallAllocations
=
opt_randomize_small
;
if
(
aParams
)
{
uint32_t
flags
=
aParams
-
>
mFlags
&
ARENA_FLAG_RANDOMIZE_SMALL_MASK
;
switch
(
flags
)
{
case
ARENA_FLAG_RANDOMIZE_SMALL_ENABLED
:
mRandomizeSmallAllocations
=
true
;
break
;
case
ARENA_FLAG_RANDOMIZE_SMALL_DISABLED
:
mRandomizeSmallAllocations
=
false
;
break
;
case
ARENA_FLAG_RANDOMIZE_SMALL_DEFAULT
:
default
:
break
;
}
}
mPRNG
=
nullptr
;
mIsPrivate
=
aIsPrivate
;
mNumDirty
=
0
;
mMaxDirty
=
(
aParams
&
&
aParams
-
>
mMaxDirty
)
?
aParams
-
>
mMaxDirty
:
(
opt_dirty_max
/
8
)
;
mRunsAvail
.
Init
(
)
;
SizeClass
sizeClass
(
1
)
;
for
(
i
=
0
;
;
i
+
+
)
{
arena_bin_t
&
bin
=
mBins
[
i
]
;
bin
.
Init
(
sizeClass
)
;
if
(
sizeClass
.
Size
(
)
=
=
gMaxBinClass
)
{
break
;
}
sizeClass
=
sizeClass
.
Next
(
)
;
}
MOZ_ASSERT
(
i
=
=
NUM_SMALL_CLASSES
-
1
)
;
#
if
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
mMagic
=
ARENA_MAGIC
;
#
endif
}
arena_t
:
:
~
arena_t
(
)
{
size_t
i
;
MutexAutoLock
lock
(
mLock
)
;
MOZ_RELEASE_ASSERT
(
!
mLink
.
Left
(
)
&
&
!
mLink
.
Right
(
)
"
Arena
is
still
registered
"
)
;
MOZ_RELEASE_ASSERT
(
!
mStats
.
allocated_small
&
&
!
mStats
.
allocated_large
"
Arena
is
not
empty
"
)
;
if
(
mSpare
)
{
chunk_dealloc
(
mSpare
kChunkSize
ARENA_CHUNK
)
;
}
for
(
i
=
0
;
i
<
NUM_SMALL_CLASSES
;
i
+
+
)
{
MOZ_RELEASE_ASSERT
(
!
mBins
[
i
]
.
mNonFullRuns
.
First
(
)
"
Bin
is
not
empty
"
)
;
}
#
ifdef
MOZ_DEBUG
{
MutexAutoLock
lock
(
huge_mtx
)
;
for
(
auto
node
:
huge
.
iter
(
)
)
{
MOZ_RELEASE_ASSERT
(
node
-
>
mArenaId
!
=
mId
"
Arena
has
huge
allocations
"
)
;
}
}
#
endif
mId
=
0
;
}
arena_t
*
ArenaCollection
:
:
CreateArena
(
bool
aIsPrivate
arena_params_t
*
aParams
)
{
arena_t
*
ret
=
new
(
fallible
)
arena_t
(
aParams
aIsPrivate
)
;
if
(
!
ret
)
{
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Error
initializing
arena
\
n
"
)
;
return
mDefaultArena
;
}
MutexAutoLock
lock
(
mLock
)
;
if
(
!
aIsPrivate
)
{
ret
-
>
mId
=
mLastPublicArenaId
+
+
;
mArenas
.
Insert
(
ret
)
;
return
ret
;
}
while
(
true
)
{
mozilla
:
:
Maybe
<
uint64_t
>
maybeRandomId
=
mozilla
:
:
RandomUint64
(
)
;
MOZ_RELEASE_ASSERT
(
maybeRandomId
.
isSome
(
)
)
;
if
(
!
maybeRandomId
.
value
(
)
)
{
continue
;
}
arena_t
*
existingArena
=
GetByIdInternal
(
maybeRandomId
.
value
(
)
true
)
;
if
(
!
existingArena
)
{
ret
-
>
mId
=
static_cast
<
arena_id_t
>
(
maybeRandomId
.
value
(
)
)
;
mPrivateArenas
.
Insert
(
ret
)
;
return
ret
;
}
}
}
void
*
arena_t
:
:
MallocHuge
(
size_t
aSize
bool
aZero
)
{
return
PallocHuge
(
aSize
kChunkSize
aZero
)
;
}
void
*
arena_t
:
:
PallocHuge
(
size_t
aSize
size_t
aAlignment
bool
aZero
)
{
void
*
ret
;
size_t
csize
;
size_t
psize
;
extent_node_t
*
node
;
bool
zeroed
;
csize
=
CHUNK_CEILING
(
aSize
+
gPageSize
)
;
if
(
csize
<
aSize
)
{
return
nullptr
;
}
node
=
ExtentAlloc
:
:
alloc
(
)
;
if
(
!
node
)
{
return
nullptr
;
}
ret
=
chunk_alloc
(
csize
aAlignment
false
&
zeroed
)
;
if
(
!
ret
)
{
ExtentAlloc
:
:
dealloc
(
node
)
;
return
nullptr
;
}
psize
=
PAGE_CEILING
(
aSize
)
;
if
(
aZero
)
{
chunk_ensure_zero
(
ret
psize
zeroed
)
;
}
node
-
>
mAddr
=
ret
;
node
-
>
mSize
=
psize
;
node
-
>
mArena
=
this
;
node
-
>
mArenaId
=
mId
;
{
MutexAutoLock
lock
(
huge_mtx
)
;
huge
.
Insert
(
node
)
;
huge_allocated
+
=
psize
;
huge_mapped
+
=
csize
;
}
pages_decommit
(
(
void
*
)
(
(
uintptr_t
)
ret
+
psize
)
csize
-
psize
)
;
if
(
!
aZero
)
{
ApplyZeroOrJunk
(
ret
psize
)
;
}
return
ret
;
}
void
*
arena_t
:
:
RallocHuge
(
void
*
aPtr
size_t
aSize
size_t
aOldSize
)
{
void
*
ret
;
size_t
copysize
;
if
(
aOldSize
>
gMaxLargeClass
&
&
CHUNK_CEILING
(
aSize
+
gPageSize
)
=
=
CHUNK_CEILING
(
aOldSize
+
gPageSize
)
)
{
size_t
psize
=
PAGE_CEILING
(
aSize
)
;
if
(
aSize
<
aOldSize
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
aSize
)
kAllocPoison
aOldSize
-
aSize
)
;
}
if
(
psize
<
aOldSize
)
{
extent_node_t
key
;
pages_decommit
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
psize
)
aOldSize
-
psize
)
;
MutexAutoLock
lock
(
huge_mtx
)
;
key
.
mAddr
=
const_cast
<
void
*
>
(
aPtr
)
;
extent_node_t
*
node
=
huge
.
Search
(
&
key
)
;
MOZ_ASSERT
(
node
)
;
MOZ_ASSERT
(
node
-
>
mSize
=
=
aOldSize
)
;
MOZ_RELEASE_ASSERT
(
node
-
>
mArena
=
=
this
)
;
huge_allocated
-
=
aOldSize
-
psize
;
node
-
>
mSize
=
psize
;
}
else
if
(
psize
>
aOldSize
)
{
if
(
!
pages_commit
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
aOldSize
)
psize
-
aOldSize
)
)
{
return
nullptr
;
}
extent_node_t
key
;
MutexAutoLock
lock
(
huge_mtx
)
;
key
.
mAddr
=
const_cast
<
void
*
>
(
aPtr
)
;
extent_node_t
*
node
=
huge
.
Search
(
&
key
)
;
MOZ_ASSERT
(
node
)
;
MOZ_ASSERT
(
node
-
>
mSize
=
=
aOldSize
)
;
MOZ_RELEASE_ASSERT
(
node
-
>
mArena
=
=
this
)
;
huge_allocated
+
=
psize
-
aOldSize
;
node
-
>
mSize
=
psize
;
}
if
(
aSize
>
aOldSize
)
{
ApplyZeroOrJunk
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
aOldSize
)
aSize
-
aOldSize
)
;
}
return
aPtr
;
}
ret
=
(
mIsPrivate
?
this
:
choose_arena
(
aSize
)
)
-
>
MallocHuge
(
aSize
false
)
;
if
(
!
ret
)
{
return
nullptr
;
}
copysize
=
(
aSize
<
aOldSize
)
?
aSize
:
aOldSize
;
#
ifdef
VM_COPY_MIN
if
(
copysize
>
=
VM_COPY_MIN
)
{
pages_copy
(
ret
aPtr
copysize
)
;
}
else
#
endif
{
memcpy
(
ret
aPtr
copysize
)
;
}
idalloc
(
aPtr
this
)
;
return
ret
;
}
static
void
huge_dalloc
(
void
*
aPtr
arena_t
*
aArena
)
{
extent_node_t
*
node
;
size_t
mapped
=
0
;
{
extent_node_t
key
;
MutexAutoLock
lock
(
huge_mtx
)
;
key
.
mAddr
=
aPtr
;
node
=
huge
.
Search
(
&
key
)
;
MOZ_RELEASE_ASSERT
(
node
"
Double
-
free
?
"
)
;
MOZ_ASSERT
(
node
-
>
mAddr
=
=
aPtr
)
;
MOZ_RELEASE_ASSERT
(
!
aArena
|
|
node
-
>
mArena
=
=
aArena
)
;
MOZ_RELEASE_ASSERT
(
node
-
>
mArenaId
=
=
node
-
>
mArena
-
>
mId
)
;
huge
.
Remove
(
node
)
;
mapped
=
CHUNK_CEILING
(
node
-
>
mSize
+
gPageSize
)
;
huge_allocated
-
=
node
-
>
mSize
;
huge_mapped
-
=
mapped
;
}
chunk_dealloc
(
node
-
>
mAddr
mapped
HUGE_CHUNK
)
;
ExtentAlloc
:
:
dealloc
(
node
)
;
}
static
size_t
GetKernelPageSize
(
)
{
static
size_t
kernel_page_size
=
(
[
]
(
)
{
#
ifdef
XP_WIN
SYSTEM_INFO
info
;
GetSystemInfo
(
&
info
)
;
return
info
.
dwPageSize
;
#
else
long
result
=
sysconf
(
_SC_PAGESIZE
)
;
MOZ_ASSERT
(
result
!
=
-
1
)
;
return
result
;
#
endif
}
)
(
)
;
return
kernel_page_size
;
}
static
bool
malloc_init_hard
(
)
{
unsigned
i
;
const
char
*
opts
;
AutoLock
<
StaticMutex
>
lock
(
gInitLock
)
;
if
(
malloc_initialized
)
{
return
true
;
}
if
(
!
thread_arena
.
init
(
)
)
{
return
true
;
}
const
size_t
result
=
GetKernelPageSize
(
)
;
MOZ_ASSERT
(
(
(
result
-
1
)
&
result
)
=
=
0
)
;
#
ifdef
MALLOC_STATIC_PAGESIZE
if
(
gPageSize
%
result
)
{
_malloc_message
(
_getprogname
(
)
"
Compile
-
time
page
size
does
not
divide
the
runtime
one
.
\
n
"
)
;
MOZ_CRASH
(
)
;
}
#
else
gRealPageSize
=
gPageSize
=
result
;
#
endif
if
(
(
opts
=
getenv
(
"
MALLOC_OPTIONS
"
)
)
)
{
for
(
i
=
0
;
opts
[
i
]
!
=
'
\
0
'
;
i
+
+
)
{
unsigned
j
nreps
;
bool
nseen
;
for
(
nreps
=
0
nseen
=
false
;
;
i
+
+
nseen
=
true
)
{
switch
(
opts
[
i
]
)
{
case
'
0
'
:
case
'
1
'
:
case
'
2
'
:
case
'
3
'
:
case
'
4
'
:
case
'
5
'
:
case
'
6
'
:
case
'
7
'
:
case
'
8
'
:
case
'
9
'
:
nreps
*
=
10
;
nreps
+
=
opts
[
i
]
-
'
0
'
;
break
;
default
:
goto
MALLOC_OUT
;
}
}
MALLOC_OUT
:
if
(
nseen
=
=
false
)
{
nreps
=
1
;
}
for
(
j
=
0
;
j
<
nreps
;
j
+
+
)
{
switch
(
opts
[
i
]
)
{
case
'
f
'
:
opt_dirty_max
>
>
=
1
;
break
;
case
'
F
'
:
if
(
opt_dirty_max
=
=
0
)
{
opt_dirty_max
=
1
;
}
else
if
(
(
opt_dirty_max
<
<
1
)
!
=
0
)
{
opt_dirty_max
<
<
=
1
;
}
break
;
#
ifdef
MOZ_DEBUG
case
'
j
'
:
opt_junk
=
false
;
break
;
case
'
J
'
:
opt_junk
=
true
;
break
;
case
'
z
'
:
opt_zero
=
false
;
break
;
case
'
Z
'
:
opt_zero
=
true
;
break
;
#
ifndef
MALLOC_STATIC_PAGESIZE
case
'
P
'
:
if
(
gPageSize
<
64_KiB
)
{
gPageSize
<
<
=
1
;
}
break
;
#
endif
#
endif
case
'
r
'
:
opt_randomize_small
=
false
;
break
;
case
'
R
'
:
opt_randomize_small
=
true
;
break
;
default
:
{
char
cbuf
[
2
]
;
cbuf
[
0
]
=
opts
[
i
]
;
cbuf
[
1
]
=
'
\
0
'
;
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Unsupported
character
"
"
in
malloc
options
:
'
"
cbuf
"
'
\
n
"
)
;
}
}
}
}
}
#
ifndef
MALLOC_STATIC_PAGESIZE
DefineGlobals
(
)
;
#
endif
gRecycledSize
=
0
;
chunks_mtx
.
Init
(
)
;
gChunksBySize
.
Init
(
)
;
gChunksByAddress
.
Init
(
)
;
huge_mtx
.
Init
(
)
;
huge
.
Init
(
)
;
huge_allocated
=
0
;
huge_mapped
=
0
;
base_mapped
=
0
;
base_committed
=
0
;
base_mtx
.
Init
(
)
;
if
(
!
gArenas
.
Init
(
)
)
{
return
false
;
}
thread_arena
.
set
(
gArenas
.
GetDefault
(
)
)
;
if
(
!
gChunkRTree
.
Init
(
)
)
{
return
false
;
}
malloc_initialized
=
true
;
Debug
:
:
jemalloc_ptr_info
(
nullptr
)
;
#
if
!
defined
(
XP_WIN
)
&
&
!
defined
(
XP_DARWIN
)
pthread_atfork
(
_malloc_prefork
_malloc_postfork_parent
_malloc_postfork_child
)
;
#
endif
return
true
;
}
struct
BaseAllocator
{
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
inline
return_type
name
(
__VA_ARGS__
)
;
#
define
MALLOC_FUNCS
MALLOC_FUNCS_MALLOC_BASE
#
include
"
malloc_decls
.
h
"
explicit
BaseAllocator
(
arena_t
*
aArena
)
:
mArena
(
aArena
)
{
}
private
:
arena_t
*
mArena
;
}
;
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
template
<
>
\
inline
return_type
MozJemalloc
:
:
name
(
\
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
{
\
BaseAllocator
allocator
(
nullptr
)
;
\
return
allocator
.
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
define
MALLOC_FUNCS
MALLOC_FUNCS_MALLOC_BASE
#
include
"
malloc_decls
.
h
"
inline
void
*
BaseAllocator
:
:
malloc
(
size_t
aSize
)
{
void
*
ret
;
arena_t
*
arena
;
if
(
!
malloc_init
(
)
)
{
ret
=
nullptr
;
goto
RETURN
;
}
if
(
aSize
=
=
0
)
{
aSize
=
1
;
}
arena
=
mArena
?
mArena
:
choose_arena
(
aSize
)
;
ret
=
arena
-
>
Malloc
(
aSize
false
)
;
RETURN
:
if
(
!
ret
)
{
errno
=
ENOMEM
;
}
return
ret
;
}
inline
void
*
BaseAllocator
:
:
memalign
(
size_t
aAlignment
size_t
aSize
)
{
MOZ_ASSERT
(
(
(
aAlignment
-
1
)
&
aAlignment
)
=
=
0
)
;
if
(
!
malloc_init
(
)
)
{
return
nullptr
;
}
if
(
aSize
=
=
0
)
{
aSize
=
1
;
}
aAlignment
=
aAlignment
<
sizeof
(
void
*
)
?
sizeof
(
void
*
)
:
aAlignment
;
arena_t
*
arena
=
mArena
?
mArena
:
choose_arena
(
aSize
)
;
return
arena
-
>
Palloc
(
aAlignment
aSize
)
;
}
inline
void
*
BaseAllocator
:
:
calloc
(
size_t
aNum
size_t
aSize
)
{
void
*
ret
;
if
(
malloc_init
(
)
)
{
CheckedInt
<
size_t
>
checkedSize
=
CheckedInt
<
size_t
>
(
aNum
)
*
aSize
;
if
(
checkedSize
.
isValid
(
)
)
{
size_t
allocSize
=
checkedSize
.
value
(
)
;
if
(
allocSize
=
=
0
)
{
allocSize
=
1
;
}
arena_t
*
arena
=
mArena
?
mArena
:
choose_arena
(
allocSize
)
;
ret
=
arena
-
>
Malloc
(
allocSize
true
)
;
}
else
{
ret
=
nullptr
;
}
}
else
{
ret
=
nullptr
;
}
if
(
!
ret
)
{
errno
=
ENOMEM
;
}
return
ret
;
}
inline
void
*
BaseAllocator
:
:
realloc
(
void
*
aPtr
size_t
aSize
)
{
void
*
ret
;
if
(
aSize
=
=
0
)
{
aSize
=
1
;
}
if
(
aPtr
)
{
MOZ_RELEASE_ASSERT
(
malloc_initialized
)
;
auto
info
=
AllocInfo
:
:
Get
(
aPtr
)
;
auto
arena
=
info
.
Arena
(
)
;
MOZ_RELEASE_ASSERT
(
!
mArena
|
|
arena
=
=
mArena
)
;
ret
=
arena
-
>
Ralloc
(
aPtr
aSize
info
.
Size
(
)
)
;
}
else
{
if
(
!
malloc_init
(
)
)
{
ret
=
nullptr
;
}
else
{
arena_t
*
arena
=
mArena
?
mArena
:
choose_arena
(
aSize
)
;
ret
=
arena
-
>
Malloc
(
aSize
false
)
;
}
}
if
(
!
ret
)
{
errno
=
ENOMEM
;
}
return
ret
;
}
inline
void
BaseAllocator
:
:
free
(
void
*
aPtr
)
{
size_t
offset
;
offset
=
GetChunkOffsetForPtr
(
aPtr
)
;
if
(
offset
!
=
0
)
{
MOZ_RELEASE_ASSERT
(
malloc_initialized
)
;
arena_dalloc
(
aPtr
offset
mArena
)
;
}
else
if
(
aPtr
)
{
MOZ_RELEASE_ASSERT
(
malloc_initialized
)
;
huge_dalloc
(
aPtr
mArena
)
;
}
}
template
<
void
*
(
*
memalign
)
(
size_t
size_t
)
>
struct
AlignedAllocator
{
static
inline
int
posix_memalign
(
void
*
*
aMemPtr
size_t
aAlignment
size_t
aSize
)
{
void
*
result
;
if
(
(
(
aAlignment
-
1
)
&
aAlignment
)
!
=
0
|
|
aAlignment
<
sizeof
(
void
*
)
)
{
return
EINVAL
;
}
result
=
memalign
(
aAlignment
aSize
)
;
if
(
!
result
)
{
return
ENOMEM
;
}
*
aMemPtr
=
result
;
return
0
;
}
static
inline
void
*
aligned_alloc
(
size_t
aAlignment
size_t
aSize
)
{
if
(
aSize
%
aAlignment
)
{
return
nullptr
;
}
return
memalign
(
aAlignment
aSize
)
;
}
static
inline
void
*
valloc
(
size_t
aSize
)
{
return
memalign
(
GetKernelPageSize
(
)
aSize
)
;
}
}
;
template
<
>
inline
int
MozJemalloc
:
:
posix_memalign
(
void
*
*
aMemPtr
size_t
aAlignment
size_t
aSize
)
{
return
AlignedAllocator
<
memalign
>
:
:
posix_memalign
(
aMemPtr
aAlignment
aSize
)
;
}
template
<
>
inline
void
*
MozJemalloc
:
:
aligned_alloc
(
size_t
aAlignment
size_t
aSize
)
{
return
AlignedAllocator
<
memalign
>
:
:
aligned_alloc
(
aAlignment
aSize
)
;
}
template
<
>
inline
void
*
MozJemalloc
:
:
valloc
(
size_t
aSize
)
{
return
AlignedAllocator
<
memalign
>
:
:
valloc
(
aSize
)
;
}
template
<
>
inline
size_t
MozJemalloc
:
:
malloc_good_size
(
size_t
aSize
)
{
if
(
aSize
<
=
gMaxLargeClass
)
{
aSize
=
SizeClass
(
aSize
)
.
Size
(
)
;
}
else
{
aSize
=
PAGE_CEILING
(
aSize
)
;
}
return
aSize
;
}
template
<
>
inline
size_t
MozJemalloc
:
:
malloc_usable_size
(
usable_ptr_t
aPtr
)
{
return
AllocInfo
:
:
GetValidated
(
aPtr
)
.
Size
(
)
;
}
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_stats_internal
(
jemalloc_stats_t
*
aStats
jemalloc_bin_stats_t
*
aBinStats
)
{
size_t
non_arena_mapped
chunk_header_size
;
if
(
!
aStats
)
{
return
;
}
if
(
!
malloc_init
(
)
)
{
memset
(
aStats
0
sizeof
(
*
aStats
)
)
;
return
;
}
if
(
aBinStats
)
{
memset
(
aBinStats
0
sizeof
(
jemalloc_bin_stats_t
)
*
NUM_SMALL_CLASSES
)
;
}
aStats
-
>
opt_junk
=
opt_junk
;
aStats
-
>
opt_zero
=
opt_zero
;
aStats
-
>
quantum
=
kQuantum
;
aStats
-
>
quantum_max
=
kMaxQuantumClass
;
aStats
-
>
quantum_wide
=
kQuantumWide
;
aStats
-
>
quantum_wide_max
=
kMaxQuantumWideClass
;
aStats
-
>
subpage_max
=
gMaxSubPageClass
;
aStats
-
>
large_max
=
gMaxLargeClass
;
aStats
-
>
chunksize
=
kChunkSize
;
aStats
-
>
page_size
=
gPageSize
;
aStats
-
>
dirty_max
=
opt_dirty_max
;
aStats
-
>
narenas
=
0
;
aStats
-
>
mapped
=
0
;
aStats
-
>
allocated
=
0
;
aStats
-
>
waste
=
0
;
aStats
-
>
page_cache
=
0
;
aStats
-
>
bookkeeping
=
0
;
aStats
-
>
bin_unused
=
0
;
non_arena_mapped
=
0
;
{
MutexAutoLock
lock
(
huge_mtx
)
;
non_arena_mapped
+
=
huge_mapped
;
aStats
-
>
allocated
+
=
huge_allocated
;
MOZ_ASSERT
(
huge_mapped
>
=
huge_allocated
)
;
}
{
MutexAutoLock
lock
(
base_mtx
)
;
non_arena_mapped
+
=
base_mapped
;
aStats
-
>
bookkeeping
+
=
base_committed
;
MOZ_ASSERT
(
base_mapped
>
=
base_committed
)
;
}
gArenas
.
mLock
.
Lock
(
)
;
for
(
auto
arena
:
gArenas
.
iter
(
)
)
{
size_t
arena_mapped
arena_allocated
arena_committed
arena_dirty
j
arena_unused
arena_headers
;
arena_headers
=
0
;
arena_unused
=
0
;
{
MutexAutoLock
lock
(
arena
-
>
mLock
)
;
arena_mapped
=
arena
-
>
mStats
.
mapped
;
arena_committed
=
arena
-
>
mStats
.
committed
<
<
gPageSize2Pow
;
arena_allocated
=
arena
-
>
mStats
.
allocated_small
+
arena
-
>
mStats
.
allocated_large
;
arena_dirty
=
arena
-
>
mNumDirty
<
<
gPageSize2Pow
;
for
(
j
=
0
;
j
<
NUM_SMALL_CLASSES
;
j
+
+
)
{
arena_bin_t
*
bin
=
&
arena
-
>
mBins
[
j
]
;
size_t
bin_unused
=
0
;
size_t
num_non_full_runs
=
0
;
for
(
auto
mapelm
:
bin
-
>
mNonFullRuns
.
iter
(
)
)
{
arena_run_t
*
run
=
(
arena_run_t
*
)
(
mapelm
-
>
bits
&
~
gPageSizeMask
)
;
bin_unused
+
=
run
-
>
mNumFree
*
bin
-
>
mSizeClass
;
num_non_full_runs
+
+
;
}
if
(
bin
-
>
mCurrentRun
)
{
bin_unused
+
=
bin
-
>
mCurrentRun
-
>
mNumFree
*
bin
-
>
mSizeClass
;
num_non_full_runs
+
+
;
}
arena_unused
+
=
bin_unused
;
arena_headers
+
=
bin
-
>
mNumRuns
*
bin
-
>
mRunFirstRegionOffset
;
if
(
aBinStats
)
{
aBinStats
[
j
]
.
size
=
bin
-
>
mSizeClass
;
aBinStats
[
j
]
.
num_non_full_runs
+
=
num_non_full_runs
;
aBinStats
[
j
]
.
num_runs
+
=
bin
-
>
mNumRuns
;
aBinStats
[
j
]
.
bytes_unused
+
=
bin_unused
;
aBinStats
[
j
]
.
bytes_total
+
=
bin
-
>
mNumRuns
*
(
bin
-
>
mRunSize
-
bin
-
>
mRunFirstRegionOffset
)
;
aBinStats
[
j
]
.
bytes_per_run
=
bin
-
>
mRunSize
;
}
}
}
MOZ_ASSERT
(
arena_mapped
>
=
arena_committed
)
;
MOZ_ASSERT
(
arena_committed
>
=
arena_allocated
+
arena_dirty
)
;
aStats
-
>
mapped
+
=
arena_mapped
;
aStats
-
>
allocated
+
=
arena_allocated
;
aStats
-
>
page_cache
+
=
arena_dirty
;
aStats
-
>
waste
+
=
arena_committed
-
arena_allocated
-
arena_dirty
-
arena_unused
-
arena_headers
;
aStats
-
>
bin_unused
+
=
arena_unused
;
aStats
-
>
bookkeeping
+
=
arena_headers
;
aStats
-
>
narenas
+
+
;
}
gArenas
.
mLock
.
Unlock
(
)
;
chunk_header_size
=
(
(
aStats
-
>
mapped
/
aStats
-
>
chunksize
)
*
gChunkHeaderNumPages
)
<
<
gPageSize2Pow
;
aStats
-
>
mapped
+
=
non_arena_mapped
;
aStats
-
>
bookkeeping
+
=
chunk_header_size
;
aStats
-
>
waste
-
=
chunk_header_size
;
MOZ_ASSERT
(
aStats
-
>
mapped
>
=
aStats
-
>
allocated
+
aStats
-
>
waste
+
aStats
-
>
page_cache
+
aStats
-
>
bookkeeping
)
;
}
template
<
>
inline
size_t
MozJemalloc
:
:
jemalloc_stats_num_bins
(
)
{
return
NUM_SMALL_CLASSES
;
}
#
ifdef
MALLOC_DOUBLE_PURGE
static
void
hard_purge_chunk
(
arena_chunk_t
*
aChunk
)
{
for
(
size_t
i
=
gChunkHeaderNumPages
;
i
<
gChunkNumPages
;
i
+
+
)
{
size_t
npages
;
for
(
npages
=
0
;
aChunk
-
>
map
[
i
+
npages
]
.
bits
&
CHUNK_MAP_MADVISED
&
&
i
+
npages
<
gChunkNumPages
;
npages
+
+
)
{
MOZ_DIAGNOSTIC_ASSERT
(
!
(
aChunk
-
>
map
[
i
+
npages
]
.
bits
&
CHUNK_MAP_DECOMMITTED
)
)
;
aChunk
-
>
map
[
i
+
npages
]
.
bits
^
=
CHUNK_MAP_MADVISED_OR_DECOMMITTED
;
}
if
(
npages
>
0
)
{
pages_decommit
(
(
(
char
*
)
aChunk
)
+
(
i
<
<
gPageSize2Pow
)
npages
<
<
gPageSize2Pow
)
;
Unused
<
<
pages_commit
(
(
(
char
*
)
aChunk
)
+
(
i
<
<
gPageSize2Pow
)
npages
<
<
gPageSize2Pow
)
;
}
i
+
=
npages
;
}
}
void
arena_t
:
:
HardPurge
(
)
{
MutexAutoLock
lock
(
mLock
)
;
while
(
!
mChunksMAdvised
.
isEmpty
(
)
)
{
arena_chunk_t
*
chunk
=
mChunksMAdvised
.
popFront
(
)
;
hard_purge_chunk
(
chunk
)
;
}
}
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_purge_freed_pages
(
)
{
if
(
malloc_initialized
)
{
MutexAutoLock
lock
(
gArenas
.
mLock
)
;
for
(
auto
arena
:
gArenas
.
iter
(
)
)
{
arena
-
>
HardPurge
(
)
;
}
}
}
#
else
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_purge_freed_pages
(
)
{
}
#
endif
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_free_dirty_pages
(
void
)
{
if
(
malloc_initialized
)
{
MutexAutoLock
lock
(
gArenas
.
mLock
)
;
for
(
auto
arena
:
gArenas
.
iter
(
)
)
{
MutexAutoLock
arena_lock
(
arena
-
>
mLock
)
;
arena
-
>
Purge
(
true
)
;
}
}
}
inline
arena_t
*
ArenaCollection
:
:
GetByIdInternal
(
arena_id_t
aArenaId
bool
aIsPrivate
)
{
mozilla
:
:
AlignedStorage2
<
arena_t
>
key
;
key
.
addr
(
)
-
>
mId
=
aArenaId
;
return
(
aIsPrivate
?
mPrivateArenas
:
mArenas
)
.
Search
(
key
.
addr
(
)
)
;
}
inline
arena_t
*
ArenaCollection
:
:
GetById
(
arena_id_t
aArenaId
bool
aIsPrivate
)
{
if
(
!
malloc_initialized
)
{
return
nullptr
;
}
MutexAutoLock
lock
(
mLock
)
;
arena_t
*
result
=
GetByIdInternal
(
aArenaId
aIsPrivate
)
;
MOZ_RELEASE_ASSERT
(
result
)
;
return
result
;
}
template
<
>
inline
arena_id_t
MozJemalloc
:
:
moz_create_arena_with_params
(
arena_params_t
*
aParams
)
{
if
(
malloc_init
(
)
)
{
arena_t
*
arena
=
gArenas
.
CreateArena
(
true
aParams
)
;
return
arena
-
>
mId
;
}
return
0
;
}
template
<
>
inline
void
MozJemalloc
:
:
moz_dispose_arena
(
arena_id_t
aArenaId
)
{
arena_t
*
arena
=
gArenas
.
GetById
(
aArenaId
true
)
;
MOZ_RELEASE_ASSERT
(
arena
)
;
gArenas
.
DisposeArena
(
arena
)
;
}
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
template
<
>
\
inline
return_type
MozJemalloc
:
:
moz_arena_
#
#
name
(
\
arena_id_t
aArenaId
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
{
\
BaseAllocator
allocator
(
\
gArenas
.
GetById
(
aArenaId
/
*
IsPrivate
=
*
/
true
)
)
;
\
return
allocator
.
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
define
MALLOC_FUNCS
MALLOC_FUNCS_MALLOC_BASE
#
include
"
malloc_decls
.
h
"
#
ifndef
XP_WIN
FORK_HOOK
void
_malloc_prefork
(
void
)
NO_THREAD_SAFETY_ANALYSIS
{
gArenas
.
mLock
.
Lock
(
)
;
for
(
auto
arena
:
gArenas
.
iter
(
)
)
{
arena
-
>
mLock
.
Lock
(
)
;
}
base_mtx
.
Lock
(
)
;
huge_mtx
.
Lock
(
)
;
}
FORK_HOOK
void
_malloc_postfork_parent
(
void
)
NO_THREAD_SAFETY_ANALYSIS
{
huge_mtx
.
Unlock
(
)
;
base_mtx
.
Unlock
(
)
;
for
(
auto
arena
:
gArenas
.
iter
(
)
)
{
arena
-
>
mLock
.
Unlock
(
)
;
}
gArenas
.
mLock
.
Unlock
(
)
;
}
FORK_HOOK
void
_malloc_postfork_child
(
void
)
{
huge_mtx
.
Init
(
)
;
base_mtx
.
Init
(
)
;
for
(
auto
arena
:
gArenas
.
iter
(
)
)
{
arena
-
>
mLock
.
Init
(
)
;
}
gArenas
.
mLock
.
Init
(
)
;
}
#
endif
#
ifdef
MOZ_REPLACE_MALLOC
#
ifdef
XP_DARWIN
#
define
MOZ_REPLACE_WEAK
__attribute__
(
(
weak_import
)
)
#
elif
defined
(
XP_WIN
)
|
|
defined
(
ANDROID
)
#
define
MOZ_DYNAMIC_REPLACE_INIT
#
define
replace_init
replace_init_decl
#
elif
defined
(
__GNUC__
)
#
define
MOZ_REPLACE_WEAK
__attribute__
(
(
weak
)
)
#
endif
#
include
"
replace_malloc
.
h
"
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
MozJemalloc
:
:
name
static
const
malloc_table_t
gDefaultMallocTable
=
{
#
include
"
malloc_decls
.
h
"
}
;
static
malloc_table_t
gOriginalMallocTable
=
{
#
include
"
malloc_decls
.
h
"
}
;
static
malloc_table_t
gDynamicMallocTable
=
{
#
include
"
malloc_decls
.
h
"
}
;
static
Atomic
<
malloc_table_t
const
*
mozilla
:
:
MemoryOrdering
:
:
Relaxed
>
gMallocTablePtr
;
#
ifdef
MOZ_DYNAMIC_REPLACE_INIT
#
undef
replace_init
typedef
decltype
(
replace_init_decl
)
replace_init_impl_t
;
static
replace_init_impl_t
*
replace_init
=
nullptr
;
#
endif
#
ifdef
XP_WIN
typedef
HMODULE
replace_malloc_handle_t
;
static
replace_malloc_handle_t
replace_malloc_handle
(
)
{
wchar_t
replace_malloc_lib
[
1024
]
;
if
(
GetEnvironmentVariableW
(
L
"
MOZ_REPLACE_MALLOC_LIB
"
replace_malloc_lib
ArrayLength
(
replace_malloc_lib
)
)
>
0
)
{
return
LoadLibraryW
(
replace_malloc_lib
)
;
}
return
nullptr
;
}
#
define
REPLACE_MALLOC_GET_INIT_FUNC
(
handle
)
\
(
replace_init_impl_t
*
)
GetProcAddress
(
handle
"
replace_init
"
)
#
elif
defined
(
ANDROID
)
#
include
<
dlfcn
.
h
>
typedef
void
*
replace_malloc_handle_t
;
static
replace_malloc_handle_t
replace_malloc_handle
(
)
{
const
char
*
replace_malloc_lib
=
getenv
(
"
MOZ_REPLACE_MALLOC_LIB
"
)
;
if
(
replace_malloc_lib
&
&
*
replace_malloc_lib
)
{
return
dlopen
(
replace_malloc_lib
RTLD_LAZY
)
;
}
return
nullptr
;
}
#
define
REPLACE_MALLOC_GET_INIT_FUNC
(
handle
)
\
(
replace_init_impl_t
*
)
dlsym
(
handle
"
replace_init
"
)
#
endif
static
void
replace_malloc_init_funcs
(
malloc_table_t
*
)
;
#
ifdef
MOZ_REPLACE_MALLOC_STATIC
extern
"
C
"
void
logalloc_init
(
malloc_table_t
*
ReplaceMallocBridge
*
*
)
;
extern
"
C
"
void
dmd_init
(
malloc_table_t
*
ReplaceMallocBridge
*
*
)
;
extern
"
C
"
void
phc_init
(
malloc_table_t
*
ReplaceMallocBridge
*
*
)
;
#
endif
bool
Equals
(
const
malloc_table_t
&
aTable1
const
malloc_table_t
&
aTable2
)
{
return
memcmp
(
&
aTable1
&
aTable2
sizeof
(
malloc_table_t
)
)
=
=
0
;
}
static
ReplaceMallocBridge
*
gReplaceMallocBridge
=
nullptr
;
static
void
init
(
)
{
malloc_table_t
tempTable
=
gDefaultMallocTable
;
#
ifdef
MOZ_DYNAMIC_REPLACE_INIT
replace_malloc_handle_t
handle
=
replace_malloc_handle
(
)
;
if
(
handle
)
{
replace_init
=
REPLACE_MALLOC_GET_INIT_FUNC
(
handle
)
;
}
#
endif
gMallocTablePtr
=
&
gDefaultMallocTable
;
if
(
replace_init
)
{
replace_init
(
&
tempTable
&
gReplaceMallocBridge
)
;
}
#
ifdef
MOZ_REPLACE_MALLOC_STATIC
if
(
Equals
(
tempTable
gDefaultMallocTable
)
)
{
logalloc_init
(
&
tempTable
&
gReplaceMallocBridge
)
;
}
#
ifdef
MOZ_DMD
if
(
Equals
(
tempTable
gDefaultMallocTable
)
)
{
dmd_init
(
&
tempTable
&
gReplaceMallocBridge
)
;
}
#
endif
#
ifdef
MOZ_PHC
if
(
Equals
(
tempTable
gDefaultMallocTable
)
)
{
phc_init
(
&
tempTable
&
gReplaceMallocBridge
)
;
}
#
endif
#
endif
if
(
!
Equals
(
tempTable
gDefaultMallocTable
)
)
{
replace_malloc_init_funcs
(
&
tempTable
)
;
}
gOriginalMallocTable
=
tempTable
;
gMallocTablePtr
=
&
gOriginalMallocTable
;
}
MOZ_JEMALLOC_API
void
jemalloc_replace_dynamic
(
jemalloc_init_func
replace_init_func
)
{
if
(
replace_init_func
)
{
malloc_table_t
tempTable
=
gOriginalMallocTable
;
(
*
replace_init_func
)
(
&
tempTable
&
gReplaceMallocBridge
)
;
if
(
!
Equals
(
tempTable
gOriginalMallocTable
)
)
{
replace_malloc_init_funcs
(
&
tempTable
)
;
gMallocTablePtr
=
&
gOriginalMallocTable
;
gDynamicMallocTable
=
tempTable
;
gMallocTablePtr
=
&
gDynamicMallocTable
;
}
}
else
{
gMallocTablePtr
=
&
gOriginalMallocTable
;
}
}
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
template
<
>
\
inline
return_type
ReplaceMalloc
:
:
name
(
\
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
{
\
if
(
MOZ_UNLIKELY
(
!
gMallocTablePtr
)
)
{
\
init
(
)
;
\
}
\
return
(
*
gMallocTablePtr
)
.
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
include
"
malloc_decls
.
h
"
MOZ_JEMALLOC_API
struct
ReplaceMallocBridge
*
get_bridge
(
void
)
{
if
(
MOZ_UNLIKELY
(
!
gMallocTablePtr
)
)
{
init
(
)
;
}
return
gReplaceMallocBridge
;
}
static
void
replace_malloc_init_funcs
(
malloc_table_t
*
table
)
{
if
(
table
-
>
posix_memalign
=
=
MozJemalloc
:
:
posix_memalign
&
&
table
-
>
memalign
!
=
MozJemalloc
:
:
memalign
)
{
table
-
>
posix_memalign
=
AlignedAllocator
<
ReplaceMalloc
:
:
memalign
>
:
:
posix_memalign
;
}
if
(
table
-
>
aligned_alloc
=
=
MozJemalloc
:
:
aligned_alloc
&
&
table
-
>
memalign
!
=
MozJemalloc
:
:
memalign
)
{
table
-
>
aligned_alloc
=
AlignedAllocator
<
ReplaceMalloc
:
:
memalign
>
:
:
aligned_alloc
;
}
if
(
table
-
>
valloc
=
=
MozJemalloc
:
:
valloc
&
&
table
-
>
memalign
!
=
MozJemalloc
:
:
memalign
)
{
table
-
>
valloc
=
AlignedAllocator
<
ReplaceMalloc
:
:
memalign
>
:
:
valloc
;
}
if
(
table
-
>
moz_create_arena_with_params
=
=
MozJemalloc
:
:
moz_create_arena_with_params
&
&
table
-
>
malloc
!
=
MozJemalloc
:
:
malloc
)
{
#
define
MALLOC_DECL
(
name
.
.
.
)
\
table
-
>
name
=
DummyArenaAllocator
<
ReplaceMalloc
>
:
:
name
;
#
define
MALLOC_FUNCS
MALLOC_FUNCS_ARENA_BASE
#
include
"
malloc_decls
.
h
"
}
if
(
table
-
>
moz_arena_malloc
=
=
MozJemalloc
:
:
moz_arena_malloc
&
&
table
-
>
malloc
!
=
MozJemalloc
:
:
malloc
)
{
#
define
MALLOC_DECL
(
name
.
.
.
)
\
table
-
>
name
=
DummyArenaAllocator
<
ReplaceMalloc
>
:
:
name
;
#
define
MALLOC_FUNCS
MALLOC_FUNCS_ARENA_ALLOC
#
include
"
malloc_decls
.
h
"
}
}
#
endif
#
define
GENERIC_MALLOC_DECL2_MINGW
(
name
name_impl
return_type
.
.
.
)
\
return_type
name
(
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
\
__attribute__
(
(
alias
(
MOZ_STRINGIFY
(
name_impl
)
)
)
)
;
#
define
GENERIC_MALLOC_DECL2
(
attributes
name
name_impl
return_type
.
.
.
)
\
return_type
name_impl
(
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
attributes
{
\
return
DefaultMalloc
:
:
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
ifndef
__MINGW32__
#
define
GENERIC_MALLOC_DECL
(
attributes
name
return_type
.
.
.
)
\
GENERIC_MALLOC_DECL2
(
attributes
name
name
#
#
_impl
return_type
\
#
#
__VA_ARGS__
)
#
else
#
define
GENERIC_MALLOC_DECL
(
attributes
name
return_type
.
.
.
)
\
GENERIC_MALLOC_DECL2
(
attributes
name
name
#
#
_impl
return_type
\
#
#
__VA_ARGS__
)
\
GENERIC_MALLOC_DECL2_MINGW
(
name
name
#
#
_impl
return_type
#
#
__VA_ARGS__
)
#
endif
#
define
NOTHROW_MALLOC_DECL
(
.
.
.
)
\
MOZ_MEMORY_API
MACRO_CALL
(
GENERIC_MALLOC_DECL
(
noexcept
(
true
)
__VA_ARGS__
)
)
#
define
MALLOC_DECL
(
.
.
.
)
\
MOZ_MEMORY_API
MACRO_CALL
(
GENERIC_MALLOC_DECL
(
__VA_ARGS__
)
)
#
define
MALLOC_FUNCS
MALLOC_FUNCS_MALLOC
#
include
"
malloc_decls
.
h
"
#
undef
GENERIC_MALLOC_DECL
#
define
GENERIC_MALLOC_DECL
(
attributes
name
return_type
.
.
.
)
\
GENERIC_MALLOC_DECL2
(
attributes
name
name
return_type
#
#
__VA_ARGS__
)
#
define
MALLOC_DECL
(
.
.
.
)
\
MOZ_JEMALLOC_API
MACRO_CALL
(
GENERIC_MALLOC_DECL
(
__VA_ARGS__
)
)
#
define
MALLOC_FUNCS
(
MALLOC_FUNCS_JEMALLOC
|
MALLOC_FUNCS_ARENA
)
#
include
"
malloc_decls
.
h
"
#
ifdef
HAVE_DLOPEN
#
include
<
dlfcn
.
h
>
#
endif
#
if
defined
(
__GLIBC__
)
&
&
!
defined
(
__UCLIBC__
)
extern
"
C
"
{
MOZ_EXPORT
void
(
*
__free_hook
)
(
void
*
)
=
free_impl
;
MOZ_EXPORT
void
*
(
*
__malloc_hook
)
(
size_t
)
=
malloc_impl
;
MOZ_EXPORT
void
*
(
*
__realloc_hook
)
(
void
*
size_t
)
=
realloc_impl
;
MOZ_EXPORT
void
*
(
*
__memalign_hook
)
(
size_t
size_t
)
=
memalign_impl
;
}
#
elif
defined
(
RTLD_DEEPBIND
)
#
error
\
"
Interposing
malloc
is
unsafe
on
this
system
without
libc
malloc
hooks
.
"
#
endif
#
ifdef
XP_WIN
MOZ_EXPORT
void
*
_recalloc
(
void
*
aPtr
size_t
aCount
size_t
aSize
)
{
size_t
oldsize
=
aPtr
?
AllocInfo
:
:
Get
(
aPtr
)
.
Size
(
)
:
0
;
CheckedInt
<
size_t
>
checkedSize
=
CheckedInt
<
size_t
>
(
aCount
)
*
aSize
;
if
(
!
checkedSize
.
isValid
(
)
)
{
return
nullptr
;
}
size_t
newsize
=
checkedSize
.
value
(
)
;
aPtr
=
DefaultMalloc
:
:
realloc
(
aPtr
newsize
)
;
if
(
aPtr
&
&
oldsize
<
newsize
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
oldsize
)
0
newsize
-
oldsize
)
;
}
return
aPtr
;
}
MOZ_EXPORT
void
*
_expand
(
void
*
aPtr
size_t
newsize
)
{
if
(
AllocInfo
:
:
Get
(
aPtr
)
.
Size
(
)
>
=
newsize
)
{
return
aPtr
;
}
return
nullptr
;
}
MOZ_EXPORT
size_t
_msize
(
void
*
aPtr
)
{
return
DefaultMalloc
:
:
malloc_usable_size
(
aPtr
)
;
}
#
endif
