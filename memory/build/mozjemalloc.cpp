#
include
"
mozmemory_wrap
.
h
"
#
include
"
mozjemalloc
.
h
"
#
include
"
mozilla
/
Sprintf
.
h
"
#
include
"
mozilla
/
Likely
.
h
"
#
include
"
mozilla
/
MacroArgs
.
h
"
#
include
"
mozilla
/
DoublyLinkedList
.
h
"
#
ifdef
ANDROID
#
define
NO_TLS
#
endif
#
ifdef
XP_DARWIN
#
define
MALLOC_DOUBLE_PURGE
#
endif
#
include
<
sys
/
types
.
h
>
#
include
<
errno
.
h
>
#
include
<
stdlib
.
h
>
#
include
<
limits
.
h
>
#
include
<
stdarg
.
h
>
#
include
<
stdio
.
h
>
#
include
<
string
.
h
>
#
include
<
algorithm
>
#
ifdef
XP_WIN
#
define
_CRT_SPINCOUNT
5000
#
include
<
io
.
h
>
#
include
<
windows
.
h
>
#
include
<
intrin
.
h
>
#
define
SIZE_T_MAX
SIZE_MAX
#
define
STDERR_FILENO
2
#
pragma
intrinsic
(
_BitScanForward
)
static
__forceinline
int
ffs
(
int
x
)
{
unsigned
long
i
;
if
(
_BitScanForward
(
&
i
x
)
!
=
0
)
return
(
i
+
1
)
;
return
(
0
)
;
}
static
char
mozillaMallocOptionsBuf
[
64
]
;
#
define
getenv
xgetenv
static
char
*
getenv
(
const
char
*
name
)
{
if
(
GetEnvironmentVariableA
(
name
(
LPSTR
)
&
mozillaMallocOptionsBuf
sizeof
(
mozillaMallocOptionsBuf
)
)
>
0
)
return
(
mozillaMallocOptionsBuf
)
;
return
nullptr
;
}
#
if
defined
(
_WIN64
)
typedef
long
long
ssize_t
;
#
else
typedef
long
ssize_t
;
#
endif
#
define
MALLOC_DECOMMIT
#
endif
#
ifndef
XP_WIN
#
ifndef
XP_SOLARIS
#
include
<
sys
/
cdefs
.
h
>
#
endif
#
include
<
sys
/
mman
.
h
>
#
ifndef
MADV_FREE
#
define
MADV_FREE
MADV_DONTNEED
#
endif
#
ifndef
MAP_NOSYNC
#
define
MAP_NOSYNC
0
#
endif
#
include
<
sys
/
param
.
h
>
#
include
<
sys
/
time
.
h
>
#
include
<
sys
/
types
.
h
>
#
if
!
defined
(
XP_SOLARIS
)
&
&
!
defined
(
ANDROID
)
#
include
<
sys
/
sysctl
.
h
>
#
endif
#
include
<
sys
/
uio
.
h
>
#
include
<
errno
.
h
>
#
include
<
limits
.
h
>
#
ifndef
SIZE_T_MAX
#
define
SIZE_T_MAX
SIZE_MAX
#
endif
#
include
<
pthread
.
h
>
#
include
<
sched
.
h
>
#
include
<
stdarg
.
h
>
#
include
<
stdio
.
h
>
#
include
<
stdbool
.
h
>
#
include
<
stdint
.
h
>
#
include
<
stdlib
.
h
>
#
include
<
string
.
h
>
#
ifndef
XP_DARWIN
#
include
<
strings
.
h
>
#
endif
#
include
<
unistd
.
h
>
#
ifdef
XP_DARWIN
#
include
<
libkern
/
OSAtomic
.
h
>
#
include
<
mach
/
mach_error
.
h
>
#
include
<
mach
/
mach_init
.
h
>
#
include
<
mach
/
vm_map
.
h
>
#
include
<
malloc
/
malloc
.
h
>
#
endif
#
endif
#
include
"
mozilla
/
ThreadLocal
.
h
"
#
include
"
mozjemalloc_types
.
h
"
#
if
(
defined
(
XP_LINUX
)
&
&
!
defined
(
__alpha__
)
)
|
|
\
(
defined
(
__FreeBSD_kernel__
)
&
&
defined
(
__GLIBC__
)
)
#
include
<
sys
/
syscall
.
h
>
#
if
defined
(
SYS_mmap
)
|
|
defined
(
SYS_mmap2
)
static
inline
void
*
_mmap
(
void
*
addr
size_t
length
int
prot
int
flags
int
fd
off_t
offset
)
{
#
ifdef
__s390__
struct
{
void
*
addr
;
size_t
length
;
long
prot
;
long
flags
;
long
fd
;
off_t
offset
;
}
args
=
{
addr
length
prot
flags
fd
offset
}
;
return
(
void
*
)
syscall
(
SYS_mmap
&
args
)
;
#
else
#
if
defined
(
ANDROID
)
&
&
defined
(
__aarch64__
)
&
&
defined
(
SYS_mmap2
)
#
undef
SYS_mmap2
#
endif
#
ifdef
SYS_mmap2
return
(
void
*
)
syscall
(
SYS_mmap2
addr
length
prot
flags
fd
offset
>
>
12
)
;
#
else
return
(
void
*
)
syscall
(
SYS_mmap
addr
length
prot
flags
fd
offset
)
;
#
endif
#
endif
}
#
define
mmap
_mmap
#
define
munmap
(
a
l
)
syscall
(
SYS_munmap
a
l
)
#
endif
#
endif
#
ifdef
XP_WIN
#
define
RB_NO_C99_VARARRAYS
#
endif
#
include
"
rb
.
h
"
#
ifdef
MOZ_DEBUG
#
ifdef
inline
#
undef
inline
#
endif
#
define
inline
#
endif
#
define
STRERROR_BUF
64
#
define
QUANTUM_2POW_MIN
4
#
if
defined
(
_WIN64
)
|
|
defined
(
__LP64__
)
#
define
SIZEOF_PTR_2POW
3
#
else
#
define
SIZEOF_PTR_2POW
2
#
endif
#
define
SIZEOF_PTR
(
1U
<
<
SIZEOF_PTR_2POW
)
#
ifndef
SIZEOF_INT_2POW
#
define
SIZEOF_INT_2POW
2
#
endif
#
define
CHUNK_2POW_DEFAULT
20
#
define
DIRTY_MAX_DEFAULT
(
1U
<
<
8
)
#
define
CACHELINE_2POW
6
#
define
CACHELINE
(
(
size_t
)
(
1U
<
<
CACHELINE_2POW
)
)
#
ifdef
XP_WIN
#
define
TINY_MIN_2POW
(
sizeof
(
void
*
)
=
=
8
?
4
:
3
)
#
else
#
define
TINY_MIN_2POW
(
sizeof
(
void
*
)
=
=
8
?
3
:
2
)
#
endif
#
define
SMALL_MAX_2POW_DEFAULT
9
#
define
SMALL_MAX_DEFAULT
(
1U
<
<
SMALL_MAX_2POW_DEFAULT
)
#
define
RUN_BFP
12
#
define
RUN_MAX_OVRHD
0x0000003dU
#
define
RUN_MAX_OVRHD_RELAX
0x00001800U
#
if
defined
(
MALLOC_DECOMMIT
)
&
&
defined
(
MALLOC_DOUBLE_PURGE
)
#
error
MALLOC_DECOMMIT
and
MALLOC_DOUBLE_PURGE
are
mutually
exclusive
.
#
endif
#
if
defined
(
XP_WIN
)
#
define
malloc_mutex_t
CRITICAL_SECTION
#
define
malloc_spinlock_t
CRITICAL_SECTION
#
elif
defined
(
XP_DARWIN
)
struct
malloc_mutex_t
{
OSSpinLock
lock
;
}
;
struct
malloc_spinlock_t
{
OSSpinLock
lock
;
}
;
#
else
typedef
pthread_mutex_t
malloc_mutex_t
;
typedef
pthread_mutex_t
malloc_spinlock_t
;
#
endif
static
bool
malloc_initialized
=
false
;
#
if
defined
(
XP_WIN
)
#
elif
defined
(
XP_DARWIN
)
static
malloc_mutex_t
init_lock
=
{
OS_SPINLOCK_INIT
}
;
#
elif
defined
(
XP_LINUX
)
&
&
!
defined
(
ANDROID
)
static
malloc_mutex_t
init_lock
=
PTHREAD_ADAPTIVE_MUTEX_INITIALIZER_NP
;
#
else
static
malloc_mutex_t
init_lock
=
PTHREAD_MUTEX_INITIALIZER
;
#
endif
struct
malloc_bin_stats_t
{
unsigned
long
curruns
;
}
;
struct
arena_stats_t
{
size_t
mapped
;
size_t
committed
;
size_t
allocated_small
;
size_t
allocated_large
;
}
;
enum
ChunkType
{
UNKNOWN_CHUNK
ZEROED_CHUNK
ARENA_CHUNK
HUGE_CHUNK
RECYCLED_CHUNK
}
;
struct
extent_node_t
{
rb_node
(
extent_node_t
)
link_szad
;
rb_node
(
extent_node_t
)
link_ad
;
void
*
addr
;
size_t
size
;
ChunkType
chunk_type
;
}
;
typedef
rb_tree
(
extent_node_t
)
extent_tree_t
;
#
if
(
SIZEOF_PTR
=
=
4
)
#
define
MALLOC_RTREE_NODESIZE
(
1U
<
<
14
)
#
else
#
define
MALLOC_RTREE_NODESIZE
CACHELINE
#
endif
struct
malloc_rtree_t
{
malloc_spinlock_t
lock
;
void
*
*
root
;
unsigned
height
;
unsigned
level2bits
[
1
]
;
}
;
struct
arena_t
;
struct
arena_bin_t
;
struct
arena_chunk_map_t
{
rb_node
(
arena_chunk_map_t
)
link
;
size_t
bits
;
#
define
CHUNK_MAP_MADVISED
(
(
size_t
)
0x40U
)
#
define
CHUNK_MAP_DECOMMITTED
(
(
size_t
)
0x20U
)
#
define
CHUNK_MAP_MADVISED_OR_DECOMMITTED
(
CHUNK_MAP_MADVISED
|
CHUNK_MAP_DECOMMITTED
)
#
define
CHUNK_MAP_KEY
(
(
size_t
)
0x10U
)
#
define
CHUNK_MAP_DIRTY
(
(
size_t
)
0x08U
)
#
define
CHUNK_MAP_ZEROED
(
(
size_t
)
0x04U
)
#
define
CHUNK_MAP_LARGE
(
(
size_t
)
0x02U
)
#
define
CHUNK_MAP_ALLOCATED
(
(
size_t
)
0x01U
)
}
;
typedef
rb_tree
(
arena_chunk_map_t
)
arena_avail_tree_t
;
typedef
rb_tree
(
arena_chunk_map_t
)
arena_run_tree_t
;
struct
arena_chunk_t
{
arena_t
*
arena
;
rb_node
(
arena_chunk_t
)
link_dirty
;
#
ifdef
MALLOC_DOUBLE_PURGE
mozilla
:
:
DoublyLinkedListElement
<
arena_chunk_t
>
chunks_madvised_elem
;
#
endif
size_t
ndirty
;
arena_chunk_map_t
map
[
1
]
;
}
;
typedef
rb_tree
(
arena_chunk_t
)
arena_chunk_tree_t
;
#
ifdef
MALLOC_DOUBLE_PURGE
namespace
mozilla
{
template
<
>
struct
GetDoublyLinkedListElement
<
arena_chunk_t
>
{
static
DoublyLinkedListElement
<
arena_chunk_t
>
&
Get
(
arena_chunk_t
*
aThis
)
{
return
aThis
-
>
chunks_madvised_elem
;
}
}
;
}
#
endif
struct
arena_run_t
{
#
if
defined
(
MOZ_DEBUG
)
|
|
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
uint32_t
magic
;
#
define
ARENA_RUN_MAGIC
0x384adf93
#
endif
arena_bin_t
*
bin
;
unsigned
regs_minelm
;
unsigned
nfree
;
unsigned
regs_mask
[
1
]
;
}
;
struct
arena_bin_t
{
arena_run_t
*
runcur
;
arena_run_tree_t
runs
;
size_t
reg_size
;
size_t
run_size
;
uint32_t
nregs
;
uint32_t
regs_mask_nelms
;
uint32_t
reg0_offset
;
malloc_bin_stats_t
stats
;
}
;
struct
arena_t
{
#
if
defined
(
MOZ_DEBUG
)
|
|
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
uint32_t
mMagic
;
#
define
ARENA_MAGIC
0x947d3d24
#
endif
malloc_spinlock_t
mLock
;
arena_stats_t
mStats
;
arena_chunk_tree_t
mChunksDirty
;
private
:
#
ifdef
MALLOC_DOUBLE_PURGE
mozilla
:
:
DoublyLinkedList
<
arena_chunk_t
>
mChunksMAdvised
;
#
endif
arena_chunk_t
*
mSpare
;
public
:
size_t
mNumDirty
;
size_t
mMaxDirty
;
arena_avail_tree_t
mRunsAvail
;
arena_bin_t
mBins
[
1
]
;
bool
Init
(
)
;
private
:
void
InitChunk
(
arena_chunk_t
*
aChunk
bool
aZeroed
)
;
void
DeallocChunk
(
arena_chunk_t
*
aChunk
)
;
public
:
arena_run_t
*
AllocRun
(
arena_bin_t
*
aBin
size_t
aSize
bool
aLarge
bool
aZero
)
;
void
DallocRun
(
arena_run_t
*
aRun
bool
aDirty
)
;
void
Purge
(
bool
aAll
)
;
void
HardPurge
(
)
;
}
;
#
if
!
defined
(
__ia64__
)
&
&
!
defined
(
__sparc__
)
&
&
!
defined
(
__mips__
)
&
&
!
defined
(
__aarch64__
)
#
define
MALLOC_STATIC_SIZES
1
#
endif
#
ifdef
MALLOC_STATIC_SIZES
#
if
(
defined
(
SOLARIS
)
|
|
defined
(
__FreeBSD__
)
)
&
&
\
(
defined
(
__sparc
)
|
|
defined
(
__sparcv9
)
|
|
defined
(
__ia64
)
)
#
define
pagesize_2pow
(
(
size_t
)
13
)
#
elif
defined
(
__powerpc64__
)
#
define
pagesize_2pow
(
(
size_t
)
16
)
#
else
#
define
pagesize_2pow
(
(
size_t
)
12
)
#
endif
#
define
pagesize
(
(
size_t
)
1
<
<
pagesize_2pow
)
#
define
pagesize_mask
(
pagesize
-
1
)
#
define
QUANTUM_DEFAULT
(
(
size_t
)
1
<
<
QUANTUM_2POW_MIN
)
static
const
size_t
quantum
=
QUANTUM_DEFAULT
;
static
const
size_t
quantum_mask
=
QUANTUM_DEFAULT
-
1
;
static
const
size_t
small_min
=
(
QUANTUM_DEFAULT
>
>
1
)
+
1
;
static
const
size_t
small_max
=
(
size_t
)
SMALL_MAX_DEFAULT
;
static
const
size_t
bin_maxclass
=
pagesize
>
>
1
;
static
const
unsigned
ntbins
=
(
unsigned
)
(
QUANTUM_2POW_MIN
-
TINY_MIN_2POW
)
;
static
const
unsigned
nqbins
=
(
unsigned
)
(
SMALL_MAX_DEFAULT
>
>
QUANTUM_2POW_MIN
)
;
static
const
unsigned
nsbins
=
(
unsigned
)
(
pagesize_2pow
-
SMALL_MAX_2POW_DEFAULT
-
1
)
;
#
else
static
size_t
pagesize
;
static
size_t
pagesize_mask
;
static
size_t
pagesize_2pow
;
static
size_t
bin_maxclass
;
static
unsigned
ntbins
;
static
unsigned
nqbins
;
static
unsigned
nsbins
;
static
size_t
small_min
;
static
size_t
small_max
;
static
size_t
quantum
;
static
size_t
quantum_mask
;
#
endif
#
define
calculate_arena_header_size
(
)
\
(
sizeof
(
arena_chunk_t
)
+
sizeof
(
arena_chunk_map_t
)
*
(
chunk_npages
-
1
)
)
#
define
calculate_arena_header_pages
(
)
\
(
(
calculate_arena_header_size
(
)
>
>
pagesize_2pow
)
+
\
(
(
calculate_arena_header_size
(
)
&
pagesize_mask
)
?
1
:
0
)
)
#
define
calculate_arena_maxclass
(
)
\
(
chunksize
-
(
arena_chunk_header_npages
<
<
pagesize_2pow
)
)
#
define
CHUNK_RECYCLE_LIMIT
128
#
ifdef
MALLOC_STATIC_SIZES
#
define
CHUNKSIZE_DEFAULT
(
(
size_t
)
1
<
<
CHUNK_2POW_DEFAULT
)
static
const
size_t
chunksize
=
CHUNKSIZE_DEFAULT
;
static
const
size_t
chunksize_mask
=
CHUNKSIZE_DEFAULT
-
1
;
static
const
size_t
chunk_npages
=
CHUNKSIZE_DEFAULT
>
>
pagesize_2pow
;
#
define
arena_chunk_header_npages
calculate_arena_header_pages
(
)
#
define
arena_maxclass
calculate_arena_maxclass
(
)
static
const
size_t
recycle_limit
=
CHUNK_RECYCLE_LIMIT
*
CHUNKSIZE_DEFAULT
;
#
else
static
size_t
chunksize
;
static
size_t
chunksize_mask
;
static
size_t
chunk_npages
;
static
size_t
arena_chunk_header_npages
;
static
size_t
arena_maxclass
;
static
size_t
recycle_limit
;
#
endif
static
size_t
recycled_size
;
static
malloc_rtree_t
*
chunk_rtree
;
static
malloc_mutex_t
chunks_mtx
;
static
extent_tree_t
chunks_szad_mmap
;
static
extent_tree_t
chunks_ad_mmap
;
static
malloc_mutex_t
huge_mtx
;
static
extent_tree_t
huge
;
static
uint64_t
huge_nmalloc
;
static
uint64_t
huge_ndalloc
;
static
size_t
huge_allocated
;
static
size_t
huge_mapped
;
static
void
*
base_pages
;
static
void
*
base_next_addr
;
static
void
*
base_next_decommitted
;
static
void
*
base_past_addr
;
static
extent_node_t
*
base_nodes
;
static
malloc_mutex_t
base_mtx
;
static
size_t
base_mapped
;
static
size_t
base_committed
;
static
arena_t
*
*
arenas
;
static
unsigned
narenas
;
static
malloc_spinlock_t
arenas_lock
;
#
ifndef
NO_TLS
#
if
!
defined
(
XP_WIN
)
&
&
!
defined
(
XP_DARWIN
)
static
MOZ_THREAD_LOCAL
(
arena_t
*
)
thread_arena
;
#
else
static
mozilla
:
:
detail
:
:
ThreadLocal
<
arena_t
*
mozilla
:
:
detail
:
:
ThreadLocalKeyStorage
>
thread_arena
;
#
endif
#
endif
const
uint8_t
kAllocJunk
=
0xe4
;
const
uint8_t
kAllocPoison
=
0xe5
;
#
ifdef
MOZ_DEBUG
static
bool
opt_junk
=
true
;
static
bool
opt_zero
=
false
;
#
else
static
const
bool
opt_junk
=
false
;
static
const
bool
opt_zero
=
false
;
#
endif
static
size_t
opt_dirty_max
=
DIRTY_MAX_DEFAULT
;
#
ifdef
MALLOC_STATIC_SIZES
#
define
opt_quantum_2pow
QUANTUM_2POW_MIN
#
define
opt_small_max_2pow
SMALL_MAX_2POW_DEFAULT
#
define
opt_chunk_2pow
CHUNK_2POW_DEFAULT
#
else
static
size_t
opt_quantum_2pow
=
QUANTUM_2POW_MIN
;
static
size_t
opt_small_max_2pow
=
SMALL_MAX_2POW_DEFAULT
;
static
size_t
opt_chunk_2pow
=
CHUNK_2POW_DEFAULT
;
#
endif
static
void
*
chunk_alloc
(
size_t
size
size_t
alignment
bool
base
bool
*
zeroed
=
nullptr
)
;
static
void
chunk_dealloc
(
void
*
chunk
size_t
size
ChunkType
chunk_type
)
;
static
void
chunk_ensure_zero
(
void
*
ptr
size_t
size
bool
zeroed
)
;
static
arena_t
*
arenas_extend
(
)
;
static
void
*
huge_malloc
(
size_t
size
bool
zero
)
;
static
void
*
huge_palloc
(
size_t
size
size_t
alignment
bool
zero
)
;
static
void
*
huge_ralloc
(
void
*
ptr
size_t
size
size_t
oldsize
)
;
static
void
huge_dalloc
(
void
*
ptr
)
;
#
ifdef
XP_WIN
extern
"
C
"
#
else
static
#
endif
bool
malloc_init_hard
(
void
)
;
#
ifdef
XP_DARWIN
#
define
FORK_HOOK
extern
"
C
"
#
else
#
define
FORK_HOOK
static
#
endif
FORK_HOOK
void
_malloc_prefork
(
void
)
;
FORK_HOOK
void
_malloc_postfork_parent
(
void
)
;
FORK_HOOK
void
_malloc_postfork_child
(
void
)
;
static
inline
size_t
load_acquire_z
(
size_t
*
p
)
{
volatile
size_t
result
=
*
p
;
#
ifdef
XP_WIN
volatile
long
dummy
=
0
;
InterlockedExchange
(
&
dummy
1
)
;
#
else
__sync_synchronize
(
)
;
#
endif
return
result
;
}
static
void
_malloc_message
(
const
char
*
p
)
{
#
if
!
defined
(
XP_WIN
)
#
define
_write
write
#
endif
if
(
_write
(
STDERR_FILENO
p
(
unsigned
int
)
strlen
(
p
)
)
<
0
)
return
;
}
template
<
typename
.
.
.
Args
>
static
void
_malloc_message
(
const
char
*
p
Args
.
.
.
args
)
{
_malloc_message
(
p
)
;
_malloc_message
(
args
.
.
.
)
;
}
#
include
"
mozilla
/
Assertions
.
h
"
#
include
"
mozilla
/
Attributes
.
h
"
#
include
"
mozilla
/
TaggedAnonymousMemory
.
h
"
#
ifdef
ANDROID
extern
"
C
"
MOZ_EXPORT
int
pthread_atfork
(
void
(
*
)
(
void
)
void
(
*
)
(
void
)
void
(
*
)
(
void
)
)
;
#
endif
static
bool
malloc_mutex_init
(
malloc_mutex_t
*
mutex
)
{
#
if
defined
(
XP_WIN
)
if
(
!
InitializeCriticalSectionAndSpinCount
(
mutex
_CRT_SPINCOUNT
)
)
return
(
true
)
;
#
elif
defined
(
XP_DARWIN
)
mutex
-
>
lock
=
OS_SPINLOCK_INIT
;
#
elif
defined
(
XP_LINUX
)
&
&
!
defined
(
ANDROID
)
pthread_mutexattr_t
attr
;
if
(
pthread_mutexattr_init
(
&
attr
)
!
=
0
)
return
(
true
)
;
pthread_mutexattr_settype
(
&
attr
PTHREAD_MUTEX_ADAPTIVE_NP
)
;
if
(
pthread_mutex_init
(
mutex
&
attr
)
!
=
0
)
{
pthread_mutexattr_destroy
(
&
attr
)
;
return
(
true
)
;
}
pthread_mutexattr_destroy
(
&
attr
)
;
#
else
if
(
pthread_mutex_init
(
mutex
nullptr
)
!
=
0
)
return
(
true
)
;
#
endif
return
(
false
)
;
}
static
inline
void
malloc_mutex_lock
(
malloc_mutex_t
*
mutex
)
{
#
if
defined
(
XP_WIN
)
EnterCriticalSection
(
mutex
)
;
#
elif
defined
(
XP_DARWIN
)
OSSpinLockLock
(
&
mutex
-
>
lock
)
;
#
else
pthread_mutex_lock
(
mutex
)
;
#
endif
}
static
inline
void
malloc_mutex_unlock
(
malloc_mutex_t
*
mutex
)
{
#
if
defined
(
XP_WIN
)
LeaveCriticalSection
(
mutex
)
;
#
elif
defined
(
XP_DARWIN
)
OSSpinLockUnlock
(
&
mutex
-
>
lock
)
;
#
else
pthread_mutex_unlock
(
mutex
)
;
#
endif
}
#
if
(
defined
(
__GNUC__
)
)
__attribute__
(
(
unused
)
)
#
endif
static
bool
malloc_spin_init
(
malloc_spinlock_t
*
lock
)
{
#
if
defined
(
XP_WIN
)
if
(
!
InitializeCriticalSectionAndSpinCount
(
lock
_CRT_SPINCOUNT
)
)
return
(
true
)
;
#
elif
defined
(
XP_DARWIN
)
lock
-
>
lock
=
OS_SPINLOCK_INIT
;
#
elif
defined
(
XP_LINUX
)
&
&
!
defined
(
ANDROID
)
pthread_mutexattr_t
attr
;
if
(
pthread_mutexattr_init
(
&
attr
)
!
=
0
)
return
(
true
)
;
pthread_mutexattr_settype
(
&
attr
PTHREAD_MUTEX_ADAPTIVE_NP
)
;
if
(
pthread_mutex_init
(
lock
&
attr
)
!
=
0
)
{
pthread_mutexattr_destroy
(
&
attr
)
;
return
(
true
)
;
}
pthread_mutexattr_destroy
(
&
attr
)
;
#
else
if
(
pthread_mutex_init
(
lock
nullptr
)
!
=
0
)
return
(
true
)
;
#
endif
return
(
false
)
;
}
static
inline
void
malloc_spin_lock
(
malloc_spinlock_t
*
lock
)
{
#
if
defined
(
XP_WIN
)
EnterCriticalSection
(
lock
)
;
#
elif
defined
(
XP_DARWIN
)
OSSpinLockLock
(
&
lock
-
>
lock
)
;
#
else
pthread_mutex_lock
(
lock
)
;
#
endif
}
static
inline
void
malloc_spin_unlock
(
malloc_spinlock_t
*
lock
)
{
#
if
defined
(
XP_WIN
)
LeaveCriticalSection
(
lock
)
;
#
elif
defined
(
XP_DARWIN
)
OSSpinLockUnlock
(
&
lock
-
>
lock
)
;
#
else
pthread_mutex_unlock
(
lock
)
;
#
endif
}
#
if
!
defined
(
XP_DARWIN
)
#
define
malloc_spin_init
malloc_mutex_init
#
define
malloc_spin_lock
malloc_mutex_lock
#
define
malloc_spin_unlock
malloc_mutex_unlock
#
endif
#
define
CHUNK_ADDR2BASE
(
a
)
\
(
(
void
*
)
(
(
uintptr_t
)
(
a
)
&
~
chunksize_mask
)
)
#
define
CHUNK_ADDR2OFFSET
(
a
)
\
(
(
size_t
)
(
(
uintptr_t
)
(
a
)
&
chunksize_mask
)
)
#
define
CHUNK_CEILING
(
s
)
\
(
(
(
s
)
+
chunksize_mask
)
&
~
chunksize_mask
)
#
define
CACHELINE_CEILING
(
s
)
\
(
(
(
s
)
+
(
CACHELINE
-
1
)
)
&
~
(
CACHELINE
-
1
)
)
#
define
QUANTUM_CEILING
(
a
)
\
(
(
(
a
)
+
quantum_mask
)
&
~
quantum_mask
)
#
define
PAGE_CEILING
(
s
)
\
(
(
(
s
)
+
pagesize_mask
)
&
~
pagesize_mask
)
static
inline
size_t
pow2_ceil
(
size_t
x
)
{
x
-
-
;
x
|
=
x
>
>
1
;
x
|
=
x
>
>
2
;
x
|
=
x
>
>
4
;
x
|
=
x
>
>
8
;
x
|
=
x
>
>
16
;
#
if
(
SIZEOF_PTR
=
=
8
)
x
|
=
x
>
>
32
;
#
endif
x
+
+
;
return
(
x
)
;
}
static
inline
const
char
*
_getprogname
(
void
)
{
return
(
"
<
jemalloc
>
"
)
;
}
static
inline
void
pages_decommit
(
void
*
addr
size_t
size
)
{
#
ifdef
XP_WIN
size_t
pages_size
=
std
:
:
min
(
size
chunksize
-
CHUNK_ADDR2OFFSET
(
(
uintptr_t
)
addr
)
)
;
while
(
size
>
0
)
{
if
(
!
VirtualFree
(
addr
pages_size
MEM_DECOMMIT
)
)
MOZ_CRASH
(
)
;
addr
=
(
void
*
)
(
(
uintptr_t
)
addr
+
pages_size
)
;
size
-
=
pages_size
;
pages_size
=
std
:
:
min
(
size
chunksize
)
;
}
#
else
if
(
mmap
(
addr
size
PROT_NONE
MAP_FIXED
|
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
=
=
MAP_FAILED
)
MOZ_CRASH
(
)
;
MozTagAnonymousMemory
(
addr
size
"
jemalloc
-
decommitted
"
)
;
#
endif
}
static
inline
void
pages_commit
(
void
*
addr
size_t
size
)
{
#
ifdef
XP_WIN
size_t
pages_size
=
std
:
:
min
(
size
chunksize
-
CHUNK_ADDR2OFFSET
(
(
uintptr_t
)
addr
)
)
;
while
(
size
>
0
)
{
if
(
!
VirtualAlloc
(
addr
pages_size
MEM_COMMIT
PAGE_READWRITE
)
)
MOZ_CRASH
(
)
;
addr
=
(
void
*
)
(
(
uintptr_t
)
addr
+
pages_size
)
;
size
-
=
pages_size
;
pages_size
=
std
:
:
min
(
size
chunksize
)
;
}
#
else
if
(
mmap
(
addr
size
PROT_READ
|
PROT_WRITE
MAP_FIXED
|
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
=
=
MAP_FAILED
)
MOZ_CRASH
(
)
;
MozTagAnonymousMemory
(
addr
size
"
jemalloc
"
)
;
#
endif
}
static
bool
base_pages_alloc
(
size_t
minsize
)
{
size_t
csize
;
size_t
pminsize
;
MOZ_ASSERT
(
minsize
!
=
0
)
;
csize
=
CHUNK_CEILING
(
minsize
)
;
base_pages
=
chunk_alloc
(
csize
chunksize
true
)
;
if
(
!
base_pages
)
return
(
true
)
;
base_next_addr
=
base_pages
;
base_past_addr
=
(
void
*
)
(
(
uintptr_t
)
base_pages
+
csize
)
;
pminsize
=
PAGE_CEILING
(
minsize
)
;
base_next_decommitted
=
(
void
*
)
(
(
uintptr_t
)
base_pages
+
pminsize
)
;
#
if
defined
(
MALLOC_DECOMMIT
)
if
(
pminsize
<
csize
)
pages_decommit
(
base_next_decommitted
csize
-
pminsize
)
;
#
endif
base_mapped
+
=
csize
;
base_committed
+
=
pminsize
;
return
(
false
)
;
}
static
void
*
base_alloc
(
size_t
size
)
{
void
*
ret
;
size_t
csize
;
csize
=
CACHELINE_CEILING
(
size
)
;
malloc_mutex_lock
(
&
base_mtx
)
;
if
(
(
uintptr_t
)
base_next_addr
+
csize
>
(
uintptr_t
)
base_past_addr
)
{
if
(
base_pages_alloc
(
csize
)
)
{
malloc_mutex_unlock
(
&
base_mtx
)
;
return
nullptr
;
}
}
ret
=
base_next_addr
;
base_next_addr
=
(
void
*
)
(
(
uintptr_t
)
base_next_addr
+
csize
)
;
if
(
(
uintptr_t
)
base_next_addr
>
(
uintptr_t
)
base_next_decommitted
)
{
void
*
pbase_next_addr
=
(
void
*
)
(
PAGE_CEILING
(
(
uintptr_t
)
base_next_addr
)
)
;
#
ifdef
MALLOC_DECOMMIT
pages_commit
(
base_next_decommitted
(
uintptr_t
)
pbase_next_addr
-
(
uintptr_t
)
base_next_decommitted
)
;
#
endif
base_next_decommitted
=
pbase_next_addr
;
base_committed
+
=
(
uintptr_t
)
pbase_next_addr
-
(
uintptr_t
)
base_next_decommitted
;
}
malloc_mutex_unlock
(
&
base_mtx
)
;
return
(
ret
)
;
}
static
void
*
base_calloc
(
size_t
number
size_t
size
)
{
void
*
ret
;
ret
=
base_alloc
(
number
*
size
)
;
memset
(
ret
0
number
*
size
)
;
return
(
ret
)
;
}
static
extent_node_t
*
base_node_alloc
(
void
)
{
extent_node_t
*
ret
;
malloc_mutex_lock
(
&
base_mtx
)
;
if
(
base_nodes
)
{
ret
=
base_nodes
;
base_nodes
=
*
(
extent_node_t
*
*
)
ret
;
malloc_mutex_unlock
(
&
base_mtx
)
;
}
else
{
malloc_mutex_unlock
(
&
base_mtx
)
;
ret
=
(
extent_node_t
*
)
base_alloc
(
sizeof
(
extent_node_t
)
)
;
}
return
(
ret
)
;
}
static
void
base_node_dealloc
(
extent_node_t
*
node
)
{
malloc_mutex_lock
(
&
base_mtx
)
;
*
(
extent_node_t
*
*
)
node
=
base_nodes
;
base_nodes
=
node
;
malloc_mutex_unlock
(
&
base_mtx
)
;
}
static
inline
int
extent_szad_comp
(
extent_node_t
*
a
extent_node_t
*
b
)
{
int
ret
;
size_t
a_size
=
a
-
>
size
;
size_t
b_size
=
b
-
>
size
;
ret
=
(
a_size
>
b_size
)
-
(
a_size
<
b_size
)
;
if
(
ret
=
=
0
)
{
uintptr_t
a_addr
=
(
uintptr_t
)
a
-
>
addr
;
uintptr_t
b_addr
=
(
uintptr_t
)
b
-
>
addr
;
ret
=
(
a_addr
>
b_addr
)
-
(
a_addr
<
b_addr
)
;
}
return
(
ret
)
;
}
rb_wrap
(
static
extent_tree_szad_
extent_tree_t
extent_node_t
link_szad
extent_szad_comp
)
static
inline
int
extent_ad_comp
(
extent_node_t
*
a
extent_node_t
*
b
)
{
uintptr_t
a_addr
=
(
uintptr_t
)
a
-
>
addr
;
uintptr_t
b_addr
=
(
uintptr_t
)
b
-
>
addr
;
return
(
(
a_addr
>
b_addr
)
-
(
a_addr
<
b_addr
)
)
;
}
rb_wrap
(
static
extent_tree_ad_
extent_tree_t
extent_node_t
link_ad
extent_ad_comp
)
static
inline
int
extent_bounds_comp
(
extent_node_t
*
aKey
extent_node_t
*
aNode
)
{
uintptr_t
key_addr
=
(
uintptr_t
)
aKey
-
>
addr
;
uintptr_t
node_addr
=
(
uintptr_t
)
aNode
-
>
addr
;
size_t
node_size
=
aNode
-
>
size
;
if
(
node_addr
<
=
key_addr
&
&
key_addr
<
node_addr
+
node_size
)
{
return
0
;
}
return
(
(
key_addr
>
node_addr
)
-
(
key_addr
<
node_addr
)
)
;
}
static
extent_node_t
*
extent_tree_bounds_search
(
extent_tree_t
*
tree
extent_node_t
*
key
)
{
extent_node_t
*
ret
;
rb_search
(
extent_node_t
link_ad
extent_bounds_comp
tree
key
ret
)
;
return
ret
;
}
#
ifdef
XP_WIN
static
void
*
pages_map
(
void
*
addr
size_t
size
)
{
void
*
ret
=
nullptr
;
ret
=
VirtualAlloc
(
addr
size
MEM_COMMIT
|
MEM_RESERVE
PAGE_READWRITE
)
;
return
(
ret
)
;
}
static
void
pages_unmap
(
void
*
addr
size_t
size
)
{
if
(
VirtualFree
(
addr
0
MEM_RELEASE
)
=
=
0
)
{
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Error
in
VirtualFree
(
)
\
n
"
)
;
}
}
#
else
static
void
*
pages_map
(
void
*
addr
size_t
size
)
{
void
*
ret
;
#
if
defined
(
__ia64__
)
|
|
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
)
bool
check_placement
=
true
;
if
(
!
addr
)
{
addr
=
(
void
*
)
0x0000070000000000
;
check_placement
=
false
;
}
#
endif
#
if
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
const
uintptr_t
start
=
0x0000070000000000ULL
;
const
uintptr_t
end
=
0x0000800000000000ULL
;
uintptr_t
hint
;
void
*
region
=
MAP_FAILED
;
for
(
hint
=
start
;
region
=
=
MAP_FAILED
&
&
hint
+
size
<
=
end
;
hint
+
=
chunksize
)
{
region
=
mmap
(
(
void
*
)
hint
size
PROT_READ
|
PROT_WRITE
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
;
if
(
region
!
=
MAP_FAILED
)
{
if
(
(
(
size_t
)
region
+
(
size
-
1
)
)
&
0xffff800000000000
)
{
if
(
munmap
(
region
size
)
)
{
MOZ_ASSERT
(
errno
=
=
ENOMEM
)
;
}
region
=
MAP_FAILED
;
}
}
}
ret
=
region
;
#
else
ret
=
mmap
(
addr
size
PROT_READ
|
PROT_WRITE
MAP_PRIVATE
|
MAP_ANON
-
1
0
)
;
MOZ_ASSERT
(
ret
)
;
#
endif
if
(
ret
=
=
MAP_FAILED
)
{
ret
=
nullptr
;
}
#
if
defined
(
__ia64__
)
|
|
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
)
else
if
(
(
long
long
)
ret
&
0xffff800000000000
)
{
munmap
(
ret
size
)
;
ret
=
nullptr
;
}
else
if
(
check_placement
&
&
ret
!
=
addr
)
{
#
else
else
if
(
addr
&
&
ret
!
=
addr
)
{
#
endif
if
(
munmap
(
ret
size
)
=
=
-
1
)
{
char
buf
[
STRERROR_BUF
]
;
if
(
strerror_r
(
errno
buf
sizeof
(
buf
)
)
=
=
0
)
{
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Error
in
munmap
(
)
:
"
buf
"
\
n
"
)
;
}
}
ret
=
nullptr
;
}
if
(
ret
)
{
MozTagAnonymousMemory
(
ret
size
"
jemalloc
"
)
;
}
#
if
defined
(
__ia64__
)
|
|
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
)
MOZ_ASSERT
(
!
ret
|
|
(
!
check_placement
&
&
ret
)
|
|
(
check_placement
&
&
ret
=
=
addr
)
)
;
#
else
MOZ_ASSERT
(
!
ret
|
|
(
!
addr
&
&
ret
!
=
addr
)
|
|
(
addr
&
&
ret
=
=
addr
)
)
;
#
endif
return
(
ret
)
;
}
static
void
pages_unmap
(
void
*
addr
size_t
size
)
{
if
(
munmap
(
addr
size
)
=
=
-
1
)
{
char
buf
[
STRERROR_BUF
]
;
if
(
strerror_r
(
errno
buf
sizeof
(
buf
)
)
=
=
0
)
{
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Error
in
munmap
(
)
:
"
buf
"
\
n
"
)
;
}
}
}
#
endif
#
ifdef
XP_DARWIN
#
define
VM_COPY_MIN
(
pagesize
<
<
5
)
static
inline
void
pages_copy
(
void
*
dest
const
void
*
src
size_t
n
)
{
MOZ_ASSERT
(
(
void
*
)
(
(
uintptr_t
)
dest
&
~
pagesize_mask
)
=
=
dest
)
;
MOZ_ASSERT
(
n
>
=
VM_COPY_MIN
)
;
MOZ_ASSERT
(
(
void
*
)
(
(
uintptr_t
)
src
&
~
pagesize_mask
)
=
=
src
)
;
vm_copy
(
mach_task_self
(
)
(
vm_address_t
)
src
(
vm_size_t
)
n
(
vm_address_t
)
dest
)
;
}
#
endif
static
inline
malloc_rtree_t
*
malloc_rtree_new
(
unsigned
bits
)
{
malloc_rtree_t
*
ret
;
unsigned
bits_per_level
height
i
;
bits_per_level
=
ffs
(
pow2_ceil
(
(
MALLOC_RTREE_NODESIZE
/
sizeof
(
void
*
)
)
)
)
-
1
;
height
=
bits
/
bits_per_level
;
if
(
height
*
bits_per_level
!
=
bits
)
height
+
+
;
MOZ_DIAGNOSTIC_ASSERT
(
height
*
bits_per_level
>
=
bits
)
;
ret
=
(
malloc_rtree_t
*
)
base_calloc
(
1
sizeof
(
malloc_rtree_t
)
+
(
sizeof
(
unsigned
)
*
(
height
-
1
)
)
)
;
if
(
!
ret
)
return
nullptr
;
malloc_spin_init
(
&
ret
-
>
lock
)
;
ret
-
>
height
=
height
;
if
(
bits_per_level
*
height
>
bits
)
ret
-
>
level2bits
[
0
]
=
bits
%
bits_per_level
;
else
ret
-
>
level2bits
[
0
]
=
bits_per_level
;
for
(
i
=
1
;
i
<
height
;
i
+
+
)
ret
-
>
level2bits
[
i
]
=
bits_per_level
;
ret
-
>
root
=
(
void
*
*
)
base_calloc
(
1
sizeof
(
void
*
)
<
<
ret
-
>
level2bits
[
0
]
)
;
if
(
!
ret
-
>
root
)
{
return
nullptr
;
}
return
(
ret
)
;
}
#
define
MALLOC_RTREE_GET_GENERATE
(
f
)
\
static
inline
void
*
\
f
(
malloc_rtree_t
*
rtree
uintptr_t
key
)
\
{
\
void
*
ret
;
\
uintptr_t
subkey
;
\
unsigned
i
lshift
height
bits
;
\
void
*
*
node
*
*
child
;
\
\
MALLOC_RTREE_LOCK
(
&
rtree
-
>
lock
)
;
\
for
(
i
=
lshift
=
0
height
=
rtree
-
>
height
node
=
rtree
-
>
root
;
\
i
<
height
-
1
;
\
i
+
+
lshift
+
=
bits
node
=
child
)
{
\
bits
=
rtree
-
>
level2bits
[
i
]
;
\
subkey
=
(
key
<
<
lshift
)
>
>
(
(
SIZEOF_PTR
<
<
3
)
-
bits
)
;
\
child
=
(
void
*
*
)
node
[
subkey
]
;
\
if
(
!
child
)
{
\
MALLOC_RTREE_UNLOCK
(
&
rtree
-
>
lock
)
;
\
return
nullptr
;
\
}
\
}
\
\
/
*
\
*
node
is
a
leaf
so
it
contains
values
rather
than
node
\
*
pointers
.
\
*
/
\
bits
=
rtree
-
>
level2bits
[
i
]
;
\
subkey
=
(
key
<
<
lshift
)
>
>
(
(
SIZEOF_PTR
<
<
3
)
-
bits
)
;
\
ret
=
node
[
subkey
]
;
\
MALLOC_RTREE_UNLOCK
(
&
rtree
-
>
lock
)
;
\
\
MALLOC_RTREE_GET_VALIDATE
\
return
(
ret
)
;
\
}
#
ifdef
MOZ_DEBUG
#
define
MALLOC_RTREE_LOCK
(
l
)
malloc_spin_lock
(
l
)
#
define
MALLOC_RTREE_UNLOCK
(
l
)
malloc_spin_unlock
(
l
)
#
define
MALLOC_RTREE_GET_VALIDATE
MALLOC_RTREE_GET_GENERATE
(
malloc_rtree_get_locked
)
#
undef
MALLOC_RTREE_LOCK
#
undef
MALLOC_RTREE_UNLOCK
#
undef
MALLOC_RTREE_GET_VALIDATE
#
endif
#
define
MALLOC_RTREE_LOCK
(
l
)
#
define
MALLOC_RTREE_UNLOCK
(
l
)
#
ifdef
MOZ_DEBUG
#
define
MALLOC_RTREE_GET_VALIDATE
\
MOZ_ASSERT
(
malloc_rtree_get_locked
(
rtree
key
)
=
=
ret
)
;
#
else
#
define
MALLOC_RTREE_GET_VALIDATE
#
endif
MALLOC_RTREE_GET_GENERATE
(
malloc_rtree_get
)
#
undef
MALLOC_RTREE_LOCK
#
undef
MALLOC_RTREE_UNLOCK
#
undef
MALLOC_RTREE_GET_VALIDATE
static
inline
bool
malloc_rtree_set
(
malloc_rtree_t
*
rtree
uintptr_t
key
void
*
val
)
{
uintptr_t
subkey
;
unsigned
i
lshift
height
bits
;
void
*
*
node
*
*
child
;
malloc_spin_lock
(
&
rtree
-
>
lock
)
;
for
(
i
=
lshift
=
0
height
=
rtree
-
>
height
node
=
rtree
-
>
root
;
i
<
height
-
1
;
i
+
+
lshift
+
=
bits
node
=
child
)
{
bits
=
rtree
-
>
level2bits
[
i
]
;
subkey
=
(
key
<
<
lshift
)
>
>
(
(
SIZEOF_PTR
<
<
3
)
-
bits
)
;
child
=
(
void
*
*
)
node
[
subkey
]
;
if
(
!
child
)
{
child
=
(
void
*
*
)
base_calloc
(
1
sizeof
(
void
*
)
<
<
rtree
-
>
level2bits
[
i
+
1
]
)
;
if
(
!
child
)
{
malloc_spin_unlock
(
&
rtree
-
>
lock
)
;
return
(
true
)
;
}
node
[
subkey
]
=
child
;
}
}
bits
=
rtree
-
>
level2bits
[
i
]
;
subkey
=
(
key
<
<
lshift
)
>
>
(
(
SIZEOF_PTR
<
<
3
)
-
bits
)
;
node
[
subkey
]
=
val
;
malloc_spin_unlock
(
&
rtree
-
>
lock
)
;
return
(
false
)
;
}
#
define
ALIGNMENT_ADDR2OFFSET
(
a
alignment
)
\
(
(
size_t
)
(
(
uintptr_t
)
(
a
)
&
(
alignment
-
1
)
)
)
#
define
ALIGNMENT_CEILING
(
s
alignment
)
\
(
(
(
s
)
+
(
alignment
-
1
)
)
&
(
~
(
alignment
-
1
)
)
)
static
void
*
pages_trim
(
void
*
addr
size_t
alloc_size
size_t
leadsize
size_t
size
)
{
void
*
ret
=
(
void
*
)
(
(
uintptr_t
)
addr
+
leadsize
)
;
MOZ_ASSERT
(
alloc_size
>
=
leadsize
+
size
)
;
#
ifdef
XP_WIN
{
void
*
new_addr
;
pages_unmap
(
addr
alloc_size
)
;
new_addr
=
pages_map
(
ret
size
)
;
if
(
new_addr
=
=
ret
)
return
(
ret
)
;
if
(
new_addr
)
pages_unmap
(
new_addr
size
)
;
return
nullptr
;
}
#
else
{
size_t
trailsize
=
alloc_size
-
leadsize
-
size
;
if
(
leadsize
!
=
0
)
pages_unmap
(
addr
leadsize
)
;
if
(
trailsize
!
=
0
)
pages_unmap
(
(
void
*
)
(
(
uintptr_t
)
ret
+
size
)
trailsize
)
;
return
(
ret
)
;
}
#
endif
}
static
void
*
chunk_alloc_mmap_slow
(
size_t
size
size_t
alignment
)
{
void
*
ret
*
pages
;
size_t
alloc_size
leadsize
;
alloc_size
=
size
+
alignment
-
pagesize
;
if
(
alloc_size
<
size
)
return
nullptr
;
do
{
pages
=
pages_map
(
nullptr
alloc_size
)
;
if
(
!
pages
)
return
nullptr
;
leadsize
=
ALIGNMENT_CEILING
(
(
uintptr_t
)
pages
alignment
)
-
(
uintptr_t
)
pages
;
ret
=
pages_trim
(
pages
alloc_size
leadsize
size
)
;
}
while
(
!
ret
)
;
MOZ_ASSERT
(
ret
)
;
return
(
ret
)
;
}
static
void
*
chunk_alloc_mmap
(
size_t
size
size_t
alignment
)
{
void
*
ret
;
size_t
offset
;
ret
=
pages_map
(
nullptr
size
)
;
if
(
!
ret
)
return
nullptr
;
offset
=
ALIGNMENT_ADDR2OFFSET
(
ret
alignment
)
;
if
(
offset
!
=
0
)
{
pages_unmap
(
ret
size
)
;
return
(
chunk_alloc_mmap_slow
(
size
alignment
)
)
;
}
MOZ_ASSERT
(
ret
)
;
return
(
ret
)
;
}
static
bool
pages_purge
(
void
*
addr
size_t
length
bool
force_zero
)
{
#
ifdef
MALLOC_DECOMMIT
pages_decommit
(
addr
length
)
;
return
true
;
#
else
#
ifndef
XP_LINUX
if
(
force_zero
)
memset
(
addr
0
length
)
;
#
endif
#
ifdef
XP_WIN
size_t
pages_size
=
std
:
:
min
(
length
chunksize
-
CHUNK_ADDR2OFFSET
(
(
uintptr_t
)
addr
)
)
;
while
(
length
>
0
)
{
VirtualAlloc
(
addr
pages_size
MEM_RESET
PAGE_READWRITE
)
;
addr
=
(
void
*
)
(
(
uintptr_t
)
addr
+
pages_size
)
;
length
-
=
pages_size
;
pages_size
=
std
:
:
min
(
length
chunksize
)
;
}
return
force_zero
;
#
else
#
ifdef
XP_LINUX
#
define
JEMALLOC_MADV_PURGE
MADV_DONTNEED
#
define
JEMALLOC_MADV_ZEROS
true
#
else
#
define
JEMALLOC_MADV_PURGE
MADV_FREE
#
define
JEMALLOC_MADV_ZEROS
force_zero
#
endif
int
err
=
madvise
(
addr
length
JEMALLOC_MADV_PURGE
)
;
return
JEMALLOC_MADV_ZEROS
&
&
err
=
=
0
;
#
undef
JEMALLOC_MADV_PURGE
#
undef
JEMALLOC_MADV_ZEROS
#
endif
#
endif
}
static
void
*
chunk_recycle
(
extent_tree_t
*
chunks_szad
extent_tree_t
*
chunks_ad
size_t
size
size_t
alignment
bool
base
bool
*
zeroed
)
{
void
*
ret
;
extent_node_t
*
node
;
extent_node_t
key
;
size_t
alloc_size
leadsize
trailsize
;
ChunkType
chunk_type
;
if
(
base
)
{
return
nullptr
;
}
alloc_size
=
size
+
alignment
-
chunksize
;
if
(
alloc_size
<
size
)
return
nullptr
;
key
.
addr
=
nullptr
;
key
.
size
=
alloc_size
;
malloc_mutex_lock
(
&
chunks_mtx
)
;
node
=
extent_tree_szad_nsearch
(
chunks_szad
&
key
)
;
if
(
!
node
)
{
malloc_mutex_unlock
(
&
chunks_mtx
)
;
return
nullptr
;
}
leadsize
=
ALIGNMENT_CEILING
(
(
uintptr_t
)
node
-
>
addr
alignment
)
-
(
uintptr_t
)
node
-
>
addr
;
MOZ_ASSERT
(
node
-
>
size
>
=
leadsize
+
size
)
;
trailsize
=
node
-
>
size
-
leadsize
-
size
;
ret
=
(
void
*
)
(
(
uintptr_t
)
node
-
>
addr
+
leadsize
)
;
chunk_type
=
node
-
>
chunk_type
;
if
(
zeroed
)
{
*
zeroed
=
(
chunk_type
=
=
ZEROED_CHUNK
)
;
}
extent_tree_szad_remove
(
chunks_szad
node
)
;
extent_tree_ad_remove
(
chunks_ad
node
)
;
if
(
leadsize
!
=
0
)
{
node
-
>
size
=
leadsize
;
extent_tree_szad_insert
(
chunks_szad
node
)
;
extent_tree_ad_insert
(
chunks_ad
node
)
;
node
=
nullptr
;
}
if
(
trailsize
!
=
0
)
{
if
(
!
node
)
{
malloc_mutex_unlock
(
&
chunks_mtx
)
;
node
=
base_node_alloc
(
)
;
if
(
!
node
)
{
chunk_dealloc
(
ret
size
chunk_type
)
;
return
nullptr
;
}
malloc_mutex_lock
(
&
chunks_mtx
)
;
}
node
-
>
addr
=
(
void
*
)
(
(
uintptr_t
)
(
ret
)
+
size
)
;
node
-
>
size
=
trailsize
;
node
-
>
chunk_type
=
chunk_type
;
extent_tree_szad_insert
(
chunks_szad
node
)
;
extent_tree_ad_insert
(
chunks_ad
node
)
;
node
=
nullptr
;
}
recycled_size
-
=
size
;
malloc_mutex_unlock
(
&
chunks_mtx
)
;
if
(
node
)
base_node_dealloc
(
node
)
;
#
ifdef
MALLOC_DECOMMIT
pages_commit
(
ret
size
)
;
if
(
zeroed
)
{
*
zeroed
=
true
;
}
#
endif
return
(
ret
)
;
}
#
ifdef
XP_WIN
#
define
CAN_RECYCLE
(
size
)
(
size
=
=
chunksize
)
#
else
#
define
CAN_RECYCLE
(
size
)
true
#
endif
static
void
*
chunk_alloc
(
size_t
size
size_t
alignment
bool
base
bool
*
zeroed
)
{
void
*
ret
;
MOZ_ASSERT
(
size
!
=
0
)
;
MOZ_ASSERT
(
(
size
&
chunksize_mask
)
=
=
0
)
;
MOZ_ASSERT
(
alignment
!
=
0
)
;
MOZ_ASSERT
(
(
alignment
&
chunksize_mask
)
=
=
0
)
;
if
(
CAN_RECYCLE
(
size
)
)
{
ret
=
chunk_recycle
(
&
chunks_szad_mmap
&
chunks_ad_mmap
size
alignment
base
zeroed
)
;
if
(
ret
)
goto
RETURN
;
}
ret
=
chunk_alloc_mmap
(
size
alignment
)
;
if
(
zeroed
)
*
zeroed
=
true
;
if
(
ret
)
{
goto
RETURN
;
}
ret
=
nullptr
;
RETURN
:
if
(
ret
&
&
base
=
=
false
)
{
if
(
malloc_rtree_set
(
chunk_rtree
(
uintptr_t
)
ret
ret
)
)
{
chunk_dealloc
(
ret
size
UNKNOWN_CHUNK
)
;
return
nullptr
;
}
}
MOZ_ASSERT
(
CHUNK_ADDR2BASE
(
ret
)
=
=
ret
)
;
return
(
ret
)
;
}
static
void
chunk_ensure_zero
(
void
*
ptr
size_t
size
bool
zeroed
)
{
if
(
zeroed
=
=
false
)
memset
(
ptr
0
size
)
;
#
ifdef
MOZ_DEBUG
else
{
size_t
i
;
size_t
*
p
=
(
size_t
*
)
(
uintptr_t
)
ret
;
for
(
i
=
0
;
i
<
size
/
sizeof
(
size_t
)
;
i
+
+
)
MOZ_ASSERT
(
p
[
i
]
=
=
0
)
;
}
#
endif
}
static
void
chunk_record
(
extent_tree_t
*
chunks_szad
extent_tree_t
*
chunks_ad
void
*
chunk
size_t
size
ChunkType
chunk_type
)
{
extent_node_t
*
xnode
*
node
*
prev
*
xprev
key
;
if
(
chunk_type
!
=
ZEROED_CHUNK
)
{
if
(
pages_purge
(
chunk
size
chunk_type
=
=
HUGE_CHUNK
)
)
{
chunk_type
=
ZEROED_CHUNK
;
}
}
xnode
=
base_node_alloc
(
)
;
xprev
=
nullptr
;
malloc_mutex_lock
(
&
chunks_mtx
)
;
key
.
addr
=
(
void
*
)
(
(
uintptr_t
)
chunk
+
size
)
;
node
=
extent_tree_ad_nsearch
(
chunks_ad
&
key
)
;
if
(
node
&
&
node
-
>
addr
=
=
key
.
addr
)
{
extent_tree_szad_remove
(
chunks_szad
node
)
;
node
-
>
addr
=
chunk
;
node
-
>
size
+
=
size
;
if
(
node
-
>
chunk_type
!
=
chunk_type
)
{
node
-
>
chunk_type
=
RECYCLED_CHUNK
;
}
extent_tree_szad_insert
(
chunks_szad
node
)
;
}
else
{
if
(
!
xnode
)
{
goto
label_return
;
}
node
=
xnode
;
xnode
=
nullptr
;
node
-
>
addr
=
chunk
;
node
-
>
size
=
size
;
node
-
>
chunk_type
=
chunk_type
;
extent_tree_ad_insert
(
chunks_ad
node
)
;
extent_tree_szad_insert
(
chunks_szad
node
)
;
}
prev
=
extent_tree_ad_prev
(
chunks_ad
node
)
;
if
(
prev
&
&
(
void
*
)
(
(
uintptr_t
)
prev
-
>
addr
+
prev
-
>
size
)
=
=
chunk
)
{
extent_tree_szad_remove
(
chunks_szad
prev
)
;
extent_tree_ad_remove
(
chunks_ad
prev
)
;
extent_tree_szad_remove
(
chunks_szad
node
)
;
node
-
>
addr
=
prev
-
>
addr
;
node
-
>
size
+
=
prev
-
>
size
;
if
(
node
-
>
chunk_type
!
=
prev
-
>
chunk_type
)
{
node
-
>
chunk_type
=
RECYCLED_CHUNK
;
}
extent_tree_szad_insert
(
chunks_szad
node
)
;
xprev
=
prev
;
}
recycled_size
+
=
size
;
label_return
:
malloc_mutex_unlock
(
&
chunks_mtx
)
;
if
(
xnode
)
base_node_dealloc
(
xnode
)
;
if
(
xprev
)
base_node_dealloc
(
xprev
)
;
}
static
void
chunk_dealloc
(
void
*
chunk
size_t
size
ChunkType
type
)
{
MOZ_ASSERT
(
chunk
)
;
MOZ_ASSERT
(
CHUNK_ADDR2BASE
(
chunk
)
=
=
chunk
)
;
MOZ_ASSERT
(
size
!
=
0
)
;
MOZ_ASSERT
(
(
size
&
chunksize_mask
)
=
=
0
)
;
malloc_rtree_set
(
chunk_rtree
(
uintptr_t
)
chunk
nullptr
)
;
if
(
CAN_RECYCLE
(
size
)
)
{
size_t
recycled_so_far
=
load_acquire_z
(
&
recycled_size
)
;
if
(
recycled_so_far
<
recycle_limit
)
{
size_t
recycle_remaining
=
recycle_limit
-
recycled_so_far
;
size_t
to_recycle
;
if
(
size
>
recycle_remaining
)
{
to_recycle
=
recycle_remaining
;
pages_trim
(
chunk
size
0
to_recycle
)
;
}
else
{
to_recycle
=
size
;
}
chunk_record
(
&
chunks_szad_mmap
&
chunks_ad_mmap
chunk
to_recycle
type
)
;
return
;
}
}
pages_unmap
(
chunk
size
)
;
}
#
undef
CAN_RECYCLE
static
inline
arena_t
*
thread_local_arena
(
bool
enabled
)
{
#
ifndef
NO_TLS
arena_t
*
arena
;
if
(
enabled
)
{
arena
=
arenas_extend
(
)
;
}
else
{
malloc_spin_lock
(
&
arenas_lock
)
;
arena
=
arenas
[
0
]
;
malloc_spin_unlock
(
&
arenas_lock
)
;
}
thread_arena
.
set
(
arena
)
;
return
arena
;
#
else
return
arenas
[
0
]
;
#
endif
}
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_thread_local_arena
(
bool
aEnabled
)
{
thread_local_arena
(
aEnabled
)
;
}
static
inline
arena_t
*
choose_arena
(
size_t
size
)
{
arena_t
*
ret
=
nullptr
;
#
ifndef
NO_TLS
if
(
size
<
=
small_max
)
{
ret
=
thread_arena
.
get
(
)
;
}
if
(
!
ret
)
{
ret
=
thread_local_arena
(
false
)
;
}
#
else
ret
=
arenas
[
0
]
;
#
endif
MOZ_DIAGNOSTIC_ASSERT
(
ret
)
;
return
(
ret
)
;
}
static
inline
int
arena_chunk_comp
(
arena_chunk_t
*
a
arena_chunk_t
*
b
)
{
uintptr_t
a_chunk
=
(
uintptr_t
)
a
;
uintptr_t
b_chunk
=
(
uintptr_t
)
b
;
MOZ_ASSERT
(
a
)
;
MOZ_ASSERT
(
b
)
;
return
(
(
a_chunk
>
b_chunk
)
-
(
a_chunk
<
b_chunk
)
)
;
}
rb_wrap
(
static
arena_chunk_tree_dirty_
arena_chunk_tree_t
arena_chunk_t
link_dirty
arena_chunk_comp
)
static
inline
int
arena_run_comp
(
arena_chunk_map_t
*
a
arena_chunk_map_t
*
b
)
{
uintptr_t
a_mapelm
=
(
uintptr_t
)
a
;
uintptr_t
b_mapelm
=
(
uintptr_t
)
b
;
MOZ_ASSERT
(
a
)
;
MOZ_ASSERT
(
b
)
;
return
(
(
a_mapelm
>
b_mapelm
)
-
(
a_mapelm
<
b_mapelm
)
)
;
}
rb_wrap
(
static
arena_run_tree_
arena_run_tree_t
arena_chunk_map_t
link
arena_run_comp
)
static
inline
int
arena_avail_comp
(
arena_chunk_map_t
*
a
arena_chunk_map_t
*
b
)
{
int
ret
;
size_t
a_size
=
a
-
>
bits
&
~
pagesize_mask
;
size_t
b_size
=
b
-
>
bits
&
~
pagesize_mask
;
ret
=
(
a_size
>
b_size
)
-
(
a_size
<
b_size
)
;
if
(
ret
=
=
0
)
{
uintptr_t
a_mapelm
b_mapelm
;
if
(
(
a
-
>
bits
&
CHUNK_MAP_KEY
)
=
=
0
)
a_mapelm
=
(
uintptr_t
)
a
;
else
{
a_mapelm
=
0
;
}
b_mapelm
=
(
uintptr_t
)
b
;
ret
=
(
a_mapelm
>
b_mapelm
)
-
(
a_mapelm
<
b_mapelm
)
;
}
return
(
ret
)
;
}
rb_wrap
(
static
arena_avail_tree_
arena_avail_tree_t
arena_chunk_map_t
link
arena_avail_comp
)
static
inline
void
*
arena_run_reg_alloc
(
arena_run_t
*
run
arena_bin_t
*
bin
)
{
void
*
ret
;
unsigned
i
mask
bit
regind
;
MOZ_ASSERT
(
run
-
>
magic
=
=
ARENA_RUN_MAGIC
)
;
MOZ_ASSERT
(
run
-
>
regs_minelm
<
bin
-
>
regs_mask_nelms
)
;
i
=
run
-
>
regs_minelm
;
mask
=
run
-
>
regs_mask
[
i
]
;
if
(
mask
!
=
0
)
{
bit
=
ffs
(
(
int
)
mask
)
-
1
;
regind
=
(
(
i
<
<
(
SIZEOF_INT_2POW
+
3
)
)
+
bit
)
;
MOZ_ASSERT
(
regind
<
bin
-
>
nregs
)
;
ret
=
(
void
*
)
(
(
(
uintptr_t
)
run
)
+
bin
-
>
reg0_offset
+
(
bin
-
>
reg_size
*
regind
)
)
;
mask
^
=
(
1U
<
<
bit
)
;
run
-
>
regs_mask
[
i
]
=
mask
;
return
(
ret
)
;
}
for
(
i
+
+
;
i
<
bin
-
>
regs_mask_nelms
;
i
+
+
)
{
mask
=
run
-
>
regs_mask
[
i
]
;
if
(
mask
!
=
0
)
{
bit
=
ffs
(
(
int
)
mask
)
-
1
;
regind
=
(
(
i
<
<
(
SIZEOF_INT_2POW
+
3
)
)
+
bit
)
;
MOZ_ASSERT
(
regind
<
bin
-
>
nregs
)
;
ret
=
(
void
*
)
(
(
(
uintptr_t
)
run
)
+
bin
-
>
reg0_offset
+
(
bin
-
>
reg_size
*
regind
)
)
;
mask
^
=
(
1U
<
<
bit
)
;
run
-
>
regs_mask
[
i
]
=
mask
;
run
-
>
regs_minelm
=
i
;
return
(
ret
)
;
}
}
MOZ_DIAGNOSTIC_ASSERT
(
0
)
;
return
nullptr
;
}
static
inline
void
arena_run_reg_dalloc
(
arena_run_t
*
run
arena_bin_t
*
bin
void
*
ptr
size_t
size
)
{
#
define
SIZE_INV_SHIFT
21
#
define
SIZE_INV
(
s
)
(
(
(
1U
<
<
SIZE_INV_SHIFT
)
/
(
s
<
<
QUANTUM_2POW_MIN
)
)
+
1
)
static
const
unsigned
size_invs
[
]
=
{
SIZE_INV
(
3
)
SIZE_INV
(
4
)
SIZE_INV
(
5
)
SIZE_INV
(
6
)
SIZE_INV
(
7
)
SIZE_INV
(
8
)
SIZE_INV
(
9
)
SIZE_INV
(
10
)
SIZE_INV
(
11
)
SIZE_INV
(
12
)
SIZE_INV
(
13
)
SIZE_INV
(
14
)
SIZE_INV
(
15
)
SIZE_INV
(
16
)
SIZE_INV
(
17
)
SIZE_INV
(
18
)
SIZE_INV
(
19
)
SIZE_INV
(
20
)
SIZE_INV
(
21
)
SIZE_INV
(
22
)
SIZE_INV
(
23
)
SIZE_INV
(
24
)
SIZE_INV
(
25
)
SIZE_INV
(
26
)
SIZE_INV
(
27
)
SIZE_INV
(
28
)
SIZE_INV
(
29
)
SIZE_INV
(
30
)
SIZE_INV
(
31
)
#
if
(
QUANTUM_2POW_MIN
<
4
)
SIZE_INV
(
32
)
SIZE_INV
(
33
)
SIZE_INV
(
34
)
SIZE_INV
(
35
)
SIZE_INV
(
36
)
SIZE_INV
(
37
)
SIZE_INV
(
38
)
SIZE_INV
(
39
)
SIZE_INV
(
40
)
SIZE_INV
(
41
)
SIZE_INV
(
42
)
SIZE_INV
(
43
)
SIZE_INV
(
44
)
SIZE_INV
(
45
)
SIZE_INV
(
46
)
SIZE_INV
(
47
)
SIZE_INV
(
48
)
SIZE_INV
(
49
)
SIZE_INV
(
50
)
SIZE_INV
(
51
)
SIZE_INV
(
52
)
SIZE_INV
(
53
)
SIZE_INV
(
54
)
SIZE_INV
(
55
)
SIZE_INV
(
56
)
SIZE_INV
(
57
)
SIZE_INV
(
58
)
SIZE_INV
(
59
)
SIZE_INV
(
60
)
SIZE_INV
(
61
)
SIZE_INV
(
62
)
SIZE_INV
(
63
)
#
endif
}
;
unsigned
diff
regind
elm
bit
;
MOZ_ASSERT
(
run
-
>
magic
=
=
ARENA_RUN_MAGIC
)
;
MOZ_ASSERT
(
(
(
sizeof
(
size_invs
)
)
/
sizeof
(
unsigned
)
)
+
3
>
=
(
SMALL_MAX_DEFAULT
>
>
QUANTUM_2POW_MIN
)
)
;
diff
=
(
unsigned
)
(
(
uintptr_t
)
ptr
-
(
uintptr_t
)
run
-
bin
-
>
reg0_offset
)
;
if
(
(
size
&
(
size
-
1
)
)
=
=
0
)
{
static
const
unsigned
char
log2_table
[
]
=
{
0
1
0
2
0
0
0
3
0
0
0
0
0
0
0
4
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
5
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
6
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
7
}
;
if
(
size
<
=
128
)
regind
=
(
diff
>
>
log2_table
[
size
-
1
]
)
;
else
if
(
size
<
=
32768
)
regind
=
diff
>
>
(
8
+
log2_table
[
(
size
>
>
8
)
-
1
]
)
;
else
{
regind
=
diff
/
size
;
}
}
else
if
(
size
<
=
(
(
sizeof
(
size_invs
)
/
sizeof
(
unsigned
)
)
<
<
QUANTUM_2POW_MIN
)
+
2
)
{
regind
=
size_invs
[
(
size
>
>
QUANTUM_2POW_MIN
)
-
3
]
*
diff
;
regind
>
>
=
SIZE_INV_SHIFT
;
}
else
{
regind
=
diff
/
size
;
}
;
MOZ_DIAGNOSTIC_ASSERT
(
diff
=
=
regind
*
size
)
;
MOZ_DIAGNOSTIC_ASSERT
(
regind
<
bin
-
>
nregs
)
;
elm
=
regind
>
>
(
SIZEOF_INT_2POW
+
3
)
;
if
(
elm
<
run
-
>
regs_minelm
)
run
-
>
regs_minelm
=
elm
;
bit
=
regind
-
(
elm
<
<
(
SIZEOF_INT_2POW
+
3
)
)
;
MOZ_DIAGNOSTIC_ASSERT
(
(
run
-
>
regs_mask
[
elm
]
&
(
1U
<
<
bit
)
)
=
=
0
)
;
run
-
>
regs_mask
[
elm
]
|
=
(
1U
<
<
bit
)
;
#
undef
SIZE_INV
#
undef
SIZE_INV_SHIFT
}
static
void
arena_run_split
(
arena_t
*
arena
arena_run_t
*
run
size_t
size
bool
large
bool
zero
)
{
arena_chunk_t
*
chunk
;
size_t
old_ndirty
run_ind
total_pages
need_pages
rem_pages
i
;
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
run
)
;
old_ndirty
=
chunk
-
>
ndirty
;
run_ind
=
(
unsigned
)
(
(
(
uintptr_t
)
run
-
(
uintptr_t
)
chunk
)
>
>
pagesize_2pow
)
;
total_pages
=
(
chunk
-
>
map
[
run_ind
]
.
bits
&
~
pagesize_mask
)
>
>
pagesize_2pow
;
need_pages
=
(
size
>
>
pagesize_2pow
)
;
MOZ_ASSERT
(
need_pages
>
0
)
;
MOZ_ASSERT
(
need_pages
<
=
total_pages
)
;
rem_pages
=
total_pages
-
need_pages
;
arena_avail_tree_remove
(
&
arena
-
>
mRunsAvail
&
chunk
-
>
map
[
run_ind
]
)
;
if
(
rem_pages
>
0
)
{
chunk
-
>
map
[
run_ind
+
need_pages
]
.
bits
=
(
rem_pages
<
<
pagesize_2pow
)
|
(
chunk
-
>
map
[
run_ind
+
need_pages
]
.
bits
&
pagesize_mask
)
;
chunk
-
>
map
[
run_ind
+
total_pages
-
1
]
.
bits
=
(
rem_pages
<
<
pagesize_2pow
)
|
(
chunk
-
>
map
[
run_ind
+
total_pages
-
1
]
.
bits
&
pagesize_mask
)
;
arena_avail_tree_insert
(
&
arena
-
>
mRunsAvail
&
chunk
-
>
map
[
run_ind
+
need_pages
]
)
;
}
for
(
i
=
0
;
i
<
need_pages
;
i
+
+
)
{
if
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_MADVISED_OR_DECOMMITTED
)
{
size_t
j
;
for
(
j
=
0
;
i
+
j
<
need_pages
&
&
(
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
CHUNK_MAP_MADVISED_OR_DECOMMITTED
)
;
j
+
+
)
{
MOZ_ASSERT
(
!
(
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
CHUNK_MAP_DECOMMITTED
&
&
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
CHUNK_MAP_MADVISED
)
)
;
chunk
-
>
map
[
run_ind
+
i
+
j
]
.
bits
&
=
~
CHUNK_MAP_MADVISED_OR_DECOMMITTED
;
}
#
ifdef
MALLOC_DECOMMIT
pages_commit
(
(
void
*
)
(
(
uintptr_t
)
chunk
+
(
(
run_ind
+
i
)
<
<
pagesize_2pow
)
)
(
j
<
<
pagesize_2pow
)
)
;
#
endif
arena
-
>
mStats
.
committed
+
=
j
;
#
ifndef
MALLOC_DECOMMIT
}
#
else
}
else
/
*
No
need
to
zero
since
commit
zeros
.
*
/
#
endif
if
(
zero
)
{
if
(
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_ZEROED
)
=
=
0
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
chunk
+
(
(
run_ind
+
i
)
<
<
pagesize_2pow
)
)
0
pagesize
)
;
}
}
if
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_DIRTY
)
{
chunk
-
>
ndirty
-
-
;
arena
-
>
mNumDirty
-
-
;
}
if
(
large
)
{
chunk
-
>
map
[
run_ind
+
i
]
.
bits
=
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
}
else
{
chunk
-
>
map
[
run_ind
+
i
]
.
bits
=
(
size_t
)
run
|
CHUNK_MAP_ALLOCATED
;
}
}
if
(
large
)
chunk
-
>
map
[
run_ind
]
.
bits
|
=
size
;
if
(
chunk
-
>
ndirty
=
=
0
&
&
old_ndirty
>
0
)
arena_chunk_tree_dirty_remove
(
&
arena
-
>
mChunksDirty
chunk
)
;
}
void
arena_t
:
:
InitChunk
(
arena_chunk_t
*
aChunk
bool
aZeroed
)
{
size_t
i
;
size_t
flags
=
aZeroed
?
CHUNK_MAP_DECOMMITTED
|
CHUNK_MAP_ZEROED
:
CHUNK_MAP_MADVISED
;
mStats
.
mapped
+
=
chunksize
;
aChunk
-
>
arena
=
this
;
aChunk
-
>
ndirty
=
0
;
#
ifdef
MALLOC_DECOMMIT
arena_run_t
*
run
=
(
arena_run_t
*
)
(
uintptr_t
(
aChunk
)
+
(
arena_chunk_header_npages
<
<
pagesize_2pow
)
)
;
#
endif
for
(
i
=
0
;
i
<
arena_chunk_header_npages
;
i
+
+
)
{
aChunk
-
>
map
[
i
]
.
bits
=
0
;
}
aChunk
-
>
map
[
i
]
.
bits
=
arena_maxclass
|
flags
;
for
(
i
+
+
;
i
<
chunk_npages
-
1
;
i
+
+
)
{
aChunk
-
>
map
[
i
]
.
bits
=
flags
;
}
aChunk
-
>
map
[
chunk_npages
-
1
]
.
bits
=
arena_maxclass
|
flags
;
#
ifdef
MALLOC_DECOMMIT
pages_decommit
(
run
arena_maxclass
)
;
#
endif
mStats
.
committed
+
=
arena_chunk_header_npages
;
arena_avail_tree_insert
(
&
mRunsAvail
&
aChunk
-
>
map
[
arena_chunk_header_npages
]
)
;
#
ifdef
MALLOC_DOUBLE_PURGE
new
(
&
aChunk
-
>
chunks_madvised_elem
)
mozilla
:
:
DoublyLinkedListElement
<
arena_chunk_t
>
(
)
;
#
endif
}
void
arena_t
:
:
DeallocChunk
(
arena_chunk_t
*
aChunk
)
{
if
(
mSpare
)
{
if
(
mSpare
-
>
ndirty
>
0
)
{
arena_chunk_tree_dirty_remove
(
&
aChunk
-
>
arena
-
>
mChunksDirty
mSpare
)
;
mNumDirty
-
=
mSpare
-
>
ndirty
;
mStats
.
committed
-
=
mSpare
-
>
ndirty
;
}
#
ifdef
MALLOC_DOUBLE_PURGE
if
(
mChunksMAdvised
.
ElementProbablyInList
(
mSpare
)
)
{
mChunksMAdvised
.
remove
(
mSpare
)
;
}
#
endif
chunk_dealloc
(
(
void
*
)
mSpare
chunksize
ARENA_CHUNK
)
;
mStats
.
mapped
-
=
chunksize
;
mStats
.
committed
-
=
arena_chunk_header_npages
;
}
arena_avail_tree_remove
(
&
mRunsAvail
&
aChunk
-
>
map
[
arena_chunk_header_npages
]
)
;
mSpare
=
aChunk
;
}
arena_run_t
*
arena_t
:
:
AllocRun
(
arena_bin_t
*
aBin
size_t
aSize
bool
aLarge
bool
aZero
)
{
arena_run_t
*
run
;
arena_chunk_map_t
*
mapelm
;
arena_chunk_map_t
key
;
MOZ_ASSERT
(
aSize
<
=
arena_maxclass
)
;
MOZ_ASSERT
(
(
aSize
&
pagesize_mask
)
=
=
0
)
;
key
.
bits
=
aSize
|
CHUNK_MAP_KEY
;
mapelm
=
arena_avail_tree_nsearch
(
&
mRunsAvail
&
key
)
;
if
(
mapelm
)
{
arena_chunk_t
*
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
mapelm
)
;
size_t
pageind
=
(
uintptr_t
(
mapelm
)
-
uintptr_t
(
chunk
-
>
map
)
)
/
sizeof
(
arena_chunk_map_t
)
;
run
=
(
arena_run_t
*
)
(
uintptr_t
(
chunk
)
+
(
pageind
<
<
pagesize_2pow
)
)
;
arena_run_split
(
this
run
aSize
aLarge
aZero
)
;
return
run
;
}
if
(
mSpare
)
{
arena_chunk_t
*
chunk
=
mSpare
;
mSpare
=
nullptr
;
run
=
(
arena_run_t
*
)
(
uintptr_t
(
chunk
)
+
(
arena_chunk_header_npages
<
<
pagesize_2pow
)
)
;
arena_avail_tree_insert
(
&
mRunsAvail
&
chunk
-
>
map
[
arena_chunk_header_npages
]
)
;
arena_run_split
(
this
run
aSize
aLarge
aZero
)
;
return
run
;
}
{
bool
zeroed
;
arena_chunk_t
*
chunk
=
(
arena_chunk_t
*
)
chunk_alloc
(
chunksize
chunksize
false
&
zeroed
)
;
if
(
!
chunk
)
{
return
nullptr
;
}
InitChunk
(
chunk
zeroed
)
;
run
=
(
arena_run_t
*
)
(
uintptr_t
(
chunk
)
+
(
arena_chunk_header_npages
<
<
pagesize_2pow
)
)
;
}
arena_run_split
(
this
run
aSize
aLarge
aZero
)
;
return
run
;
}
void
arena_t
:
:
Purge
(
bool
aAll
)
{
arena_chunk_t
*
chunk
;
size_t
i
npages
;
size_t
dirty_max
=
aAll
?
1
:
mMaxDirty
;
#
ifdef
MOZ_DEBUG
size_t
ndirty
=
0
;
rb_foreach_begin
(
arena_chunk_t
link_dirty
&
mChunksDirty
chunk
)
{
ndirty
+
=
chunk
-
>
ndirty
;
}
rb_foreach_end
(
arena_chunk_t
link_dirty
&
mChunksDirty
chunk
)
MOZ_ASSERT
(
ndirty
=
=
mNumDirty
)
;
#
endif
MOZ_DIAGNOSTIC_ASSERT
(
aAll
|
|
(
mNumDirty
>
mMaxDirty
)
)
;
while
(
mNumDirty
>
(
dirty_max
>
>
1
)
)
{
#
ifdef
MALLOC_DOUBLE_PURGE
bool
madvised
=
false
;
#
endif
chunk
=
arena_chunk_tree_dirty_last
(
&
mChunksDirty
)
;
MOZ_DIAGNOSTIC_ASSERT
(
chunk
)
;
for
(
i
=
chunk_npages
-
1
;
chunk
-
>
ndirty
>
0
;
i
-
-
)
{
MOZ_DIAGNOSTIC_ASSERT
(
i
>
=
arena_chunk_header_npages
)
;
if
(
chunk
-
>
map
[
i
]
.
bits
&
CHUNK_MAP_DIRTY
)
{
#
ifdef
MALLOC_DECOMMIT
const
size_t
free_operation
=
CHUNK_MAP_DECOMMITTED
;
#
else
const
size_t
free_operation
=
CHUNK_MAP_MADVISED
;
#
endif
MOZ_ASSERT
(
(
chunk
-
>
map
[
i
]
.
bits
&
CHUNK_MAP_MADVISED_OR_DECOMMITTED
)
=
=
0
)
;
chunk
-
>
map
[
i
]
.
bits
^
=
free_operation
|
CHUNK_MAP_DIRTY
;
for
(
npages
=
1
;
i
>
arena_chunk_header_npages
&
&
(
chunk
-
>
map
[
i
-
1
]
.
bits
&
CHUNK_MAP_DIRTY
)
;
npages
+
+
)
{
i
-
-
;
MOZ_ASSERT
(
(
chunk
-
>
map
[
i
]
.
bits
&
CHUNK_MAP_MADVISED_OR_DECOMMITTED
)
=
=
0
)
;
chunk
-
>
map
[
i
]
.
bits
^
=
free_operation
|
CHUNK_MAP_DIRTY
;
}
chunk
-
>
ndirty
-
=
npages
;
mNumDirty
-
=
npages
;
#
ifdef
MALLOC_DECOMMIT
pages_decommit
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
i
<
<
pagesize_2pow
)
)
(
npages
<
<
pagesize_2pow
)
)
;
#
endif
mStats
.
committed
-
=
npages
;
#
ifndef
MALLOC_DECOMMIT
madvise
(
(
void
*
)
(
uintptr_t
(
chunk
)
+
(
i
<
<
pagesize_2pow
)
)
(
npages
<
<
pagesize_2pow
)
MADV_FREE
)
;
#
ifdef
MALLOC_DOUBLE_PURGE
madvised
=
true
;
#
endif
#
endif
if
(
mNumDirty
<
=
(
dirty_max
>
>
1
)
)
{
break
;
}
}
}
if
(
chunk
-
>
ndirty
=
=
0
)
{
arena_chunk_tree_dirty_remove
(
&
mChunksDirty
chunk
)
;
}
#
ifdef
MALLOC_DOUBLE_PURGE
if
(
madvised
)
{
if
(
mChunksMAdvised
.
ElementProbablyInList
(
chunk
)
)
{
mChunksMAdvised
.
remove
(
chunk
)
;
}
mChunksMAdvised
.
pushFront
(
chunk
)
;
}
#
endif
}
}
void
arena_t
:
:
DallocRun
(
arena_run_t
*
aRun
bool
aDirty
)
{
arena_chunk_t
*
chunk
;
size_t
size
run_ind
run_pages
;
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
aRun
)
;
run_ind
=
(
size_t
)
(
(
uintptr_t
(
aRun
)
-
uintptr_t
(
chunk
)
)
>
>
pagesize_2pow
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run_ind
>
=
arena_chunk_header_npages
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run_ind
<
chunk_npages
)
;
if
(
(
chunk
-
>
map
[
run_ind
]
.
bits
&
CHUNK_MAP_LARGE
)
!
=
0
)
size
=
chunk
-
>
map
[
run_ind
]
.
bits
&
~
pagesize_mask
;
else
size
=
aRun
-
>
bin
-
>
run_size
;
run_pages
=
(
size
>
>
pagesize_2pow
)
;
if
(
aDirty
)
{
size_t
i
;
for
(
i
=
0
;
i
<
run_pages
;
i
+
+
)
{
MOZ_DIAGNOSTIC_ASSERT
(
(
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
CHUNK_MAP_DIRTY
)
=
=
0
)
;
chunk
-
>
map
[
run_ind
+
i
]
.
bits
=
CHUNK_MAP_DIRTY
;
}
if
(
chunk
-
>
ndirty
=
=
0
)
{
arena_chunk_tree_dirty_insert
(
&
mChunksDirty
chunk
)
;
}
chunk
-
>
ndirty
+
=
run_pages
;
mNumDirty
+
=
run_pages
;
}
else
{
size_t
i
;
for
(
i
=
0
;
i
<
run_pages
;
i
+
+
)
{
chunk
-
>
map
[
run_ind
+
i
]
.
bits
&
=
~
(
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
)
;
}
}
chunk
-
>
map
[
run_ind
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
]
.
bits
&
pagesize_mask
)
;
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
pagesize_mask
)
;
if
(
run_ind
+
run_pages
<
chunk_npages
&
&
(
chunk
-
>
map
[
run_ind
+
run_pages
]
.
bits
&
CHUNK_MAP_ALLOCATED
)
=
=
0
)
{
size_t
nrun_size
=
chunk
-
>
map
[
run_ind
+
run_pages
]
.
bits
&
~
pagesize_mask
;
arena_avail_tree_remove
(
&
mRunsAvail
&
chunk
-
>
map
[
run_ind
+
run_pages
]
)
;
size
+
=
nrun_size
;
run_pages
=
size
>
>
pagesize_2pow
;
MOZ_DIAGNOSTIC_ASSERT
(
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
~
pagesize_mask
)
=
=
nrun_size
)
;
chunk
-
>
map
[
run_ind
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
]
.
bits
&
pagesize_mask
)
;
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
pagesize_mask
)
;
}
if
(
run_ind
>
arena_chunk_header_npages
&
&
(
chunk
-
>
map
[
run_ind
-
1
]
.
bits
&
CHUNK_MAP_ALLOCATED
)
=
=
0
)
{
size_t
prun_size
=
chunk
-
>
map
[
run_ind
-
1
]
.
bits
&
~
pagesize_mask
;
run_ind
-
=
prun_size
>
>
pagesize_2pow
;
arena_avail_tree_remove
(
&
mRunsAvail
&
chunk
-
>
map
[
run_ind
]
)
;
size
+
=
prun_size
;
run_pages
=
size
>
>
pagesize_2pow
;
MOZ_DIAGNOSTIC_ASSERT
(
(
chunk
-
>
map
[
run_ind
]
.
bits
&
~
pagesize_mask
)
=
=
prun_size
)
;
chunk
-
>
map
[
run_ind
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
]
.
bits
&
pagesize_mask
)
;
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
=
size
|
(
chunk
-
>
map
[
run_ind
+
run_pages
-
1
]
.
bits
&
pagesize_mask
)
;
}
arena_avail_tree_insert
(
&
mRunsAvail
&
chunk
-
>
map
[
run_ind
]
)
;
if
(
(
chunk
-
>
map
[
arena_chunk_header_npages
]
.
bits
&
(
~
pagesize_mask
|
CHUNK_MAP_ALLOCATED
)
)
=
=
arena_maxclass
)
{
DeallocChunk
(
chunk
)
;
}
if
(
mNumDirty
>
mMaxDirty
)
{
Purge
(
false
)
;
}
}
static
void
arena_run_trim_head
(
arena_t
*
arena
arena_chunk_t
*
chunk
arena_run_t
*
run
size_t
oldsize
size_t
newsize
)
{
size_t
pageind
=
(
(
uintptr_t
)
run
-
(
uintptr_t
)
chunk
)
>
>
pagesize_2pow
;
size_t
head_npages
=
(
oldsize
-
newsize
)
>
>
pagesize_2pow
;
MOZ_ASSERT
(
oldsize
>
newsize
)
;
chunk
-
>
map
[
pageind
]
.
bits
=
(
oldsize
-
newsize
)
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
chunk
-
>
map
[
pageind
+
head_npages
]
.
bits
=
newsize
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
arena
-
>
DallocRun
(
run
false
)
;
}
static
void
arena_run_trim_tail
(
arena_t
*
arena
arena_chunk_t
*
chunk
arena_run_t
*
run
size_t
oldsize
size_t
newsize
bool
dirty
)
{
size_t
pageind
=
(
(
uintptr_t
)
run
-
(
uintptr_t
)
chunk
)
>
>
pagesize_2pow
;
size_t
npages
=
newsize
>
>
pagesize_2pow
;
MOZ_ASSERT
(
oldsize
>
newsize
)
;
chunk
-
>
map
[
pageind
]
.
bits
=
newsize
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
chunk
-
>
map
[
pageind
+
npages
]
.
bits
=
(
oldsize
-
newsize
)
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
arena
-
>
DallocRun
(
(
arena_run_t
*
)
(
uintptr_t
(
run
)
+
newsize
)
dirty
)
;
}
static
arena_run_t
*
arena_bin_nonfull_run_get
(
arena_t
*
arena
arena_bin_t
*
bin
)
{
arena_chunk_map_t
*
mapelm
;
arena_run_t
*
run
;
unsigned
i
remainder
;
mapelm
=
arena_run_tree_first
(
&
bin
-
>
runs
)
;
if
(
mapelm
)
{
arena_run_tree_remove
(
&
bin
-
>
runs
mapelm
)
;
run
=
(
arena_run_t
*
)
(
mapelm
-
>
bits
&
~
pagesize_mask
)
;
return
(
run
)
;
}
run
=
arena
-
>
AllocRun
(
bin
bin
-
>
run_size
false
false
)
;
if
(
!
run
)
return
nullptr
;
if
(
run
=
=
bin
-
>
runcur
)
return
(
run
)
;
run
-
>
bin
=
bin
;
for
(
i
=
0
;
i
<
bin
-
>
regs_mask_nelms
-
1
;
i
+
+
)
run
-
>
regs_mask
[
i
]
=
UINT_MAX
;
remainder
=
bin
-
>
nregs
&
(
(
1U
<
<
(
SIZEOF_INT_2POW
+
3
)
)
-
1
)
;
if
(
remainder
=
=
0
)
run
-
>
regs_mask
[
i
]
=
UINT_MAX
;
else
{
run
-
>
regs_mask
[
i
]
=
(
UINT_MAX
>
>
(
(
1U
<
<
(
SIZEOF_INT_2POW
+
3
)
)
-
remainder
)
)
;
}
run
-
>
regs_minelm
=
0
;
run
-
>
nfree
=
bin
-
>
nregs
;
#
if
defined
(
MOZ_DEBUG
)
|
|
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
run
-
>
magic
=
ARENA_RUN_MAGIC
;
#
endif
bin
-
>
stats
.
curruns
+
+
;
return
(
run
)
;
}
static
inline
void
*
arena_bin_malloc_easy
(
arena_t
*
arena
arena_bin_t
*
bin
arena_run_t
*
run
)
{
void
*
ret
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
magic
=
=
ARENA_RUN_MAGIC
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
nfree
>
0
)
;
ret
=
arena_run_reg_alloc
(
run
bin
)
;
MOZ_DIAGNOSTIC_ASSERT
(
ret
)
;
run
-
>
nfree
-
-
;
return
(
ret
)
;
}
static
void
*
arena_bin_malloc_hard
(
arena_t
*
arena
arena_bin_t
*
bin
)
{
bin
-
>
runcur
=
arena_bin_nonfull_run_get
(
arena
bin
)
;
if
(
!
bin
-
>
runcur
)
return
nullptr
;
MOZ_DIAGNOSTIC_ASSERT
(
bin
-
>
runcur
-
>
magic
=
=
ARENA_RUN_MAGIC
)
;
MOZ_DIAGNOSTIC_ASSERT
(
bin
-
>
runcur
-
>
nfree
>
0
)
;
return
(
arena_bin_malloc_easy
(
arena
bin
bin
-
>
runcur
)
)
;
}
static
size_t
arena_bin_run_size_calc
(
arena_bin_t
*
bin
size_t
min_run_size
)
{
size_t
try_run_size
good_run_size
;
unsigned
good_nregs
good_mask_nelms
good_reg0_offset
;
unsigned
try_nregs
try_mask_nelms
try_reg0_offset
;
MOZ_ASSERT
(
min_run_size
>
=
pagesize
)
;
MOZ_ASSERT
(
min_run_size
<
=
arena_maxclass
)
;
try_run_size
=
min_run_size
;
try_nregs
=
(
(
try_run_size
-
sizeof
(
arena_run_t
)
)
/
bin
-
>
reg_size
)
+
1
;
do
{
try_nregs
-
-
;
try_mask_nelms
=
(
try_nregs
>
>
(
SIZEOF_INT_2POW
+
3
)
)
+
(
(
try_nregs
&
(
(
1U
<
<
(
SIZEOF_INT_2POW
+
3
)
)
-
1
)
)
?
1
:
0
)
;
try_reg0_offset
=
try_run_size
-
(
try_nregs
*
bin
-
>
reg_size
)
;
}
while
(
sizeof
(
arena_run_t
)
+
(
sizeof
(
unsigned
)
*
(
try_mask_nelms
-
1
)
)
>
try_reg0_offset
)
;
do
{
good_run_size
=
try_run_size
;
good_nregs
=
try_nregs
;
good_mask_nelms
=
try_mask_nelms
;
good_reg0_offset
=
try_reg0_offset
;
try_run_size
+
=
pagesize
;
try_nregs
=
(
(
try_run_size
-
sizeof
(
arena_run_t
)
)
/
bin
-
>
reg_size
)
+
1
;
do
{
try_nregs
-
-
;
try_mask_nelms
=
(
try_nregs
>
>
(
SIZEOF_INT_2POW
+
3
)
)
+
(
(
try_nregs
&
(
(
1U
<
<
(
SIZEOF_INT_2POW
+
3
)
)
-
1
)
)
?
1
:
0
)
;
try_reg0_offset
=
try_run_size
-
(
try_nregs
*
bin
-
>
reg_size
)
;
}
while
(
sizeof
(
arena_run_t
)
+
(
sizeof
(
unsigned
)
*
(
try_mask_nelms
-
1
)
)
>
try_reg0_offset
)
;
}
while
(
try_run_size
<
=
arena_maxclass
&
&
RUN_MAX_OVRHD
*
(
bin
-
>
reg_size
<
<
3
)
>
RUN_MAX_OVRHD_RELAX
&
&
(
try_reg0_offset
<
<
RUN_BFP
)
>
RUN_MAX_OVRHD
*
try_run_size
)
;
MOZ_ASSERT
(
sizeof
(
arena_run_t
)
+
(
sizeof
(
unsigned
)
*
(
good_mask_nelms
-
1
)
)
<
=
good_reg0_offset
)
;
MOZ_ASSERT
(
(
good_mask_nelms
<
<
(
SIZEOF_INT_2POW
+
3
)
)
>
=
good_nregs
)
;
bin
-
>
run_size
=
good_run_size
;
bin
-
>
nregs
=
good_nregs
;
bin
-
>
regs_mask_nelms
=
good_mask_nelms
;
bin
-
>
reg0_offset
=
good_reg0_offset
;
return
(
good_run_size
)
;
}
static
inline
void
*
arena_malloc_small
(
arena_t
*
arena
size_t
size
bool
zero
)
{
void
*
ret
;
arena_bin_t
*
bin
;
arena_run_t
*
run
;
if
(
size
<
small_min
)
{
size
=
pow2_ceil
(
size
)
;
bin
=
&
arena
-
>
mBins
[
ffs
(
(
int
)
(
size
>
>
(
TINY_MIN_2POW
+
1
)
)
)
]
;
if
(
size
<
(
1U
<
<
TINY_MIN_2POW
)
)
size
=
(
1U
<
<
TINY_MIN_2POW
)
;
}
else
if
(
size
<
=
small_max
)
{
size
=
QUANTUM_CEILING
(
size
)
;
bin
=
&
arena
-
>
mBins
[
ntbins
+
(
size
>
>
opt_quantum_2pow
)
-
1
]
;
}
else
{
size
=
pow2_ceil
(
size
)
;
bin
=
&
arena
-
>
mBins
[
ntbins
+
nqbins
+
(
ffs
(
(
int
)
(
size
>
>
opt_small_max_2pow
)
)
-
2
)
]
;
}
MOZ_DIAGNOSTIC_ASSERT
(
size
=
=
bin
-
>
reg_size
)
;
malloc_spin_lock
(
&
arena
-
>
mLock
)
;
if
(
(
run
=
bin
-
>
runcur
)
&
&
run
-
>
nfree
>
0
)
ret
=
arena_bin_malloc_easy
(
arena
bin
run
)
;
else
ret
=
arena_bin_malloc_hard
(
arena
bin
)
;
if
(
!
ret
)
{
malloc_spin_unlock
(
&
arena
-
>
mLock
)
;
return
nullptr
;
}
arena
-
>
mStats
.
allocated_small
+
=
size
;
malloc_spin_unlock
(
&
arena
-
>
mLock
)
;
if
(
zero
=
=
false
)
{
if
(
opt_junk
)
memset
(
ret
kAllocJunk
size
)
;
else
if
(
opt_zero
)
memset
(
ret
0
size
)
;
}
else
memset
(
ret
0
size
)
;
return
(
ret
)
;
}
static
void
*
arena_malloc_large
(
arena_t
*
arena
size_t
size
bool
zero
)
{
void
*
ret
;
size
=
PAGE_CEILING
(
size
)
;
malloc_spin_lock
(
&
arena
-
>
mLock
)
;
ret
=
arena
-
>
AllocRun
(
nullptr
size
true
zero
)
;
if
(
!
ret
)
{
malloc_spin_unlock
(
&
arena
-
>
mLock
)
;
return
nullptr
;
}
arena
-
>
mStats
.
allocated_large
+
=
size
;
malloc_spin_unlock
(
&
arena
-
>
mLock
)
;
if
(
zero
=
=
false
)
{
if
(
opt_junk
)
memset
(
ret
kAllocJunk
size
)
;
else
if
(
opt_zero
)
memset
(
ret
0
size
)
;
}
return
(
ret
)
;
}
static
inline
void
*
arena_malloc
(
arena_t
*
arena
size_t
size
bool
zero
)
{
MOZ_ASSERT
(
arena
)
;
MOZ_DIAGNOSTIC_ASSERT
(
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
MOZ_ASSERT
(
size
!
=
0
)
;
MOZ_ASSERT
(
QUANTUM_CEILING
(
size
)
<
=
arena_maxclass
)
;
if
(
size
<
=
bin_maxclass
)
{
return
(
arena_malloc_small
(
arena
size
zero
)
)
;
}
else
return
(
arena_malloc_large
(
arena
size
zero
)
)
;
}
static
inline
void
*
imalloc
(
size_t
size
)
{
MOZ_ASSERT
(
size
!
=
0
)
;
if
(
size
<
=
arena_maxclass
)
return
(
arena_malloc
(
choose_arena
(
size
)
size
false
)
)
;
else
return
(
huge_malloc
(
size
false
)
)
;
}
static
inline
void
*
icalloc
(
size_t
size
)
{
if
(
size
<
=
arena_maxclass
)
return
(
arena_malloc
(
choose_arena
(
size
)
size
true
)
)
;
else
return
(
huge_malloc
(
size
true
)
)
;
}
static
void
*
arena_palloc
(
arena_t
*
arena
size_t
alignment
size_t
size
size_t
alloc_size
)
{
void
*
ret
;
size_t
offset
;
arena_chunk_t
*
chunk
;
MOZ_ASSERT
(
(
size
&
pagesize_mask
)
=
=
0
)
;
MOZ_ASSERT
(
(
alignment
&
pagesize_mask
)
=
=
0
)
;
malloc_spin_lock
(
&
arena
-
>
mLock
)
;
ret
=
arena
-
>
AllocRun
(
nullptr
alloc_size
true
false
)
;
if
(
!
ret
)
{
malloc_spin_unlock
(
&
arena
-
>
mLock
)
;
return
nullptr
;
}
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
ret
)
;
offset
=
(
uintptr_t
)
ret
&
(
alignment
-
1
)
;
MOZ_ASSERT
(
(
offset
&
pagesize_mask
)
=
=
0
)
;
MOZ_ASSERT
(
offset
<
alloc_size
)
;
if
(
offset
=
=
0
)
arena_run_trim_tail
(
arena
chunk
(
arena_run_t
*
)
ret
alloc_size
size
false
)
;
else
{
size_t
leadsize
trailsize
;
leadsize
=
alignment
-
offset
;
if
(
leadsize
>
0
)
{
arena_run_trim_head
(
arena
chunk
(
arena_run_t
*
)
ret
alloc_size
alloc_size
-
leadsize
)
;
ret
=
(
void
*
)
(
(
uintptr_t
)
ret
+
leadsize
)
;
}
trailsize
=
alloc_size
-
leadsize
-
size
;
if
(
trailsize
!
=
0
)
{
MOZ_ASSERT
(
trailsize
<
alloc_size
)
;
arena_run_trim_tail
(
arena
chunk
(
arena_run_t
*
)
ret
size
+
trailsize
size
false
)
;
}
}
arena
-
>
mStats
.
allocated_large
+
=
size
;
malloc_spin_unlock
(
&
arena
-
>
mLock
)
;
if
(
opt_junk
)
memset
(
ret
kAllocJunk
size
)
;
else
if
(
opt_zero
)
memset
(
ret
0
size
)
;
return
(
ret
)
;
}
static
inline
void
*
ipalloc
(
size_t
alignment
size_t
size
)
{
void
*
ret
;
size_t
ceil_size
;
ceil_size
=
ALIGNMENT_CEILING
(
size
alignment
)
;
if
(
ceil_size
<
size
)
{
return
nullptr
;
}
if
(
ceil_size
<
=
pagesize
|
|
(
alignment
<
=
pagesize
&
&
ceil_size
<
=
arena_maxclass
)
)
ret
=
arena_malloc
(
choose_arena
(
size
)
ceil_size
false
)
;
else
{
size_t
run_size
;
alignment
=
PAGE_CEILING
(
alignment
)
;
ceil_size
=
PAGE_CEILING
(
size
)
;
if
(
ceil_size
<
size
|
|
ceil_size
+
alignment
<
ceil_size
)
{
return
nullptr
;
}
if
(
ceil_size
>
=
alignment
)
run_size
=
ceil_size
+
alignment
-
pagesize
;
else
{
run_size
=
(
alignment
<
<
1
)
-
pagesize
;
}
if
(
run_size
<
=
arena_maxclass
)
{
ret
=
arena_palloc
(
choose_arena
(
size
)
alignment
ceil_size
run_size
)
;
}
else
if
(
alignment
<
=
chunksize
)
ret
=
huge_malloc
(
ceil_size
false
)
;
else
ret
=
huge_palloc
(
ceil_size
alignment
false
)
;
}
MOZ_ASSERT
(
(
(
uintptr_t
)
ret
&
(
alignment
-
1
)
)
=
=
0
)
;
return
(
ret
)
;
}
static
size_t
arena_salloc
(
const
void
*
ptr
)
{
size_t
ret
;
arena_chunk_t
*
chunk
;
size_t
pageind
mapbits
;
MOZ_ASSERT
(
ptr
)
;
MOZ_ASSERT
(
CHUNK_ADDR2BASE
(
ptr
)
!
=
ptr
)
;
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
ptr
)
;
pageind
=
(
(
(
uintptr_t
)
ptr
-
(
uintptr_t
)
chunk
)
>
>
pagesize_2pow
)
;
mapbits
=
chunk
-
>
map
[
pageind
]
.
bits
;
MOZ_DIAGNOSTIC_ASSERT
(
(
mapbits
&
CHUNK_MAP_ALLOCATED
)
!
=
0
)
;
if
(
(
mapbits
&
CHUNK_MAP_LARGE
)
=
=
0
)
{
arena_run_t
*
run
=
(
arena_run_t
*
)
(
mapbits
&
~
pagesize_mask
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
magic
=
=
ARENA_RUN_MAGIC
)
;
ret
=
run
-
>
bin
-
>
reg_size
;
}
else
{
ret
=
mapbits
&
~
pagesize_mask
;
MOZ_DIAGNOSTIC_ASSERT
(
ret
!
=
0
)
;
}
return
(
ret
)
;
}
static
inline
size_t
isalloc_validate
(
const
void
*
ptr
)
{
if
(
malloc_initialized
=
=
false
)
{
return
0
;
}
arena_chunk_t
*
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
ptr
)
;
if
(
!
chunk
)
{
return
0
;
}
if
(
!
malloc_rtree_get
(
chunk_rtree
(
uintptr_t
)
chunk
)
)
{
return
0
;
}
if
(
chunk
!
=
ptr
)
{
MOZ_DIAGNOSTIC_ASSERT
(
chunk
-
>
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
return
arena_salloc
(
ptr
)
;
}
else
{
size_t
ret
;
extent_node_t
*
node
;
extent_node_t
key
;
key
.
addr
=
(
void
*
)
chunk
;
malloc_mutex_lock
(
&
huge_mtx
)
;
node
=
extent_tree_ad_search
(
&
huge
&
key
)
;
if
(
node
)
ret
=
node
-
>
size
;
else
ret
=
0
;
malloc_mutex_unlock
(
&
huge_mtx
)
;
return
ret
;
}
}
static
inline
size_t
isalloc
(
const
void
*
ptr
)
{
size_t
ret
;
arena_chunk_t
*
chunk
;
MOZ_ASSERT
(
ptr
)
;
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
ptr
)
;
if
(
chunk
!
=
ptr
)
{
MOZ_ASSERT
(
chunk
-
>
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
ret
=
arena_salloc
(
ptr
)
;
}
else
{
extent_node_t
*
node
key
;
malloc_mutex_lock
(
&
huge_mtx
)
;
key
.
addr
=
const_cast
<
void
*
>
(
ptr
)
;
node
=
extent_tree_ad_search
(
&
huge
&
key
)
;
MOZ_DIAGNOSTIC_ASSERT
(
node
)
;
ret
=
node
-
>
size
;
malloc_mutex_unlock
(
&
huge_mtx
)
;
}
return
(
ret
)
;
}
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_ptr_info
(
const
void
*
aPtr
jemalloc_ptr_info_t
*
aInfo
)
{
arena_chunk_t
*
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
aPtr
)
;
if
(
!
chunk
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
}
;
return
;
}
extent_node_t
*
node
;
extent_node_t
key
;
malloc_mutex_lock
(
&
huge_mtx
)
;
key
.
addr
=
const_cast
<
void
*
>
(
aPtr
)
;
node
=
extent_tree_bounds_search
(
&
huge
&
key
)
;
if
(
node
)
{
*
aInfo
=
{
TagLiveHuge
node
-
>
addr
node
-
>
size
}
;
}
malloc_mutex_unlock
(
&
huge_mtx
)
;
if
(
node
)
{
return
;
}
if
(
!
malloc_rtree_get
(
chunk_rtree
(
uintptr_t
)
chunk
)
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
}
;
return
;
}
MOZ_DIAGNOSTIC_ASSERT
(
chunk
-
>
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
size_t
pageind
=
(
(
(
uintptr_t
)
aPtr
-
(
uintptr_t
)
chunk
)
>
>
pagesize_2pow
)
;
if
(
pageind
<
arena_chunk_header_npages
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
}
;
return
;
}
size_t
mapbits
=
chunk
-
>
map
[
pageind
]
.
bits
;
if
(
!
(
mapbits
&
CHUNK_MAP_ALLOCATED
)
)
{
PtrInfoTag
tag
=
TagFreedPageDirty
;
if
(
mapbits
&
CHUNK_MAP_DIRTY
)
tag
=
TagFreedPageDirty
;
else
if
(
mapbits
&
CHUNK_MAP_DECOMMITTED
)
tag
=
TagFreedPageDecommitted
;
else
if
(
mapbits
&
CHUNK_MAP_MADVISED
)
tag
=
TagFreedPageMadvised
;
else
if
(
mapbits
&
CHUNK_MAP_ZEROED
)
tag
=
TagFreedPageZeroed
;
else
MOZ_CRASH
(
)
;
void
*
pageaddr
=
(
void
*
)
(
uintptr_t
(
aPtr
)
&
~
pagesize_mask
)
;
*
aInfo
=
{
tag
pageaddr
pagesize
}
;
return
;
}
if
(
mapbits
&
CHUNK_MAP_LARGE
)
{
size_t
size
;
while
(
true
)
{
size
=
mapbits
&
~
pagesize_mask
;
if
(
size
!
=
0
)
{
break
;
}
pageind
-
-
;
MOZ_DIAGNOSTIC_ASSERT
(
pageind
>
=
arena_chunk_header_npages
)
;
if
(
pageind
<
arena_chunk_header_npages
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
}
;
return
;
}
mapbits
=
chunk
-
>
map
[
pageind
]
.
bits
;
MOZ_DIAGNOSTIC_ASSERT
(
mapbits
&
CHUNK_MAP_LARGE
)
;
if
(
!
(
mapbits
&
CHUNK_MAP_LARGE
)
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
}
;
return
;
}
}
void
*
addr
=
(
(
char
*
)
chunk
)
+
(
pageind
<
<
pagesize_2pow
)
;
*
aInfo
=
{
TagLiveLarge
addr
size
}
;
return
;
}
auto
run
=
(
arena_run_t
*
)
(
mapbits
&
~
pagesize_mask
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
magic
=
=
ARENA_RUN_MAGIC
)
;
size_t
size
=
run
-
>
bin
-
>
reg_size
;
uintptr_t
reg0_addr
=
(
uintptr_t
)
run
+
run
-
>
bin
-
>
reg0_offset
;
if
(
aPtr
<
(
void
*
)
reg0_addr
)
{
*
aInfo
=
{
TagUnknown
nullptr
0
}
;
return
;
}
unsigned
regind
=
(
(
uintptr_t
)
aPtr
-
reg0_addr
)
/
size
;
void
*
addr
=
(
void
*
)
(
reg0_addr
+
regind
*
size
)
;
unsigned
elm
=
regind
>
>
(
SIZEOF_INT_2POW
+
3
)
;
unsigned
bit
=
regind
-
(
elm
<
<
(
SIZEOF_INT_2POW
+
3
)
)
;
PtrInfoTag
tag
=
(
(
run
-
>
regs_mask
[
elm
]
&
(
1U
<
<
bit
)
)
)
?
TagFreedSmall
:
TagLiveSmall
;
*
aInfo
=
{
tag
addr
size
}
;
}
static
inline
void
arena_dalloc_small
(
arena_t
*
arena
arena_chunk_t
*
chunk
void
*
ptr
arena_chunk_map_t
*
mapelm
)
{
arena_run_t
*
run
;
arena_bin_t
*
bin
;
size_t
size
;
run
=
(
arena_run_t
*
)
(
mapelm
-
>
bits
&
~
pagesize_mask
)
;
MOZ_DIAGNOSTIC_ASSERT
(
run
-
>
magic
=
=
ARENA_RUN_MAGIC
)
;
bin
=
run
-
>
bin
;
size
=
bin
-
>
reg_size
;
memset
(
ptr
kAllocPoison
size
)
;
arena_run_reg_dalloc
(
run
bin
ptr
size
)
;
run
-
>
nfree
+
+
;
if
(
run
-
>
nfree
=
=
bin
-
>
nregs
)
{
if
(
run
=
=
bin
-
>
runcur
)
bin
-
>
runcur
=
nullptr
;
else
if
(
bin
-
>
nregs
!
=
1
)
{
size_t
run_pageind
=
(
(
(
uintptr_t
)
run
-
(
uintptr_t
)
chunk
)
)
>
>
pagesize_2pow
;
arena_chunk_map_t
*
run_mapelm
=
&
chunk
-
>
map
[
run_pageind
]
;
MOZ_DIAGNOSTIC_ASSERT
(
arena_run_tree_search
(
&
bin
-
>
runs
run_mapelm
)
=
=
run_mapelm
)
;
arena_run_tree_remove
(
&
bin
-
>
runs
run_mapelm
)
;
}
#
if
defined
(
MOZ_DEBUG
)
|
|
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
run
-
>
magic
=
0
;
#
endif
arena
-
>
DallocRun
(
run
true
)
;
bin
-
>
stats
.
curruns
-
-
;
}
else
if
(
run
-
>
nfree
=
=
1
&
&
run
!
=
bin
-
>
runcur
)
{
if
(
!
bin
-
>
runcur
)
bin
-
>
runcur
=
run
;
else
if
(
(
uintptr_t
)
run
<
(
uintptr_t
)
bin
-
>
runcur
)
{
if
(
bin
-
>
runcur
-
>
nfree
>
0
)
{
arena_chunk_t
*
runcur_chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
bin
-
>
runcur
)
;
size_t
runcur_pageind
=
(
(
(
uintptr_t
)
bin
-
>
runcur
-
(
uintptr_t
)
runcur_chunk
)
)
>
>
pagesize_2pow
;
arena_chunk_map_t
*
runcur_mapelm
=
&
runcur_chunk
-
>
map
[
runcur_pageind
]
;
MOZ_DIAGNOSTIC_ASSERT
(
!
arena_run_tree_search
(
&
bin
-
>
runs
runcur_mapelm
)
)
;
arena_run_tree_insert
(
&
bin
-
>
runs
runcur_mapelm
)
;
}
bin
-
>
runcur
=
run
;
}
else
{
size_t
run_pageind
=
(
(
(
uintptr_t
)
run
-
(
uintptr_t
)
chunk
)
)
>
>
pagesize_2pow
;
arena_chunk_map_t
*
run_mapelm
=
&
chunk
-
>
map
[
run_pageind
]
;
MOZ_DIAGNOSTIC_ASSERT
(
arena_run_tree_search
(
&
bin
-
>
runs
run_mapelm
)
=
=
nullptr
)
;
arena_run_tree_insert
(
&
bin
-
>
runs
run_mapelm
)
;
}
}
arena
-
>
mStats
.
allocated_small
-
=
size
;
}
static
void
arena_dalloc_large
(
arena_t
*
arena
arena_chunk_t
*
chunk
void
*
ptr
)
{
size_t
pageind
=
(
(
uintptr_t
)
ptr
-
(
uintptr_t
)
chunk
)
>
>
pagesize_2pow
;
size_t
size
=
chunk
-
>
map
[
pageind
]
.
bits
&
~
pagesize_mask
;
memset
(
ptr
kAllocPoison
size
)
;
arena
-
>
mStats
.
allocated_large
-
=
size
;
arena
-
>
DallocRun
(
(
arena_run_t
*
)
ptr
true
)
;
}
static
inline
void
arena_dalloc
(
void
*
ptr
size_t
offset
)
{
arena_chunk_t
*
chunk
;
arena_t
*
arena
;
size_t
pageind
;
arena_chunk_map_t
*
mapelm
;
MOZ_ASSERT
(
ptr
)
;
MOZ_ASSERT
(
offset
!
=
0
)
;
MOZ_ASSERT
(
CHUNK_ADDR2OFFSET
(
ptr
)
=
=
offset
)
;
chunk
=
(
arena_chunk_t
*
)
(
(
uintptr_t
)
ptr
-
offset
)
;
arena
=
chunk
-
>
arena
;
MOZ_ASSERT
(
arena
)
;
MOZ_DIAGNOSTIC_ASSERT
(
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
malloc_spin_lock
(
&
arena
-
>
mLock
)
;
pageind
=
offset
>
>
pagesize_2pow
;
mapelm
=
&
chunk
-
>
map
[
pageind
]
;
MOZ_DIAGNOSTIC_ASSERT
(
(
mapelm
-
>
bits
&
CHUNK_MAP_ALLOCATED
)
!
=
0
)
;
if
(
(
mapelm
-
>
bits
&
CHUNK_MAP_LARGE
)
=
=
0
)
{
arena_dalloc_small
(
arena
chunk
ptr
mapelm
)
;
}
else
{
arena_dalloc_large
(
arena
chunk
ptr
)
;
}
malloc_spin_unlock
(
&
arena
-
>
mLock
)
;
}
static
inline
void
idalloc
(
void
*
ptr
)
{
size_t
offset
;
MOZ_ASSERT
(
ptr
)
;
offset
=
CHUNK_ADDR2OFFSET
(
ptr
)
;
if
(
offset
!
=
0
)
arena_dalloc
(
ptr
offset
)
;
else
huge_dalloc
(
ptr
)
;
}
static
void
arena_ralloc_large_shrink
(
arena_t
*
arena
arena_chunk_t
*
chunk
void
*
ptr
size_t
size
size_t
oldsize
)
{
MOZ_ASSERT
(
size
<
oldsize
)
;
malloc_spin_lock
(
&
arena
-
>
mLock
)
;
arena_run_trim_tail
(
arena
chunk
(
arena_run_t
*
)
ptr
oldsize
size
true
)
;
arena
-
>
mStats
.
allocated_large
-
=
oldsize
-
size
;
malloc_spin_unlock
(
&
arena
-
>
mLock
)
;
}
static
bool
arena_ralloc_large_grow
(
arena_t
*
arena
arena_chunk_t
*
chunk
void
*
ptr
size_t
size
size_t
oldsize
)
{
size_t
pageind
=
(
(
uintptr_t
)
ptr
-
(
uintptr_t
)
chunk
)
>
>
pagesize_2pow
;
size_t
npages
=
oldsize
>
>
pagesize_2pow
;
malloc_spin_lock
(
&
arena
-
>
mLock
)
;
MOZ_DIAGNOSTIC_ASSERT
(
oldsize
=
=
(
chunk
-
>
map
[
pageind
]
.
bits
&
~
pagesize_mask
)
)
;
MOZ_ASSERT
(
size
>
oldsize
)
;
if
(
pageind
+
npages
<
chunk_npages
&
&
(
chunk
-
>
map
[
pageind
+
npages
]
.
bits
&
CHUNK_MAP_ALLOCATED
)
=
=
0
&
&
(
chunk
-
>
map
[
pageind
+
npages
]
.
bits
&
~
pagesize_mask
)
>
=
size
-
oldsize
)
{
arena_run_split
(
arena
(
arena_run_t
*
)
(
(
uintptr_t
)
chunk
+
(
(
pageind
+
npages
)
<
<
pagesize_2pow
)
)
size
-
oldsize
true
false
)
;
chunk
-
>
map
[
pageind
]
.
bits
=
size
|
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
chunk
-
>
map
[
pageind
+
npages
]
.
bits
=
CHUNK_MAP_LARGE
|
CHUNK_MAP_ALLOCATED
;
arena
-
>
mStats
.
allocated_large
+
=
size
-
oldsize
;
malloc_spin_unlock
(
&
arena
-
>
mLock
)
;
return
(
false
)
;
}
malloc_spin_unlock
(
&
arena
-
>
mLock
)
;
return
(
true
)
;
}
static
bool
arena_ralloc_large
(
void
*
ptr
size_t
size
size_t
oldsize
)
{
size_t
psize
;
psize
=
PAGE_CEILING
(
size
)
;
if
(
psize
=
=
oldsize
)
{
if
(
size
<
oldsize
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
ptr
+
size
)
kAllocPoison
oldsize
-
size
)
;
}
return
(
false
)
;
}
else
{
arena_chunk_t
*
chunk
;
arena_t
*
arena
;
chunk
=
(
arena_chunk_t
*
)
CHUNK_ADDR2BASE
(
ptr
)
;
arena
=
chunk
-
>
arena
;
MOZ_DIAGNOSTIC_ASSERT
(
arena
-
>
mMagic
=
=
ARENA_MAGIC
)
;
if
(
psize
<
oldsize
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
ptr
+
size
)
kAllocPoison
oldsize
-
size
)
;
arena_ralloc_large_shrink
(
arena
chunk
ptr
psize
oldsize
)
;
return
(
false
)
;
}
else
{
bool
ret
=
arena_ralloc_large_grow
(
arena
chunk
ptr
psize
oldsize
)
;
if
(
ret
=
=
false
&
&
opt_zero
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
ptr
+
oldsize
)
0
size
-
oldsize
)
;
}
return
(
ret
)
;
}
}
}
static
void
*
arena_ralloc
(
void
*
ptr
size_t
size
size_t
oldsize
)
{
void
*
ret
;
size_t
copysize
;
if
(
size
<
small_min
)
{
if
(
oldsize
<
small_min
&
&
ffs
(
(
int
)
(
pow2_ceil
(
size
)
>
>
(
TINY_MIN_2POW
+
1
)
)
)
=
=
ffs
(
(
int
)
(
pow2_ceil
(
oldsize
)
>
>
(
TINY_MIN_2POW
+
1
)
)
)
)
goto
IN_PLACE
;
}
else
if
(
size
<
=
small_max
)
{
if
(
oldsize
>
=
small_min
&
&
oldsize
<
=
small_max
&
&
(
QUANTUM_CEILING
(
size
)
>
>
opt_quantum_2pow
)
=
=
(
QUANTUM_CEILING
(
oldsize
)
>
>
opt_quantum_2pow
)
)
goto
IN_PLACE
;
}
else
if
(
size
<
=
bin_maxclass
)
{
if
(
oldsize
>
small_max
&
&
oldsize
<
=
bin_maxclass
&
&
pow2_ceil
(
size
)
=
=
pow2_ceil
(
oldsize
)
)
goto
IN_PLACE
;
}
else
if
(
oldsize
>
bin_maxclass
&
&
oldsize
<
=
arena_maxclass
)
{
MOZ_ASSERT
(
size
>
bin_maxclass
)
;
if
(
arena_ralloc_large
(
ptr
size
oldsize
)
=
=
false
)
return
(
ptr
)
;
}
ret
=
arena_malloc
(
choose_arena
(
size
)
size
false
)
;
if
(
!
ret
)
return
nullptr
;
copysize
=
(
size
<
oldsize
)
?
size
:
oldsize
;
#
ifdef
VM_COPY_MIN
if
(
copysize
>
=
VM_COPY_MIN
)
pages_copy
(
ret
ptr
copysize
)
;
else
#
endif
memcpy
(
ret
ptr
copysize
)
;
idalloc
(
ptr
)
;
return
(
ret
)
;
IN_PLACE
:
if
(
size
<
oldsize
)
memset
(
(
void
*
)
(
(
uintptr_t
)
ptr
+
size
)
kAllocPoison
oldsize
-
size
)
;
else
if
(
opt_zero
&
&
size
>
oldsize
)
memset
(
(
void
*
)
(
(
uintptr_t
)
ptr
+
oldsize
)
0
size
-
oldsize
)
;
return
(
ptr
)
;
}
static
inline
void
*
iralloc
(
void
*
ptr
size_t
size
)
{
size_t
oldsize
;
MOZ_ASSERT
(
ptr
)
;
MOZ_ASSERT
(
size
!
=
0
)
;
oldsize
=
isalloc
(
ptr
)
;
if
(
size
<
=
arena_maxclass
)
return
(
arena_ralloc
(
ptr
size
oldsize
)
)
;
else
return
(
huge_ralloc
(
ptr
size
oldsize
)
)
;
}
bool
arena_t
:
:
Init
(
)
{
unsigned
i
;
arena_bin_t
*
bin
;
size_t
prev_run_size
;
if
(
malloc_spin_init
(
&
mLock
)
)
return
true
;
memset
(
&
mStats
0
sizeof
(
arena_stats_t
)
)
;
arena_chunk_tree_dirty_new
(
&
mChunksDirty
)
;
#
ifdef
MALLOC_DOUBLE_PURGE
new
(
&
mChunksMAdvised
)
mozilla
:
:
DoublyLinkedList
<
arena_chunk_t
>
(
)
;
#
endif
mSpare
=
nullptr
;
mNumDirty
=
0
;
mMaxDirty
=
opt_dirty_max
>
>
3
;
arena_avail_tree_new
(
&
mRunsAvail
)
;
prev_run_size
=
pagesize
;
for
(
i
=
0
;
i
<
ntbins
;
i
+
+
)
{
bin
=
&
mBins
[
i
]
;
bin
-
>
runcur
=
nullptr
;
arena_run_tree_new
(
&
bin
-
>
runs
)
;
bin
-
>
reg_size
=
(
1ULL
<
<
(
TINY_MIN_2POW
+
i
)
)
;
prev_run_size
=
arena_bin_run_size_calc
(
bin
prev_run_size
)
;
memset
(
&
bin
-
>
stats
0
sizeof
(
malloc_bin_stats_t
)
)
;
}
for
(
;
i
<
ntbins
+
nqbins
;
i
+
+
)
{
bin
=
&
mBins
[
i
]
;
bin
-
>
runcur
=
nullptr
;
arena_run_tree_new
(
&
bin
-
>
runs
)
;
bin
-
>
reg_size
=
quantum
*
(
i
-
ntbins
+
1
)
;
prev_run_size
=
arena_bin_run_size_calc
(
bin
prev_run_size
)
;
memset
(
&
bin
-
>
stats
0
sizeof
(
malloc_bin_stats_t
)
)
;
}
for
(
;
i
<
ntbins
+
nqbins
+
nsbins
;
i
+
+
)
{
bin
=
&
mBins
[
i
]
;
bin
-
>
runcur
=
nullptr
;
arena_run_tree_new
(
&
bin
-
>
runs
)
;
bin
-
>
reg_size
=
(
small_max
<
<
(
i
-
(
ntbins
+
nqbins
)
+
1
)
)
;
prev_run_size
=
arena_bin_run_size_calc
(
bin
prev_run_size
)
;
memset
(
&
bin
-
>
stats
0
sizeof
(
malloc_bin_stats_t
)
)
;
}
#
if
defined
(
MOZ_DEBUG
)
|
|
defined
(
MOZ_DIAGNOSTIC_ASSERT_ENABLED
)
mMagic
=
ARENA_MAGIC
;
#
endif
return
false
;
}
static
inline
arena_t
*
arenas_fallback
(
)
{
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Error
initializing
arena
\
n
"
)
;
return
arenas
[
0
]
;
}
static
arena_t
*
arenas_extend
(
)
{
const
size_t
arenas_growth
=
16
;
arena_t
*
ret
;
ret
=
(
arena_t
*
)
base_alloc
(
sizeof
(
arena_t
)
+
(
sizeof
(
arena_bin_t
)
*
(
ntbins
+
nqbins
+
nsbins
-
1
)
)
)
;
if
(
!
ret
|
|
ret
-
>
Init
(
)
)
{
return
arenas_fallback
(
)
;
}
malloc_spin_lock
(
&
arenas_lock
)
;
if
(
narenas
%
arenas_growth
=
=
0
)
{
size_t
max_arenas
=
(
(
narenas
+
arenas_growth
)
/
arenas_growth
)
*
arenas_growth
;
arena_t
*
*
new_arenas
=
(
arena_t
*
*
)
base_alloc
(
sizeof
(
arena_t
*
)
*
max_arenas
)
;
if
(
!
new_arenas
)
{
ret
=
arenas
?
arenas_fallback
(
)
:
nullptr
;
malloc_spin_unlock
(
&
arenas_lock
)
;
return
(
ret
)
;
}
memcpy
(
new_arenas
arenas
narenas
*
sizeof
(
arena_t
*
)
)
;
memset
(
new_arenas
+
narenas
0
sizeof
(
arena_t
*
)
*
(
max_arenas
-
narenas
)
)
;
arenas
=
new_arenas
;
}
arenas
[
narenas
+
+
]
=
ret
;
malloc_spin_unlock
(
&
arenas_lock
)
;
return
(
ret
)
;
}
static
void
*
huge_malloc
(
size_t
size
bool
zero
)
{
return
huge_palloc
(
size
chunksize
zero
)
;
}
static
void
*
huge_palloc
(
size_t
size
size_t
alignment
bool
zero
)
{
void
*
ret
;
size_t
csize
;
size_t
psize
;
extent_node_t
*
node
;
bool
zeroed
;
csize
=
CHUNK_CEILING
(
size
)
;
if
(
csize
=
=
0
)
{
return
nullptr
;
}
node
=
base_node_alloc
(
)
;
if
(
!
node
)
return
nullptr
;
ret
=
chunk_alloc
(
csize
alignment
false
&
zeroed
)
;
if
(
!
ret
)
{
base_node_dealloc
(
node
)
;
return
nullptr
;
}
if
(
zero
)
{
chunk_ensure_zero
(
ret
csize
zeroed
)
;
}
node
-
>
addr
=
ret
;
psize
=
PAGE_CEILING
(
size
)
;
node
-
>
size
=
psize
;
malloc_mutex_lock
(
&
huge_mtx
)
;
extent_tree_ad_insert
(
&
huge
node
)
;
huge_nmalloc
+
+
;
huge_allocated
+
=
psize
;
huge_mapped
+
=
csize
;
malloc_mutex_unlock
(
&
huge_mtx
)
;
#
ifdef
MALLOC_DECOMMIT
if
(
csize
-
psize
>
0
)
pages_decommit
(
(
void
*
)
(
(
uintptr_t
)
ret
+
psize
)
csize
-
psize
)
;
#
endif
if
(
zero
=
=
false
)
{
if
(
opt_junk
)
#
ifdef
MALLOC_DECOMMIT
memset
(
ret
kAllocJunk
psize
)
;
#
else
memset
(
ret
kAllocJunk
csize
)
;
#
endif
else
if
(
opt_zero
)
#
ifdef
MALLOC_DECOMMIT
memset
(
ret
0
psize
)
;
#
else
memset
(
ret
0
csize
)
;
#
endif
}
return
(
ret
)
;
}
static
void
*
huge_ralloc
(
void
*
ptr
size_t
size
size_t
oldsize
)
{
void
*
ret
;
size_t
copysize
;
if
(
oldsize
>
arena_maxclass
&
&
CHUNK_CEILING
(
size
)
=
=
CHUNK_CEILING
(
oldsize
)
)
{
size_t
psize
=
PAGE_CEILING
(
size
)
;
if
(
size
<
oldsize
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
ptr
+
size
)
kAllocPoison
oldsize
-
size
)
;
}
#
ifdef
MALLOC_DECOMMIT
if
(
psize
<
oldsize
)
{
extent_node_t
*
node
key
;
pages_decommit
(
(
void
*
)
(
(
uintptr_t
)
ptr
+
psize
)
oldsize
-
psize
)
;
malloc_mutex_lock
(
&
huge_mtx
)
;
key
.
addr
=
const_cast
<
void
*
>
(
ptr
)
;
node
=
extent_tree_ad_search
(
&
huge
&
key
)
;
MOZ_ASSERT
(
node
)
;
MOZ_ASSERT
(
node
-
>
size
=
=
oldsize
)
;
huge_allocated
-
=
oldsize
-
psize
;
node
-
>
size
=
psize
;
malloc_mutex_unlock
(
&
huge_mtx
)
;
}
else
if
(
psize
>
oldsize
)
{
pages_commit
(
(
void
*
)
(
(
uintptr_t
)
ptr
+
oldsize
)
psize
-
oldsize
)
;
}
#
endif
if
(
psize
>
oldsize
)
{
extent_node_t
*
node
key
;
malloc_mutex_lock
(
&
huge_mtx
)
;
key
.
addr
=
const_cast
<
void
*
>
(
ptr
)
;
node
=
extent_tree_ad_search
(
&
huge
&
key
)
;
MOZ_ASSERT
(
node
)
;
MOZ_ASSERT
(
node
-
>
size
=
=
oldsize
)
;
huge_allocated
+
=
psize
-
oldsize
;
node
-
>
size
=
psize
;
malloc_mutex_unlock
(
&
huge_mtx
)
;
}
if
(
opt_zero
&
&
size
>
oldsize
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
ptr
+
oldsize
)
0
size
-
oldsize
)
;
}
return
(
ptr
)
;
}
ret
=
huge_malloc
(
size
false
)
;
if
(
!
ret
)
return
nullptr
;
copysize
=
(
size
<
oldsize
)
?
size
:
oldsize
;
#
ifdef
VM_COPY_MIN
if
(
copysize
>
=
VM_COPY_MIN
)
pages_copy
(
ret
ptr
copysize
)
;
else
#
endif
memcpy
(
ret
ptr
copysize
)
;
idalloc
(
ptr
)
;
return
(
ret
)
;
}
static
void
huge_dalloc
(
void
*
ptr
)
{
extent_node_t
*
node
key
;
malloc_mutex_lock
(
&
huge_mtx
)
;
key
.
addr
=
ptr
;
node
=
extent_tree_ad_search
(
&
huge
&
key
)
;
MOZ_ASSERT
(
node
)
;
MOZ_ASSERT
(
node
-
>
addr
=
=
ptr
)
;
extent_tree_ad_remove
(
&
huge
node
)
;
huge_ndalloc
+
+
;
huge_allocated
-
=
node
-
>
size
;
huge_mapped
-
=
CHUNK_CEILING
(
node
-
>
size
)
;
malloc_mutex_unlock
(
&
huge_mtx
)
;
chunk_dealloc
(
node
-
>
addr
CHUNK_CEILING
(
node
-
>
size
)
HUGE_CHUNK
)
;
base_node_dealloc
(
node
)
;
}
#
if
defined
(
XP_WIN
)
#
define
malloc_init
(
)
false
#
else
static
inline
bool
malloc_init
(
void
)
{
if
(
malloc_initialized
=
=
false
)
return
(
malloc_init_hard
(
)
)
;
return
(
false
)
;
}
#
endif
static
size_t
GetKernelPageSize
(
)
{
static
size_t
kernel_page_size
=
(
[
]
(
)
{
#
ifdef
XP_WIN
SYSTEM_INFO
info
;
GetSystemInfo
(
&
info
)
;
return
info
.
dwPageSize
;
#
else
long
result
=
sysconf
(
_SC_PAGESIZE
)
;
MOZ_ASSERT
(
result
!
=
-
1
)
;
return
result
;
#
endif
}
)
(
)
;
return
kernel_page_size
;
}
#
if
!
defined
(
XP_WIN
)
static
#
endif
bool
malloc_init_hard
(
void
)
{
unsigned
i
;
const
char
*
opts
;
long
result
;
#
ifndef
XP_WIN
malloc_mutex_lock
(
&
init_lock
)
;
#
endif
if
(
malloc_initialized
)
{
#
ifndef
XP_WIN
malloc_mutex_unlock
(
&
init_lock
)
;
#
endif
return
false
;
}
#
ifndef
NO_TLS
if
(
!
thread_arena
.
init
(
)
)
{
return
false
;
}
#
endif
result
=
GetKernelPageSize
(
)
;
MOZ_ASSERT
(
(
(
result
-
1
)
&
result
)
=
=
0
)
;
#
ifdef
MALLOC_STATIC_SIZES
if
(
pagesize
%
(
size_t
)
result
)
{
_malloc_message
(
_getprogname
(
)
"
Compile
-
time
page
size
does
not
divide
the
runtime
one
.
\
n
"
)
;
MOZ_CRASH
(
)
;
}
#
else
pagesize
=
(
size_t
)
result
;
pagesize_mask
=
(
size_t
)
result
-
1
;
pagesize_2pow
=
ffs
(
(
int
)
result
)
-
1
;
#
endif
if
(
(
opts
=
getenv
(
"
MALLOC_OPTIONS
"
)
)
)
{
for
(
i
=
0
;
opts
[
i
]
!
=
'
\
0
'
;
i
+
+
)
{
unsigned
j
nreps
;
bool
nseen
;
for
(
nreps
=
0
nseen
=
false
;
;
i
+
+
nseen
=
true
)
{
switch
(
opts
[
i
]
)
{
case
'
0
'
:
case
'
1
'
:
case
'
2
'
:
case
'
3
'
:
case
'
4
'
:
case
'
5
'
:
case
'
6
'
:
case
'
7
'
:
case
'
8
'
:
case
'
9
'
:
nreps
*
=
10
;
nreps
+
=
opts
[
i
]
-
'
0
'
;
break
;
default
:
goto
MALLOC_OUT
;
}
}
MALLOC_OUT
:
if
(
nseen
=
=
false
)
nreps
=
1
;
for
(
j
=
0
;
j
<
nreps
;
j
+
+
)
{
switch
(
opts
[
i
]
)
{
case
'
f
'
:
opt_dirty_max
>
>
=
1
;
break
;
case
'
F
'
:
if
(
opt_dirty_max
=
=
0
)
opt_dirty_max
=
1
;
else
if
(
(
opt_dirty_max
<
<
1
)
!
=
0
)
opt_dirty_max
<
<
=
1
;
break
;
#
ifdef
MOZ_DEBUG
case
'
j
'
:
opt_junk
=
false
;
break
;
case
'
J
'
:
opt_junk
=
true
;
break
;
#
endif
#
ifndef
MALLOC_STATIC_SIZES
case
'
k
'
:
if
(
opt_chunk_2pow
>
pagesize_2pow
+
1
)
opt_chunk_2pow
-
-
;
break
;
case
'
K
'
:
if
(
opt_chunk_2pow
+
1
<
(
sizeof
(
size_t
)
<
<
3
)
)
opt_chunk_2pow
+
+
;
break
;
#
endif
#
ifndef
MALLOC_STATIC_SIZES
case
'
q
'
:
if
(
opt_quantum_2pow
>
QUANTUM_2POW_MIN
)
opt_quantum_2pow
-
-
;
break
;
case
'
Q
'
:
if
(
opt_quantum_2pow
<
pagesize_2pow
-
1
)
opt_quantum_2pow
+
+
;
break
;
case
'
s
'
:
if
(
opt_small_max_2pow
>
QUANTUM_2POW_MIN
)
opt_small_max_2pow
-
-
;
break
;
case
'
S
'
:
if
(
opt_small_max_2pow
<
pagesize_2pow
-
1
)
opt_small_max_2pow
+
+
;
break
;
#
endif
#
ifdef
MOZ_DEBUG
case
'
z
'
:
opt_zero
=
false
;
break
;
case
'
Z
'
:
opt_zero
=
true
;
break
;
#
endif
default
:
{
char
cbuf
[
2
]
;
cbuf
[
0
]
=
opts
[
i
]
;
cbuf
[
1
]
=
'
\
0
'
;
_malloc_message
(
_getprogname
(
)
"
:
(
malloc
)
Unsupported
character
"
"
in
malloc
options
:
'
"
cbuf
"
'
\
n
"
)
;
}
}
}
}
}
#
ifndef
MALLOC_STATIC_SIZES
if
(
opt_small_max_2pow
<
opt_quantum_2pow
)
{
opt_small_max_2pow
=
opt_quantum_2pow
;
}
small_max
=
(
1U
<
<
opt_small_max_2pow
)
;
bin_maxclass
=
(
pagesize
>
>
1
)
;
MOZ_ASSERT
(
opt_quantum_2pow
>
=
TINY_MIN_2POW
)
;
ntbins
=
opt_quantum_2pow
-
TINY_MIN_2POW
;
MOZ_ASSERT
(
ntbins
<
=
opt_quantum_2pow
)
;
nqbins
=
(
small_max
>
>
opt_quantum_2pow
)
;
nsbins
=
pagesize_2pow
-
opt_small_max_2pow
-
1
;
quantum
=
(
1U
<
<
opt_quantum_2pow
)
;
quantum_mask
=
quantum
-
1
;
if
(
ntbins
>
0
)
{
small_min
=
(
quantum
>
>
1
)
+
1
;
}
else
{
small_min
=
1
;
}
MOZ_ASSERT
(
small_min
<
=
quantum
)
;
chunksize
=
(
1LU
<
<
opt_chunk_2pow
)
;
chunksize_mask
=
chunksize
-
1
;
chunk_npages
=
(
chunksize
>
>
pagesize_2pow
)
;
arena_chunk_header_npages
=
calculate_arena_header_pages
(
)
;
arena_maxclass
=
calculate_arena_maxclass
(
)
;
recycle_limit
=
CHUNK_RECYCLE_LIMIT
*
chunksize
;
#
endif
recycled_size
=
0
;
MOZ_ASSERT
(
quantum
>
=
sizeof
(
void
*
)
)
;
MOZ_ASSERT
(
quantum
<
=
pagesize
)
;
MOZ_ASSERT
(
chunksize
>
=
pagesize
)
;
MOZ_ASSERT
(
quantum
*
4
<
=
chunksize
)
;
malloc_mutex_init
(
&
chunks_mtx
)
;
extent_tree_szad_new
(
&
chunks_szad_mmap
)
;
extent_tree_ad_new
(
&
chunks_ad_mmap
)
;
malloc_mutex_init
(
&
huge_mtx
)
;
extent_tree_ad_new
(
&
huge
)
;
huge_nmalloc
=
0
;
huge_ndalloc
=
0
;
huge_allocated
=
0
;
huge_mapped
=
0
;
base_mapped
=
0
;
base_committed
=
0
;
base_nodes
=
nullptr
;
malloc_mutex_init
(
&
base_mtx
)
;
malloc_spin_init
(
&
arenas_lock
)
;
arenas_extend
(
)
;
if
(
!
arenas
|
|
!
arenas
[
0
]
)
{
#
ifndef
XP_WIN
malloc_mutex_unlock
(
&
init_lock
)
;
#
endif
return
true
;
}
arenas
[
0
]
-
>
mMaxDirty
=
opt_dirty_max
;
#
ifndef
NO_TLS
thread_arena
.
set
(
arenas
[
0
]
)
;
#
endif
chunk_rtree
=
malloc_rtree_new
(
(
SIZEOF_PTR
<
<
3
)
-
opt_chunk_2pow
)
;
if
(
!
chunk_rtree
)
{
return
true
;
}
malloc_initialized
=
true
;
#
if
!
defined
(
XP_WIN
)
&
&
!
defined
(
XP_DARWIN
)
pthread_atfork
(
_malloc_prefork
_malloc_postfork_parent
_malloc_postfork_child
)
;
#
endif
#
ifndef
XP_WIN
malloc_mutex_unlock
(
&
init_lock
)
;
#
endif
return
false
;
}
template
<
>
inline
void
*
MozJemalloc
:
:
malloc
(
size_t
aSize
)
{
void
*
ret
;
if
(
malloc_init
(
)
)
{
ret
=
nullptr
;
goto
RETURN
;
}
if
(
aSize
=
=
0
)
{
aSize
=
1
;
}
ret
=
imalloc
(
aSize
)
;
RETURN
:
if
(
!
ret
)
{
errno
=
ENOMEM
;
}
return
ret
;
}
template
<
>
inline
void
*
MozJemalloc
:
:
memalign
(
size_t
aAlignment
size_t
aSize
)
{
void
*
ret
;
MOZ_ASSERT
(
(
(
aAlignment
-
1
)
&
aAlignment
)
=
=
0
)
;
if
(
malloc_init
(
)
)
{
return
nullptr
;
}
if
(
aSize
=
=
0
)
{
aSize
=
1
;
}
aAlignment
=
aAlignment
<
sizeof
(
void
*
)
?
sizeof
(
void
*
)
:
aAlignment
;
ret
=
ipalloc
(
aAlignment
aSize
)
;
return
ret
;
}
template
<
void
*
(
*
memalign
)
(
size_t
size_t
)
>
struct
AlignedAllocator
{
static
inline
int
posix_memalign
(
void
*
*
aMemPtr
size_t
aAlignment
size_t
aSize
)
{
void
*
result
;
if
(
(
(
aAlignment
-
1
)
&
aAlignment
)
!
=
0
|
|
aAlignment
<
sizeof
(
void
*
)
)
{
return
EINVAL
;
}
result
=
memalign
(
aAlignment
aSize
)
;
if
(
!
result
)
{
return
ENOMEM
;
}
*
aMemPtr
=
result
;
return
0
;
}
static
inline
void
*
aligned_alloc
(
size_t
aAlignment
size_t
aSize
)
{
if
(
aSize
%
aAlignment
)
{
return
nullptr
;
}
return
memalign
(
aAlignment
aSize
)
;
}
static
inline
void
*
valloc
(
size_t
aSize
)
{
return
memalign
(
GetKernelPageSize
(
)
aSize
)
;
}
}
;
template
<
>
inline
int
MozJemalloc
:
:
posix_memalign
(
void
*
*
aMemPtr
size_t
aAlignment
size_t
aSize
)
{
return
AlignedAllocator
<
memalign
>
:
:
posix_memalign
(
aMemPtr
aAlignment
aSize
)
;
}
template
<
>
inline
void
*
MozJemalloc
:
:
aligned_alloc
(
size_t
aAlignment
size_t
aSize
)
{
return
AlignedAllocator
<
memalign
>
:
:
aligned_alloc
(
aAlignment
aSize
)
;
}
template
<
>
inline
void
*
MozJemalloc
:
:
valloc
(
size_t
aSize
)
{
return
AlignedAllocator
<
memalign
>
:
:
valloc
(
aSize
)
;
}
template
<
>
inline
void
*
MozJemalloc
:
:
calloc
(
size_t
aNum
size_t
aSize
)
{
void
*
ret
;
size_t
num_size
;
if
(
malloc_init
(
)
)
{
num_size
=
0
;
ret
=
nullptr
;
goto
RETURN
;
}
num_size
=
aNum
*
aSize
;
if
(
num_size
=
=
0
)
{
num_size
=
1
;
}
else
if
(
(
(
aNum
|
aSize
)
&
(
SIZE_T_MAX
<
<
(
sizeof
(
size_t
)
<
<
2
)
)
)
&
&
(
num_size
/
aSize
!
=
aNum
)
)
{
ret
=
nullptr
;
goto
RETURN
;
}
ret
=
icalloc
(
num_size
)
;
RETURN
:
if
(
!
ret
)
{
errno
=
ENOMEM
;
}
return
ret
;
}
template
<
>
inline
void
*
MozJemalloc
:
:
realloc
(
void
*
aPtr
size_t
aSize
)
{
void
*
ret
;
if
(
aSize
=
=
0
)
{
aSize
=
1
;
}
if
(
aPtr
)
{
MOZ_ASSERT
(
malloc_initialized
)
;
ret
=
iralloc
(
aPtr
aSize
)
;
if
(
!
ret
)
{
errno
=
ENOMEM
;
}
}
else
{
if
(
malloc_init
(
)
)
{
ret
=
nullptr
;
}
else
{
ret
=
imalloc
(
aSize
)
;
}
if
(
!
ret
)
{
errno
=
ENOMEM
;
}
}
return
ret
;
}
template
<
>
inline
void
MozJemalloc
:
:
free
(
void
*
aPtr
)
{
size_t
offset
;
MOZ_ASSERT
(
CHUNK_ADDR2OFFSET
(
nullptr
)
=
=
0
)
;
offset
=
CHUNK_ADDR2OFFSET
(
aPtr
)
;
if
(
offset
!
=
0
)
{
arena_dalloc
(
aPtr
offset
)
;
}
else
if
(
aPtr
)
{
huge_dalloc
(
aPtr
)
;
}
}
template
<
>
inline
size_t
MozJemalloc
:
:
malloc_good_size
(
size_t
aSize
)
{
if
(
aSize
<
small_min
)
{
aSize
=
pow2_ceil
(
aSize
)
;
if
(
aSize
<
(
1U
<
<
TINY_MIN_2POW
)
)
aSize
=
(
1U
<
<
TINY_MIN_2POW
)
;
}
else
if
(
aSize
<
=
small_max
)
{
aSize
=
QUANTUM_CEILING
(
aSize
)
;
}
else
if
(
aSize
<
=
bin_maxclass
)
{
aSize
=
pow2_ceil
(
aSize
)
;
}
else
if
(
aSize
<
=
arena_maxclass
)
{
aSize
=
PAGE_CEILING
(
aSize
)
;
}
else
{
aSize
=
PAGE_CEILING
(
aSize
)
;
}
return
aSize
;
}
template
<
>
inline
size_t
MozJemalloc
:
:
malloc_usable_size
(
usable_ptr_t
aPtr
)
{
return
isalloc_validate
(
aPtr
)
;
}
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_stats
(
jemalloc_stats_t
*
aStats
)
{
size_t
i
non_arena_mapped
chunk_header_size
;
MOZ_ASSERT
(
aStats
)
;
aStats
-
>
opt_junk
=
opt_junk
;
aStats
-
>
opt_zero
=
opt_zero
;
aStats
-
>
narenas
=
narenas
;
aStats
-
>
quantum
=
quantum
;
aStats
-
>
small_max
=
small_max
;
aStats
-
>
large_max
=
arena_maxclass
;
aStats
-
>
chunksize
=
chunksize
;
aStats
-
>
page_size
=
pagesize
;
aStats
-
>
dirty_max
=
opt_dirty_max
;
aStats
-
>
mapped
=
0
;
aStats
-
>
allocated
=
0
;
aStats
-
>
waste
=
0
;
aStats
-
>
page_cache
=
0
;
aStats
-
>
bookkeeping
=
0
;
aStats
-
>
bin_unused
=
0
;
non_arena_mapped
=
0
;
malloc_mutex_lock
(
&
huge_mtx
)
;
non_arena_mapped
+
=
huge_mapped
;
aStats
-
>
allocated
+
=
huge_allocated
;
MOZ_ASSERT
(
huge_mapped
>
=
huge_allocated
)
;
malloc_mutex_unlock
(
&
huge_mtx
)
;
malloc_mutex_lock
(
&
base_mtx
)
;
non_arena_mapped
+
=
base_mapped
;
aStats
-
>
bookkeeping
+
=
base_committed
;
MOZ_ASSERT
(
base_mapped
>
=
base_committed
)
;
malloc_mutex_unlock
(
&
base_mtx
)
;
malloc_spin_lock
(
&
arenas_lock
)
;
for
(
i
=
0
;
i
<
narenas
;
i
+
+
)
{
arena_t
*
arena
=
arenas
[
i
]
;
size_t
arena_mapped
arena_allocated
arena_committed
arena_dirty
j
arena_unused
arena_headers
;
arena_run_t
*
run
;
arena_chunk_map_t
*
mapelm
;
if
(
!
arena
)
{
continue
;
}
arena_headers
=
0
;
arena_unused
=
0
;
malloc_spin_lock
(
&
arena
-
>
mLock
)
;
arena_mapped
=
arena
-
>
mStats
.
mapped
;
arena_committed
=
arena
-
>
mStats
.
committed
<
<
pagesize_2pow
;
arena_allocated
=
arena
-
>
mStats
.
allocated_small
+
arena
-
>
mStats
.
allocated_large
;
arena_dirty
=
arena
-
>
mNumDirty
<
<
pagesize_2pow
;
for
(
j
=
0
;
j
<
ntbins
+
nqbins
+
nsbins
;
j
+
+
)
{
arena_bin_t
*
bin
=
&
arena
-
>
mBins
[
j
]
;
size_t
bin_unused
=
0
;
rb_foreach_begin
(
arena_chunk_map_t
link
&
bin
-
>
runs
mapelm
)
{
run
=
(
arena_run_t
*
)
(
mapelm
-
>
bits
&
~
pagesize_mask
)
;
bin_unused
+
=
run
-
>
nfree
*
bin
-
>
reg_size
;
}
rb_foreach_end
(
arena_chunk_map_t
link
&
bin
-
>
runs
mapelm
)
if
(
bin
-
>
runcur
)
{
bin_unused
+
=
bin
-
>
runcur
-
>
nfree
*
bin
-
>
reg_size
;
}
arena_unused
+
=
bin_unused
;
arena_headers
+
=
bin
-
>
stats
.
curruns
*
bin
-
>
reg0_offset
;
}
malloc_spin_unlock
(
&
arena
-
>
mLock
)
;
MOZ_ASSERT
(
arena_mapped
>
=
arena_committed
)
;
MOZ_ASSERT
(
arena_committed
>
=
arena_allocated
+
arena_dirty
)
;
aStats
-
>
mapped
+
=
arena_mapped
;
aStats
-
>
allocated
+
=
arena_allocated
;
aStats
-
>
page_cache
+
=
arena_dirty
;
aStats
-
>
waste
+
=
arena_committed
-
arena_allocated
-
arena_dirty
-
arena_unused
-
arena_headers
;
aStats
-
>
bin_unused
+
=
arena_unused
;
aStats
-
>
bookkeeping
+
=
arena_headers
;
}
malloc_spin_unlock
(
&
arenas_lock
)
;
chunk_header_size
=
(
(
aStats
-
>
mapped
/
aStats
-
>
chunksize
)
*
arena_chunk_header_npages
)
<
<
pagesize_2pow
;
aStats
-
>
mapped
+
=
non_arena_mapped
;
aStats
-
>
bookkeeping
+
=
chunk_header_size
;
aStats
-
>
waste
-
=
chunk_header_size
;
MOZ_ASSERT
(
aStats
-
>
mapped
>
=
aStats
-
>
allocated
+
aStats
-
>
waste
+
aStats
-
>
page_cache
+
aStats
-
>
bookkeeping
)
;
}
#
ifdef
MALLOC_DOUBLE_PURGE
static
void
hard_purge_chunk
(
arena_chunk_t
*
chunk
)
{
size_t
i
;
for
(
i
=
arena_chunk_header_npages
;
i
<
chunk_npages
;
i
+
+
)
{
size_t
npages
;
for
(
npages
=
0
;
chunk
-
>
map
[
i
+
npages
]
.
bits
&
CHUNK_MAP_MADVISED
&
&
i
+
npages
<
chunk_npages
;
npages
+
+
)
{
MOZ_DIAGNOSTIC_ASSERT
(
!
(
chunk
-
>
map
[
i
+
npages
]
.
bits
&
CHUNK_MAP_DECOMMITTED
)
)
;
chunk
-
>
map
[
i
+
npages
]
.
bits
^
=
CHUNK_MAP_MADVISED_OR_DECOMMITTED
;
}
if
(
npages
>
0
)
{
pages_decommit
(
(
(
char
*
)
chunk
)
+
(
i
<
<
pagesize_2pow
)
npages
<
<
pagesize_2pow
)
;
pages_commit
(
(
(
char
*
)
chunk
)
+
(
i
<
<
pagesize_2pow
)
npages
<
<
pagesize_2pow
)
;
}
i
+
=
npages
;
}
}
void
arena_t
:
:
HardPurge
(
)
{
malloc_spin_lock
(
&
mLock
)
;
while
(
!
mChunksMAdvised
.
isEmpty
(
)
)
{
arena_chunk_t
*
chunk
=
mChunksMAdvised
.
popFront
(
)
;
hard_purge_chunk
(
chunk
)
;
}
malloc_spin_unlock
(
&
mLock
)
;
}
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_purge_freed_pages
(
)
{
size_t
i
;
malloc_spin_lock
(
&
arenas_lock
)
;
for
(
i
=
0
;
i
<
narenas
;
i
+
+
)
{
arena_t
*
arena
=
arenas
[
i
]
;
if
(
arena
)
{
arena
-
>
HardPurge
(
)
;
}
}
malloc_spin_unlock
(
&
arenas_lock
)
;
}
#
else
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_purge_freed_pages
(
)
{
}
#
endif
template
<
>
inline
void
MozJemalloc
:
:
jemalloc_free_dirty_pages
(
void
)
{
size_t
i
;
malloc_spin_lock
(
&
arenas_lock
)
;
for
(
i
=
0
;
i
<
narenas
;
i
+
+
)
{
arena_t
*
arena
=
arenas
[
i
]
;
if
(
arena
)
{
malloc_spin_lock
(
&
arena
-
>
mLock
)
;
arena
-
>
Purge
(
true
)
;
malloc_spin_unlock
(
&
arena
-
>
mLock
)
;
}
}
malloc_spin_unlock
(
&
arenas_lock
)
;
}
#
ifndef
XP_DARWIN
static
#
endif
void
_malloc_prefork
(
void
)
{
unsigned
i
;
malloc_spin_lock
(
&
arenas_lock
)
;
for
(
i
=
0
;
i
<
narenas
;
i
+
+
)
{
if
(
arenas
[
i
]
)
malloc_spin_lock
(
&
arenas
[
i
]
-
>
mLock
)
;
}
malloc_mutex_lock
(
&
base_mtx
)
;
malloc_mutex_lock
(
&
huge_mtx
)
;
}
#
ifndef
XP_DARWIN
static
#
endif
void
_malloc_postfork_parent
(
void
)
{
unsigned
i
;
malloc_mutex_unlock
(
&
huge_mtx
)
;
malloc_mutex_unlock
(
&
base_mtx
)
;
for
(
i
=
0
;
i
<
narenas
;
i
+
+
)
{
if
(
arenas
[
i
]
)
malloc_spin_unlock
(
&
arenas
[
i
]
-
>
mLock
)
;
}
malloc_spin_unlock
(
&
arenas_lock
)
;
}
#
ifndef
XP_DARWIN
static
#
endif
void
_malloc_postfork_child
(
void
)
{
unsigned
i
;
malloc_mutex_init
(
&
huge_mtx
)
;
malloc_mutex_init
(
&
base_mtx
)
;
for
(
i
=
0
;
i
<
narenas
;
i
+
+
)
{
if
(
arenas
[
i
]
)
malloc_spin_init
(
&
arenas
[
i
]
-
>
mLock
)
;
}
malloc_spin_init
(
&
arenas_lock
)
;
}
#
define
MACRO_CALL
(
a
b
)
a
b
#
define
MACRO_CALL2
(
a
b
)
a
b
#
define
ARGS_HELPER
(
name
.
.
.
)
MACRO_CALL2
(
\
MOZ_PASTE_PREFIX_AND_ARG_COUNT
(
name
#
#
__VA_ARGS__
)
\
(
__VA_ARGS__
)
)
#
define
TYPED_ARGS0
(
)
#
define
TYPED_ARGS1
(
t1
)
t1
arg1
#
define
TYPED_ARGS2
(
t1
t2
)
TYPED_ARGS1
(
t1
)
t2
arg2
#
define
TYPED_ARGS3
(
t1
t2
t3
)
TYPED_ARGS2
(
t1
t2
)
t3
arg3
#
define
ARGS0
(
)
#
define
ARGS1
(
t1
)
arg1
#
define
ARGS2
(
t1
t2
)
ARGS1
(
t1
)
arg2
#
define
ARGS3
(
t1
t2
t3
)
ARGS2
(
t1
t2
)
arg3
#
ifdef
MOZ_REPLACE_MALLOC
#
ifdef
XP_DARWIN
#
define
MOZ_REPLACE_WEAK
__attribute__
(
(
weak_import
)
)
#
elif
defined
(
XP_WIN
)
|
|
defined
(
MOZ_WIDGET_ANDROID
)
#
define
MOZ_NO_REPLACE_FUNC_DECL
#
elif
defined
(
__GNUC__
)
#
define
MOZ_REPLACE_WEAK
__attribute__
(
(
weak
)
)
#
endif
#
include
"
replace_malloc
.
h
"
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
MozJemalloc
:
:
name
static
const
malloc_table_t
malloc_table
=
{
#
include
"
malloc_decls
.
h
"
}
;
static
malloc_table_t
replace_malloc_table
;
#
ifdef
MOZ_NO_REPLACE_FUNC_DECL
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
typedef
return_type
(
name
#
#
_impl_t
)
(
__VA_ARGS__
)
;
\
name
#
#
_impl_t
*
replace_
#
#
name
=
nullptr
;
#
define
MALLOC_FUNCS
(
MALLOC_FUNCS_INIT
|
MALLOC_FUNCS_BRIDGE
)
#
include
"
malloc_decls
.
h
"
#
endif
#
ifdef
XP_WIN
typedef
HMODULE
replace_malloc_handle_t
;
static
replace_malloc_handle_t
replace_malloc_handle
(
)
{
char
replace_malloc_lib
[
1024
]
;
if
(
GetEnvironmentVariableA
(
"
MOZ_REPLACE_MALLOC_LIB
"
(
LPSTR
)
&
replace_malloc_lib
sizeof
(
replace_malloc_lib
)
)
>
0
)
{
return
LoadLibraryA
(
replace_malloc_lib
)
;
}
return
nullptr
;
}
#
define
REPLACE_MALLOC_GET_FUNC
(
handle
name
)
\
(
name
#
#
_impl_t
*
)
GetProcAddress
(
handle
"
replace_
"
#
name
)
#
elif
defined
(
ANDROID
)
#
include
<
dlfcn
.
h
>
typedef
void
*
replace_malloc_handle_t
;
static
replace_malloc_handle_t
replace_malloc_handle
(
)
{
const
char
*
replace_malloc_lib
=
getenv
(
"
MOZ_REPLACE_MALLOC_LIB
"
)
;
if
(
replace_malloc_lib
&
&
*
replace_malloc_lib
)
{
return
dlopen
(
replace_malloc_lib
RTLD_LAZY
)
;
}
return
nullptr
;
}
#
define
REPLACE_MALLOC_GET_FUNC
(
handle
name
)
\
(
name
#
#
_impl_t
*
)
dlsym
(
handle
"
replace_
"
#
name
)
#
else
typedef
bool
replace_malloc_handle_t
;
static
replace_malloc_handle_t
replace_malloc_handle
(
)
{
return
true
;
}
#
define
REPLACE_MALLOC_GET_FUNC
(
handle
name
)
\
replace_
#
#
name
#
endif
static
void
replace_malloc_init_funcs
(
)
;
static
int
replace_malloc_initialized
=
0
;
static
void
init
(
)
{
replace_malloc_init_funcs
(
)
;
replace_malloc_initialized
=
1
;
if
(
replace_init
)
{
replace_init
(
&
malloc_table
)
;
}
}
#
define
MALLOC_DECL
(
name
return_type
.
.
.
)
\
template
<
>
inline
return_type
\
ReplaceMalloc
:
:
name
(
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
\
{
\
if
(
MOZ_UNLIKELY
(
!
replace_malloc_initialized
)
)
{
\
init
(
)
;
\
}
\
return
replace_malloc_table
.
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
define
MALLOC_FUNCS
(
MALLOC_FUNCS_MALLOC
|
MALLOC_FUNCS_JEMALLOC
)
#
include
"
malloc_decls
.
h
"
MOZ_JEMALLOC_API
struct
ReplaceMallocBridge
*
get_bridge
(
void
)
{
if
(
MOZ_UNLIKELY
(
!
replace_malloc_initialized
)
)
init
(
)
;
if
(
MOZ_LIKELY
(
!
replace_get_bridge
)
)
return
nullptr
;
return
replace_get_bridge
(
)
;
}
static
void
replace_malloc_init_funcs
(
)
{
replace_malloc_handle_t
handle
=
replace_malloc_handle
(
)
;
if
(
handle
)
{
#
ifdef
MOZ_NO_REPLACE_FUNC_DECL
#
define
MALLOC_DECL
(
name
.
.
.
)
\
replace_
#
#
name
=
REPLACE_MALLOC_GET_FUNC
(
handle
name
)
;
#
define
MALLOC_FUNCS
(
MALLOC_FUNCS_INIT
|
MALLOC_FUNCS_BRIDGE
)
#
include
"
malloc_decls
.
h
"
#
endif
#
define
MALLOC_DECL
(
name
.
.
.
)
\
replace_malloc_table
.
name
=
REPLACE_MALLOC_GET_FUNC
(
handle
name
)
;
#
include
"
malloc_decls
.
h
"
}
if
(
!
replace_malloc_table
.
posix_memalign
&
&
replace_malloc_table
.
memalign
)
{
replace_malloc_table
.
posix_memalign
=
AlignedAllocator
<
ReplaceMalloc
:
:
memalign
>
:
:
posix_memalign
;
}
if
(
!
replace_malloc_table
.
aligned_alloc
&
&
replace_malloc_table
.
memalign
)
{
replace_malloc_table
.
aligned_alloc
=
AlignedAllocator
<
ReplaceMalloc
:
:
memalign
>
:
:
aligned_alloc
;
}
if
(
!
replace_malloc_table
.
valloc
&
&
replace_malloc_table
.
memalign
)
{
replace_malloc_table
.
valloc
=
AlignedAllocator
<
ReplaceMalloc
:
:
memalign
>
:
:
valloc
;
}
#
define
MALLOC_DECL
(
name
.
.
.
)
\
if
(
!
replace_malloc_table
.
name
)
{
\
replace_malloc_table
.
name
=
MozJemalloc
:
:
name
;
\
}
#
include
"
malloc_decls
.
h
"
}
#
endif
#
define
GENERIC_MALLOC_DECL2
(
name
name_impl
return_type
.
.
.
)
\
return_type
name_impl
(
ARGS_HELPER
(
TYPED_ARGS
#
#
__VA_ARGS__
)
)
\
{
\
return
DefaultMalloc
:
:
name
(
ARGS_HELPER
(
ARGS
#
#
__VA_ARGS__
)
)
;
\
}
#
define
GENERIC_MALLOC_DECL
(
name
return_type
.
.
.
)
\
GENERIC_MALLOC_DECL2
(
name
name
#
#
_impl
return_type
#
#
__VA_ARGS__
)
#
define
MALLOC_DECL
(
.
.
.
)
MOZ_MEMORY_API
MACRO_CALL
(
GENERIC_MALLOC_DECL
(
__VA_ARGS__
)
)
#
define
MALLOC_FUNCS
MALLOC_FUNCS_MALLOC
#
include
"
malloc_decls
.
h
"
#
undef
GENERIC_MALLOC_DECL
#
define
GENERIC_MALLOC_DECL
(
name
return_type
.
.
.
)
\
GENERIC_MALLOC_DECL2
(
name
name
return_type
#
#
__VA_ARGS__
)
#
define
MALLOC_DECL
(
.
.
.
)
MOZ_JEMALLOC_API
MACRO_CALL
(
GENERIC_MALLOC_DECL
(
__VA_ARGS__
)
)
#
define
MALLOC_FUNCS
MALLOC_FUNCS_JEMALLOC
#
include
"
malloc_decls
.
h
"
#
ifdef
HAVE_DLOPEN
#
include
<
dlfcn
.
h
>
#
endif
#
if
defined
(
__GLIBC__
)
&
&
!
defined
(
__UCLIBC__
)
extern
"
C
"
{
MOZ_EXPORT
void
(
*
__free_hook
)
(
void
*
)
=
free_impl
;
MOZ_EXPORT
void
*
(
*
__malloc_hook
)
(
size_t
)
=
malloc_impl
;
MOZ_EXPORT
void
*
(
*
__realloc_hook
)
(
void
*
size_t
)
=
realloc_impl
;
MOZ_EXPORT
void
*
(
*
__memalign_hook
)
(
size_t
size_t
)
=
memalign_impl
;
}
#
elif
defined
(
RTLD_DEEPBIND
)
#
error
"
Interposing
malloc
is
unsafe
on
this
system
without
libc
malloc
hooks
.
"
#
endif
#
ifdef
XP_WIN
void
*
_recalloc
(
void
*
aPtr
size_t
aCount
size_t
aSize
)
{
size_t
oldsize
=
aPtr
?
isalloc
(
aPtr
)
:
0
;
size_t
newsize
=
aCount
*
aSize
;
aPtr
=
DefaultMalloc
:
:
realloc
(
aPtr
newsize
)
;
if
(
aPtr
&
&
oldsize
<
newsize
)
{
memset
(
(
void
*
)
(
(
uintptr_t
)
aPtr
+
oldsize
)
0
newsize
-
oldsize
)
;
}
return
aPtr
;
}
void
*
_expand
(
void
*
aPtr
size_t
newsize
)
{
if
(
isalloc
(
aPtr
)
>
=
newsize
)
{
return
aPtr
;
}
return
nullptr
;
}
size_t
_msize
(
void
*
aPtr
)
{
return
DefaultMalloc
:
:
malloc_usable_size
(
aPtr
)
;
}
BOOL
APIENTRY
DllMain
(
HINSTANCE
hModule
DWORD
reason
LPVOID
lpReserved
)
{
switch
(
reason
)
{
case
DLL_PROCESS_ATTACH
:
DisableThreadLibraryCalls
(
hModule
)
;
malloc_init_hard
(
)
;
break
;
case
DLL_PROCESS_DETACH
:
break
;
}
return
TRUE
;
}
#
endif
