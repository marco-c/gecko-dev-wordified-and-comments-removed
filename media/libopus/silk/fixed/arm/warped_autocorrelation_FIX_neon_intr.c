#
ifdef
HAVE_CONFIG_H
#
include
"
config
.
h
"
#
endif
#
include
<
arm_neon
.
h
>
#
ifdef
OPUS_CHECK_ASM
#
include
<
string
.
h
>
#
endif
#
include
"
stack_alloc
.
h
"
#
include
"
main_FIX
.
h
"
static
OPUS_INLINE
void
calc_corr
(
const
opus_int32
*
const
input_QS
opus_int64
*
const
corr_QC
const
opus_int
offset
const
int32x4_t
state_QS_s32x4
)
{
int64x2_t
corr_QC_s64x2
[
2
]
t_s64x2
[
2
]
;
const
int32x4_t
input_QS_s32x4
=
vld1q_s32
(
input_QS
+
offset
)
;
corr_QC_s64x2
[
0
]
=
vld1q_s64
(
corr_QC
+
offset
+
0
)
;
corr_QC_s64x2
[
1
]
=
vld1q_s64
(
corr_QC
+
offset
+
2
)
;
t_s64x2
[
0
]
=
vmull_s32
(
vget_low_s32
(
state_QS_s32x4
)
vget_low_s32
(
input_QS_s32x4
)
)
;
t_s64x2
[
1
]
=
vmull_s32
(
vget_high_s32
(
state_QS_s32x4
)
vget_high_s32
(
input_QS_s32x4
)
)
;
corr_QC_s64x2
[
0
]
=
vsraq_n_s64
(
corr_QC_s64x2
[
0
]
t_s64x2
[
0
]
2
*
QS
-
QC
)
;
corr_QC_s64x2
[
1
]
=
vsraq_n_s64
(
corr_QC_s64x2
[
1
]
t_s64x2
[
1
]
2
*
QS
-
QC
)
;
vst1q_s64
(
corr_QC
+
offset
+
0
corr_QC_s64x2
[
0
]
)
;
vst1q_s64
(
corr_QC
+
offset
+
2
corr_QC_s64x2
[
1
]
)
;
}
static
OPUS_INLINE
int32x4_t
calc_state
(
const
int32x4_t
state_QS0_s32x4
const
int32x4_t
state_QS0_1_s32x4
const
int32x4_t
state_QS1_1_s32x4
const
int32x4_t
warping_Q16_s32x4
)
{
int32x4_t
t_s32x4
=
vsubq_s32
(
state_QS0_s32x4
state_QS0_1_s32x4
)
;
t_s32x4
=
vqdmulhq_s32
(
t_s32x4
warping_Q16_s32x4
)
;
return
vaddq_s32
(
state_QS1_1_s32x4
t_s32x4
)
;
}
void
silk_warped_autocorrelation_FIX_neon
(
opus_int32
*
corr
opus_int
*
scale
const
opus_int16
*
input
const
opus_int
warping_Q16
const
opus_int
length
const
opus_int
order
)
{
if
(
(
MAX_SHAPE_LPC_ORDER
>
24
)
|
|
(
order
<
6
)
)
{
silk_warped_autocorrelation_FIX_c
(
corr
scale
input
warping_Q16
length
order
)
;
}
else
{
opus_int
n
i
lsh
;
opus_int64
corr_QC
[
MAX_SHAPE_LPC_ORDER
+
1
]
=
{
0
}
;
opus_int64
corr_QC_orderT
;
int64x2_t
lsh_s64x2
;
const
opus_int
orderT
=
(
order
+
3
)
&
~
3
;
opus_int64
*
corr_QCT
;
opus_int32
*
input_QS
;
VARDECL
(
opus_int32
input_QST
)
;
VARDECL
(
opus_int32
state
)
;
SAVE_STACK
;
silk_assert
(
(
order
&
1
)
=
=
0
)
;
silk_assert
(
2
*
QS
-
QC
>
=
0
)
;
ALLOC
(
input_QST
length
+
2
*
MAX_SHAPE_LPC_ORDER
+
4
opus_int32
)
;
input_QS
=
input_QST
;
vst1q_s32
(
input_QS
vdupq_n_s32
(
0
)
)
;
input_QS
+
=
4
;
vst1q_s32
(
input_QS
vdupq_n_s32
(
0
)
)
;
input_QS
+
=
4
;
vst1q_s32
(
input_QS
vdupq_n_s32
(
0
)
)
;
input_QS
+
=
4
;
vst1q_s32
(
input_QS
vdupq_n_s32
(
0
)
)
;
input_QS
+
=
4
;
vst1q_s32
(
input_QS
vdupq_n_s32
(
0
)
)
;
input_QS
+
=
4
;
vst1q_s32
(
input_QS
vdupq_n_s32
(
0
)
)
;
input_QS
+
=
4
;
for
(
n
=
0
;
n
<
length
-
7
;
n
+
=
8
input_QS
+
=
8
)
{
const
int16x8_t
t0_s16x4
=
vld1q_s16
(
input
+
n
)
;
vst1q_s32
(
input_QS
+
0
vshll_n_s16
(
vget_low_s16
(
t0_s16x4
)
QS
)
)
;
vst1q_s32
(
input_QS
+
4
vshll_n_s16
(
vget_high_s16
(
t0_s16x4
)
QS
)
)
;
}
for
(
;
n
<
length
;
n
+
+
input_QS
+
+
)
{
input_QS
[
0
]
=
silk_LSHIFT32
(
(
opus_int32
)
input
[
n
]
QS
)
;
}
vst1q_s32
(
input_QS
vdupq_n_s32
(
0
)
)
;
input_QS
+
=
4
;
vst1q_s32
(
input_QS
vdupq_n_s32
(
0
)
)
;
input_QS
+
=
4
;
vst1q_s32
(
input_QS
vdupq_n_s32
(
0
)
)
;
input_QS
+
=
4
;
vst1q_s32
(
input_QS
vdupq_n_s32
(
0
)
)
;
input_QS
+
=
4
;
vst1q_s32
(
input_QS
vdupq_n_s32
(
0
)
)
;
input_QS
+
=
4
;
vst1q_s32
(
input_QS
vdupq_n_s32
(
0
)
)
;
input_QS
+
=
4
;
vst1q_s32
(
input_QS
vdupq_n_s32
(
0
)
)
;
input_QS
=
input_QST
+
MAX_SHAPE_LPC_ORDER
-
orderT
;
{
const
int32x4_t
warping_Q16_s32x4
=
vdupq_n_s32
(
warping_Q16
<
<
15
)
;
const
opus_int32
*
in
=
input_QS
+
orderT
;
opus_int
o
=
orderT
;
int32x4_t
state_QS_s32x4
[
3
]
[
2
]
;
ALLOC
(
state
length
+
order
+
4
opus_int32
)
;
state_QS_s32x4
[
2
]
[
1
]
=
vdupq_n_s32
(
0
)
;
do
{
state_QS_s32x4
[
0
]
[
0
]
=
state_QS_s32x4
[
0
]
[
1
]
=
state_QS_s32x4
[
1
]
[
0
]
=
state_QS_s32x4
[
1
]
[
1
]
=
vdupq_n_s32
(
0
)
;
n
=
0
;
do
{
calc_corr
(
input_QS
+
n
corr_QC
o
-
8
state_QS_s32x4
[
0
]
[
0
]
)
;
calc_corr
(
input_QS
+
n
corr_QC
o
-
4
state_QS_s32x4
[
0
]
[
1
]
)
;
state_QS_s32x4
[
2
]
[
1
]
=
vld1q_s32
(
in
+
n
)
;
vst1q_lane_s32
(
state
+
n
state_QS_s32x4
[
0
]
[
0
]
0
)
;
state_QS_s32x4
[
2
]
[
0
]
=
vextq_s32
(
state_QS_s32x4
[
0
]
[
0
]
state_QS_s32x4
[
0
]
[
1
]
1
)
;
state_QS_s32x4
[
2
]
[
1
]
=
vextq_s32
(
state_QS_s32x4
[
0
]
[
1
]
state_QS_s32x4
[
2
]
[
1
]
1
)
;
state_QS_s32x4
[
0
]
[
0
]
=
calc_state
(
state_QS_s32x4
[
0
]
[
0
]
state_QS_s32x4
[
2
]
[
0
]
state_QS_s32x4
[
1
]
[
0
]
warping_Q16_s32x4
)
;
state_QS_s32x4
[
0
]
[
1
]
=
calc_state
(
state_QS_s32x4
[
0
]
[
1
]
state_QS_s32x4
[
2
]
[
1
]
state_QS_s32x4
[
1
]
[
1
]
warping_Q16_s32x4
)
;
state_QS_s32x4
[
1
]
[
0
]
=
state_QS_s32x4
[
2
]
[
0
]
;
state_QS_s32x4
[
1
]
[
1
]
=
state_QS_s32x4
[
2
]
[
1
]
;
}
while
(
+
+
n
<
(
length
+
order
)
)
;
in
=
state
;
o
-
=
8
;
}
while
(
o
>
4
)
;
if
(
o
)
{
opus_int32
*
stateT
=
state
;
silk_assert
(
o
=
=
4
)
;
state_QS_s32x4
[
0
]
[
0
]
=
state_QS_s32x4
[
1
]
[
0
]
=
vdupq_n_s32
(
0
)
;
n
=
length
+
order
;
do
{
calc_corr
(
input_QS
corr_QC
0
state_QS_s32x4
[
0
]
[
0
]
)
;
state_QS_s32x4
[
2
]
[
0
]
=
vld1q_s32
(
stateT
)
;
vst1q_lane_s32
(
stateT
state_QS_s32x4
[
0
]
[
0
]
0
)
;
state_QS_s32x4
[
2
]
[
0
]
=
vextq_s32
(
state_QS_s32x4
[
0
]
[
0
]
state_QS_s32x4
[
2
]
[
0
]
1
)
;
state_QS_s32x4
[
0
]
[
0
]
=
calc_state
(
state_QS_s32x4
[
0
]
[
0
]
state_QS_s32x4
[
2
]
[
0
]
state_QS_s32x4
[
1
]
[
0
]
warping_Q16_s32x4
)
;
state_QS_s32x4
[
1
]
[
0
]
=
state_QS_s32x4
[
2
]
[
0
]
;
input_QS
+
+
;
stateT
+
+
;
}
while
(
-
-
n
)
;
}
}
{
const
opus_int16
*
inputT
=
input
;
int32x4_t
t_s32x4
;
int64x1_t
t_s64x1
;
int64x2_t
t_s64x2
=
vdupq_n_s64
(
0
)
;
for
(
n
=
0
;
n
<
=
length
-
8
;
n
+
=
8
)
{
int16x8_t
input_s16x8
=
vld1q_s16
(
inputT
)
;
t_s32x4
=
vmull_s16
(
vget_low_s16
(
input_s16x8
)
vget_low_s16
(
input_s16x8
)
)
;
t_s32x4
=
vmlal_s16
(
t_s32x4
vget_high_s16
(
input_s16x8
)
vget_high_s16
(
input_s16x8
)
)
;
t_s64x2
=
vaddw_s32
(
t_s64x2
vget_low_s32
(
t_s32x4
)
)
;
t_s64x2
=
vaddw_s32
(
t_s64x2
vget_high_s32
(
t_s32x4
)
)
;
inputT
+
=
8
;
}
t_s64x1
=
vadd_s64
(
vget_low_s64
(
t_s64x2
)
vget_high_s64
(
t_s64x2
)
)
;
corr_QC_orderT
=
vget_lane_s64
(
t_s64x1
0
)
;
for
(
;
n
<
length
;
n
+
+
)
{
corr_QC_orderT
+
=
silk_SMULL
(
input
[
n
]
input
[
n
]
)
;
}
corr_QC_orderT
=
silk_LSHIFT64
(
corr_QC_orderT
QC
)
;
corr_QC
[
orderT
]
=
corr_QC_orderT
;
}
corr_QCT
=
corr_QC
+
orderT
-
order
;
lsh
=
silk_CLZ64
(
corr_QC_orderT
)
-
35
;
lsh
=
silk_LIMIT
(
lsh
-
12
-
QC
30
-
QC
)
;
*
scale
=
-
(
QC
+
lsh
)
;
silk_assert
(
*
scale
>
=
-
30
&
&
*
scale
<
=
12
)
;
lsh_s64x2
=
vdupq_n_s64
(
lsh
)
;
for
(
i
=
0
;
i
<
=
order
-
3
;
i
+
=
4
)
{
int32x4_t
corr_s32x4
;
int64x2_t
corr_QC0_s64x2
corr_QC1_s64x2
;
corr_QC0_s64x2
=
vld1q_s64
(
corr_QCT
+
i
)
;
corr_QC1_s64x2
=
vld1q_s64
(
corr_QCT
+
i
+
2
)
;
corr_QC0_s64x2
=
vshlq_s64
(
corr_QC0_s64x2
lsh_s64x2
)
;
corr_QC1_s64x2
=
vshlq_s64
(
corr_QC1_s64x2
lsh_s64x2
)
;
corr_s32x4
=
vcombine_s32
(
vmovn_s64
(
corr_QC1_s64x2
)
vmovn_s64
(
corr_QC0_s64x2
)
)
;
corr_s32x4
=
vrev64q_s32
(
corr_s32x4
)
;
vst1q_s32
(
corr
+
order
-
i
-
3
corr_s32x4
)
;
}
if
(
lsh
>
=
0
)
{
for
(
;
i
<
order
+
1
;
i
+
+
)
{
corr
[
order
-
i
]
=
(
opus_int32
)
silk_CHECK_FIT32
(
silk_LSHIFT64
(
corr_QCT
[
i
]
lsh
)
)
;
}
}
else
{
for
(
;
i
<
order
+
1
;
i
+
+
)
{
corr
[
order
-
i
]
=
(
opus_int32
)
silk_CHECK_FIT32
(
silk_RSHIFT64
(
corr_QCT
[
i
]
-
lsh
)
)
;
}
}
silk_assert
(
corr_QCT
[
order
]
>
=
0
)
;
RESTORE_STACK
;
}
#
ifdef
OPUS_CHECK_ASM
{
opus_int32
corr_c
[
MAX_SHAPE_LPC_ORDER
+
1
]
;
opus_int
scale_c
;
silk_warped_autocorrelation_FIX_c
(
corr_c
&
scale_c
input
warping_Q16
length
order
)
;
silk_assert
(
!
memcmp
(
corr_c
corr
sizeof
(
corr_c
[
0
]
)
*
(
order
+
1
)
)
)
;
silk_assert
(
scale_c
=
=
*
scale
)
;
}
#
endif
}
