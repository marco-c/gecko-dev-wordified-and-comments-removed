#
ifndef
AVUTIL_MEM_INTERNAL_H
#
define
AVUTIL_MEM_INTERNAL_H
#
include
"
config
.
h
"
#
include
<
stdint
.
h
>
#
include
"
avassert
.
h
"
#
include
"
mem
.
h
"
#
include
"
version
.
h
"
#
if
!
FF_API_DECLARE_ALIGNED
#
if
defined
(
__INTEL_COMPILER
)
&
&
__INTEL_COMPILER
<
1110
|
|
defined
(
__SUNPRO_C
)
#
define
DECLARE_ALIGNED
(
n
t
v
)
t
__attribute__
(
(
aligned
(
n
)
)
)
v
#
define
DECLARE_ASM_ALIGNED
(
n
t
v
)
t
__attribute__
(
(
aligned
(
n
)
)
)
v
#
define
DECLARE_ASM_CONST
(
n
t
v
)
const
t
__attribute__
(
(
aligned
(
n
)
)
)
v
#
elif
defined
(
__DJGPP__
)
#
define
DECLARE_ALIGNED
(
n
t
v
)
t
__attribute__
(
(
aligned
(
FFMIN
(
n
16
)
)
)
)
v
#
define
DECLARE_ASM_ALIGNED
(
n
t
v
)
t
av_used
__attribute__
(
(
aligned
(
FFMIN
(
n
16
)
)
)
)
v
#
define
DECLARE_ASM_CONST
(
n
t
v
)
static
const
t
av_used
__attribute__
(
(
aligned
(
FFMIN
(
n
16
)
)
)
)
v
#
elif
defined
(
__GNUC__
)
|
|
defined
(
__clang__
)
#
define
DECLARE_ALIGNED
(
n
t
v
)
t
__attribute__
(
(
aligned
(
n
)
)
)
v
#
define
DECLARE_ASM_ALIGNED
(
n
t
v
)
t
av_used
__attribute__
(
(
aligned
(
n
)
)
)
v
#
define
DECLARE_ASM_CONST
(
n
t
v
)
static
const
t
av_used
__attribute__
(
(
aligned
(
n
)
)
)
v
#
elif
defined
(
_MSC_VER
)
#
define
DECLARE_ALIGNED
(
n
t
v
)
__declspec
(
align
(
n
)
)
t
v
#
define
DECLARE_ASM_ALIGNED
(
n
t
v
)
__declspec
(
align
(
n
)
)
t
v
#
define
DECLARE_ASM_CONST
(
n
t
v
)
__declspec
(
align
(
n
)
)
static
const
t
v
#
else
#
define
DECLARE_ALIGNED
(
n
t
v
)
t
v
#
define
DECLARE_ASM_ALIGNED
(
n
t
v
)
t
v
#
define
DECLARE_ASM_CONST
(
n
t
v
)
static
const
t
v
#
endif
#
endif
#
define
E1
(
x
)
x
#
define
LOCAL_ALIGNED_A
(
a
t
v
s
o
.
.
.
)
\
uint8_t
la_
#
#
v
[
sizeof
(
t
s
o
)
+
(
a
)
]
;
\
t
(
*
v
)
o
=
(
void
*
)
FFALIGN
(
(
uintptr_t
)
la_
#
#
v
a
)
#
define
LOCAL_ALIGNED_D
(
a
t
v
s
o
.
.
.
)
\
DECLARE_ALIGNED
(
a
t
la_
#
#
v
)
s
o
;
\
t
(
*
v
)
o
=
la_
#
#
v
#
define
LOCAL_ALIGNED
(
a
t
v
.
.
.
)
LOCAL_ALIGNED_
#
#
a
(
t
v
__VA_ARGS__
)
#
if
HAVE_LOCAL_ALIGNED
#
define
LOCAL_ALIGNED_4
(
t
v
.
.
.
)
E1
(
LOCAL_ALIGNED_D
(
4
t
v
__VA_ARGS__
)
)
#
else
#
define
LOCAL_ALIGNED_4
(
t
v
.
.
.
)
E1
(
LOCAL_ALIGNED_A
(
4
t
v
__VA_ARGS__
)
)
#
endif
#
if
HAVE_LOCAL_ALIGNED
#
define
LOCAL_ALIGNED_8
(
t
v
.
.
.
)
E1
(
LOCAL_ALIGNED_D
(
8
t
v
__VA_ARGS__
)
)
#
else
#
define
LOCAL_ALIGNED_8
(
t
v
.
.
.
)
E1
(
LOCAL_ALIGNED_A
(
8
t
v
__VA_ARGS__
)
)
#
endif
#
if
HAVE_LOCAL_ALIGNED
#
define
LOCAL_ALIGNED_16
(
t
v
.
.
.
)
E1
(
LOCAL_ALIGNED_D
(
16
t
v
__VA_ARGS__
)
)
#
else
#
define
LOCAL_ALIGNED_16
(
t
v
.
.
.
)
E1
(
LOCAL_ALIGNED_A
(
16
t
v
__VA_ARGS__
)
)
#
endif
#
if
HAVE_LOCAL_ALIGNED
#
define
LOCAL_ALIGNED_32
(
t
v
.
.
.
)
E1
(
LOCAL_ALIGNED_D
(
32
t
v
__VA_ARGS__
)
)
#
else
#
define
LOCAL_ALIGNED_32
(
t
v
.
.
.
)
E1
(
LOCAL_ALIGNED_A
(
32
t
v
__VA_ARGS__
)
)
#
endif
static
inline
int
ff_fast_malloc
(
void
*
ptr
unsigned
int
*
size
size_t
min_size
int
zero_realloc
)
{
void
*
val
;
memcpy
(
&
val
ptr
sizeof
(
val
)
)
;
if
(
min_size
<
=
*
size
)
{
av_assert0
(
val
|
|
!
min_size
)
;
return
0
;
}
min_size
=
FFMAX
(
min_size
+
min_size
/
16
+
32
min_size
)
;
av_freep
(
ptr
)
;
val
=
zero_realloc
?
av_mallocz
(
min_size
)
:
av_malloc
(
min_size
)
;
memcpy
(
ptr
&
val
sizeof
(
val
)
)
;
if
(
!
val
)
min_size
=
0
;
*
size
=
min_size
;
return
1
;
}
#
endif
