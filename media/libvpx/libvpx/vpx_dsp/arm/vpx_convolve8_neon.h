#
ifndef
VPX_VPX_DSP_ARM_VPX_CONVOLVE8_NEON_H_
#
define
VPX_VPX_DSP_ARM_VPX_CONVOLVE8_NEON_H_
#
include
<
arm_neon
.
h
>
#
include
"
.
/
vpx_config
.
h
"
#
include
"
.
/
vpx_dsp_rtcd
.
h
"
#
include
"
vpx_dsp
/
vpx_filter
.
h
"
#
if
VPX_ARCH_AARCH64
&
&
defined
(
__ARM_FEATURE_DOTPROD
)
void
vpx_convolve8_2d_horiz_neon_dotprod
(
const
uint8_t
*
src
ptrdiff_t
src_stride
uint8_t
*
dst
ptrdiff_t
dst_stride
const
InterpKernel
*
filter
int
x0_q4
int
x_step_q4
int
y0_q4
int
y_step_q4
int
w
int
h
)
;
static
INLINE
int16x4_t
convolve8_4_sdot_partial
(
const
int8x16_t
samples_lo
const
int8x16_t
samples_hi
const
int32x4_t
correction
const
int8x8_t
filters
)
{
int32x4_t
sum
;
sum
=
vdotq_lane_s32
(
correction
samples_lo
filters
0
)
;
sum
=
vdotq_lane_s32
(
sum
samples_hi
filters
1
)
;
return
vqmovn_s32
(
sum
)
;
}
static
INLINE
int16x4_t
convolve8_4_sdot
(
uint8x16_t
samples
const
int8x8_t
filters
const
int32x4_t
correction
const
uint8x16_t
range_limit
const
uint8x16x2_t
permute_tbl
)
{
int8x16_t
clamped_samples
permuted_samples
[
2
]
;
int32x4_t
sum
;
clamped_samples
=
vreinterpretq_s8_u8
(
vsubq_u8
(
samples
range_limit
)
)
;
permuted_samples
[
0
]
=
vqtbl1q_s8
(
clamped_samples
permute_tbl
.
val
[
0
]
)
;
permuted_samples
[
1
]
=
vqtbl1q_s8
(
clamped_samples
permute_tbl
.
val
[
1
]
)
;
sum
=
vdotq_lane_s32
(
correction
permuted_samples
[
0
]
filters
0
)
;
sum
=
vdotq_lane_s32
(
sum
permuted_samples
[
1
]
filters
1
)
;
return
vqmovn_s32
(
sum
)
;
}
static
INLINE
uint8x8_t
convolve8_8_sdot_partial
(
const
int8x16_t
samples0_lo
const
int8x16_t
samples0_hi
const
int8x16_t
samples1_lo
const
int8x16_t
samples1_hi
const
int32x4_t
correction
const
int8x8_t
filters
)
{
int32x4_t
sum0
sum1
;
int16x8_t
sum
;
sum0
=
vdotq_lane_s32
(
correction
samples0_lo
filters
0
)
;
sum0
=
vdotq_lane_s32
(
sum0
samples0_hi
filters
1
)
;
sum1
=
vdotq_lane_s32
(
correction
samples1_lo
filters
0
)
;
sum1
=
vdotq_lane_s32
(
sum1
samples1_hi
filters
1
)
;
sum
=
vcombine_s16
(
vqmovn_s32
(
sum0
)
vqmovn_s32
(
sum1
)
)
;
return
vqrshrun_n_s16
(
sum
FILTER_BITS
)
;
}
static
INLINE
uint8x8_t
convolve8_8_sdot
(
uint8x16_t
samples
const
int8x8_t
filters
const
int32x4_t
correction
const
uint8x16_t
range_limit
const
uint8x16x3_t
permute_tbl
)
{
int8x16_t
clamped_samples
permuted_samples
[
3
]
;
int32x4_t
sum0
sum1
;
int16x8_t
sum
;
clamped_samples
=
vreinterpretq_s8_u8
(
vsubq_u8
(
samples
range_limit
)
)
;
permuted_samples
[
0
]
=
vqtbl1q_s8
(
clamped_samples
permute_tbl
.
val
[
0
]
)
;
permuted_samples
[
1
]
=
vqtbl1q_s8
(
clamped_samples
permute_tbl
.
val
[
1
]
)
;
permuted_samples
[
2
]
=
vqtbl1q_s8
(
clamped_samples
permute_tbl
.
val
[
2
]
)
;
sum0
=
vdotq_lane_s32
(
correction
permuted_samples
[
0
]
filters
0
)
;
sum0
=
vdotq_lane_s32
(
sum0
permuted_samples
[
1
]
filters
1
)
;
sum1
=
vdotq_lane_s32
(
correction
permuted_samples
[
1
]
filters
0
)
;
sum1
=
vdotq_lane_s32
(
sum1
permuted_samples
[
2
]
filters
1
)
;
sum
=
vcombine_s16
(
vqmovn_s32
(
sum0
)
vqmovn_s32
(
sum1
)
)
;
return
vqrshrun_n_s16
(
sum
FILTER_BITS
)
;
}
#
endif
#
if
VPX_ARCH_AARCH64
&
&
defined
(
__ARM_FEATURE_MATMUL_INT8
)
void
vpx_convolve8_2d_horiz_neon_i8mm
(
const
uint8_t
*
src
ptrdiff_t
src_stride
uint8_t
*
dst
ptrdiff_t
dst_stride
const
InterpKernel
*
filter
int
x0_q4
int
x_step_q4
int
y0_q4
int
y_step_q4
int
w
int
h
)
;
static
INLINE
int16x4_t
convolve8_4_usdot_partial
(
const
uint8x16_t
samples_lo
const
uint8x16_t
samples_hi
const
int8x8_t
filters
)
{
int32x4_t
sum
;
sum
=
vusdotq_lane_s32
(
vdupq_n_s32
(
0
)
samples_lo
filters
0
)
;
sum
=
vusdotq_lane_s32
(
sum
samples_hi
filters
1
)
;
return
vqmovn_s32
(
sum
)
;
}
static
INLINE
int16x4_t
convolve8_4_usdot
(
uint8x16_t
samples
const
int8x8_t
filters
const
uint8x16x2_t
permute_tbl
)
{
uint8x16_t
permuted_samples
[
2
]
;
int32x4_t
sum
;
permuted_samples
[
0
]
=
vqtbl1q_u8
(
samples
permute_tbl
.
val
[
0
]
)
;
permuted_samples
[
1
]
=
vqtbl1q_u8
(
samples
permute_tbl
.
val
[
1
]
)
;
sum
=
vusdotq_lane_s32
(
vdupq_n_s32
(
0
)
permuted_samples
[
0
]
filters
0
)
;
sum
=
vusdotq_lane_s32
(
sum
permuted_samples
[
1
]
filters
1
)
;
return
vqmovn_s32
(
sum
)
;
}
static
INLINE
uint8x8_t
convolve8_8_usdot_partial
(
const
uint8x16_t
samples0_lo
const
uint8x16_t
samples0_hi
const
uint8x16_t
samples1_lo
const
uint8x16_t
samples1_hi
const
int8x8_t
filters
)
{
int32x4_t
sum0
sum1
;
int16x8_t
sum
;
sum0
=
vusdotq_lane_s32
(
vdupq_n_s32
(
0
)
samples0_lo
filters
0
)
;
sum0
=
vusdotq_lane_s32
(
sum0
samples0_hi
filters
1
)
;
sum1
=
vusdotq_lane_s32
(
vdupq_n_s32
(
0
)
samples1_lo
filters
0
)
;
sum1
=
vusdotq_lane_s32
(
sum1
samples1_hi
filters
1
)
;
sum
=
vcombine_s16
(
vqmovn_s32
(
sum0
)
vqmovn_s32
(
sum1
)
)
;
return
vqrshrun_n_s16
(
sum
FILTER_BITS
)
;
}
static
INLINE
uint8x8_t
convolve8_8_usdot
(
uint8x16_t
samples
const
int8x8_t
filters
const
uint8x16x3_t
permute_tbl
)
{
uint8x16_t
permuted_samples
[
3
]
;
int32x4_t
sum0
sum1
;
int16x8_t
sum
;
permuted_samples
[
0
]
=
vqtbl1q_u8
(
samples
permute_tbl
.
val
[
0
]
)
;
permuted_samples
[
1
]
=
vqtbl1q_u8
(
samples
permute_tbl
.
val
[
1
]
)
;
permuted_samples
[
2
]
=
vqtbl1q_u8
(
samples
permute_tbl
.
val
[
2
]
)
;
sum0
=
vusdotq_lane_s32
(
vdupq_n_s32
(
0
)
permuted_samples
[
0
]
filters
0
)
;
sum0
=
vusdotq_lane_s32
(
sum0
permuted_samples
[
1
]
filters
1
)
;
sum1
=
vusdotq_lane_s32
(
vdupq_n_s32
(
0
)
permuted_samples
[
1
]
filters
0
)
;
sum1
=
vusdotq_lane_s32
(
sum1
permuted_samples
[
2
]
filters
1
)
;
sum
=
vcombine_s16
(
vqmovn_s32
(
sum0
)
vqmovn_s32
(
sum1
)
)
;
return
vqrshrun_n_s16
(
sum
FILTER_BITS
)
;
}
#
endif
static
INLINE
int16x4_t
convolve8_4
(
const
int16x4_t
s0
const
int16x4_t
s1
const
int16x4_t
s2
const
int16x4_t
s3
const
int16x4_t
s4
const
int16x4_t
s5
const
int16x4_t
s6
const
int16x4_t
s7
const
int16x8_t
filters
)
{
const
int16x4_t
filters_lo
=
vget_low_s16
(
filters
)
;
const
int16x4_t
filters_hi
=
vget_high_s16
(
filters
)
;
int16x4_t
sum
;
sum
=
vmul_lane_s16
(
s0
filters_lo
0
)
;
sum
=
vmla_lane_s16
(
sum
s1
filters_lo
1
)
;
sum
=
vmla_lane_s16
(
sum
s2
filters_lo
2
)
;
sum
=
vmla_lane_s16
(
sum
s5
filters_hi
1
)
;
sum
=
vmla_lane_s16
(
sum
s6
filters_hi
2
)
;
sum
=
vmla_lane_s16
(
sum
s7
filters_hi
3
)
;
sum
=
vqadd_s16
(
sum
vmul_lane_s16
(
s3
filters_lo
3
)
)
;
sum
=
vqadd_s16
(
sum
vmul_lane_s16
(
s4
filters_hi
0
)
)
;
return
sum
;
}
static
INLINE
uint8x8_t
convolve8_8
(
const
int16x8_t
s0
const
int16x8_t
s1
const
int16x8_t
s2
const
int16x8_t
s3
const
int16x8_t
s4
const
int16x8_t
s5
const
int16x8_t
s6
const
int16x8_t
s7
const
int16x8_t
filters
)
{
const
int16x4_t
filters_lo
=
vget_low_s16
(
filters
)
;
const
int16x4_t
filters_hi
=
vget_high_s16
(
filters
)
;
int16x8_t
sum
;
sum
=
vmulq_lane_s16
(
s0
filters_lo
0
)
;
sum
=
vmlaq_lane_s16
(
sum
s1
filters_lo
1
)
;
sum
=
vmlaq_lane_s16
(
sum
s2
filters_lo
2
)
;
sum
=
vmlaq_lane_s16
(
sum
s5
filters_hi
1
)
;
sum
=
vmlaq_lane_s16
(
sum
s6
filters_hi
2
)
;
sum
=
vmlaq_lane_s16
(
sum
s7
filters_hi
3
)
;
sum
=
vqaddq_s16
(
sum
vmulq_lane_s16
(
s3
filters_lo
3
)
)
;
sum
=
vqaddq_s16
(
sum
vmulq_lane_s16
(
s4
filters_hi
0
)
)
;
return
vqrshrun_n_s16
(
sum
FILTER_BITS
)
;
}
static
INLINE
uint8x8_t
scale_filter_8
(
const
uint8x8_t
*
const
s
const
int16x8_t
filters
)
{
int16x8_t
ss
[
8
]
;
ss
[
0
]
=
vreinterpretq_s16_u16
(
vmovl_u8
(
s
[
0
]
)
)
;
ss
[
1
]
=
vreinterpretq_s16_u16
(
vmovl_u8
(
s
[
1
]
)
)
;
ss
[
2
]
=
vreinterpretq_s16_u16
(
vmovl_u8
(
s
[
2
]
)
)
;
ss
[
3
]
=
vreinterpretq_s16_u16
(
vmovl_u8
(
s
[
3
]
)
)
;
ss
[
4
]
=
vreinterpretq_s16_u16
(
vmovl_u8
(
s
[
4
]
)
)
;
ss
[
5
]
=
vreinterpretq_s16_u16
(
vmovl_u8
(
s
[
5
]
)
)
;
ss
[
6
]
=
vreinterpretq_s16_u16
(
vmovl_u8
(
s
[
6
]
)
)
;
ss
[
7
]
=
vreinterpretq_s16_u16
(
vmovl_u8
(
s
[
7
]
)
)
;
return
convolve8_8
(
ss
[
0
]
ss
[
1
]
ss
[
2
]
ss
[
3
]
ss
[
4
]
ss
[
5
]
ss
[
6
]
ss
[
7
]
filters
)
;
}
#
endif
