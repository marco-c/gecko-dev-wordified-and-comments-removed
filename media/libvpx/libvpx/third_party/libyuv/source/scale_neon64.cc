#
include
"
libyuv
/
scale
.
h
"
#
include
"
libyuv
/
row
.
h
"
#
include
"
libyuv
/
scale_row
.
h
"
#
ifdef
__cplusplus
namespace
libyuv
{
extern
"
C
"
{
#
endif
#
if
!
defined
(
LIBYUV_DISABLE_NEON
)
&
&
defined
(
__aarch64__
)
void
ScaleRowDown2_NEON
(
const
uint8
*
src_ptr
ptrdiff_t
src_stride
uint8
*
dst
int
dst_width
)
{
asm
volatile
(
"
1
:
\
n
"
MEMACCESS
(
0
)
"
ld2
{
v0
.
16b
v1
.
16b
}
[
%
0
]
#
32
\
n
"
"
subs
%
2
%
2
#
16
\
n
"
MEMACCESS
(
1
)
"
st1
{
v1
.
16b
}
[
%
1
]
#
16
\
n
"
"
b
.
gt
1b
\
n
"
:
"
+
r
"
(
src_ptr
)
/
/
%
0
"
+
r
"
(
dst
)
/
/
%
1
"
+
r
"
(
dst_width
)
/
/
%
2
:
:
"
v0
"
"
v1
"
/
/
Clobber
List
)
;
}
void
ScaleRowDown2Box_NEON
(
const
uint8
*
src_ptr
ptrdiff_t
src_stride
uint8
*
dst
int
dst_width
)
{
asm
volatile
(
"
add
%
1
%
1
%
0
\
n
"
"
1
:
\
n
"
MEMACCESS
(
0
)
"
ld1
{
v0
.
16b
v1
.
16b
}
[
%
0
]
#
32
\
n
"
MEMACCESS
(
1
)
"
ld1
{
v2
.
16b
v3
.
16b
}
[
%
1
]
#
32
\
n
"
"
subs
%
3
%
3
#
16
\
n
"
"
uaddlp
v0
.
8h
v0
.
16b
\
n
"
"
uaddlp
v1
.
8h
v1
.
16b
\
n
"
"
uadalp
v0
.
8h
v2
.
16b
\
n
"
"
uadalp
v1
.
8h
v3
.
16b
\
n
"
"
rshrn
v0
.
8b
v0
.
8h
#
2
\
n
"
"
rshrn2
v0
.
16b
v1
.
8h
#
2
\
n
"
MEMACCESS
(
2
)
"
st1
{
v0
.
16b
}
[
%
2
]
#
16
\
n
"
"
b
.
gt
1b
\
n
"
:
"
+
r
"
(
src_ptr
)
"
+
r
"
(
src_stride
)
"
+
r
"
(
dst
)
"
+
r
"
(
dst_width
)
:
:
"
v0
"
"
v1
"
"
v2
"
"
v3
"
)
;
}
void
ScaleRowDown4_NEON
(
const
uint8
*
src_ptr
ptrdiff_t
src_stride
uint8
*
dst_ptr
int
dst_width
)
{
asm
volatile
(
"
1
:
\
n
"
MEMACCESS
(
0
)
"
ld4
{
v0
.
8b
v1
.
8b
v2
.
8b
v3
.
8b
}
[
%
0
]
#
32
\
n
"
"
subs
%
2
%
2
#
8
\
n
"
MEMACCESS
(
1
)
"
st1
{
v2
.
8b
}
[
%
1
]
#
8
\
n
"
"
b
.
gt
1b
\
n
"
:
"
+
r
"
(
src_ptr
)
"
+
r
"
(
dst_ptr
)
"
+
r
"
(
dst_width
)
:
:
"
v0
"
"
v1
"
"
v2
"
"
v3
"
"
memory
"
"
cc
"
)
;
}
void
ScaleRowDown4Box_NEON
(
const
uint8
*
src_ptr
ptrdiff_t
src_stride
uint8
*
dst_ptr
int
dst_width
)
{
const
uint8
*
src_ptr1
=
src_ptr
+
src_stride
;
const
uint8
*
src_ptr2
=
src_ptr
+
src_stride
*
2
;
const
uint8
*
src_ptr3
=
src_ptr
+
src_stride
*
3
;
asm
volatile
(
"
1
:
\
n
"
MEMACCESS
(
0
)
"
ld1
{
v0
.
16b
}
[
%
0
]
#
16
\
n
"
MEMACCESS
(
3
)
"
ld1
{
v1
.
16b
}
[
%
2
]
#
16
\
n
"
MEMACCESS
(
4
)
"
ld1
{
v2
.
16b
}
[
%
3
]
#
16
\
n
"
MEMACCESS
(
5
)
"
ld1
{
v3
.
16b
}
[
%
4
]
#
16
\
n
"
"
subs
%
5
%
5
#
4
\
n
"
"
uaddlp
v0
.
8h
v0
.
16b
\
n
"
"
uadalp
v0
.
8h
v1
.
16b
\
n
"
"
uadalp
v0
.
8h
v2
.
16b
\
n
"
"
uadalp
v0
.
8h
v3
.
16b
\
n
"
"
addp
v0
.
8h
v0
.
8h
v0
.
8h
\
n
"
"
rshrn
v0
.
8b
v0
.
8h
#
4
\
n
"
MEMACCESS
(
1
)
"
st1
{
v0
.
s
}
[
0
]
[
%
1
]
#
4
\
n
"
"
b
.
gt
1b
\
n
"
:
"
+
r
"
(
src_ptr
)
"
+
r
"
(
dst_ptr
)
"
+
r
"
(
src_ptr1
)
"
+
r
"
(
src_ptr2
)
"
+
r
"
(
src_ptr3
)
"
+
r
"
(
dst_width
)
:
:
"
v0
"
"
v1
"
"
v2
"
"
v3
"
"
memory
"
"
cc
"
)
;
}
void
ScaleRowDown34_NEON
(
const
uint8
*
src_ptr
ptrdiff_t
src_stride
uint8
*
dst_ptr
int
dst_width
)
{
asm
volatile
(
"
1
:
\
n
"
MEMACCESS
(
0
)
"
ld4
{
v0
.
8b
v1
.
8b
v2
.
8b
v3
.
8b
}
[
%
0
]
#
32
\
n
"
"
subs
%
2
%
2
#
24
\
n
"
"
orr
v2
.
16b
v3
.
16b
v3
.
16b
\
n
"
MEMACCESS
(
1
)
"
st3
{
v0
.
8b
v1
.
8b
v2
.
8b
}
[
%
1
]
#
24
\
n
"
"
b
.
gt
1b
\
n
"
:
"
+
r
"
(
src_ptr
)
/
/
%
0
"
+
r
"
(
dst_ptr
)
/
/
%
1
"
+
r
"
(
dst_width
)
/
/
%
2
:
:
"
v0
"
"
v1
"
"
v2
"
"
v3
"
"
memory
"
"
cc
"
)
;
}
void
ScaleRowDown34_0_Box_NEON
(
const
uint8
*
src_ptr
ptrdiff_t
src_stride
uint8
*
dst_ptr
int
dst_width
)
{
asm
volatile
(
"
movi
v20
.
8b
#
3
\
n
"
"
add
%
3
%
3
%
0
\
n
"
"
1
:
\
n
"
MEMACCESS
(
0
)
"
ld4
{
v0
.
8b
v1
.
8b
v2
.
8b
v3
.
8b
}
[
%
0
]
#
32
\
n
"
MEMACCESS
(
3
)
"
ld4
{
v4
.
8b
v5
.
8b
v6
.
8b
v7
.
8b
}
[
%
3
]
#
32
\
n
"
"
subs
%
2
%
2
#
24
\
n
"
"
ushll
v16
.
8h
v4
.
8b
#
0
\
n
"
"
ushll
v17
.
8h
v5
.
8b
#
0
\
n
"
"
ushll
v18
.
8h
v6
.
8b
#
0
\
n
"
"
ushll
v19
.
8h
v7
.
8b
#
0
\
n
"
"
umlal
v16
.
8h
v0
.
8b
v20
.
8b
\
n
"
"
umlal
v17
.
8h
v1
.
8b
v20
.
8b
\
n
"
"
umlal
v18
.
8h
v2
.
8b
v20
.
8b
\
n
"
"
umlal
v19
.
8h
v3
.
8b
v20
.
8b
\
n
"
"
uqrshrn
v0
.
8b
v16
.
8h
#
2
\
n
"
"
uqrshrn
v1
.
8b
v17
.
8h
#
2
\
n
"
"
uqrshrn
v2
.
8b
v18
.
8h
#
2
\
n
"
"
uqrshrn
v3
.
8b
v19
.
8h
#
2
\
n
"
"
ushll
v16
.
8h
v1
.
8b
#
0
\
n
"
"
umlal
v16
.
8h
v0
.
8b
v20
.
8b
\
n
"
"
uqrshrn
v0
.
8b
v16
.
8h
#
2
\
n
"
"
urhadd
v1
.
8b
v1
.
8b
v2
.
8b
\
n
"
"
ushll
v16
.
8h
v2
.
8b
#
0
\
n
"
"
umlal
v16
.
8h
v3
.
8b
v20
.
8b
\
n
"
"
uqrshrn
v2
.
8b
v16
.
8h
#
2
\
n
"
MEMACCESS
(
1
)
"
st3
{
v0
.
8b
v1
.
8b
v2
.
8b
}
[
%
1
]
#
24
\
n
"
"
b
.
gt
1b
\
n
"
:
"
+
r
"
(
src_ptr
)
/
/
%
0
"
+
r
"
(
dst_ptr
)
/
/
%
1
"
+
r
"
(
dst_width
)
/
/
%
2
"
+
r
"
(
src_stride
)
/
/
%
3
:
:
"
v0
"
"
v1
"
"
v2
"
"
v3
"
"
v4
"
"
v5
"
"
v6
"
"
v7
"
"
v16
"
"
v17
"
"
v18
"
"
v19
"
"
v20
"
"
memory
"
"
cc
"
)
;
}
void
ScaleRowDown34_1_Box_NEON
(
const
uint8
*
src_ptr
ptrdiff_t
src_stride
uint8
*
dst_ptr
int
dst_width
)
{
asm
volatile
(
"
movi
v20
.
8b
#
3
\
n
"
"
add
%
3
%
3
%
0
\
n
"
"
1
:
\
n
"
MEMACCESS
(
0
)
"
ld4
{
v0
.
8b
v1
.
8b
v2
.
8b
v3
.
8b
}
[
%
0
]
#
32
\
n
"
MEMACCESS
(
3
)
"
ld4
{
v4
.
8b
v5
.
8b
v6
.
8b
v7
.
8b
}
[
%
3
]
#
32
\
n
"
"
subs
%
2
%
2
#
24
\
n
"
"
urhadd
v0
.
8b
v0
.
8b
v4
.
8b
\
n
"
"
urhadd
v1
.
8b
v1
.
8b
v5
.
8b
\
n
"
"
urhadd
v2
.
8b
v2
.
8b
v6
.
8b
\
n
"
"
urhadd
v3
.
8b
v3
.
8b
v7
.
8b
\
n
"
"
ushll
v4
.
8h
v1
.
8b
#
0
\
n
"
"
umlal
v4
.
8h
v0
.
8b
v20
.
8b
\
n
"
"
uqrshrn
v0
.
8b
v4
.
8h
#
2
\
n
"
"
urhadd
v1
.
8b
v1
.
8b
v2
.
8b
\
n
"
"
ushll
v4
.
8h
v2
.
8b
#
0
\
n
"
"
umlal
v4
.
8h
v3
.
8b
v20
.
8b
\
n
"
"
uqrshrn
v2
.
8b
v4
.
8h
#
2
\
n
"
MEMACCESS
(
1
)
"
st3
{
v0
.
8b
v1
.
8b
v2
.
8b
}
[
%
1
]
#
24
\
n
"
"
b
.
gt
1b
\
n
"
:
"
+
r
"
(
src_ptr
)
/
/
%
0
"
+
r
"
(
dst_ptr
)
/
/
%
1
"
+
r
"
(
dst_width
)
/
/
%
2
"
+
r
"
(
src_stride
)
/
/
%
3
:
:
"
v0
"
"
v1
"
"
v2
"
"
v3
"
"
v4
"
"
v5
"
"
v6
"
"
v7
"
"
v20
"
"
memory
"
"
cc
"
)
;
}
static
uvec8
kShuf38
=
{
0
3
6
8
11
14
16
19
22
24
27
30
0
0
0
0
}
;
static
uvec8
kShuf38_2
=
{
0
16
32
2
18
33
4
20
34
6
22
35
0
0
0
0
}
;
static
vec16
kMult38_Div6
=
{
65536
/
12
65536
/
12
65536
/
12
65536
/
12
65536
/
12
65536
/
12
65536
/
12
65536
/
12
}
;
static
vec16
kMult38_Div9
=
{
65536
/
18
65536
/
18
65536
/
18
65536
/
18
65536
/
18
65536
/
18
65536
/
18
65536
/
18
}
;
void
ScaleRowDown38_NEON
(
const
uint8
*
src_ptr
ptrdiff_t
src_stride
uint8
*
dst_ptr
int
dst_width
)
{
asm
volatile
(
MEMACCESS
(
3
)
"
ld1
{
v3
.
16b
}
[
%
3
]
\
n
"
"
1
:
\
n
"
MEMACCESS
(
0
)
"
ld1
{
v0
.
16b
v1
.
16b
}
[
%
0
]
#
32
\
n
"
"
subs
%
2
%
2
#
12
\
n
"
"
tbl
v2
.
16b
{
v0
.
16b
v1
.
16b
}
v3
.
16b
\
n
"
MEMACCESS
(
1
)
"
st1
{
v2
.
8b
}
[
%
1
]
#
8
\
n
"
MEMACCESS
(
1
)
"
st1
{
v2
.
s
}
[
2
]
[
%
1
]
#
4
\
n
"
"
b
.
gt
1b
\
n
"
:
"
+
r
"
(
src_ptr
)
"
+
r
"
(
dst_ptr
)
"
+
r
"
(
dst_width
)
:
"
r
"
(
&
kShuf38
)
:
"
v0
"
"
v1
"
"
v2
"
"
v3
"
"
memory
"
"
cc
"
)
;
}
void
OMITFP
ScaleRowDown38_3_Box_NEON
(
const
uint8
*
src_ptr
ptrdiff_t
src_stride
uint8
*
dst_ptr
int
dst_width
)
{
const
uint8
*
src_ptr1
=
src_ptr
+
src_stride
*
2
;
ptrdiff_t
tmp_src_stride
=
src_stride
;
asm
volatile
(
MEMACCESS
(
5
)
"
ld1
{
v29
.
8h
}
[
%
5
]
\
n
"
MEMACCESS
(
6
)
"
ld1
{
v30
.
16b
}
[
%
6
]
\
n
"
MEMACCESS
(
7
)
"
ld1
{
v31
.
8h
}
[
%
7
]
\
n
"
"
add
%
2
%
2
%
0
\
n
"
"
1
:
\
n
"
MEMACCESS
(
0
)
"
ld4
{
v0
.
8b
v1
.
8b
v2
.
8b
v3
.
8b
}
[
%
0
]
#
32
\
n
"
MEMACCESS
(
3
)
"
ld4
{
v4
.
8b
v5
.
8b
v6
.
8b
v7
.
8b
}
[
%
2
]
#
32
\
n
"
MEMACCESS
(
4
)
"
ld4
{
v16
.
8b
v17
.
8b
v18
.
8b
v19
.
8b
}
[
%
3
]
#
32
\
n
"
"
subs
%
4
%
4
#
12
\
n
"
"
trn1
v20
.
8b
v0
.
8b
v1
.
8b
\
n
"
"
trn2
v21
.
8b
v0
.
8b
v1
.
8b
\
n
"
"
trn1
v22
.
8b
v4
.
8b
v5
.
8b
\
n
"
"
trn2
v23
.
8b
v4
.
8b
v5
.
8b
\
n
"
"
trn1
v24
.
8b
v16
.
8b
v17
.
8b
\
n
"
"
trn2
v25
.
8b
v16
.
8b
v17
.
8b
\
n
"
"
trn1
v0
.
8b
v2
.
8b
v3
.
8b
\
n
"
"
trn2
v1
.
8b
v2
.
8b
v3
.
8b
\
n
"
"
trn1
v4
.
8b
v6
.
8b
v7
.
8b
\
n
"
"
trn2
v5
.
8b
v6
.
8b
v7
.
8b
\
n
"
"
trn1
v16
.
8b
v18
.
8b
v19
.
8b
\
n
"
"
trn2
v17
.
8b
v18
.
8b
v19
.
8b
\
n
"
"
uaddlp
v20
.
4h
v20
.
8b
\
n
"
"
uaddlp
v21
.
4h
v21
.
8b
\
n
"
"
uaddlp
v22
.
4h
v22
.
8b
\
n
"
"
uaddlp
v23
.
4h
v23
.
8b
\
n
"
"
uaddlp
v24
.
4h
v24
.
8b
\
n
"
"
uaddlp
v25
.
4h
v25
.
8b
\
n
"
"
uaddlp
v1
.
4h
v1
.
8b
\
n
"
"
uaddlp
v5
.
4h
v5
.
8b
\
n
"
"
uaddlp
v17
.
4h
v17
.
8b
\
n
"
"
add
v20
.
4h
v20
.
4h
v22
.
4h
\
n
"
"
add
v21
.
4h
v21
.
4h
v23
.
4h
\
n
"
"
add
v20
.
4h
v20
.
4h
v24
.
4h
\
n
"
"
add
v21
.
4h
v21
.
4h
v25
.
4h
\
n
"
"
add
v2
.
4h
v1
.
4h
v5
.
4h
\
n
"
"
add
v2
.
4h
v2
.
4h
v17
.
4h
\
n
"
"
sqrdmulh
v2
.
8h
v2
.
8h
v29
.
8h
\
n
"
"
xtn
v2
.
8b
v2
.
8h
\
n
"
"
ushll
v16
.
8h
v16
.
8b
#
0
\
n
"
"
uaddl
v0
.
8h
v0
.
8b
v4
.
8b
\
n
"
"
add
v0
.
8h
v0
.
8h
v16
.
8h
\
n
"
"
trn1
v1
.
8h
v0
.
8h
v0
.
8h
\
n
"
"
trn2
v4
.
8h
v0
.
8h
v0
.
8h
\
n
"
"
xtn
v0
.
4h
v1
.
4s
\
n
"
"
xtn
v4
.
4h
v4
.
4s
\
n
"
"
add
v20
.
8h
v20
.
8h
v0
.
8h
\
n
"
"
add
v21
.
8h
v21
.
8h
v4
.
8h
\
n
"
"
sqrdmulh
v0
.
8h
v20
.
8h
v31
.
8h
\
n
"
"
sqrdmulh
v1
.
8h
v21
.
8h
v31
.
8h
\
n
"
"
tbl
v3
.
16b
{
v0
.
16b
v1
.
16b
v2
.
16b
}
v30
.
16b
\
n
"
MEMACCESS
(
1
)
"
st1
{
v3
.
8b
}
[
%
1
]
#
8
\
n
"
MEMACCESS
(
1
)
"
st1
{
v3
.
s
}
[
2
]
[
%
1
]
#
4
\
n
"
"
b
.
gt
1b
\
n
"
:
"
+
r
"
(
src_ptr
)
"
+
r
"
(
dst_ptr
)
"
+
r
"
(
tmp_src_stride
)
"
+
r
"
(
src_ptr1
)
"
+
r
"
(
dst_width
)
:
"
r
"
(
&
kMult38_Div6
)
"
r
"
(
&
kShuf38_2
)
"
r
"
(
&
kMult38_Div9
)
:
"
v0
"
"
v1
"
"
v2
"
"
v3
"
"
v4
"
"
v5
"
"
v6
"
"
v7
"
"
v16
"
"
v17
"
"
v18
"
"
v19
"
"
v20
"
"
v21
"
"
v22
"
"
v23
"
"
v24
"
"
v25
"
"
v29
"
"
v30
"
"
v31
"
"
memory
"
"
cc
"
)
;
}
void
ScaleRowDown38_2_Box_NEON
(
const
uint8
*
src_ptr
ptrdiff_t
src_stride
uint8
*
dst_ptr
int
dst_width
)
{
ptrdiff_t
tmp_src_stride
=
src_stride
;
asm
volatile
(
MEMACCESS
(
4
)
"
ld1
{
v30
.
8h
}
[
%
4
]
\
n
"
MEMACCESS
(
5
)
"
ld1
{
v31
.
16b
}
[
%
5
]
\
n
"
"
add
%
2
%
2
%
0
\
n
"
"
1
:
\
n
"
MEMACCESS
(
0
)
"
ld4
{
v0
.
8b
v1
.
8b
v2
.
8b
v3
.
8b
}
[
%
0
]
#
32
\
n
"
MEMACCESS
(
3
)
"
ld4
{
v4
.
8b
v5
.
8b
v6
.
8b
v7
.
8b
}
[
%
2
]
#
32
\
n
"
"
subs
%
3
%
3
#
12
\
n
"
"
trn1
v16
.
8b
v0
.
8b
v1
.
8b
\
n
"
"
trn2
v17
.
8b
v0
.
8b
v1
.
8b
\
n
"
"
trn1
v18
.
8b
v4
.
8b
v5
.
8b
\
n
"
"
trn2
v19
.
8b
v4
.
8b
v5
.
8b
\
n
"
"
trn1
v0
.
8b
v2
.
8b
v3
.
8b
\
n
"
"
trn2
v1
.
8b
v2
.
8b
v3
.
8b
\
n
"
"
trn1
v4
.
8b
v6
.
8b
v7
.
8b
\
n
"
"
trn2
v5
.
8b
v6
.
8b
v7
.
8b
\
n
"
"
uaddlp
v16
.
4h
v16
.
8b
\
n
"
"
uaddlp
v17
.
4h
v17
.
8b
\
n
"
"
uaddlp
v18
.
4h
v18
.
8b
\
n
"
"
uaddlp
v19
.
4h
v19
.
8b
\
n
"
"
uaddlp
v1
.
4h
v1
.
8b
\
n
"
"
uaddlp
v5
.
4h
v5
.
8b
\
n
"
"
add
v16
.
4h
v16
.
4h
v18
.
4h
\
n
"
"
add
v17
.
4h
v17
.
4h
v19
.
4h
\
n
"
"
add
v2
.
4h
v1
.
4h
v5
.
4h
\
n
"
"
uqrshrn
v2
.
8b
v2
.
8h
#
2
\
n
"
"
uaddl
v0
.
8h
v0
.
8b
v4
.
8b
\
n
"
"
trn1
v1
.
8h
v0
.
8h
v0
.
8h
\
n
"
"
trn2
v4
.
8h
v0
.
8h
v0
.
8h
\
n
"
"
xtn
v0
.
4h
v1
.
4s
\
n
"
"
xtn
v4
.
4h
v4
.
4s
\
n
"
"
add
v16
.
8h
v16
.
8h
v0
.
8h
\
n
"
"
add
v17
.
8h
v17
.
8h
v4
.
8h
\
n
"
"
sqrdmulh
v0
.
8h
v16
.
8h
v30
.
8h
\
n
"
"
sqrdmulh
v1
.
8h
v17
.
8h
v30
.
8h
\
n
"
"
tbl
v3
.
16b
{
v0
.
16b
v1
.
16b
v2
.
16b
}
v31
.
16b
\
n
"
MEMACCESS
(
1
)
"
st1
{
v3
.
8b
}
[
%
1
]
#
8
\
n
"
MEMACCESS
(
1
)
"
st1
{
v3
.
s
}
[
2
]
[
%
1
]
#
4
\
n
"
"
b
.
gt
1b
\
n
"
:
"
+
r
"
(
src_ptr
)
"
+
r
"
(
dst_ptr
)
"
+
r
"
(
tmp_src_stride
)
"
+
r
"
(
dst_width
)
:
"
r
"
(
&
kMult38_Div6
)
"
r
"
(
&
kShuf38_2
)
:
"
v0
"
"
v1
"
"
v2
"
"
v3
"
"
v4
"
"
v5
"
"
v6
"
"
v7
"
"
v16
"
"
v17
"
"
v18
"
"
v19
"
"
v30
"
"
v31
"
"
memory
"
"
cc
"
)
;
}
void
ScaleFilterRows_NEON
(
uint8
*
dst_ptr
const
uint8
*
src_ptr
ptrdiff_t
src_stride
int
dst_width
int
source_y_fraction
)
{
int
y_fraction
=
256
-
source_y_fraction
;
asm
volatile
(
"
cmp
%
4
#
0
\
n
"
"
b
.
eq
100f
\
n
"
"
add
%
2
%
2
%
1
\
n
"
"
cmp
%
4
#
64
\
n
"
"
b
.
eq
75f
\
n
"
"
cmp
%
4
#
128
\
n
"
"
b
.
eq
50f
\
n
"
"
cmp
%
4
#
192
\
n
"
"
b
.
eq
25f
\
n
"
"
dup
v5
.
8b
%
w4
\
n
"
"
dup
v4
.
8b
%
w5
\
n
"
"
1
:
\
n
"
MEMACCESS
(
1
)
"
ld1
{
v0
.
16b
}
[
%
1
]
#
16
\
n
"
MEMACCESS
(
2
)
"
ld1
{
v1
.
16b
}
[
%
2
]
#
16
\
n
"
"
subs
%
3
%
3
#
16
\
n
"
"
umull
v6
.
8h
v0
.
8b
v4
.
8b
\
n
"
"
umull2
v7
.
8h
v0
.
16b
v4
.
16b
\
n
"
"
umlal
v6
.
8h
v1
.
8b
v5
.
8b
\
n
"
"
umlal2
v7
.
8h
v1
.
16b
v5
.
16b
\
n
"
"
rshrn
v0
.
8b
v6
.
8h
#
8
\
n
"
"
rshrn2
v0
.
16b
v7
.
8h
#
8
\
n
"
MEMACCESS
(
0
)
"
st1
{
v0
.
16b
}
[
%
0
]
#
16
\
n
"
"
b
.
gt
1b
\
n
"
"
b
99f
\
n
"
"
25
:
\
n
"
MEMACCESS
(
1
)
"
ld1
{
v0
.
16b
}
[
%
1
]
#
16
\
n
"
MEMACCESS
(
2
)
"
ld1
{
v1
.
16b
}
[
%
2
]
#
16
\
n
"
"
subs
%
3
%
3
#
16
\
n
"
"
urhadd
v0
.
16b
v0
.
16b
v1
.
16b
\
n
"
"
urhadd
v0
.
16b
v0
.
16b
v1
.
16b
\
n
"
MEMACCESS
(
0
)
"
st1
{
v0
.
16b
}
[
%
0
]
#
16
\
n
"
"
b
.
gt
25b
\
n
"
"
b
99f
\
n
"
"
50
:
\
n
"
MEMACCESS
(
1
)
"
ld1
{
v0
.
16b
}
[
%
1
]
#
16
\
n
"
MEMACCESS
(
2
)
"
ld1
{
v1
.
16b
}
[
%
2
]
#
16
\
n
"
"
subs
%
3
%
3
#
16
\
n
"
"
urhadd
v0
.
16b
v0
.
16b
v1
.
16b
\
n
"
MEMACCESS
(
0
)
"
st1
{
v0
.
16b
}
[
%
0
]
#
16
\
n
"
"
b
.
gt
50b
\
n
"
"
b
99f
\
n
"
"
75
:
\
n
"
MEMACCESS
(
1
)
"
ld1
{
v1
.
16b
}
[
%
1
]
#
16
\
n
"
MEMACCESS
(
2
)
"
ld1
{
v0
.
16b
}
[
%
2
]
#
16
\
n
"
"
subs
%
3
%
3
#
16
\
n
"
"
urhadd
v0
.
16b
v0
.
16b
v1
.
16b
\
n
"
"
urhadd
v0
.
16b
v0
.
16b
v1
.
16b
\
n
"
MEMACCESS
(
0
)
"
st1
{
v0
.
16b
}
[
%
0
]
#
16
\
n
"
"
b
.
gt
75b
\
n
"
"
b
99f
\
n
"
"
100
:
\
n
"
MEMACCESS
(
1
)
"
ld1
{
v0
.
16b
}
[
%
1
]
#
16
\
n
"
"
subs
%
3
%
3
#
16
\
n
"
MEMACCESS
(
0
)
"
st1
{
v0
.
16b
}
[
%
0
]
#
16
\
n
"
"
b
.
gt
100b
\
n
"
"
99
:
\
n
"
MEMACCESS
(
0
)
"
st1
{
v0
.
b
}
[
15
]
[
%
0
]
\
n
"
:
"
+
r
"
(
dst_ptr
)
"
+
r
"
(
src_ptr
)
"
+
r
"
(
src_stride
)
"
+
r
"
(
dst_width
)
"
+
r
"
(
source_y_fraction
)
"
+
r
"
(
y_fraction
)
:
:
"
v0
"
"
v1
"
"
v2
"
"
v3
"
"
v4
"
"
v5
"
"
v6
"
"
v7
"
"
memory
"
"
cc
"
)
;
}
void
ScaleARGBRowDown2_NEON
(
const
uint8
*
src_ptr
ptrdiff_t
src_stride
uint8
*
dst
int
dst_width
)
{
asm
volatile
(
"
1
:
\
n
"
MEMACCESS
(
0
)
"
ld2
{
v0
.
4s
v1
.
4s
}
[
%
0
]
#
32
\
n
"
MEMACCESS
(
0
)
"
ld2
{
v2
.
4s
v3
.
4s
}
[
%
0
]
#
32
\
n
"
"
subs
%
2
%
2
#
8
\
n
"
MEMACCESS
(
1
)
"
st1
{
v1
.
16b
}
[
%
1
]
#
16
\
n
"
MEMACCESS
(
1
)
"
st1
{
v3
.
16b
}
[
%
1
]
#
16
\
n
"
"
b
.
gt
1b
\
n
"
:
"
+
r
"
(
src_ptr
)
"
+
r
"
(
dst
)
"
+
r
"
(
dst_width
)
:
:
"
memory
"
"
cc
"
"
v0
"
"
v1
"
"
v2
"
"
v3
"
)
;
}
void
ScaleARGBRowDown2Box_NEON
(
const
uint8
*
src_ptr
ptrdiff_t
src_stride
uint8
*
dst
int
dst_width
)
{
asm
volatile
(
"
add
%
1
%
1
%
0
\
n
"
"
1
:
\
n
"
MEMACCESS
(
0
)
"
ld4
{
v0
.
16b
v1
.
16b
v2
.
16b
v3
.
16b
}
[
%
0
]
#
64
\
n
"
"
subs
%
3
%
3
#
8
\
n
"
"
uaddlp
v0
.
8h
v0
.
16b
\
n
"
"
uaddlp
v1
.
8h
v1
.
16b
\
n
"
"
uaddlp
v2
.
8h
v2
.
16b
\
n
"
"
uaddlp
v3
.
8h
v3
.
16b
\
n
"
MEMACCESS
(
1
)
"
ld4
{
v16
.
16b
v17
.
16b
v18
.
16b
v19
.
16b
}
[
%
1
]
#
64
\
n
"
"
uadalp
v0
.
8h
v16
.
16b
\
n
"
"
uadalp
v1
.
8h
v17
.
16b
\
n
"
"
uadalp
v2
.
8h
v18
.
16b
\
n
"
"
uadalp
v3
.
8h
v19
.
16b
\
n
"
"
rshrn
v0
.
8b
v0
.
8h
#
2
\
n
"
"
rshrn
v1
.
8b
v1
.
8h
#
2
\
n
"
"
rshrn
v2
.
8b
v2
.
8h
#
2
\
n
"
"
rshrn
v3
.
8b
v3
.
8h
#
2
\
n
"
MEMACCESS
(
2
)
"
st4
{
v0
.
8b
v1
.
8b
v2
.
8b
v3
.
8b
}
[
%
2
]
#
32
\
n
"
"
b
.
gt
1b
\
n
"
:
"
+
r
"
(
src_ptr
)
"
+
r
"
(
src_stride
)
"
+
r
"
(
dst
)
"
+
r
"
(
dst_width
)
:
:
"
memory
"
"
cc
"
"
v0
"
"
v1
"
"
v2
"
"
v3
"
"
v16
"
"
v17
"
"
v18
"
"
v19
"
)
;
}
void
ScaleARGBRowDownEven_NEON
(
const
uint8
*
src_argb
ptrdiff_t
src_stride
int
src_stepx
uint8
*
dst_argb
int
dst_width
)
{
asm
volatile
(
"
1
:
\
n
"
MEMACCESS
(
0
)
"
ld1
{
v0
.
s
}
[
0
]
[
%
0
]
%
3
\
n
"
MEMACCESS
(
0
)
"
ld1
{
v0
.
s
}
[
1
]
[
%
0
]
%
3
\
n
"
MEMACCESS
(
0
)
"
ld1
{
v0
.
s
}
[
2
]
[
%
0
]
%
3
\
n
"
MEMACCESS
(
0
)
"
ld1
{
v0
.
s
}
[
3
]
[
%
0
]
%
3
\
n
"
"
subs
%
2
%
2
#
4
\
n
"
MEMACCESS
(
1
)
"
st1
{
v0
.
16b
}
[
%
1
]
#
16
\
n
"
"
b
.
gt
1b
\
n
"
:
"
+
r
"
(
src_argb
)
"
+
r
"
(
dst_argb
)
"
+
r
"
(
dst_width
)
:
"
r
"
(
static_cast
<
ptrdiff_t
>
(
src_stepx
*
4
)
)
:
"
memory
"
"
cc
"
"
v0
"
)
;
}
void
ScaleARGBRowDownEvenBox_NEON
(
const
uint8
*
src_argb
ptrdiff_t
src_stride
int
src_stepx
uint8
*
dst_argb
int
dst_width
)
{
asm
volatile
(
"
add
%
1
%
1
%
0
\
n
"
"
1
:
\
n
"
MEMACCESS
(
0
)
"
ld1
{
v0
.
8b
}
[
%
0
]
%
4
\
n
"
MEMACCESS
(
1
)
"
ld1
{
v1
.
8b
}
[
%
1
]
%
4
\
n
"
MEMACCESS
(
0
)
"
ld1
{
v2
.
8b
}
[
%
0
]
%
4
\
n
"
MEMACCESS
(
1
)
"
ld1
{
v3
.
8b
}
[
%
1
]
%
4
\
n
"
MEMACCESS
(
0
)
"
ld1
{
v4
.
8b
}
[
%
0
]
%
4
\
n
"
MEMACCESS
(
1
)
"
ld1
{
v5
.
8b
}
[
%
1
]
%
4
\
n
"
MEMACCESS
(
0
)
"
ld1
{
v6
.
8b
}
[
%
0
]
%
4
\
n
"
MEMACCESS
(
1
)
"
ld1
{
v7
.
8b
}
[
%
1
]
%
4
\
n
"
"
uaddl
v0
.
8h
v0
.
8b
v1
.
8b
\
n
"
"
uaddl
v2
.
8h
v2
.
8b
v3
.
8b
\
n
"
"
uaddl
v4
.
8h
v4
.
8b
v5
.
8b
\
n
"
"
uaddl
v6
.
8h
v6
.
8b
v7
.
8b
\
n
"
"
mov
v16
.
d
[
1
]
v0
.
d
[
1
]
\
n
"
"
mov
v0
.
d
[
1
]
v2
.
d
[
0
]
\
n
"
"
mov
v2
.
d
[
0
]
v16
.
d
[
1
]
\
n
"
"
mov
v16
.
d
[
1
]
v4
.
d
[
1
]
\
n
"
"
mov
v4
.
d
[
1
]
v6
.
d
[
0
]
\
n
"
"
mov
v6
.
d
[
0
]
v16
.
d
[
1
]
\
n
"
"
add
v0
.
8h
v0
.
8h
v2
.
8h
\
n
"
"
add
v4
.
8h
v4
.
8h
v6
.
8h
\
n
"
"
rshrn
v0
.
8b
v0
.
8h
#
2
\
n
"
"
rshrn2
v0
.
16b
v4
.
8h
#
2
\
n
"
"
subs
%
3
%
3
#
4
\
n
"
MEMACCESS
(
2
)
"
st1
{
v0
.
16b
}
[
%
2
]
#
16
\
n
"
"
b
.
gt
1b
\
n
"
:
"
+
r
"
(
src_argb
)
"
+
r
"
(
src_stride
)
"
+
r
"
(
dst_argb
)
"
+
r
"
(
dst_width
)
:
"
r
"
(
src_stepx
*
4
)
:
"
memory
"
"
cc
"
"
v0
"
"
v1
"
"
v2
"
"
v3
"
"
v4
"
"
v5
"
"
v6
"
"
v7
"
"
v16
"
)
;
}
#
endif
#
ifdef
__cplusplus
}
}
#
endif
