#
include
<
assert
.
h
>
#
include
<
immintrin
.
h
>
#
include
"
.
/
vp9_rtcd
.
h
"
#
include
"
.
/
vpx_dsp_rtcd
.
h
"
#
include
"
vp9
/
encoder
/
vp9_temporal_filter
.
h
"
static
INLINE
void
highbd_shuffle_12tap_filter_avx2
(
const
int16_t
*
filter
__m256i
*
f
)
{
const
__m256i
f_low
=
_mm256_broadcastsi128_si256
(
_mm_loadu_si128
(
(
const
__m128i
*
)
filter
)
)
;
const
__m256i
f_high
=
_mm256_broadcastsi128_si256
(
_mm_loadl_epi64
(
(
const
__m128i
*
)
(
filter
+
8
)
)
)
;
f
[
0
]
=
_mm256_shuffle_epi32
(
f_low
0x00
)
;
f
[
1
]
=
_mm256_shuffle_epi32
(
f_low
0x55
)
;
f
[
2
]
=
_mm256_shuffle_epi32
(
f_low
0xaa
)
;
f
[
3
]
=
_mm256_shuffle_epi32
(
f_low
0xff
)
;
f
[
4
]
=
_mm256_shuffle_epi32
(
f_high
0x00
)
;
f
[
5
]
=
_mm256_shuffle_epi32
(
f_high
0x55
)
;
}
static
INLINE
__m256i
highbd_convolve_12tap
(
const
__m256i
*
s
const
__m256i
*
f
)
{
const
__m256i
res_0
=
_mm256_madd_epi16
(
s
[
0
]
f
[
0
]
)
;
const
__m256i
res_1
=
_mm256_madd_epi16
(
s
[
1
]
f
[
1
]
)
;
const
__m256i
res_2
=
_mm256_madd_epi16
(
s
[
2
]
f
[
2
]
)
;
const
__m256i
res_3
=
_mm256_madd_epi16
(
s
[
3
]
f
[
3
]
)
;
const
__m256i
res_4
=
_mm256_madd_epi16
(
s
[
4
]
f
[
4
]
)
;
const
__m256i
res_5
=
_mm256_madd_epi16
(
s
[
5
]
f
[
5
]
)
;
const
__m256i
res
=
_mm256_add_epi32
(
_mm256_add_epi32
(
res_0
res_1
)
_mm256_add_epi32
(
_mm256_add_epi32
(
res_2
res_3
)
_mm256_add_epi32
(
res_4
res_5
)
)
)
;
return
res
;
}
static
INLINE
void
reuse_src_data_avx2
(
const
__m256i
*
src
__m256i
*
des
)
{
des
[
0
]
=
src
[
0
]
;
des
[
1
]
=
src
[
1
]
;
des
[
2
]
=
src
[
2
]
;
des
[
3
]
=
src
[
3
]
;
des
[
4
]
=
src
[
4
]
;
}
void
vpx_highbd_convolve_copy_12_avx2
(
const
uint16_t
*
src
ptrdiff_t
src_stride
uint16_t
*
dst
ptrdiff_t
dst_stride
const
InterpKernel12
*
filter
int
x0_q4
int
x_step_q4
int
y0_q4
int
y_step_q4
int
w
int
h
int
bd
)
{
(
void
)
filter
;
vpx_highbd_convolve_copy_avx2
(
src
src_stride
dst
dst_stride
NULL
x0_q4
x_step_q4
y0_q4
y_step_q4
w
h
bd
)
;
}
void
vpx_highbd_convolve_horiz_12_avx2
(
const
uint16_t
*
src
ptrdiff_t
src_stride
uint16_t
*
dst
ptrdiff_t
dst_stride
const
InterpKernel12
*
filter
int
x0_q4
int
x_step_q4
int
y0_q4
int
y_step_q4
int
w
int
h
int
bd
)
{
assert
(
x_step_q4
=
=
16
)
;
(
void
)
y0_q4
;
(
void
)
x_step_q4
;
(
void
)
y_step_q4
;
const
uint16_t
*
src_ptr
=
src
;
src_ptr
-
=
MAX_FILTER_TAP
/
2
-
1
;
__m256i
s
[
6
]
f
[
6
]
;
const
__m256i
rounding
=
_mm256_set1_epi32
(
1
<
<
(
FILTER_BITS
-
1
)
)
;
const
__m256i
max
=
_mm256_set1_epi16
(
(
1
<
<
bd
)
-
1
)
;
highbd_shuffle_12tap_filter_avx2
(
filter
[
x0_q4
]
f
)
;
for
(
int
j
=
0
;
j
<
w
;
j
+
=
8
)
{
for
(
int
i
=
0
;
i
<
h
;
i
+
=
2
)
{
const
__m256i
row0
=
_mm256_loadu_si256
(
(
const
__m256i
*
)
&
src_ptr
[
i
*
src_stride
+
j
]
)
;
const
__m256i
row1
=
_mm256_loadu_si256
(
(
const
__m256i
*
)
&
src_ptr
[
(
i
+
1
)
*
src_stride
+
j
]
)
;
const
__m128i
row0_16
=
_mm_loadu_si128
(
(
const
__m128i
*
)
&
src_ptr
[
i
*
src_stride
+
j
+
16
]
)
;
const
__m128i
row1_16
=
_mm_loadu_si128
(
(
const
__m128i
*
)
&
src_ptr
[
(
i
+
1
)
*
src_stride
+
j
+
16
]
)
;
const
__m256i
r0
=
_mm256_permute2x128_si256
(
row0
row1
0x20
)
;
const
__m256i
r1
=
_mm256_permute2x128_si256
(
row0
row1
0x31
)
;
const
__m256i
r2
=
_mm256_inserti128_si256
(
_mm256_castsi128_si256
(
row0_16
)
row1_16
1
)
;
s
[
0
]
=
r0
;
s
[
1
]
=
_mm256_alignr_epi8
(
r1
r0
4
)
;
s
[
2
]
=
_mm256_alignr_epi8
(
r1
r0
8
)
;
s
[
3
]
=
_mm256_alignr_epi8
(
r1
r0
12
)
;
s
[
4
]
=
r1
;
s
[
5
]
=
_mm256_alignr_epi8
(
r2
r1
4
)
;
__m256i
res_even
=
highbd_convolve_12tap
(
s
f
)
;
res_even
=
_mm256_srai_epi32
(
_mm256_add_epi32
(
res_even
rounding
)
FILTER_BITS
)
;
s
[
0
]
=
_mm256_alignr_epi8
(
r1
r0
2
)
;
s
[
1
]
=
_mm256_alignr_epi8
(
r1
r0
6
)
;
s
[
2
]
=
_mm256_alignr_epi8
(
r1
r0
10
)
;
s
[
3
]
=
_mm256_alignr_epi8
(
r1
r0
14
)
;
s
[
4
]
=
_mm256_alignr_epi8
(
r2
r1
2
)
;
s
[
5
]
=
_mm256_alignr_epi8
(
r2
r1
6
)
;
__m256i
res_odd
=
highbd_convolve_12tap
(
s
f
)
;
res_odd
=
_mm256_srai_epi32
(
_mm256_add_epi32
(
res_odd
rounding
)
FILTER_BITS
)
;
const
__m256i
res_0
=
_mm256_unpacklo_epi32
(
res_even
res_odd
)
;
const
__m256i
res_1
=
_mm256_unpackhi_epi32
(
res_even
res_odd
)
;
const
__m256i
res_2
=
_mm256_packus_epi32
(
res_0
res_1
)
;
const
__m256i
res
=
_mm256_min_epi16
(
res_2
max
)
;
_mm_storeu_si128
(
(
__m128i
*
)
&
dst
[
i
*
dst_stride
+
j
]
_mm256_castsi256_si128
(
res
)
)
;
if
(
i
+
1
<
h
)
{
_mm_storeu_si128
(
(
__m128i
*
)
(
&
dst
[
(
i
+
1
)
*
dst_stride
+
j
]
)
_mm256_extractf128_si256
(
res
1
)
)
;
}
}
}
}
void
vpx_highbd_convolve_vert_12_avx2
(
const
uint16_t
*
src
ptrdiff_t
src_stride
uint16_t
*
dst
ptrdiff_t
dst_stride
const
InterpKernel12
*
filter
int
x0_q4
int
x_step_q4
int
y0_q4
int
y_step_q4
int
w
int
h
int
bd
)
{
assert
(
y_step_q4
=
=
16
)
;
(
void
)
x0_q4
;
(
void
)
x_step_q4
;
(
void
)
y_step_q4
;
const
uint16_t
*
src_ptr
=
src
;
src_ptr
-
=
src_stride
*
(
MAX_FILTER_TAP
/
2
-
1
)
;
__m256i
s
[
12
]
f
[
6
]
;
const
__m256i
rounding
=
_mm256_set1_epi32
(
(
(
1
<
<
FILTER_BITS
)
>
>
1
)
)
;
const
__m256i
max
=
_mm256_set1_epi16
(
(
1
<
<
bd
)
-
1
)
;
highbd_shuffle_12tap_filter_avx2
(
filter
[
y0_q4
]
f
)
;
for
(
int
j
=
0
;
j
<
w
;
j
+
=
8
)
{
__m128i
s0
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
0
*
src_stride
+
j
)
)
;
__m128i
s1
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
1
*
src_stride
+
j
)
)
;
__m128i
s2
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
2
*
src_stride
+
j
)
)
;
__m128i
s3
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
3
*
src_stride
+
j
)
)
;
__m128i
s4
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
4
*
src_stride
+
j
)
)
;
__m128i
s5
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
5
*
src_stride
+
j
)
)
;
__m128i
s6
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
6
*
src_stride
+
j
)
)
;
__m128i
s7
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
7
*
src_stride
+
j
)
)
;
__m128i
s8
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
8
*
src_stride
+
j
)
)
;
__m128i
s9
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
9
*
src_stride
+
j
)
)
;
__m128i
s10t
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
10
*
src_stride
+
j
)
)
;
__m256i
r01
=
_mm256_inserti128_si256
(
_mm256_castsi128_si256
(
s0
)
s1
1
)
;
__m256i
r12
=
_mm256_inserti128_si256
(
_mm256_castsi128_si256
(
s1
)
s2
1
)
;
__m256i
r23
=
_mm256_inserti128_si256
(
_mm256_castsi128_si256
(
s2
)
s3
1
)
;
__m256i
r34
=
_mm256_inserti128_si256
(
_mm256_castsi128_si256
(
s3
)
s4
1
)
;
__m256i
r45
=
_mm256_inserti128_si256
(
_mm256_castsi128_si256
(
s4
)
s5
1
)
;
__m256i
r56
=
_mm256_inserti128_si256
(
_mm256_castsi128_si256
(
s5
)
s6
1
)
;
__m256i
r67
=
_mm256_inserti128_si256
(
_mm256_castsi128_si256
(
s6
)
s7
1
)
;
__m256i
r78
=
_mm256_inserti128_si256
(
_mm256_castsi128_si256
(
s7
)
s8
1
)
;
__m256i
r89
=
_mm256_inserti128_si256
(
_mm256_castsi128_si256
(
s8
)
s9
1
)
;
__m256i
r910
=
_mm256_inserti128_si256
(
_mm256_castsi128_si256
(
s9
)
s10t
1
)
;
s
[
0
]
=
_mm256_unpacklo_epi16
(
r01
r12
)
;
s
[
1
]
=
_mm256_unpacklo_epi16
(
r23
r34
)
;
s
[
2
]
=
_mm256_unpacklo_epi16
(
r45
r56
)
;
s
[
3
]
=
_mm256_unpacklo_epi16
(
r67
r78
)
;
s
[
4
]
=
_mm256_unpacklo_epi16
(
r89
r910
)
;
s
[
6
]
=
_mm256_unpackhi_epi16
(
r01
r12
)
;
s
[
7
]
=
_mm256_unpackhi_epi16
(
r23
r34
)
;
s
[
8
]
=
_mm256_unpackhi_epi16
(
r45
r56
)
;
s
[
9
]
=
_mm256_unpackhi_epi16
(
r67
r78
)
;
s
[
10
]
=
_mm256_unpackhi_epi16
(
r89
r910
)
;
for
(
int
i
=
0
;
i
<
h
;
i
+
=
2
)
{
const
__m128i
s10
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
(
i
+
10
)
*
src_stride
+
j
)
)
;
const
__m128i
s11
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
(
i
+
11
)
*
src_stride
+
j
)
)
;
const
__m128i
s12
=
_mm_loadu_si128
(
(
const
__m128i
*
)
(
src_ptr
+
(
i
+
12
)
*
src_stride
+
j
)
)
;
__m256i
r1011
=
_mm256_inserti128_si256
(
_mm256_castsi128_si256
(
s10
)
s11
1
)
;
__m256i
r1112
=
_mm256_inserti128_si256
(
_mm256_castsi128_si256
(
s11
)
s12
1
)
;
s
[
5
]
=
_mm256_unpacklo_epi16
(
r1011
r1112
)
;
s
[
11
]
=
_mm256_unpackhi_epi16
(
r1011
r1112
)
;
const
__m256i
res_a
=
highbd_convolve_12tap
(
s
f
)
;
__m256i
res_a_round
=
_mm256_srai_epi32
(
_mm256_add_epi32
(
res_a
rounding
)
FILTER_BITS
)
;
const
__m256i
res_b
=
highbd_convolve_12tap
(
s
+
6
f
)
;
__m256i
res_b_round
=
_mm256_srai_epi32
(
_mm256_add_epi32
(
res_b
rounding
)
FILTER_BITS
)
;
const
__m256i
res_0
=
_mm256_packus_epi32
(
res_a_round
res_b_round
)
;
const
__m256i
res
=
_mm256_min_epi16
(
res_0
max
)
;
_mm_storeu_si128
(
(
__m128i
*
)
&
dst
[
i
*
dst_stride
+
j
]
_mm256_castsi256_si128
(
res
)
)
;
_mm_storeu_si128
(
(
__m128i
*
)
(
&
dst
[
(
i
+
1
)
*
dst_stride
+
j
]
)
_mm256_extractf128_si256
(
res
1
)
)
;
reuse_src_data_avx2
(
s
+
1
s
)
;
reuse_src_data_avx2
(
s
+
7
s
+
6
)
;
}
}
}
void
vpx_highbd_convolve_12_avx2
(
const
uint16_t
*
src
ptrdiff_t
src_stride
uint16_t
*
dst
ptrdiff_t
dst_stride
const
InterpKernel12
*
filter
int
x0_q4
int
x_step_q4
int
y0_q4
int
y_step_q4
int
w
int
h
int
bd
)
{
assert
(
x_step_q4
=
=
16
&
&
y_step_q4
=
=
16
)
;
assert
(
h
=
=
32
|
|
h
=
=
16
|
|
h
=
=
8
)
;
assert
(
w
=
=
32
|
|
w
=
=
16
|
|
w
=
=
8
)
;
DECLARE_ALIGNED
(
32
uint16_t
temp
[
BW
*
(
BH
+
MAX_FILTER_TAP
-
1
)
]
)
;
const
int
temp_stride
=
BW
;
const
int
intermediate_height
=
(
(
(
h
-
1
)
*
y_step_q4
+
y0_q4
)
>
>
SUBPEL_BITS
)
+
MAX_FILTER_TAP
;
vpx_highbd_convolve_horiz_12_avx2
(
src
-
src_stride
*
(
MAX_FILTER_TAP
/
2
-
1
)
src_stride
temp
temp_stride
filter
x0_q4
x_step_q4
y0_q4
y_step_q4
w
intermediate_height
bd
)
;
vpx_highbd_convolve_vert_12_avx2
(
temp
+
temp_stride
*
(
MAX_FILTER_TAP
/
2
-
1
)
temp_stride
dst
dst_stride
filter
x0_q4
x_step_q4
y0_q4
y_step_q4
w
h
bd
)
;
}
