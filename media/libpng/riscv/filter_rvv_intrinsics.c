#
include
"
.
.
/
pngpriv
.
h
"
#
ifdef
PNG_READ_SUPPORTED
#
if
PNG_RISCV_RVV_IMPLEMENTATION
=
=
1
#
include
<
riscv_vector
.
h
>
void
png_read_filter_row_up_rvv
(
png_row_infop
row_info
png_bytep
row
png_const_bytep
prev_row
)
{
size_t
len
=
row_info
-
>
rowbytes
;
for
(
size_t
vl
;
len
>
0
;
len
-
=
vl
row
+
=
vl
prev_row
+
=
vl
)
{
vl
=
__riscv_vsetvl_e8m8
(
len
)
;
vuint8m8_t
prev_vals
=
__riscv_vle8_v_u8m8
(
prev_row
vl
)
;
vuint8m8_t
row_vals
=
__riscv_vle8_v_u8m8
(
row
vl
)
;
row_vals
=
__riscv_vadd_vv_u8m8
(
row_vals
prev_vals
vl
)
;
__riscv_vse8_v_u8m8
(
row
row_vals
vl
)
;
}
}
static
inline
void
png_read_filter_row_sub_rvv
(
size_t
len
size_t
bpp
unsigned
char
*
row
)
{
png_bytep
rp_end
=
row
+
len
;
asm
volatile
(
"
vsetvli
zero
%
0
e8
m1
"
:
:
"
r
"
(
bpp
)
)
;
asm
volatile
(
"
vle8
.
v
v0
(
%
0
)
"
:
:
"
r
"
(
row
)
)
;
row
+
=
bpp
;
while
(
row
<
rp_end
)
{
asm
volatile
(
"
vle8
.
v
v8
(
%
0
)
"
:
:
"
r
"
(
row
)
)
;
asm
volatile
(
"
vadd
.
vv
v0
v0
v8
"
)
;
asm
volatile
(
"
vse8
.
v
v0
(
%
0
)
"
:
:
"
r
"
(
row
)
)
;
row
+
=
bpp
;
}
}
void
png_read_filter_row_sub3_rvv
(
png_row_infop
row_info
png_bytep
row
png_const_bytep
prev_row
)
{
size_t
len
=
row_info
-
>
rowbytes
;
png_read_filter_row_sub_rvv
(
len
3
row
)
;
PNG_UNUSED
(
prev_row
)
}
void
png_read_filter_row_sub4_rvv
(
png_row_infop
row_info
png_bytep
row
png_const_bytep
prev_row
)
{
size_t
len
=
row_info
-
>
rowbytes
;
png_read_filter_row_sub_rvv
(
len
4
row
)
;
PNG_UNUSED
(
prev_row
)
}
static
inline
void
png_read_filter_row_avg_rvv
(
size_t
len
size_t
bpp
unsigned
char
*
row
const
unsigned
char
*
prev_row
)
{
png_bytep
rp_end
=
row
+
len
;
asm
volatile
(
"
vsetvli
zero
%
0
e8
m1
"
:
:
"
r
"
(
bpp
)
)
;
asm
volatile
(
"
vle8
.
v
v4
(
%
0
)
"
:
:
"
r
"
(
prev_row
)
)
;
prev_row
+
=
bpp
;
asm
volatile
(
"
vle8
.
v
v8
(
%
0
)
"
:
:
"
r
"
(
row
)
)
;
asm
volatile
(
"
vsrl
.
vi
v4
v4
1
"
)
;
asm
volatile
(
"
vadd
.
vv
v2
v4
v8
"
)
;
asm
volatile
(
"
vse8
.
v
v2
(
%
0
)
"
:
:
"
r
"
(
row
)
)
;
row
+
=
bpp
;
while
(
row
<
rp_end
)
{
asm
volatile
(
"
vle8
.
v
v4
(
%
0
)
"
:
:
"
r
"
(
prev_row
)
)
;
prev_row
+
=
bpp
;
asm
volatile
(
"
vle8
.
v
v8
(
%
0
)
"
:
:
"
r
"
(
row
)
)
;
asm
volatile
(
"
vwaddu
.
vv
v12
v2
v4
"
)
;
asm
volatile
(
"
vnsrl
.
wi
v2
v12
1
"
)
;
asm
volatile
(
"
vadd
.
vv
v2
v2
v8
"
)
;
asm
volatile
(
"
vse8
.
v
v2
(
%
0
)
"
:
:
"
r
"
(
row
)
)
;
row
+
=
bpp
;
}
}
void
png_read_filter_row_avg3_rvv
(
png_row_infop
row_info
png_bytep
row
png_const_bytep
prev_row
)
{
size_t
len
=
row_info
-
>
rowbytes
;
png_read_filter_row_avg_rvv
(
len
3
row
prev_row
)
;
PNG_UNUSED
(
prev_row
)
}
void
png_read_filter_row_avg4_rvv
(
png_row_infop
row_info
png_bytep
row
png_const_bytep
prev_row
)
{
size_t
len
=
row_info
-
>
rowbytes
;
png_read_filter_row_avg_rvv
(
len
4
row
prev_row
)
;
PNG_UNUSED
(
prev_row
)
}
#
define
MIN_CHUNK_LEN
256
#
define
MAX_CHUNK_LEN
2048
static
inline
vuint8m1_t
prefix_sum
(
vuint8m1_t
chunk
unsigned
char
*
carry
size_t
vl
size_t
max_chunk_len
)
{
size_t
r
;
for
(
r
=
1
;
r
<
MIN_CHUNK_LEN
;
r
<
<
=
1
)
{
vbool8_t
shift_mask
=
__riscv_vmsgeu_vx_u8m1_b8
(
__riscv_vid_v_u8m1
(
vl
)
r
vl
)
;
chunk
=
__riscv_vadd_vv_u8m1_mu
(
shift_mask
chunk
chunk
__riscv_vslideup_vx_u8m1
(
__riscv_vundefined_u8m1
(
)
chunk
r
vl
)
vl
)
;
}
for
(
r
=
MIN_CHUNK_LEN
;
r
<
MAX_CHUNK_LEN
&
&
r
<
max_chunk_len
;
r
<
<
=
1
)
{
vbool8_t
shift_mask
=
__riscv_vmsgeu_vx_u8m1_b8
(
__riscv_vid_v_u8m1
(
vl
)
r
vl
)
;
chunk
=
__riscv_vadd_vv_u8m1_mu
(
shift_mask
chunk
chunk
__riscv_vslideup_vx_u8m1
(
__riscv_vundefined_u8m1
(
)
chunk
r
vl
)
vl
)
;
}
chunk
=
__riscv_vadd_vx_u8m1
(
chunk
*
carry
vl
)
;
*
carry
=
__riscv_vmv_x_s_u8m1_u8
(
__riscv_vslidedown_vx_u8m1
(
chunk
vl
-
1
vl
)
)
;
return
chunk
;
}
static
inline
vint16m1_t
abs_diff
(
vuint16m1_t
a
vuint16m1_t
b
size_t
vl
)
{
vint16m1_t
diff
=
__riscv_vreinterpret_v_u16m1_i16m1
(
__riscv_vsub_vv_u16m1
(
a
b
vl
)
)
;
vint16m1_t
neg
=
__riscv_vneg_v_i16m1
(
diff
vl
)
;
return
__riscv_vmax_vv_i16m1
(
diff
neg
vl
)
;
}
static
inline
vint16m1_t
abs_sum
(
vint16m1_t
a
vint16m1_t
b
size_t
vl
)
{
vint16m1_t
sum
=
__riscv_vadd_vv_i16m1
(
a
b
vl
)
;
vint16m1_t
neg
=
__riscv_vneg_v_i16m1
(
sum
vl
)
;
return
__riscv_vmax_vv_i16m1
(
sum
neg
vl
)
;
}
static
inline
void
png_read_filter_row_paeth_rvv
(
size_t
len
size_t
bpp
unsigned
char
*
row
const
unsigned
char
*
prev
)
{
png_bytep
rp_end
=
row
+
len
;
asm
volatile
(
"
vsetvli
zero
%
0
e8
m1
"
:
:
"
r
"
(
bpp
)
)
;
asm
volatile
(
"
vle8
.
v
v2
(
%
0
)
"
:
:
"
r
"
(
row
)
)
;
asm
volatile
(
"
vle8
.
v
v6
(
%
0
)
"
:
:
"
r
"
(
prev
)
)
;
prev
+
=
bpp
;
asm
volatile
(
"
vadd
.
vv
v2
v2
v6
"
)
;
asm
volatile
(
"
vse8
.
v
v2
(
%
0
)
"
:
:
"
r
"
(
row
)
)
;
row
+
=
bpp
;
while
(
row
<
rp_end
)
{
asm
volatile
(
"
vle8
.
v
v4
(
%
0
)
"
:
:
"
r
"
(
prev
)
)
;
prev
+
=
bpp
;
asm
volatile
(
"
vle8
.
v
v8
(
%
0
)
"
:
:
"
r
"
(
row
)
)
;
asm
volatile
(
"
vwsubu
.
vv
v12
v4
v6
"
)
;
asm
volatile
(
"
vwsubu
.
vv
v24
v2
v6
"
)
;
asm
volatile
(
"
vsetvli
zero
%
0
e16
m2
"
:
:
"
r
"
(
bpp
)
)
;
asm
volatile
(
"
vmv
.
v
.
v
v16
v12
"
)
;
asm
volatile
(
"
vmslt
.
vx
v0
v16
zero
"
)
;
asm
volatile
(
"
vrsub
.
vx
v16
v16
zero
v0
.
t
"
)
;
asm
volatile
(
"
vmv
.
v
.
v
v20
v24
"
)
;
asm
volatile
(
"
vmslt
.
vx
v0
v20
zero
"
)
;
asm
volatile
(
"
vrsub
.
vx
v20
v20
zero
v0
.
t
"
)
;
asm
volatile
(
"
vadd
.
vv
v24
v24
v12
"
)
;
asm
volatile
(
"
vmslt
.
vx
v0
v24
zero
"
)
;
asm
volatile
(
"
vrsub
.
vx
v24
v24
zero
v0
.
t
"
)
;
asm
volatile
(
"
vmslt
.
vv
v0
v20
v16
"
)
;
asm
volatile
(
"
vmerge
.
vvm
v16
v16
v20
v0
"
)
;
asm
volatile
(
"
vmslt
.
vv
v31
v24
v16
"
)
;
asm
volatile
(
"
vsetvli
zero
%
0
e8
m1
"
:
:
"
r
"
(
bpp
)
)
;
asm
volatile
(
"
vmerge
.
vvm
v2
v2
v4
v0
"
)
;
asm
volatile
(
"
vmand
.
mm
v0
v31
v31
"
)
;
asm
volatile
(
"
vmerge
.
vvm
v2
v2
v6
v0
"
)
;
asm
volatile
(
"
vadd
.
vv
v2
v2
v8
"
)
;
asm
volatile
(
"
vse8
.
v
v2
(
%
0
)
"
:
:
"
r
"
(
row
)
)
;
row
+
=
bpp
;
asm
volatile
(
"
vmv
.
v
.
v
v6
v4
"
)
;
}
}
void
png_read_filter_row_paeth3_rvv
(
png_row_infop
row_info
png_bytep
row
png_const_bytep
prev_row
)
{
size_t
len
=
row_info
-
>
rowbytes
;
png_read_filter_row_paeth_rvv
(
len
3
row
prev_row
)
;
PNG_UNUSED
(
prev_row
)
}
void
png_read_filter_row_paeth4_rvv
(
png_row_infop
row_info
png_bytep
row
png_const_bytep
prev_row
)
{
size_t
len
=
row_info
-
>
rowbytes
;
png_read_filter_row_paeth_rvv
(
len
4
row
prev_row
)
;
PNG_UNUSED
(
prev_row
)
}
#
endif
#
endif
