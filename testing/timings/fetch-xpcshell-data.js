#
!
/
usr
/
bin
/
env
node
const
fs
=
require
(
"
fs
"
)
;
const
path
=
require
(
"
path
"
)
;
const
{
Worker
}
=
require
(
"
worker_threads
"
)
;
const
os
=
require
(
"
os
"
)
;
const
MAX_WORKERS
=
Math
.
min
(
32
os
.
cpus
(
)
.
length
)
;
const
TASKCLUSTER_BASE_URL
=
process
.
env
.
TASKCLUSTER_PROXY_URL
|
|
process
.
env
.
TASKCLUSTER_ROOT_URL
|
|
"
https
:
/
/
firefox
-
ci
-
tc
.
services
.
mozilla
.
com
"
;
const
OUTPUT_DIR
=
(
(
)
=
>
{
const
outputDirIndex
=
process
.
argv
.
findIndex
(
arg
=
>
arg
=
=
=
"
-
-
output
-
dir
"
)
;
if
(
outputDirIndex
!
=
=
-
1
&
&
outputDirIndex
+
1
<
process
.
argv
.
length
)
{
return
process
.
argv
[
outputDirIndex
+
1
]
;
}
return
"
.
/
xpcshell
-
data
"
;
}
)
(
)
;
const
PROFILE_CACHE_DIR
=
"
.
/
profile
-
cache
"
;
let
previousRunData
=
null
;
let
allJobsCache
=
null
;
if
(
!
fs
.
existsSync
(
OUTPUT_DIR
)
)
{
fs
.
mkdirSync
(
OUTPUT_DIR
{
recursive
:
true
}
)
;
}
if
(
!
fs
.
existsSync
(
PROFILE_CACHE_DIR
)
)
{
fs
.
mkdirSync
(
PROFILE_CACHE_DIR
{
recursive
:
true
}
)
;
}
function
getDateString
(
daysAgo
=
0
)
{
const
date
=
new
Date
(
)
;
date
.
setDate
(
date
.
getDate
(
)
-
daysAgo
)
;
return
date
.
toISOString
(
)
.
split
(
"
T
"
)
[
0
]
;
}
async
function
fetchJson
(
url
)
{
const
response
=
await
fetch
(
url
)
;
if
(
!
response
.
ok
)
{
return
null
;
}
return
response
.
json
(
)
;
}
async
function
fetchCommitData
(
project
revision
)
{
console
.
log
(
Fetching
commit
data
for
{
project
}
:
{
revision
}
.
.
.
)
;
const
result
=
await
fetchJson
(
https
:
/
/
treeherder
.
mozilla
.
org
/
api
/
project
/
{
project
}
/
push
/
?
full
=
true
&
count
=
10
&
revision
=
{
revision
}
)
;
if
(
!
result
|
|
!
result
.
results
|
|
result
.
results
.
length
=
=
=
0
)
{
throw
new
Error
(
No
push
found
for
revision
{
revision
}
on
project
{
project
}
)
;
}
const
pushId
=
result
.
results
[
0
]
.
id
;
console
.
log
(
Found
push
ID
:
{
pushId
}
)
;
return
pushId
;
}
async
function
fetchPushJobs
(
project
pushId
)
{
console
.
log
(
Fetching
jobs
for
push
ID
{
pushId
}
.
.
.
)
;
let
allJobs
=
[
]
;
let
propertyNames
=
[
]
;
let
url
=
https
:
/
/
treeherder
.
mozilla
.
org
/
api
/
jobs
/
?
push_id
=
{
pushId
}
;
while
(
url
)
{
const
result
=
await
fetchJson
(
url
)
;
if
(
!
result
)
{
throw
new
Error
(
Failed
to
fetch
jobs
for
push
ID
{
pushId
}
)
;
}
allJobs
=
allJobs
.
concat
(
result
.
results
|
|
[
]
)
;
if
(
!
propertyNames
.
length
)
{
propertyNames
=
result
.
job_property_names
|
|
[
]
;
}
url
=
result
.
next
;
}
const
jobTypeNameIndex
=
propertyNames
.
indexOf
(
"
job_type_name
"
)
;
const
taskIdIndex
=
propertyNames
.
indexOf
(
"
task_id
"
)
;
const
retryIdIndex
=
propertyNames
.
indexOf
(
"
retry_id
"
)
;
const
lastModifiedIndex
=
propertyNames
.
indexOf
(
"
last_modified
"
)
;
const
xpcshellJobs
=
allJobs
.
filter
(
job
=
>
job
[
jobTypeNameIndex
]
&
&
job
[
jobTypeNameIndex
]
.
includes
(
"
xpcshell
"
)
)
.
map
(
job
=
>
(
{
name
:
job
[
jobTypeNameIndex
]
task_id
:
job
[
taskIdIndex
]
retry_id
:
job
[
retryIdIndex
]
|
|
0
start_time
:
job
[
lastModifiedIndex
]
repository
:
project
}
)
)
;
console
.
log
(
Found
{
xpcshellJobs
.
length
}
xpcshell
jobs
out
of
{
allJobs
.
length
}
total
jobs
)
;
return
xpcshellJobs
;
}
async
function
fetchXpcshellData
(
targetDate
)
{
console
.
log
(
Fetching
xpcshell
test
data
for
{
targetDate
}
.
.
.
)
;
if
(
!
allJobsCache
)
{
console
.
log
(
Querying
treeherder
database
.
.
.
)
;
const
result
=
await
fetchJson
(
"
https
:
/
/
sql
.
telemetry
.
mozilla
.
org
/
api
/
queries
/
110630
/
results
.
json
?
api_key
=
Pyybfsna2r5KQkwYgSk9zqbYfc6Dv0rhxL99DFi1
"
)
;
if
(
!
result
)
{
throw
new
Error
(
"
Failed
to
fetch
data
from
treeherder
database
"
)
;
}
const
allJobs
=
result
.
query_result
.
data
.
rows
;
allJobsCache
=
allJobs
.
filter
(
job
=
>
job
.
name
.
includes
(
"
xpcshell
"
)
)
;
console
.
log
(
Cached
{
allJobsCache
.
length
}
xpcshell
jobs
from
treeherder
database
(
out
of
{
allJobs
.
length
}
total
jobs
)
)
;
}
return
allJobsCache
.
filter
(
job
=
>
job
.
start_time
.
startsWith
(
targetDate
)
)
;
}
async
function
processJobsWithWorkers
(
jobs
targetDate
=
null
)
{
if
(
jobs
.
length
=
=
=
0
)
{
return
[
]
;
}
const
dateStr
=
targetDate
?
for
{
targetDate
}
:
"
"
;
console
.
log
(
Processing
{
jobs
.
length
}
jobs
{
dateStr
}
using
{
MAX_WORKERS
}
workers
.
.
.
)
;
const
jobQueue
=
[
.
.
.
jobs
]
;
const
results
=
[
]
;
const
workers
=
[
]
;
let
completedJobs
=
0
;
let
lastProgressTime
=
0
;
return
new
Promise
(
(
resolve
reject
)
=
>
{
const
workerStates
=
new
Map
(
)
;
for
(
let
i
=
0
;
i
<
MAX_WORKERS
;
i
+
+
)
{
const
worker
=
new
Worker
(
path
.
join
(
__dirname
"
profile
-
worker
.
js
"
)
{
workerData
:
{
profileCacheDir
:
PROFILE_CACHE_DIR
taskclusterBaseUrl
:
TASKCLUSTER_BASE_URL
}
}
)
;
workers
.
push
(
worker
)
;
workerStates
.
set
(
worker
{
id
:
i
+
1
ready
:
false
jobsProcessed
:
0
}
)
;
worker
.
on
(
"
message
"
message
=
>
{
const
workerState
=
workerStates
.
get
(
worker
)
;
if
(
message
.
type
=
=
=
"
ready
"
)
{
workerState
.
ready
=
true
;
assignNextJob
(
worker
)
;
}
else
if
(
message
.
type
=
=
=
"
jobComplete
"
)
{
workerState
.
jobsProcessed
+
+
;
completedJobs
+
+
;
if
(
message
.
result
)
{
results
.
push
(
message
.
result
)
;
}
const
now
=
Date
.
now
(
)
;
if
(
completedJobs
=
=
=
1
|
|
completedJobs
=
=
=
jobs
.
length
|
|
now
-
lastProgressTime
>
=
1000
)
{
const
percentage
=
Math
.
round
(
(
completedJobs
/
jobs
.
length
)
*
100
)
;
const
paddedCompleted
=
completedJobs
.
toString
(
)
.
padStart
(
jobs
.
length
.
toString
(
)
.
length
)
;
const
paddedPercentage
=
percentage
.
toString
(
)
.
padStart
(
3
)
;
console
.
log
(
{
paddedPercentage
}
%
{
paddedCompleted
}
/
{
jobs
.
length
}
)
;
lastProgressTime
=
now
;
}
assignNextJob
(
worker
)
;
}
else
if
(
message
.
type
=
=
=
"
finished
"
)
{
checkAllComplete
(
)
;
}
else
if
(
message
.
type
=
=
=
"
error
"
)
{
reject
(
new
Error
(
Worker
{
workerState
.
id
}
error
:
{
message
.
error
}
)
)
;
}
}
)
;
worker
.
on
(
"
error
"
error
=
>
{
reject
(
new
Error
(
Worker
{
workerStates
.
get
(
worker
)
.
id
}
thread
error
:
{
error
.
message
}
)
)
;
}
)
;
worker
.
on
(
"
exit
"
code
=
>
{
if
(
code
!
=
=
0
)
{
reject
(
new
Error
(
Worker
{
workerStates
.
get
(
worker
)
.
id
}
stopped
with
exit
code
{
code
}
)
)
;
}
}
)
;
}
function
assignNextJob
(
worker
)
{
if
(
jobQueue
.
length
)
{
const
job
=
jobQueue
.
shift
(
)
;
worker
.
postMessage
(
{
type
:
"
job
"
job
}
)
;
}
else
{
worker
.
postMessage
(
{
type
:
"
shutdown
"
}
)
;
}
}
let
resolved
=
false
;
let
workersFinished
=
0
;
function
checkAllComplete
(
)
{
if
(
resolved
)
{
return
;
}
workersFinished
+
+
;
if
(
workersFinished
>
=
MAX_WORKERS
)
{
resolved
=
true
;
workers
.
forEach
(
worker
=
>
worker
.
terminate
(
)
)
;
resolve
(
results
)
;
}
}
}
)
;
}
function
shouldIncludeMessage
(
status
)
{
return
status
=
=
=
"
SKIP
"
|
|
status
.
startsWith
(
"
FAIL
"
)
;
}
function
createDataTables
(
jobResults
)
{
const
tables
=
{
jobNames
:
[
]
testPaths
:
[
]
testNames
:
[
]
repositories
:
[
]
statuses
:
[
]
taskIds
:
[
]
messages
:
[
]
crashSignatures
:
[
]
}
;
const
stringMaps
=
{
jobNames
:
new
Map
(
)
testPaths
:
new
Map
(
)
testNames
:
new
Map
(
)
repositories
:
new
Map
(
)
statuses
:
new
Map
(
)
taskIds
:
new
Map
(
)
messages
:
new
Map
(
)
crashSignatures
:
new
Map
(
)
}
;
const
taskInfo
=
{
repositoryIds
:
[
]
jobNameIds
:
[
]
}
;
const
testInfo
=
{
testPathIds
:
[
]
testNameIds
:
[
]
}
;
const
testIdMap
=
new
Map
(
)
;
const
testRuns
=
[
]
;
function
findStringIndex
(
tableName
string
)
{
const
table
=
tables
[
tableName
]
;
const
map
=
stringMaps
[
tableName
]
;
let
index
=
map
.
get
(
string
)
;
if
(
index
=
=
=
undefined
)
{
index
=
table
.
length
;
table
.
push
(
string
)
;
map
.
set
(
string
index
)
;
}
return
index
;
}
for
(
const
result
of
jobResults
)
{
if
(
!
result
|
|
!
result
.
timings
)
{
continue
;
}
const
jobNameId
=
findStringIndex
(
"
jobNames
"
result
.
jobName
)
;
const
repositoryId
=
findStringIndex
(
"
repositories
"
result
.
repository
)
;
for
(
const
timing
of
result
.
timings
)
{
const
fullPath
=
timing
.
path
;
let
testId
=
testIdMap
.
get
(
fullPath
)
;
if
(
testId
=
=
=
undefined
)
{
const
lastSlashIndex
=
fullPath
.
lastIndexOf
(
"
/
"
)
;
let
testPath
testName
;
if
(
lastSlashIndex
=
=
=
-
1
)
{
testPath
=
"
"
;
testName
=
fullPath
;
}
else
{
testPath
=
fullPath
.
substring
(
0
lastSlashIndex
)
;
testName
=
fullPath
.
substring
(
lastSlashIndex
+
1
)
;
}
const
testPathId
=
findStringIndex
(
"
testPaths
"
testPath
)
;
const
testNameId
=
findStringIndex
(
"
testNames
"
testName
)
;
testId
=
testInfo
.
testPathIds
.
length
;
testInfo
.
testPathIds
.
push
(
testPathId
)
;
testInfo
.
testNameIds
.
push
(
testNameId
)
;
testIdMap
.
set
(
fullPath
testId
)
;
}
const
statusId
=
findStringIndex
(
"
statuses
"
timing
.
status
|
|
"
UNKNOWN
"
)
;
const
taskIdString
=
{
result
.
taskId
}
.
{
result
.
retryId
}
;
const
taskIdId
=
findStringIndex
(
"
taskIds
"
taskIdString
)
;
if
(
taskInfo
.
repositoryIds
[
taskIdId
]
=
=
=
undefined
)
{
taskInfo
.
repositoryIds
[
taskIdId
]
=
repositoryId
;
taskInfo
.
jobNameIds
[
taskIdId
]
=
jobNameId
;
}
if
(
!
testRuns
[
testId
]
)
{
testRuns
[
testId
]
=
[
]
;
}
let
statusGroup
=
testRuns
[
testId
]
[
statusId
]
;
if
(
!
statusGroup
)
{
statusGroup
=
{
taskIdIds
:
[
]
durations
:
[
]
timestamps
:
[
]
}
;
if
(
shouldIncludeMessage
(
timing
.
status
)
)
{
statusGroup
.
messageIds
=
[
]
;
}
if
(
timing
.
status
=
=
=
"
CRASH
"
)
{
statusGroup
.
crashSignatureIds
=
[
]
;
statusGroup
.
minidumps
=
[
]
;
}
testRuns
[
testId
]
[
statusId
]
=
statusGroup
;
}
statusGroup
.
taskIdIds
.
push
(
taskIdId
)
;
statusGroup
.
durations
.
push
(
Math
.
round
(
timing
.
duration
)
)
;
statusGroup
.
timestamps
.
push
(
timing
.
timestamp
)
;
if
(
shouldIncludeMessage
(
timing
.
status
)
)
{
const
messageId
=
timing
.
message
?
findStringIndex
(
"
messages
"
timing
.
message
)
:
null
;
statusGroup
.
messageIds
.
push
(
messageId
)
;
}
if
(
timing
.
status
=
=
=
"
CRASH
"
)
{
const
crashSignatureId
=
timing
.
crashSignature
?
findStringIndex
(
"
crashSignatures
"
timing
.
crashSignature
)
:
null
;
statusGroup
.
crashSignatureIds
.
push
(
crashSignatureId
)
;
statusGroup
.
minidumps
.
push
(
timing
.
minidump
|
|
null
)
;
}
}
}
return
{
tables
taskInfo
testInfo
testRuns
}
;
}
function
sortStringTablesByFrequency
(
dataStructure
)
{
const
{
tables
taskInfo
testInfo
testRuns
}
=
dataStructure
;
const
frequencyCounts
=
{
jobNames
:
new
Array
(
tables
.
jobNames
.
length
)
.
fill
(
0
)
testPaths
:
new
Array
(
tables
.
testPaths
.
length
)
.
fill
(
0
)
testNames
:
new
Array
(
tables
.
testNames
.
length
)
.
fill
(
0
)
repositories
:
new
Array
(
tables
.
repositories
.
length
)
.
fill
(
0
)
statuses
:
new
Array
(
tables
.
statuses
.
length
)
.
fill
(
0
)
taskIds
:
new
Array
(
tables
.
taskIds
.
length
)
.
fill
(
0
)
messages
:
new
Array
(
tables
.
messages
.
length
)
.
fill
(
0
)
crashSignatures
:
new
Array
(
tables
.
crashSignatures
.
length
)
.
fill
(
0
)
}
;
for
(
const
jobNameId
of
taskInfo
.
jobNameIds
)
{
if
(
jobNameId
!
=
=
undefined
)
{
frequencyCounts
.
jobNames
[
jobNameId
]
+
+
;
}
}
for
(
const
repositoryId
of
taskInfo
.
repositoryIds
)
{
if
(
repositoryId
!
=
=
undefined
)
{
frequencyCounts
.
repositories
[
repositoryId
]
+
+
;
}
}
for
(
const
testPathId
of
testInfo
.
testPathIds
)
{
frequencyCounts
.
testPaths
[
testPathId
]
+
+
;
}
for
(
const
testNameId
of
testInfo
.
testNameIds
)
{
frequencyCounts
.
testNames
[
testNameId
]
+
+
;
}
for
(
const
testGroup
of
testRuns
)
{
if
(
!
testGroup
)
{
continue
;
}
testGroup
.
forEach
(
(
statusGroup
statusId
)
=
>
{
if
(
!
statusGroup
)
{
return
;
}
frequencyCounts
.
statuses
[
statusId
]
+
=
statusGroup
.
taskIdIds
.
length
;
for
(
const
taskIdId
of
statusGroup
.
taskIdIds
)
{
frequencyCounts
.
taskIds
[
taskIdId
]
+
+
;
}
if
(
statusGroup
.
messageIds
)
{
for
(
const
messageId
of
statusGroup
.
messageIds
)
{
if
(
messageId
!
=
=
null
)
{
frequencyCounts
.
messages
[
messageId
]
+
+
;
}
}
}
if
(
statusGroup
.
crashSignatureIds
)
{
for
(
const
crashSigId
of
statusGroup
.
crashSignatureIds
)
{
if
(
crashSigId
!
=
=
null
)
{
frequencyCounts
.
crashSignatures
[
crashSigId
]
+
+
;
}
}
}
}
)
;
}
const
sortedTables
=
{
}
;
const
indexMaps
=
{
}
;
for
(
const
[
tableName
table
]
of
Object
.
entries
(
tables
)
)
{
const
counts
=
frequencyCounts
[
tableName
]
;
const
indexed
=
table
.
map
(
(
value
oldIndex
)
=
>
(
{
value
oldIndex
count
:
counts
[
oldIndex
]
}
)
)
;
indexed
.
sort
(
(
a
b
)
=
>
{
if
(
b
.
count
!
=
=
a
.
count
)
{
return
b
.
count
-
a
.
count
;
}
return
a
.
value
.
localeCompare
(
b
.
value
)
;
}
)
;
sortedTables
[
tableName
]
=
indexed
.
map
(
item
=
>
item
.
value
)
;
indexMaps
[
tableName
]
=
new
Map
(
indexed
.
map
(
(
item
newIndex
)
=
>
[
item
.
oldIndex
newIndex
]
)
)
;
}
const
sortedTaskInfo
=
{
repositoryIds
:
[
]
jobNameIds
:
[
]
}
;
for
(
let
oldTaskIdId
=
0
;
oldTaskIdId
<
taskInfo
.
repositoryIds
.
length
;
oldTaskIdId
+
+
)
{
const
newTaskIdId
=
indexMaps
.
taskIds
.
get
(
oldTaskIdId
)
;
sortedTaskInfo
.
repositoryIds
[
newTaskIdId
]
=
indexMaps
.
repositories
.
get
(
taskInfo
.
repositoryIds
[
oldTaskIdId
]
)
;
sortedTaskInfo
.
jobNameIds
[
newTaskIdId
]
=
indexMaps
.
jobNames
.
get
(
taskInfo
.
jobNameIds
[
oldTaskIdId
]
)
;
}
const
sortedTestInfo
=
{
testPathIds
:
testInfo
.
testPathIds
.
map
(
oldId
=
>
indexMaps
.
testPaths
.
get
(
oldId
)
)
testNameIds
:
testInfo
.
testNameIds
.
map
(
oldId
=
>
indexMaps
.
testNames
.
get
(
oldId
)
)
}
;
const
sortedTestRuns
=
testRuns
.
map
(
testGroup
=
>
{
if
(
!
testGroup
)
{
return
testGroup
;
}
return
testGroup
.
map
(
statusGroup
=
>
{
if
(
!
statusGroup
)
{
return
statusGroup
;
}
const
remapped
=
{
taskIdIds
:
statusGroup
.
taskIdIds
.
map
(
oldId
=
>
indexMaps
.
taskIds
.
get
(
oldId
)
)
durations
:
statusGroup
.
durations
timestamps
:
statusGroup
.
timestamps
}
;
if
(
statusGroup
.
messageIds
)
{
remapped
.
messageIds
=
statusGroup
.
messageIds
.
map
(
oldId
=
>
oldId
=
=
=
null
?
null
:
indexMaps
.
messages
.
get
(
oldId
)
)
;
}
if
(
statusGroup
.
crashSignatureIds
)
{
remapped
.
crashSignatureIds
=
statusGroup
.
crashSignatureIds
.
map
(
oldId
=
>
oldId
=
=
=
null
?
null
:
indexMaps
.
crashSignatures
.
get
(
oldId
)
)
;
}
if
(
statusGroup
.
minidumps
)
{
remapped
.
minidumps
=
statusGroup
.
minidumps
;
}
return
remapped
;
}
)
;
}
)
;
const
finalTestRuns
=
sortedTestRuns
.
map
(
testGroup
=
>
{
if
(
!
testGroup
)
{
return
testGroup
;
}
const
remappedGroup
=
[
]
;
testGroup
.
forEach
(
(
statusGroup
oldStatusId
)
=
>
{
if
(
!
statusGroup
)
{
return
;
}
const
newStatusId
=
indexMaps
.
statuses
.
get
(
oldStatusId
)
;
remappedGroup
[
newStatusId
]
=
statusGroup
;
}
)
;
return
remappedGroup
;
}
)
;
return
{
tables
:
sortedTables
taskInfo
:
sortedTaskInfo
testInfo
:
sortedTestInfo
testRuns
:
finalTestRuns
}
;
}
function
createResourceUsageData
(
jobResults
)
{
const
jobNames
=
[
]
;
const
jobNameMap
=
new
Map
(
)
;
const
repositories
=
[
]
;
const
repositoryMap
=
new
Map
(
)
;
const
machineInfos
=
[
]
;
const
machineInfoMap
=
new
Map
(
)
;
const
jobDataList
=
[
]
;
for
(
const
result
of
jobResults
)
{
if
(
!
result
|
|
!
result
.
resourceUsage
)
{
continue
;
}
let
jobNameBase
=
result
.
jobName
;
let
chunkNumber
=
null
;
const
match
=
result
.
jobName
.
match
(
/
^
(
.
+
)
-
(
\
d
+
)
/
)
;
if
(
match
)
{
jobNameBase
=
match
[
1
]
;
chunkNumber
=
parseInt
(
match
[
2
]
10
)
;
}
let
jobNameId
=
jobNameMap
.
get
(
jobNameBase
)
;
if
(
jobNameId
=
=
=
undefined
)
{
jobNameId
=
jobNames
.
length
;
jobNames
.
push
(
jobNameBase
)
;
jobNameMap
.
set
(
jobNameBase
jobNameId
)
;
}
let
repositoryId
=
repositoryMap
.
get
(
result
.
repository
)
;
if
(
repositoryId
=
=
=
undefined
)
{
repositoryId
=
repositories
.
length
;
repositories
.
push
(
result
.
repository
)
;
repositoryMap
.
set
(
result
.
repository
repositoryId
)
;
}
const
machineInfo
=
result
.
resourceUsage
.
machineInfo
;
const
machineInfoKey
=
JSON
.
stringify
(
machineInfo
)
;
let
machineInfoId
=
machineInfoMap
.
get
(
machineInfoKey
)
;
if
(
machineInfoId
=
=
=
undefined
)
{
machineInfoId
=
machineInfos
.
length
;
machineInfos
.
push
(
machineInfo
)
;
machineInfoMap
.
set
(
machineInfoKey
machineInfoId
)
;
}
const
taskIdString
=
result
.
retryId
=
=
=
0
?
result
.
taskId
:
{
result
.
taskId
}
.
{
result
.
retryId
}
;
jobDataList
.
push
(
{
jobNameId
chunk
:
chunkNumber
taskId
:
taskIdString
repositoryId
startTime
:
result
.
startTime
machineInfoId
maxMemory
:
result
.
resourceUsage
.
maxMemory
idleTime
:
result
.
resourceUsage
.
idleTime
singleCoreTime
:
result
.
resourceUsage
.
singleCoreTime
cpuBuckets
:
result
.
resourceUsage
.
cpuBuckets
}
)
;
}
jobDataList
.
sort
(
(
a
b
)
=
>
a
.
startTime
-
b
.
startTime
)
;
const
jobs
=
{
jobNameIds
:
[
]
chunks
:
[
]
taskIds
:
[
]
repositoryIds
:
[
]
startTimes
:
[
]
machineInfoIds
:
[
]
maxMemories
:
[
]
idleTimes
:
[
]
singleCoreTimes
:
[
]
cpuBuckets
:
[
]
}
;
let
previousStartTime
=
0
;
for
(
const
jobData
of
jobDataList
)
{
jobs
.
jobNameIds
.
push
(
jobData
.
jobNameId
)
;
jobs
.
chunks
.
push
(
jobData
.
chunk
)
;
jobs
.
taskIds
.
push
(
jobData
.
taskId
)
;
jobs
.
repositoryIds
.
push
(
jobData
.
repositoryId
)
;
const
timeDiff
=
jobData
.
startTime
-
previousStartTime
;
jobs
.
startTimes
.
push
(
timeDiff
)
;
previousStartTime
=
jobData
.
startTime
;
jobs
.
machineInfoIds
.
push
(
jobData
.
machineInfoId
)
;
jobs
.
maxMemories
.
push
(
jobData
.
maxMemory
)
;
jobs
.
idleTimes
.
push
(
jobData
.
idleTime
)
;
jobs
.
singleCoreTimes
.
push
(
jobData
.
singleCoreTime
)
;
jobs
.
cpuBuckets
.
push
(
jobData
.
cpuBuckets
)
;
}
return
{
jobNames
repositories
machineInfos
jobs
}
;
}
function
saveJsonFile
(
data
filePath
)
{
fs
.
writeFileSync
(
filePath
JSON
.
stringify
(
data
)
)
;
const
stats
=
fs
.
statSync
(
filePath
)
;
const
fileSizeBytes
=
stats
.
size
;
if
(
fileSizeBytes
>
=
1024
*
1024
)
{
const
fileSizeMB
=
Math
.
round
(
fileSizeBytes
/
(
1024
*
1024
)
)
;
const
formattedBytes
=
fileSizeBytes
.
toLocaleString
(
)
;
console
.
log
(
Saved
{
filePath
}
-
{
fileSizeMB
}
MB
(
{
formattedBytes
}
bytes
)
)
;
}
else
{
const
fileSizeKB
=
Math
.
round
(
fileSizeBytes
/
1024
)
;
console
.
log
(
Saved
{
filePath
}
-
{
fileSizeKB
}
KB
)
;
}
}
async
function
processJobsAndCreateData
(
jobs
targetLabel
startTime
metadata
)
{
if
(
jobs
.
length
=
=
=
0
)
{
console
.
log
(
No
jobs
found
for
{
targetLabel
}
.
)
;
return
null
;
}
const
jobProcessingStart
=
Date
.
now
(
)
;
const
jobResults
=
await
processJobsWithWorkers
(
jobs
targetLabel
)
;
const
jobProcessingTime
=
Date
.
now
(
)
-
jobProcessingStart
;
console
.
log
(
Successfully
processed
{
jobResults
.
length
}
jobs
in
{
jobProcessingTime
}
ms
)
;
const
dataTablesStart
=
Date
.
now
(
)
;
let
dataStructure
=
createDataTables
(
jobResults
)
;
const
dataTablesTime
=
Date
.
now
(
)
-
dataTablesStart
;
console
.
log
(
Created
data
tables
in
{
dataTablesTime
}
ms
:
)
;
const
hasTestRuns
=
!
!
dataStructure
.
testRuns
.
length
;
if
(
!
hasTestRuns
)
{
console
.
log
(
No
test
run
data
extracted
for
{
targetLabel
}
)
;
return
null
;
}
const
totalRuns
=
dataStructure
.
testRuns
.
reduce
(
(
sum
testGroup
)
=
>
{
if
(
!
testGroup
)
{
return
sum
;
}
return
(
sum
+
testGroup
.
reduce
(
(
testSum
statusGroup
)
=
>
testSum
+
(
statusGroup
?
statusGroup
.
taskIdIds
.
length
:
0
)
0
)
)
;
}
0
)
;
console
.
log
(
{
dataStructure
.
testInfo
.
testPathIds
.
length
}
tests
{
totalRuns
}
runs
{
dataStructure
.
tables
.
taskIds
.
length
}
tasks
{
dataStructure
.
tables
.
jobNames
.
length
}
job
names
{
dataStructure
.
tables
.
statuses
.
length
}
statuses
)
;
const
sortingStart
=
Date
.
now
(
)
;
dataStructure
=
sortStringTablesByFrequency
(
dataStructure
)
;
const
sortingTime
=
Date
.
now
(
)
-
sortingStart
;
console
.
log
(
Sorted
string
tables
by
frequency
in
{
sortingTime
}
ms
)
;
for
(
const
testGroup
of
dataStructure
.
testRuns
)
{
if
(
!
testGroup
)
{
continue
;
}
for
(
const
statusGroup
of
testGroup
)
{
if
(
!
statusGroup
)
{
continue
;
}
for
(
let
i
=
0
;
i
<
statusGroup
.
timestamps
.
length
;
i
+
+
)
{
statusGroup
.
timestamps
[
i
]
=
Math
.
floor
(
statusGroup
.
timestamps
[
i
]
/
1000
)
-
startTime
;
}
const
runs
=
statusGroup
.
timestamps
.
map
(
(
ts
i
)
=
>
{
const
run
=
{
timestamp
:
ts
taskIdId
:
statusGroup
.
taskIdIds
[
i
]
duration
:
statusGroup
.
durations
[
i
]
}
;
if
(
statusGroup
.
crashSignatureIds
)
{
run
.
crashSignatureId
=
statusGroup
.
crashSignatureIds
[
i
]
;
}
if
(
statusGroup
.
minidumps
)
{
run
.
minidump
=
statusGroup
.
minidumps
[
i
]
;
}
if
(
statusGroup
.
messageIds
)
{
run
.
messageId
=
statusGroup
.
messageIds
[
i
]
;
}
return
run
;
}
)
;
runs
.
sort
(
(
a
b
)
=
>
a
.
timestamp
-
b
.
timestamp
)
;
let
previousTimestamp
=
0
;
for
(
const
run
of
runs
)
{
const
currentTimestamp
=
run
.
timestamp
;
run
.
timestamp
=
currentTimestamp
-
previousTimestamp
;
previousTimestamp
=
currentTimestamp
;
}
statusGroup
.
taskIdIds
=
runs
.
map
(
run
=
>
run
.
taskIdId
)
;
statusGroup
.
durations
=
runs
.
map
(
run
=
>
run
.
duration
)
;
statusGroup
.
timestamps
=
runs
.
map
(
run
=
>
run
.
timestamp
)
;
if
(
statusGroup
.
crashSignatureIds
)
{
statusGroup
.
crashSignatureIds
=
runs
.
map
(
run
=
>
run
.
crashSignatureId
)
;
}
if
(
statusGroup
.
minidumps
)
{
statusGroup
.
minidumps
=
runs
.
map
(
run
=
>
run
.
minidump
)
;
}
if
(
statusGroup
.
messageIds
)
{
statusGroup
.
messageIds
=
runs
.
map
(
run
=
>
run
.
messageId
)
;
}
}
}
return
{
testData
:
{
metadata
:
{
.
.
.
metadata
startTime
generatedAt
:
new
Date
(
)
.
toISOString
(
)
jobCount
:
jobs
.
length
processedJobCount
:
jobResults
.
length
}
tables
:
dataStructure
.
tables
taskInfo
:
dataStructure
.
taskInfo
testInfo
:
dataStructure
.
testInfo
testRuns
:
dataStructure
.
testRuns
}
resourceData
:
createResourceUsageData
(
jobResults
)
}
;
}
async
function
processRevisionData
(
project
revision
forceRefetch
=
false
)
{
console
.
log
(
Fetching
xpcshell
test
data
for
{
project
}
:
{
revision
}
)
;
console
.
log
(
=
=
=
Processing
{
project
}
:
{
revision
}
=
=
=
)
;
const
cacheFile
=
path
.
join
(
OUTPUT_DIR
xpcshell
-
{
project
}
-
{
revision
}
.
json
)
;
if
(
fs
.
existsSync
(
cacheFile
)
&
&
!
forceRefetch
)
{
console
.
log
(
Data
for
{
project
}
:
{
revision
}
already
exists
.
Skipping
.
)
;
return
null
;
}
if
(
forceRefetch
)
{
console
.
log
(
Force
flag
detected
re
-
fetching
data
for
{
project
}
:
{
revision
}
.
.
.
)
;
}
try
{
const
pushId
=
await
fetchCommitData
(
project
revision
)
;
const
jobs
=
await
fetchPushJobs
(
project
pushId
)
;
if
(
jobs
.
length
=
=
=
0
)
{
console
.
log
(
No
xpcshell
jobs
found
for
{
project
}
:
{
revision
}
.
)
;
return
null
;
}
const
startTime
=
jobs
.
length
?
Math
.
floor
(
new
Date
(
jobs
[
0
]
.
start_time
)
.
getTime
(
)
/
1000
)
:
Math
.
floor
(
Date
.
now
(
)
/
1000
)
;
const
output
=
await
processJobsAndCreateData
(
jobs
{
project
}
-
{
revision
}
startTime
{
project
revision
pushId
}
)
;
if
(
!
output
)
{
return
null
;
}
saveJsonFile
(
output
.
testData
cacheFile
)
;
const
resourceCacheFile
=
path
.
join
(
OUTPUT_DIR
xpcshell
-
{
project
}
-
{
revision
}
-
resources
.
json
)
;
saveJsonFile
(
output
.
resourceData
resourceCacheFile
)
;
return
output
;
}
catch
(
error
)
{
console
.
error
(
Error
processing
{
project
}
:
{
revision
}
:
error
)
;
return
null
;
}
}
async
function
fetchPreviousRunData
(
)
{
try
{
const
taskUrl
=
{
TASKCLUSTER_BASE_URL
}
/
api
/
queue
/
v1
/
task
/
{
process
.
env
.
TASK_ID
}
;
const
taskData
=
await
fetchJson
(
taskUrl
)
;
if
(
!
taskData
)
{
console
.
log
(
Failed
to
fetch
task
info
from
{
taskUrl
}
)
;
return
;
}
const
routes
=
taskData
.
routes
|
|
[
]
;
const
latestRoute
=
routes
.
find
(
route
=
>
route
.
startsWith
(
"
index
.
"
)
&
&
route
.
includes
(
"
.
latest
.
"
)
)
;
if
(
!
latestRoute
)
{
console
.
log
(
No
route
found
with
'
index
.
'
prefix
and
'
.
latest
.
'
in
name
.
Available
routes
:
{
JSON
.
stringify
(
routes
)
}
)
;
return
;
}
const
indexName
=
latestRoute
.
replace
(
/
^
index
\
.
/
"
"
)
;
console
.
log
(
Using
index
:
{
indexName
}
)
;
const
artifactsUrl
=
{
TASKCLUSTER_BASE_URL
}
/
api
/
index
/
v1
/
task
/
{
indexName
}
/
artifacts
/
public
;
const
indexUrl
=
{
artifactsUrl
}
/
index
.
json
;
console
.
log
(
Fetching
previous
run
data
from
{
indexUrl
}
)
;
const
indexData
=
await
fetchJson
(
indexUrl
)
;
if
(
!
indexData
)
{
console
.
log
(
Failed
to
fetch
index
.
json
from
{
indexUrl
}
)
;
return
;
}
const
dates
=
indexData
.
dates
|
|
[
]
;
console
.
log
(
Found
{
dates
.
length
}
dates
in
previous
run
)
;
previousRunData
=
{
dates
:
new
Set
(
dates
)
artifactsUrl
}
;
console
.
log
(
"
Previous
run
metadata
loaded
\
n
"
)
;
}
catch
(
error
)
{
console
.
log
(
Error
fetching
previous
run
metadata
:
{
error
.
message
}
)
;
}
}
async
function
processDateData
(
targetDate
forceRefetch
=
false
)
{
const
timingsFilename
=
xpcshell
-
{
targetDate
}
.
json
;
const
resourcesFilename
=
xpcshell
-
{
targetDate
}
-
resources
.
json
;
const
timingsPath
=
path
.
join
(
OUTPUT_DIR
timingsFilename
)
;
const
resourcesPath
=
path
.
join
(
OUTPUT_DIR
resourcesFilename
)
;
if
(
fs
.
existsSync
(
timingsPath
)
&
&
!
forceRefetch
)
{
console
.
log
(
Data
for
{
targetDate
}
already
exists
.
Skipping
.
)
;
return
;
}
let
jobs
;
try
{
jobs
=
await
fetchXpcshellData
(
targetDate
)
;
if
(
jobs
.
length
=
=
=
0
)
{
console
.
log
(
No
jobs
found
for
{
targetDate
}
.
)
;
return
;
}
}
catch
(
error
)
{
console
.
error
(
Error
fetching
jobs
for
{
targetDate
}
:
error
)
;
return
;
}
if
(
!
forceRefetch
&
&
previousRunData
&
&
previousRunData
.
dates
.
has
(
targetDate
)
)
{
try
{
const
[
timings
resources
]
=
await
Promise
.
all
(
[
fetchJson
(
{
previousRunData
.
artifactsUrl
}
/
{
timingsFilename
}
)
fetchJson
(
{
previousRunData
.
artifactsUrl
}
/
{
resourcesFilename
}
)
]
)
;
if
(
timings
&
&
resources
)
{
const
expectedJobCount
=
jobs
.
length
;
const
actualProcessedCount
=
timings
.
metadata
?
.
processedJobCount
;
if
(
actualProcessedCount
<
expectedJobCount
)
{
const
missingJobs
=
expectedJobCount
-
actualProcessedCount
;
console
.
log
(
Ignoring
artifact
from
previous
run
:
missing
{
missingJobs
}
jobs
(
expected
{
expectedJobCount
}
got
{
actualProcessedCount
}
)
)
;
}
else
{
console
.
log
(
Fetched
valid
artifact
from
previous
run
.
)
;
saveJsonFile
(
timings
timingsPath
)
;
saveJsonFile
(
resources
resourcesPath
)
;
return
;
}
}
else
{
console
.
log
(
Error
fetching
artifact
from
previous
run
:
artifact
not
found
)
;
}
}
catch
(
error
)
{
console
.
log
(
Error
fetching
artifact
from
previous
run
:
{
error
.
message
}
)
;
}
}
if
(
forceRefetch
)
{
console
.
log
(
Force
flag
detected
re
-
fetching
data
for
{
targetDate
}
.
.
.
)
;
}
try
{
const
startOfDay
=
new
Date
(
targetDate
+
"
T00
:
00
:
00
.
000Z
"
)
;
const
startTime
=
Math
.
floor
(
startOfDay
.
getTime
(
)
/
1000
)
;
const
output
=
await
processJobsAndCreateData
(
jobs
targetDate
startTime
{
date
:
targetDate
}
)
;
if
(
!
output
)
{
return
;
}
saveJsonFile
(
output
.
testData
timingsPath
)
;
saveJsonFile
(
output
.
resourceData
resourcesPath
)
;
}
catch
(
error
)
{
console
.
error
(
Error
processing
{
targetDate
}
:
error
)
;
}
}
async
function
main
(
)
{
const
forceRefetch
=
process
.
argv
.
includes
(
"
-
-
force
"
)
;
let
numDays
=
3
;
const
daysIndex
=
process
.
argv
.
findIndex
(
arg
=
>
arg
=
=
=
"
-
-
days
"
)
;
if
(
daysIndex
!
=
=
-
1
&
&
daysIndex
+
1
<
process
.
argv
.
length
)
{
const
daysValue
=
parseInt
(
process
.
argv
[
daysIndex
+
1
]
)
;
if
(
!
isNaN
(
daysValue
)
&
&
daysValue
>
0
&
&
daysValue
<
=
30
)
{
numDays
=
daysValue
;
}
else
{
console
.
error
(
"
Error
:
-
-
days
must
be
a
number
between
1
and
30
"
)
;
process
.
exit
(
1
)
;
}
}
if
(
process
.
env
.
TASK_ID
)
{
await
fetchPreviousRunData
(
)
;
}
const
revisionIndex
=
process
.
argv
.
findIndex
(
arg
=
>
arg
=
=
=
"
-
-
revision
"
)
;
if
(
revisionIndex
!
=
=
-
1
&
&
revisionIndex
+
1
<
process
.
argv
.
length
)
{
const
revisionArg
=
process
.
argv
[
revisionIndex
+
1
]
;
const
parts
=
revisionArg
.
split
(
"
:
"
)
;
if
(
parts
.
length
!
=
=
2
)
{
console
.
error
(
"
Error
:
-
-
revision
must
be
in
format
project
:
revision
(
e
.
g
.
try
:
abc123
or
autoland
:
def456
)
"
)
;
process
.
exit
(
1
)
;
}
const
[
project
revision
]
=
parts
;
const
output
=
await
processRevisionData
(
project
revision
forceRefetch
)
;
if
(
output
)
{
console
.
log
(
"
Successfully
processed
revision
data
.
"
)
;
}
else
{
console
.
log
(
"
\
nNo
data
was
successfully
processed
.
"
)
;
}
return
;
}
const
tryIndex
=
process
.
argv
.
findIndex
(
arg
=
>
arg
=
=
=
"
-
-
try
"
)
;
if
(
tryIndex
!
=
=
-
1
&
&
tryIndex
+
1
<
process
.
argv
.
length
)
{
const
revision
=
process
.
argv
[
tryIndex
+
1
]
;
const
output
=
await
processRevisionData
(
"
try
"
revision
forceRefetch
)
;
if
(
output
)
{
console
.
log
(
"
Successfully
processed
try
commit
data
.
"
)
;
}
else
{
console
.
log
(
"
\
nNo
data
was
successfully
processed
.
"
)
;
}
return
;
}
const
dates
=
[
]
;
for
(
let
i
=
1
;
i
<
=
numDays
;
i
+
+
)
{
dates
.
push
(
getDateString
(
i
)
)
;
}
console
.
log
(
Fetching
xpcshell
test
data
for
the
last
{
numDays
}
day
{
numDays
>
1
?
"
s
"
:
"
"
}
:
{
dates
.
join
(
"
"
)
}
)
;
for
(
const
date
of
dates
)
{
console
.
log
(
\
n
=
=
=
Processing
{
date
}
=
=
=
)
;
await
processDateData
(
date
forceRefetch
)
;
}
const
indexFile
=
path
.
join
(
OUTPUT_DIR
"
index
.
json
"
)
;
const
availableDates
=
[
]
;
const
files
=
fs
.
readdirSync
(
OUTPUT_DIR
)
;
files
.
forEach
(
file
=
>
{
const
match
=
file
.
match
(
/
^
xpcshell
-
(
\
d
{
4
}
-
\
d
{
2
}
-
\
d
{
2
}
)
\
.
json
/
)
;
if
(
match
)
{
availableDates
.
push
(
match
[
1
]
)
;
}
}
)
;
availableDates
.
sort
(
(
a
b
)
=
>
b
.
localeCompare
(
a
)
)
;
fs
.
writeFileSync
(
indexFile
JSON
.
stringify
(
{
dates
:
availableDates
}
null
2
)
)
;
console
.
log
(
\
nIndex
file
saved
as
{
indexFile
}
with
{
availableDates
.
length
}
dates
)
;
}
main
(
)
.
catch
(
console
.
error
)
;
