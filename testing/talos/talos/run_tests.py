from
__future__
import
absolute_import
print_function
import
copy
import
os
import
sys
import
time
import
traceback
import
urllib
import
mozhttpd
import
mozversion
import
utils
from
mozlog
import
get_proxy_logger
from
talos
.
config
import
get_configs
ConfigurationError
from
talos
.
mitmproxy
import
mitmproxy
from
talos
.
results
import
TalosResults
from
talos
.
ttest
import
TTest
from
talos
.
utils
import
TalosError
TalosRegression
here
=
os
.
path
.
dirname
(
os
.
path
.
realpath
(
__file__
)
)
LOG
=
get_proxy_logger
(
)
def
useBaseTestDefaults
(
base
tests
)
:
    
for
test
in
tests
:
        
for
item
in
base
:
            
if
item
not
in
test
:
                
test
[
item
]
=
base
[
item
]
                
if
test
[
item
]
is
None
:
                    
test
[
item
]
=
'
'
    
return
tests
def
set_tp_preferences
(
test
browser_config
)
:
    
if
test
[
'
tpcycles
'
]
not
in
range
(
1
1000
)
:
        
raise
TalosError
(
'
pageloader
cycles
must
be
int
1
to
1
000
'
)
    
if
'
tpmanifest
'
not
in
test
:
        
raise
TalosError
(
"
tpmanifest
not
found
in
test
:
%
s
"
%
test
)
    
if
test
[
'
gecko_profile
'
]
:
        
LOG
.
info
(
"
Gecko
profiling
is
enabled
so
talos
is
reducing
the
number
"
                 
"
of
cycles
please
disregard
reported
numbers
"
)
        
for
cycle_var
in
[
'
tppagecycles
'
'
tpcycles
'
'
cycles
'
]
:
            
if
test
[
cycle_var
]
>
2
:
                
test
[
cycle_var
]
=
2
    
CLI_bool_options
=
[
'
tpchrome
'
'
tpmozafterpaint
'
'
tploadnocache
'
'
tpscrolltest
'
'
fnbpaint
'
]
    
CLI_options
=
[
'
tpcycles
'
'
tppagecycles
'
'
tptimeout
'
'
tpmanifest
'
]
    
for
key
in
CLI_bool_options
:
        
if
key
in
test
:
            
_pref_name
=
"
talos
.
%
s
"
%
key
            
test
[
'
preferences
'
]
[
_pref_name
]
=
test
.
get
(
key
)
    
for
key
in
CLI_options
:
        
value
=
test
.
get
(
key
)
        
if
value
:
            
_pref_name
=
"
talos
.
%
s
"
%
key
            
test
[
'
preferences
'
]
[
_pref_name
]
=
value
def
setup_webserver
(
webserver
)
:
    
"
"
"
use
mozhttpd
to
setup
a
webserver
"
"
"
    
LOG
.
info
(
"
starting
webserver
on
%
r
"
%
webserver
)
    
host
port
=
webserver
.
split
(
'
:
'
)
    
return
mozhttpd
.
MozHttpd
(
host
=
host
port
=
int
(
port
)
docroot
=
here
)
def
run_tests
(
config
browser_config
)
:
    
"
"
"
Runs
the
talos
tests
on
the
given
configuration
and
generates
a
report
.
    
"
"
"
    
tests
=
config
[
'
tests
'
]
    
tests
=
useBaseTestDefaults
(
config
.
get
(
'
basetest
'
{
}
)
tests
)
    
paths
=
[
'
profile_path
'
'
tpmanifest
'
'
extensions
'
'
setup
'
'
cleanup
'
]
    
for
test
in
tests
:
        
for
path
in
paths
:
            
if
test
.
get
(
path
)
:
                
test
[
path
]
=
utils
.
interpolate
(
test
[
path
]
)
        
if
test
.
get
(
'
tpmanifest
'
)
:
            
test
[
'
tpmanifest
'
]
=
\
                
os
.
path
.
normpath
(
'
file
:
/
%
s
'
%
(
urllib
.
quote
(
test
[
'
tpmanifest
'
]
                                               
'
/
\
\
t
:
\
\
'
)
)
)
            
test
[
'
preferences
'
]
[
'
talos
.
tpmanifest
'
]
=
test
[
'
tpmanifest
'
]
        
if
test
.
get
(
'
fnbpaint
'
False
)
:
            
LOG
.
info
(
"
Test
is
using
firstNonBlankPaint
browser
pref
will
be
turned
on
"
)
            
test
[
'
preferences
'
]
[
'
dom
.
performance
.
time_to_non_blank_paint
.
enabled
'
]
=
True
        
test
[
'
setup
'
]
=
utils
.
interpolate
(
test
[
'
setup
'
]
)
        
test
[
'
cleanup
'
]
=
utils
.
interpolate
(
test
[
'
cleanup
'
]
)
        
if
not
test
.
get
(
'
profile
'
False
)
:
            
test
[
'
profile
'
]
=
config
.
get
(
'
profile
'
)
    
if
browser_config
[
'
develop
'
]
:
        
browser_config
[
'
extra_args
'
]
=
'
-
-
no
-
remote
'
    
if
browser_config
[
'
subtests
'
]
:
        
browser_config
[
'
preferences
'
]
[
'
talos
.
subtests
'
]
=
browser_config
[
'
subtests
'
]
    
if
config
.
get
(
'
code_coverage
'
False
)
:
        
if
browser_config
[
'
develop
'
]
:
            
raise
TalosError
(
'
Aborting
:
talos
-
-
code
-
coverage
flag
is
only
'
                             
'
supported
in
production
'
)
        
else
:
            
browser_config
[
'
code_coverage
'
]
=
True
    
testdate
=
config
.
get
(
'
testdate
'
'
'
)
    
if
not
browser_config
[
'
process
'
]
:
        
browser_config
[
'
process
'
]
=
\
            
os
.
path
.
basename
(
browser_config
[
'
browser_path
'
]
)
    
browser_config
[
'
extensions
'
]
=
[
utils
.
interpolate
(
i
)
                                    
for
i
in
browser_config
[
'
extensions
'
]
]
    
browser_config
[
'
bcontroller_config
'
]
=
\
        
utils
.
interpolate
(
browser_config
[
'
bcontroller_config
'
]
)
    
browser_config
[
'
browser_path
'
]
=
\
        
os
.
path
.
normpath
(
browser_config
[
'
browser_path
'
]
)
    
binary
=
browser_config
[
"
browser_path
"
]
    
version_info
=
mozversion
.
get_version
(
binary
=
binary
)
    
browser_config
[
'
browser_name
'
]
=
version_info
[
'
application_name
'
]
    
browser_config
[
'
browser_version
'
]
=
version_info
[
'
application_version
'
]
    
browser_config
[
'
buildid
'
]
=
version_info
[
'
application_buildid
'
]
    
try
:
        
browser_config
[
'
repository
'
]
=
version_info
[
'
application_repository
'
]
        
browser_config
[
'
sourcestamp
'
]
=
version_info
[
'
application_changeset
'
]
    
except
KeyError
:
        
if
not
browser_config
[
'
develop
'
]
:
            
print
(
"
Abort
:
unable
to
find
changeset
or
repository
:
%
s
"
%
version_info
)
            
sys
.
exit
(
1
)
        
else
:
            
browser_config
[
'
repository
'
]
=
'
develop
'
            
browser_config
[
'
sourcestamp
'
]
=
'
develop
'
    
if
testdate
:
        
date
=
int
(
time
.
mktime
(
time
.
strptime
(
testdate
                                             
'
%
a
%
d
%
b
%
Y
%
H
:
%
M
:
%
S
GMT
'
)
)
)
    
else
:
        
date
=
int
(
time
.
time
(
)
)
    
LOG
.
debug
(
"
using
testdate
:
%
d
"
%
date
)
    
LOG
.
debug
(
"
actual
date
:
%
d
"
%
int
(
time
.
time
(
)
)
)
    
talos_results
=
TalosResults
(
)
    
if
not
browser_config
[
'
develop
'
]
and
not
config
[
'
gecko_profile
'
]
:
        
results_urls
=
dict
(
            
output_urls
=
[
'
local
.
json
'
]
        
)
    
else
:
        
results_urls
=
dict
(
output_urls
=
[
os
.
path
.
abspath
(
'
local
.
json
'
)
]
)
    
httpd
=
setup_webserver
(
browser_config
[
'
webserver
'
]
)
    
httpd
.
start
(
)
    
talos_results
.
add_extra_option
(
'
e10s
'
)
    
if
config
[
'
enable_stylo
'
]
:
        
talos_results
.
add_extra_option
(
'
stylo
'
)
    
if
config
[
'
disable_stylo
'
]
:
        
talos_results
.
add_extra_option
(
'
stylo_disabled
'
)
    
if
config
.
get
(
'
stylothreads
'
0
)
>
0
:
        
talos_results
.
add_extra_option
(
'
%
s_thread
'
%
config
[
'
stylothreads
'
]
)
    
if
config
[
'
gecko_profile
'
]
:
        
talos_results
.
add_extra_option
(
'
geckoProfile
'
)
    
mitmproxy_recordings_list
=
config
.
get
(
'
mitmproxy
'
False
)
    
if
mitmproxy_recordings_list
is
not
False
:
        
browser_config
[
'
mitmproxy
'
]
=
True
        
mitmdump_path
=
config
.
get
(
'
mitmdumpPath
'
False
)
        
if
mitmdump_path
is
False
:
            
raise
TalosError
(
'
Aborting
:
mitmdumpPath
not
provided
on
cmd
line
but
is
required
'
)
        
mitmproxy_recording_path
=
os
.
path
.
join
(
here
'
mitmproxy
'
)
        
mitmproxy_proc
=
mitmproxy
.
start_mitmproxy_playback
(
mitmdump_path
                                                            
mitmproxy_recording_path
                                                            
mitmproxy_recordings_list
.
split
(
)
                                                            
browser_config
[
'
browser_path
'
]
)
        
scripts_path
=
os
.
environ
.
get
(
'
SCRIPTSPATH
'
)
        
LOG
.
info
(
'
scripts_path
:
%
s
'
%
str
(
scripts_path
)
)
        
mitmproxy
.
install_mitmproxy_cert
(
mitmproxy_proc
                                         
browser_config
[
'
browser_path
'
]
                                         
str
(
scripts_path
)
)
    
testname
=
None
    
timer
=
utils
.
Timer
(
)
    
LOG
.
suite_start
(
tests
=
[
test
[
'
name
'
]
for
test
in
tests
]
)
    
try
:
        
for
test
in
tests
:
            
testname
=
test
[
'
name
'
]
            
LOG
.
test_start
(
testname
)
            
if
not
test
.
get
(
'
url
'
)
:
                
test
[
'
url
'
]
=
None
                
set_tp_preferences
(
test
browser_config
)
            
mytest
=
TTest
(
)
            
if
test
.
get
(
'
firstpaint
'
False
)
or
test
.
get
(
'
userready
'
None
)
:
                
multi_value_result
=
None
                
separate_results_list
=
[
]
                
test_event_map
=
test
.
get
(
'
testeventmap
'
None
)
                
if
test_event_map
is
None
:
                    
raise
TalosError
(
"
Need
'
testeventmap
'
in
test
.
py
for
%
s
"
%
test
.
get
(
'
name
'
)
)
                
multi_value_result
=
mytest
.
runTest
(
browser_config
test
)
                
if
multi_value_result
is
None
:
                    
raise
TalosError
(
"
Abort
:
no
results
returned
for
%
s
"
%
test
.
get
(
'
name
'
)
)
                
separate_results_list
=
convert_to_separate_test_results
(
multi_value_result
                                                                         
test_event_map
)
                
for
test_result
in
separate_results_list
:
                    
talos_results
.
add
(
test_result
)
            
elif
test
.
get
(
'
base_vs_ref
'
False
)
:
                
base_and_reference_results
=
mytest
.
runTest
(
browser_config
test
)
                
talos_results
.
add
(
make_comparison_result
(
base_and_reference_results
)
)
            
else
:
                
talos_results
.
add
(
mytest
.
runTest
(
browser_config
test
)
)
            
LOG
.
test_end
(
testname
status
=
'
OK
'
)
    
except
TalosRegression
as
exc
:
        
LOG
.
error
(
"
Detected
a
regression
for
%
s
"
%
testname
)
        
LOG
.
test_end
(
testname
status
=
'
FAIL
'
message
=
str
(
exc
)
                     
stack
=
traceback
.
format_exc
(
)
)
        
return
1
    
except
Exception
as
exc
:
        
LOG
.
test_end
(
testname
status
=
'
ERROR
'
message
=
str
(
exc
)
                     
stack
=
traceback
.
format_exc
(
)
)
        
return
2
    
finally
:
        
LOG
.
suite_end
(
)
        
httpd
.
stop
(
)
    
LOG
.
info
(
"
Completed
test
suite
(
%
s
)
"
%
timer
.
elapsed
(
)
)
    
if
mitmproxy_recordings_list
is
not
False
:
        
mitmproxy
.
stop_mitmproxy_playback
(
mitmproxy_proc
)
    
if
results_urls
and
not
browser_config
[
'
no_upload_results
'
]
:
        
talos_results
.
output
(
results_urls
)
        
if
browser_config
[
'
develop
'
]
or
config
[
'
gecko_profile
'
]
:
            
print
(
"
Thanks
for
running
Talos
locally
.
Results
are
in
%
s
"
                  
%
(
results_urls
[
'
output_urls
'
]
)
)
    
return
0
def
make_comparison_result
(
base_and_reference_results
)
:
    
'
'
'
Receive
a
test
result
object
meant
to
be
used
as
a
base
vs
reference
test
.
The
result
    
object
will
have
one
test
with
two
subtests
;
instead
of
traditional
subtests
we
want
to
    
treat
them
as
separate
tests
comparing
them
together
and
reporting
the
comparison
results
.
    
Results
with
multiple
pages
used
as
subtests
would
look
like
this
normally
with
the
overall
    
result
value
being
the
mean
of
the
pages
/
subtests
:
    
PERFHERDER_DATA
:
{
"
framework
"
:
{
"
name
"
:
"
talos
"
}
"
suites
"
:
[
{
"
extraOptions
"
:
[
"
e10s
"
]
    
"
name
"
:
"
bloom_basic
"
"
lowerIsBetter
"
:
true
"
alertThreshold
"
:
5
.
0
"
value
"
:
594
.
81
    
"
subtests
"
:
[
{
"
name
"
:
"
.
html
"
"
lowerIsBetter
"
:
true
"
alertThreshold
"
:
5
.
0
"
replicates
"
:
    
[
586
.
52
.
.
.
]
"
value
"
:
586
.
52
]
"
unit
"
:
"
ms
"
}
{
"
name
"
:
"
-
ref
.
html
"
"
lowerIsBetter
"
:
true
    
"
alertThreshold
"
:
5
.
0
"
replicates
"
:
[
603
.
225
.
.
.
]
"
value
"
:
603
.
225
"
unit
"
:
"
ms
"
}
]
}
]
}
    
We
want
to
compare
the
subtests
against
eachother
(
base
vs
ref
)
and
create
a
new
single
test
    
results
object
with
the
comparison
results
that
will
look
like
traditional
single
test
results
    
like
this
:
    
PERFHERDER_DATA
:
{
"
framework
"
:
{
"
name
"
:
"
talos
"
}
"
suites
"
:
[
{
"
lowerIsBetter
"
:
true
    
"
subtests
"
:
[
{
"
name
"
:
"
"
"
lowerIsBetter
"
:
true
"
alertThreshold
"
:
5
.
0
"
replicates
"
:
    
[
16
.
705
.
.
.
]
"
value
"
:
16
.
705
"
unit
"
:
"
ms
"
}
]
"
extraOptions
"
:
[
"
e10s
"
]
"
name
"
:
    
"
bloom_basic
"
"
alertThreshold
"
:
5
.
0
}
]
}
    
'
'
'
    
comparison_result
=
copy
.
deepcopy
(
base_and_reference_results
)
    
comparison_result
.
results
[
0
]
.
results
=
[
]
    
comp_results
=
comparison_result
.
results
[
0
]
.
results
    
subtest_index
=
0
    
for
x
in
range
(
0
len
(
base_and_reference_results
.
results
[
0
]
.
results
)
2
)
:
        
results
=
base_and_reference_results
.
results
[
0
]
.
results
        
base_result_runs
=
results
[
x
]
[
'
runs
'
]
        
ref_result_runs
=
results
[
x
+
1
]
[
'
runs
'
]
        
sub_test_name
=
base_and_reference_results
.
results
[
0
]
.
results
[
x
]
[
'
page
'
]
        
comp_results
.
append
(
{
'
index
'
:
0
                             
'
runs
'
:
[
]
                             
'
page
'
:
sub_test_name
                             
'
base_runs
'
:
base_result_runs
                             
'
ref_runs
'
:
ref_result_runs
}
)
        
_index
=
0
        
for
next_ref
in
comp_results
[
subtest_index
]
[
'
ref_runs
'
]
:
            
diff
=
abs
(
next_ref
-
comp_results
[
subtest_index
]
[
'
base_runs
'
]
[
_index
]
)
            
comp_results
[
subtest_index
]
[
'
runs
'
]
.
append
(
round
(
diff
3
)
)
            
_index
+
=
1
        
subtest_index
+
=
1
    
return
comparison_result
def
convert_to_separate_test_results
(
multi_value_result
test_event_map
)
:
    
'
'
'
Receive
a
test
result
that
actually
contains
multiple
values
in
a
single
iteration
and
    
parse
it
out
in
order
to
'
fake
'
three
seprate
test
results
.
    
Incoming
result
looks
like
this
:
    
[
{
'
index
'
:
0
'
runs
'
:
{
'
event_1
'
:
[
1338
.
.
.
]
'
event_2
'
:
[
1438
.
.
.
]
'
event_3
'
:
    
[
1538
.
.
.
]
}
'
page
'
:
'
NULL
'
}
]
    
We
want
to
parse
it
out
such
that
we
have
'
faked
'
three
separate
tests
setting
test
names
    
and
taking
the
run
values
for
each
.
End
goal
is
to
have
results
reported
as
three
separate
    
tests
like
this
:
    
PERFHERDER_DATA
:
{
"
framework
"
:
{
"
name
"
:
"
talos
"
}
"
suites
"
:
[
{
"
subtests
"
:
[
{
"
replicates
"
:
    
[
1338
.
.
.
]
"
name
"
:
"
ts_paint
"
"
value
"
:
1338
}
]
"
extraOptions
"
:
[
"
e10s
"
]
"
name
"
:
    
"
ts_paint
"
}
{
"
subtests
"
:
[
{
"
replicates
"
:
[
1438
.
.
.
]
"
name
"
:
"
ts_first_paint
"
"
value
"
:
    
1438
}
]
"
extraOptions
"
:
[
"
e10s
"
]
"
name
"
:
"
ts_first_paint
"
}
{
"
subtests
"
:
[
{
"
replicates
"
:
    
[
1538
.
.
.
]
"
name
"
:
"
ts_user_ready
"
"
value
"
:
1538
}
]
"
extraOptions
"
:
[
"
e10s
"
]
"
name
"
:
    
"
ts_user_ready
"
}
]
}
    
'
'
'
    
list_of_separate_tests
=
[
]
    
for
next_test
in
test_event_map
:
        
separate_test
=
copy
.
deepcopy
(
multi_value_result
)
        
separate_test
.
test_config
[
'
name
'
]
=
next_test
[
'
name
'
]
        
for
x
in
separate_test
.
results
:
            
for
item
in
x
.
results
:
                
all_runs
=
item
[
'
runs
'
]
                
item
[
'
runs
'
]
=
all_runs
[
next_test
[
'
label
'
]
]
        
list_of_separate_tests
.
append
(
separate_test
)
    
return
list_of_separate_tests
def
main
(
args
=
sys
.
argv
[
1
:
]
)
:
    
try
:
        
config
browser_config
=
get_configs
(
)
    
except
ConfigurationError
as
exc
:
        
sys
.
exit
(
"
ERROR
:
%
s
"
%
exc
)
    
sys
.
exit
(
run_tests
(
config
browser_config
)
)
if
__name__
=
=
'
__main__
'
:
    
main
(
)
