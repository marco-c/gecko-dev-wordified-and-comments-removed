"
"
"
output
raptor
test
results
"
"
"
from
__future__
import
absolute_import
print_function
import
filters
import
json
import
os
from
abc
import
ABCMeta
abstractmethod
from
logger
.
logger
import
RaptorLogger
LOG
=
RaptorLogger
(
component
=
'
perftest
-
output
'
)
class
PerftestOutput
(
object
)
:
    
"
"
"
Abstract
base
class
to
handle
output
of
perftest
results
"
"
"
    
__metaclass__
=
ABCMeta
    
def
__init__
(
self
results
supporting_data
subtest_alert_on
)
:
        
"
"
"
        
-
results
:
list
of
RaptorTestResult
instances
        
"
"
"
        
self
.
results
=
results
        
self
.
summarized_results
=
{
}
        
self
.
supporting_data
=
supporting_data
        
self
.
summarized_supporting_data
=
[
]
        
self
.
summarized_screenshots
=
[
]
        
self
.
subtest_alert_on
=
subtest_alert_on
    
abstractmethod
    
def
summarize
(
self
test_names
)
:
        
raise
NotImplementedError
(
)
    
def
summarize_supporting_data
(
self
)
:
        
'
'
'
        
Supporting
data
was
gathered
outside
of
the
main
raptor
test
;
it
will
be
kept
        
separate
from
the
main
raptor
test
results
.
Summarize
it
appropriately
.
        
supporting_data
=
{
'
type
'
:
'
data
-
type
'
                           
'
test
'
:
'
raptor
-
test
-
ran
-
when
-
data
-
was
-
gathered
'
                           
'
unit
'
:
'
unit
that
the
values
are
in
'
                           
'
values
'
:
{
                               
'
name
'
:
value
                               
'
nameN
'
:
valueN
}
}
        
More
specifically
power
data
will
look
like
this
:
        
supporting_data
=
{
'
type
'
:
'
power
'
                           
'
test
'
:
'
raptor
-
speedometer
-
geckoview
'
                           
'
unit
'
:
'
mAh
'
                           
'
values
'
:
{
                               
'
cpu
'
:
cpu
                               
'
wifi
'
:
wifi
                               
'
screen
'
:
screen
                               
'
proportional
'
:
proportional
}
}
        
We
want
to
treat
each
value
as
a
'
subtest
'
;
and
for
the
overall
aggregated
        
test
result
we
will
add
all
of
these
subtest
values
together
.
        
'
'
'
        
if
self
.
supporting_data
is
None
:
            
return
        
self
.
summarized_supporting_data
=
[
]
        
support_data_by_type
=
{
}
        
for
data_set
in
self
.
supporting_data
:
            
data_type
=
data_set
[
'
type
'
]
            
LOG
.
info
(
"
summarizing
%
s
data
"
%
data_type
)
            
if
data_type
not
in
support_data_by_type
:
                
support_data_by_type
[
data_type
]
=
{
                    
'
framework
'
:
{
                        
'
name
'
:
'
raptor
'
                    
}
                    
'
suites
'
:
[
]
                
}
            
vals
=
[
]
            
subtests
=
[
]
            
suite
=
{
                
'
name
'
:
data_set
[
'
test
'
]
+
"
-
"
+
data_set
[
'
type
'
]
                
'
type
'
:
data_set
[
'
type
'
]
                
'
subtests
'
:
subtests
                
'
lowerIsBetter
'
:
True
                
'
unit
'
:
data_set
[
'
unit
'
]
                
'
alertThreshold
'
:
2
.
0
            
}
            
support_data_by_type
[
data_type
]
[
'
suites
'
]
.
append
(
suite
)
            
for
measurement_name
value
in
data_set
[
'
values
'
]
.
iteritems
(
)
:
                
new_subtest
=
{
}
                
new_subtest
[
'
name
'
]
=
data_type
+
"
-
"
+
measurement_name
                
new_subtest
[
'
value
'
]
=
value
                
new_subtest
[
'
lowerIsBetter
'
]
=
True
                
new_subtest
[
'
alertThreshold
'
]
=
2
.
0
                
new_subtest
[
'
unit
'
]
=
data_set
[
'
unit
'
]
                
subtests
.
append
(
new_subtest
)
                
vals
.
append
(
[
new_subtest
[
'
value
'
]
new_subtest
[
'
name
'
]
]
)
            
if
len
(
subtests
)
>
=
1
:
                
suite
[
'
value
'
]
=
self
.
construct_summary
(
                    
vals
                    
testname
=
"
supporting_data
"
                    
unit
=
data_set
[
'
unit
'
]
                
)
        
for
data_type
in
support_data_by_type
:
            
self
.
summarized_supporting_data
.
append
(
support_data_by_type
[
data_type
]
)
        
return
    
def
output
(
self
test_names
)
:
        
"
"
"
output
to
file
and
perfherder
data
json
"
"
"
        
if
os
.
getenv
(
'
MOZ_UPLOAD_DIR
'
)
:
            
results_path
=
os
.
path
.
join
(
os
.
path
.
dirname
(
os
.
environ
[
'
MOZ_UPLOAD_DIR
'
]
)
                                        
'
raptor
.
json
'
)
            
screenshot_path
=
os
.
path
.
join
(
os
.
path
.
dirname
(
os
.
environ
[
'
MOZ_UPLOAD_DIR
'
]
)
                                           
'
screenshots
.
html
'
)
        
else
:
            
results_path
=
os
.
path
.
join
(
os
.
getcwd
(
)
'
raptor
.
json
'
)
            
screenshot_path
=
os
.
path
.
join
(
os
.
getcwd
(
)
'
screenshots
.
html
'
)
        
success
=
True
        
if
self
.
summarized_results
=
=
{
}
:
            
success
=
False
            
LOG
.
error
(
"
no
summarized
raptor
results
found
for
%
s
"
%
                      
'
'
.
join
(
test_names
)
)
        
else
:
            
for
suite
in
self
.
summarized_results
[
'
suites
'
]
:
                
tname
=
suite
[
'
name
'
]
                
found
=
False
                
for
test
in
test_names
:
                    
if
tname
in
test
:
                        
found
=
True
                        
break
                
if
not
found
:
                    
success
=
False
                    
LOG
.
error
(
"
no
summarized
raptor
results
found
for
%
s
"
%
tname
)
            
with
open
(
results_path
'
w
'
)
as
f
:
                
for
result
in
self
.
summarized_results
:
                    
f
.
write
(
"
%
s
\
n
"
%
result
)
        
if
len
(
self
.
summarized_screenshots
)
>
0
:
            
with
open
(
screenshot_path
'
w
'
)
as
f
:
                
for
result
in
self
.
summarized_screenshots
:
                    
f
.
write
(
"
%
s
\
n
"
%
result
)
            
LOG
.
info
(
"
screen
captures
can
be
found
locally
at
:
%
s
"
%
screenshot_path
)
        
if
self
.
summarized_results
=
=
{
}
:
            
return
success
0
        
extra_opts
=
self
.
summarized_results
[
'
suites
'
]
[
0
]
.
get
(
'
extraOptions
'
[
]
)
        
test_type
=
self
.
summarized_results
[
'
suites
'
]
[
0
]
.
get
(
'
type
'
'
'
)
        
output_perf_data
=
True
        
not_posting
=
'
-
not
posting
regular
test
results
for
perfherder
'
        
if
'
gecko_profile
'
in
extra_opts
:
            
LOG
.
info
(
"
gecko
profiling
enabled
%
s
"
%
not_posting
)
            
output_perf_data
=
False
        
elif
test_type
=
=
'
scenario
'
:
            
LOG
.
info
(
"
scenario
test
type
was
run
%
s
"
%
not_posting
)
            
output_perf_data
=
False
        
total_perfdata
=
0
        
if
output_perf_data
:
            
if
len
(
self
.
summarized_supporting_data
)
=
=
0
:
                
LOG
.
info
(
"
PERFHERDER_DATA
:
%
s
"
%
json
.
dumps
(
self
.
summarized_results
)
)
                
total_perfdata
=
1
            
else
:
                
LOG
.
info
(
"
supporting
data
measurements
exist
-
only
posting
those
to
perfherder
"
)
        
json
.
dump
(
self
.
summarized_results
open
(
results_path
'
w
'
)
indent
=
2
                  
sort_keys
=
True
)
        
LOG
.
info
(
"
results
can
also
be
found
locally
at
:
%
s
"
%
results_path
)
        
return
success
total_perfdata
    
def
output_supporting_data
(
self
test_names
)
:
        
'
'
'
        
Supporting
data
was
gathered
outside
of
the
main
raptor
test
;
it
has
already
        
been
summarized
now
output
it
appropriately
.
        
We
want
to
output
supporting
data
in
a
completely
separate
perfherder
json
blob
and
        
in
a
corresponding
file
artifact
.
This
way
supporting
data
can
be
ingested
as
its
own
        
test
suite
in
perfherder
and
alerted
upon
if
desired
;
kept
outside
of
the
test
results
        
from
the
actual
Raptor
test
which
was
run
when
the
supporting
data
was
gathered
.
        
'
'
'
        
if
len
(
self
.
summarized_supporting_data
)
=
=
0
:
            
LOG
.
error
(
"
no
summarized
supporting
data
found
for
%
s
"
%
                      
'
'
.
join
(
test_names
)
)
            
return
False
0
        
total_perfdata
=
0
        
for
next_data_set
in
self
.
summarized_supporting_data
:
            
data_type
=
next_data_set
[
'
suites
'
]
[
0
]
[
'
type
'
]
            
if
os
.
environ
[
'
MOZ_UPLOAD_DIR
'
]
:
                
results_path
=
os
.
path
.
join
(
os
.
path
.
dirname
(
os
.
environ
[
'
MOZ_UPLOAD_DIR
'
]
)
                                            
'
raptor
-
%
s
.
json
'
%
data_type
)
            
else
:
                
results_path
=
os
.
path
.
join
(
os
.
getcwd
(
)
'
raptor
-
%
s
.
json
'
%
data_type
)
            
json
.
dump
(
next_data_set
open
(
results_path
'
w
'
)
indent
=
2
sort_keys
=
True
)
            
LOG
.
info
(
"
PERFHERDER_DATA
:
%
s
"
%
json
.
dumps
(
next_data_set
)
)
            
LOG
.
info
(
"
%
s
results
can
also
be
found
locally
at
:
%
s
"
%
(
data_type
results_path
)
)
            
total_perfdata
+
=
1
        
return
True
total_perfdata
    
def
construct_summary
(
self
vals
testname
unit
=
None
)
:
        
def
_filter
(
vals
value
=
None
)
:
            
if
value
is
None
:
                
return
[
i
for
i
j
in
vals
]
            
return
[
i
for
i
j
in
vals
if
j
=
=
value
]
        
if
testname
.
startswith
(
'
raptor
-
v8_7
'
)
:
            
return
100
*
filters
.
geometric_mean
(
_filter
(
vals
)
)
        
if
testname
.
startswith
(
'
raptor
-
speedometer
'
)
:
            
correctionFactor
=
3
            
results
=
_filter
(
vals
)
            
if
len
(
results
)
!
=
160
:
                
raise
Exception
(
"
Speedometer
has
160
subtests
found
:
%
s
instead
"
%
len
(
results
)
)
            
results
=
results
[
9
:
:
10
]
            
score
=
60
*
1000
/
filters
.
geometric_mean
(
results
)
/
correctionFactor
            
return
score
        
if
testname
.
startswith
(
'
raptor
-
stylebench
'
)
:
            
correctionFactor
=
3
            
results
=
_filter
(
vals
)
            
if
len
(
results
)
!
=
380
:
                
raise
Exception
(
"
StyleBench
requires
380
entries
found
:
%
s
instead
"
                                
%
len
(
results
)
)
            
results
=
results
[
75
:
:
76
]
            
return
60
*
1000
/
filters
.
geometric_mean
(
results
)
/
correctionFactor
        
if
testname
.
startswith
(
(
'
raptor
-
kraken
'
'
raptor
-
sunspider
'
)
)
:
            
return
sum
(
_filter
(
vals
)
)
        
if
testname
.
startswith
(
(
'
raptor
-
unity
-
webgl
'
'
raptor
-
webaudio
'
)
)
:
            
return
filters
.
mean
(
_filter
(
vals
'
Geometric
Mean
'
)
)
        
if
testname
.
startswith
(
'
raptor
-
assorted
-
dom
'
)
:
            
return
round
(
filters
.
geometric_mean
(
_filter
(
vals
)
)
2
)
        
if
testname
.
startswith
(
'
raptor
-
wasm
-
misc
'
)
:
            
return
filters
.
mean
(
_filter
(
vals
'
__total__
'
)
)
        
if
testname
.
startswith
(
'
raptor
-
wasm
-
godot
'
)
:
            
return
filters
.
mean
(
_filter
(
vals
'
first
-
interactive
'
)
)
        
if
testname
.
startswith
(
'
raptor
-
youtube
-
playback
'
)
:
            
return
round
(
filters
.
mean
(
_filter
(
vals
)
)
2
)
        
if
testname
.
startswith
(
'
supporting_data
'
)
:
            
if
unit
:
                
if
unit
in
(
'
%
'
)
:
                    
return
filters
.
mean
(
_filter
(
vals
)
)
                
elif
unit
in
(
'
W
'
'
MHz
'
)
:
                    
allavgs
=
[
]
                    
for
(
val
subtest
)
in
vals
:
                        
if
'
avg
'
in
subtest
:
                            
allavgs
.
append
(
val
)
                    
if
allavgs
:
                        
return
sum
(
allavgs
)
            
return
sum
(
_filter
(
vals
)
)
        
if
len
(
vals
)
>
1
:
            
return
round
(
filters
.
geometric_mean
(
_filter
(
vals
)
)
2
)
        
return
round
(
filters
.
mean
(
_filter
(
vals
)
)
2
)
class
RaptorOutput
(
PerftestOutput
)
:
    
"
"
"
class
for
raptor
output
"
"
"
    
def
summarize
(
self
test_names
)
:
        
suites
=
[
]
        
test_results
=
{
            
'
framework
'
:
{
                
'
name
'
:
'
raptor
'
            
}
            
'
suites
'
:
suites
        
}
        
if
len
(
self
.
results
)
=
=
0
:
            
LOG
.
error
(
"
no
raptor
test
results
found
for
%
s
"
%
                      
'
'
.
join
(
test_names
)
)
            
return
        
for
test
in
self
.
results
:
            
vals
=
[
]
            
subtests
=
[
]
            
suite
=
{
                
'
name
'
:
test
.
name
                
'
type
'
:
test
.
type
                
'
extraOptions
'
:
test
.
extra_options
                
'
subtests
'
:
subtests
                
'
lowerIsBetter
'
:
test
.
lower_is_better
                
'
unit
'
:
test
.
unit
                
'
alertThreshold
'
:
float
(
test
.
alert_threshold
)
            
}
            
if
hasattr
(
test
"
alert_change_type
"
)
:
                
suite
[
'
alertChangeType
'
]
=
test
.
alert_change_type
            
if
test
.
cold
is
True
:
                
suite
[
'
cold
'
]
=
True
                
suite
[
'
browser_cycle
'
]
=
int
(
test
.
browser_cycle
)
                
suite
[
'
expected_browser_cycles
'
]
=
int
(
test
.
expected_browser_cycles
)
            
suites
.
append
(
suite
)
            
if
test
.
type
in
(
"
pageload
"
"
scenario
"
)
:
                
for
measurement_name
replicates
in
test
.
measurements
.
iteritems
(
)
:
                    
new_subtest
=
{
}
                    
new_subtest
[
'
name
'
]
=
measurement_name
                    
new_subtest
[
'
replicates
'
]
=
replicates
                    
new_subtest
[
'
lowerIsBetter
'
]
=
test
.
subtest_lower_is_better
                    
new_subtest
[
'
alertThreshold
'
]
=
float
(
test
.
alert_threshold
)
                    
new_subtest
[
'
value
'
]
=
0
                    
new_subtest
[
'
unit
'
]
=
test
.
subtest_unit
                    
if
test
.
cold
is
False
:
                        
LOG
.
info
(
"
ignoring
the
first
%
s
value
due
to
initial
pageload
noise
"
                                 
%
measurement_name
)
                        
filtered_values
=
filters
.
ignore_first
(
new_subtest
[
'
replicates
'
]
1
)
                    
else
:
                        
filtered_values
=
new_subtest
[
'
replicates
'
]
                    
if
measurement_name
=
=
"
ttfi
"
:
                        
filtered_values
=
filters
.
ignore_negative
(
filtered_values
)
                        
if
len
(
filtered_values
)
<
1
:
                            
continue
                    
if
self
.
subtest_alert_on
is
not
None
:
                        
if
measurement_name
in
self
.
subtest_alert_on
:
                            
LOG
.
info
(
"
turning
on
subtest
alerting
for
measurement
type
:
%
s
"
                                     
%
measurement_name
)
                            
new_subtest
[
'
shouldAlert
'
]
=
True
                    
new_subtest
[
'
value
'
]
=
filters
.
median
(
filtered_values
)
                    
vals
.
append
(
[
new_subtest
[
'
value
'
]
new_subtest
[
'
name
'
]
]
)
                    
subtests
.
append
(
new_subtest
)
            
elif
test
.
type
=
=
"
benchmark
"
:
                
if
'
assorted
-
dom
'
in
test
.
measurements
:
                    
subtests
vals
=
self
.
parseAssortedDomOutput
(
test
)
                
elif
'
ares6
'
in
test
.
measurements
:
                    
subtests
vals
=
self
.
parseAresSixOutput
(
test
)
                
elif
'
jetstream2
'
in
test
.
measurements
:
                    
subtests
vals
=
self
.
parseJetstreamTwoOutput
(
test
)
                
elif
'
motionmark
'
in
test
.
measurements
:
                    
subtests
vals
=
self
.
parseMotionmarkOutput
(
test
)
                
elif
'
speedometer
'
in
test
.
measurements
:
                    
subtests
vals
=
self
.
parseSpeedometerOutput
(
test
)
                
elif
'
sunspider
'
in
test
.
measurements
:
                    
subtests
vals
=
self
.
parseSunspiderOutput
(
test
)
                
elif
'
unity
-
webgl
'
in
test
.
measurements
:
                    
subtests
vals
=
self
.
parseUnityWebGLOutput
(
test
)
                
elif
'
wasm
-
godot
'
in
test
.
measurements
:
                    
subtests
vals
=
self
.
parseWASMGodotOutput
(
test
)
                
elif
'
wasm
-
misc
'
in
test
.
measurements
:
                    
subtests
vals
=
self
.
parseWASMMiscOutput
(
test
)
                
elif
'
webaudio
'
in
test
.
measurements
:
                    
subtests
vals
=
self
.
parseWebaudioOutput
(
test
)
                
elif
'
youtube
-
playbackperf
-
test
'
in
test
.
measurements
:
                    
subtests
vals
=
self
.
parseYoutubePlaybackPerformanceOutput
(
test
)
                
suite
[
'
subtests
'
]
=
subtests
            
else
:
                
LOG
.
error
(
"
output
.
summarize
received
unsupported
test
results
type
for
%
s
"
%
                          
test
.
name
)
                
return
            
if
len
(
subtests
)
>
1
:
                
suite
[
'
value
'
]
=
self
.
construct_summary
(
vals
testname
=
test
.
name
)
            
subtests
.
sort
(
key
=
lambda
subtest
:
subtest
[
'
name
'
]
)
        
suites
.
sort
(
key
=
lambda
suite
:
suite
[
'
name
'
]
)
        
self
.
summarized_results
=
test_results
    
def
combine_browser_cycles
(
self
)
:
        
'
'
'
        
At
this
point
the
results
have
been
summarized
;
however
there
may
have
been
multiple
        
browser
cycles
(
i
.
e
.
cold
load
)
.
In
which
case
the
results
have
one
entry
for
each
        
test
for
each
browser
cycle
.
For
each
test
we
need
to
combine
the
results
for
all
        
browser
cycles
into
one
results
entry
.
        
For
example
this
is
what
the
summarized
results
suites
list
looks
like
from
a
test
that
        
was
run
with
multiple
(
two
)
browser
cycles
:
        
[
{
'
expected_browser_cycles
'
:
2
'
extraOptions
'
:
[
]
            
'
name
'
:
u
'
raptor
-
tp6m
-
amazon
-
geckoview
-
cold
'
'
lowerIsBetter
'
:
True
            
'
alertThreshold
'
:
2
.
0
'
value
'
:
1776
.
94
'
browser_cycle
'
:
1
            
'
subtests
'
:
[
{
'
name
'
:
u
'
dcf
'
'
lowerIsBetter
'
:
True
'
alertThreshold
'
:
2
.
0
                
'
value
'
:
818
'
replicates
'
:
[
818
]
'
unit
'
:
u
'
ms
'
}
{
'
name
'
:
u
'
fcp
'
                
'
lowerIsBetter
'
:
True
'
alertThreshold
'
:
2
.
0
'
value
'
:
1131
'
shouldAlert
'
:
True
                
'
replicates
'
:
[
1131
]
'
unit
'
:
u
'
ms
'
}
{
'
name
'
:
u
'
fnbpaint
'
'
lowerIsBetter
'
:
True
                
'
alertThreshold
'
:
2
.
0
'
value
'
:
1056
'
replicates
'
:
[
1056
]
'
unit
'
:
u
'
ms
'
}
                
{
'
name
'
:
u
'
ttfi
'
'
lowerIsBetter
'
:
True
'
alertThreshold
'
:
2
.
0
'
value
'
:
18074
                
'
replicates
'
:
[
18074
]
'
unit
'
:
u
'
ms
'
}
{
'
name
'
:
u
'
loadtime
'
'
lowerIsBetter
'
:
True
                
'
alertThreshold
'
:
2
.
0
'
value
'
:
1002
'
shouldAlert
'
:
True
'
replicates
'
:
[
1002
]
                
'
unit
'
:
u
'
ms
'
}
]
            
'
cold
'
:
True
'
type
'
:
u
'
pageload
'
'
unit
'
:
u
'
ms
'
}
        
{
'
expected_browser_cycles
'
:
2
'
extraOptions
'
:
[
]
            
'
name
'
:
u
'
raptor
-
tp6m
-
amazon
-
geckoview
-
cold
'
'
lowerIsBetter
'
:
True
            
'
alertThreshold
'
:
2
.
0
'
value
'
:
840
.
25
'
browser_cycle
'
:
2
            
'
subtests
'
:
[
{
'
name
'
:
u
'
dcf
'
'
lowerIsBetter
'
:
True
'
alertThreshold
'
:
2
.
0
                
'
value
'
:
462
'
replicates
'
:
[
462
]
'
unit
'
:
u
'
ms
'
}
{
'
name
'
:
u
'
fcp
'
                
'
lowerIsBetter
'
:
True
'
alertThreshold
'
:
2
.
0
'
value
'
:
718
'
shouldAlert
'
:
True
                
'
replicates
'
:
[
718
]
'
unit
'
:
u
'
ms
'
}
{
'
name
'
:
u
'
fnbpaint
'
'
lowerIsBetter
'
:
True
                
'
alertThreshold
'
:
2
.
0
'
value
'
:
676
'
replicates
'
:
[
676
]
'
unit
'
:
u
'
ms
'
}
                
{
'
name
'
:
u
'
ttfi
'
'
lowerIsBetter
'
:
True
'
alertThreshold
'
:
2
.
0
'
value
'
:
3084
                
'
replicates
'
:
[
3084
]
'
unit
'
:
u
'
ms
'
}
{
'
name
'
:
u
'
loadtime
'
'
lowerIsBetter
'
:
True
                
'
alertThreshold
'
:
2
.
0
'
value
'
:
605
'
shouldAlert
'
:
True
'
replicates
'
:
[
605
]
                
'
unit
'
:
u
'
ms
'
}
]
            
'
cold
'
:
True
'
type
'
:
u
'
pageload
'
'
unit
'
:
u
'
ms
'
}
]
        
Need
to
combine
those
into
a
single
entry
.
        
'
'
'
        
if
len
(
self
.
results
)
=
=
0
:
            
LOG
.
info
(
"
error
:
no
raptor
test
results
found
so
no
need
to
combine
browser
cycles
"
)
            
return
        
suites_to_be_combined
=
[
]
        
combined_suites
=
[
]
        
for
_index
suite
in
enumerate
(
self
.
summarized_results
.
get
(
'
suites
'
[
]
)
)
:
            
if
suite
.
get
(
'
cold
'
)
is
None
:
                
continue
            
if
suite
[
'
expected_browser_cycles
'
]
>
1
:
                
_name
=
suite
[
'
name
'
]
                
_details
=
suite
.
copy
(
)
                
suites_to_be_combined
.
append
(
{
'
name
'
:
_name
'
details
'
:
_details
}
)
                
suite
[
'
to_be_deleted
'
]
=
True
        
combined_suites
=
{
}
        
for
next_suite
in
suites_to_be_combined
:
            
suite_name
=
next_suite
[
'
details
'
]
[
'
name
'
]
            
browser_cycle
=
next_suite
[
'
details
'
]
[
'
browser_cycle
'
]
            
LOG
.
info
(
"
combining
results
from
browser
cycle
%
d
for
%
s
"
                     
%
(
browser_cycle
suite_name
)
)
            
if
suite_name
not
in
combined_suites
:
                
combined_suites
[
suite_name
]
=
next_suite
[
'
details
'
]
                
LOG
.
info
(
"
created
new
combined
result
with
intial
cycle
replicates
"
)
                
del
(
combined_suites
[
suite_name
]
[
'
cold
'
]
)
                
del
(
combined_suites
[
suite_name
]
[
'
browser_cycle
'
]
)
                
del
(
combined_suites
[
suite_name
]
[
'
expected_browser_cycles
'
]
)
            
else
:
                
for
next_subtest
in
next_suite
[
'
details
'
]
[
'
subtests
'
]
:
                    
found_subtest
=
False
                    
for
combined_subtest
in
combined_suites
[
suite_name
]
[
'
subtests
'
]
:
                        
if
combined_subtest
[
'
name
'
]
=
=
next_subtest
[
'
name
'
]
:
                            
LOG
.
info
(
"
adding
replicates
for
%
s
"
%
next_subtest
[
'
name
'
]
)
                            
combined_subtest
[
'
replicates
'
]
.
extend
(
next_subtest
[
'
replicates
'
]
)
                            
found_subtest
=
True
                    
if
not
found_subtest
:
                        
LOG
.
info
(
"
adding
replicates
for
%
s
"
%
next_subtest
[
'
name
'
]
)
                        
combined_suites
[
next_suite
[
'
details
'
]
[
'
name
'
]
]
[
'
subtests
'
]
\
                            
.
append
(
next_subtest
)
        
for
i
name
in
enumerate
(
combined_suites
)
:
            
vals
=
[
]
            
for
next_sub
in
combined_suites
[
name
]
[
'
subtests
'
]
:
                
next_sub
[
'
value
'
]
=
filters
.
median
(
next_sub
[
'
replicates
'
]
)
                
vals
.
append
(
[
next_sub
[
'
value
'
]
next_sub
[
'
name
'
]
]
)
            
if
len
(
combined_suites
[
name
]
[
'
subtests
'
]
)
>
1
:
                
combined_suites
[
name
]
[
'
value
'
]
=
self
.
construct_summary
(
vals
testname
=
name
)
            
self
.
summarized_results
[
'
suites
'
]
.
append
(
combined_suites
[
name
]
)
        
self
.
summarized_results
[
'
suites
'
]
=
[
item
for
item
in
self
.
summarized_results
[
'
suites
'
]
                                             
if
item
.
get
(
'
to_be_deleted
'
)
is
not
True
]
    
def
parseSpeedometerOutput
(
self
test
)
:
        
_subtests
=
{
}
        
data
=
test
.
measurements
[
'
speedometer
'
]
        
for
page_cycle
in
data
:
            
for
sub
replicates
in
page_cycle
[
0
]
.
iteritems
(
)
:
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
'
unit
'
:
test
.
subtest_unit
                                      
'
alertThreshold
'
:
float
(
test
.
alert_threshold
)
                                      
'
lowerIsBetter
'
:
test
.
subtest_lower_is_better
                                      
'
name
'
:
sub
                                      
'
replicates
'
:
[
]
}
                
_subtests
[
sub
]
[
'
replicates
'
]
.
extend
(
[
round
(
x
3
)
for
x
in
replicates
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
'
value
'
]
=
filters
.
median
(
_subtests
[
name
]
[
'
replicates
'
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
'
value
'
]
name
]
)
        
return
subtests
vals
    
def
parseAresSixOutput
(
self
test
)
:
        
_subtests
=
{
}
        
data
=
test
.
measurements
[
'
ares6
'
]
        
for
page_cycle
in
data
:
            
for
sub
replicates
in
page_cycle
[
0
]
.
iteritems
(
)
:
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
'
unit
'
:
test
.
subtest_unit
                                      
'
alertThreshold
'
:
float
(
test
.
alert_threshold
)
                                      
'
lowerIsBetter
'
:
test
.
subtest_lower_is_better
                                      
'
name
'
:
sub
                                      
'
replicates
'
:
[
]
}
                
_subtests
[
sub
]
[
'
replicates
'
]
.
extend
(
[
round
(
x
3
)
for
x
in
replicates
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
'
value
'
]
=
filters
.
mean
(
_subtests
[
name
]
[
'
replicates
'
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
'
value
'
]
name
]
)
        
return
subtests
vals
    
def
parseJetstreamTwoOutput
(
self
test
)
:
        
_subtests
=
{
}
        
data
=
test
.
measurements
[
'
jetstream2
'
]
        
for
page_cycle
in
data
:
            
for
sub
replicates
in
page_cycle
[
0
]
.
iteritems
(
)
:
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
'
unit
'
:
test
.
subtest_unit
                                      
'
alertThreshold
'
:
float
(
test
.
alert_threshold
)
                                      
'
lowerIsBetter
'
:
test
.
subtest_lower_is_better
                                      
'
name
'
:
sub
                                      
'
replicates
'
:
[
]
}
                
_subtests
[
sub
]
[
'
replicates
'
]
.
extend
(
[
round
(
x
3
)
for
x
in
replicates
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
'
value
'
]
=
filters
.
mean
(
_subtests
[
name
]
[
'
replicates
'
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
'
value
'
]
name
]
)
        
return
subtests
vals
    
def
parseWASMMiscOutput
(
self
test
)
:
        
'
'
'
          
{
u
'
wasm
-
misc
'
:
[
            
[
[
{
u
'
name
'
:
u
'
validate
'
u
'
time
'
:
163
.
44000000000005
}
              
.
.
.
              
{
u
'
name
'
:
u
'
__total__
'
u
'
time
'
:
63308
.
434904788155
}
]
]
            
.
.
.
            
[
[
{
u
'
name
'
:
u
'
validate
'
u
'
time
'
:
129
.
42000000000002
}
              
{
u
'
name
'
:
u
'
__total__
'
u
'
time
'
:
63181
.
24089257814
}
]
]
           
]
}
        
'
'
'
        
_subtests
=
{
}
        
data
=
test
.
measurements
[
'
wasm
-
misc
'
]
        
for
page_cycle
in
data
:
            
for
item
in
page_cycle
[
0
]
:
                
sub
=
item
[
'
name
'
]
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
'
unit
'
:
test
.
subtest_unit
                                      
'
alertThreshold
'
:
float
(
test
.
alert_threshold
)
                                      
'
lowerIsBetter
'
:
test
.
subtest_lower_is_better
                                      
'
name
'
:
sub
                                      
'
replicates
'
:
[
]
}
                
_subtests
[
sub
]
[
'
replicates
'
]
.
append
(
item
[
'
time
'
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
'
value
'
]
=
filters
.
median
(
_subtests
[
name
]
[
'
replicates
'
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
'
value
'
]
name
]
)
        
return
subtests
vals
    
def
parseWASMGodotOutput
(
self
test
)
:
        
'
'
'
            
{
u
'
wasm
-
godot
'
:
[
                
{
                  
"
name
"
:
"
wasm
-
instantiate
"
                  
"
time
"
:
349
                
}
{
                  
"
name
"
:
"
engine
-
instantiate
"
                  
"
time
"
:
1263
                
.
.
.
                
}
]
}
        
'
'
'
        
_subtests
=
{
}
        
data
=
test
.
measurements
[
'
wasm
-
godot
'
]
        
print
(
data
)
        
for
page_cycle
in
data
:
            
for
item
in
page_cycle
[
0
]
:
                
sub
=
item
[
'
name
'
]
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
'
unit
'
:
test
.
subtest_unit
                                      
'
alertThreshold
'
:
float
(
test
.
alert_threshold
)
                                      
'
lowerIsBetter
'
:
test
.
subtest_lower_is_better
                                      
'
name
'
:
sub
                                      
'
replicates
'
:
[
]
}
                
_subtests
[
sub
]
[
'
replicates
'
]
.
append
(
item
[
'
time
'
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
'
value
'
]
=
filters
.
median
(
_subtests
[
name
]
[
'
replicates
'
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
'
value
'
]
name
]
)
        
return
subtests
vals
    
def
parseWebaudioOutput
(
self
test
)
:
        
_subtests
=
{
}
        
data
=
test
.
measurements
[
'
webaudio
'
]
        
for
page_cycle
in
data
:
            
data
=
json
.
loads
(
page_cycle
[
0
]
)
            
for
item
in
data
:
                
sub
=
item
[
'
name
'
]
                
replicates
=
[
item
[
'
duration
'
]
]
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
'
unit
'
:
test
.
subtest_unit
                                      
'
alertThreshold
'
:
float
(
test
.
alert_threshold
)
                                      
'
lowerIsBetter
'
:
test
.
subtest_lower_is_better
                                      
'
name
'
:
sub
                                      
'
replicates
'
:
[
]
}
                
_subtests
[
sub
]
[
'
replicates
'
]
.
extend
(
[
round
(
x
3
)
for
x
in
replicates
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
'
value
'
]
=
filters
.
median
(
_subtests
[
name
]
[
'
replicates
'
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
'
value
'
]
name
]
)
        
print
(
subtests
)
        
return
subtests
vals
    
def
parseMotionmarkOutput
(
self
test
)
:
        
_subtests
=
{
}
        
data
=
test
.
measurements
[
'
motionmark
'
]
        
for
page_cycle
in
data
:
            
page_cycle_results
=
page_cycle
[
0
]
            
suite
=
page_cycle_results
.
keys
(
)
[
0
]
            
for
sub
in
page_cycle_results
[
suite
]
.
keys
(
)
:
                
try
:
                    
replicate
=
round
(
                        
float
(
page_cycle_results
[
suite
]
[
sub
]
[
'
frameLength
'
]
[
'
average
'
]
)
3
                    
)
                
except
TypeError
as
e
:
                    
LOG
.
warning
(
"
[
{
}
]
[
{
}
]
:
{
}
-
{
}
"
.
format
(
suite
sub
e
.
__class__
.
__name__
e
)
)
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
'
unit
'
:
test
.
subtest_unit
                                      
'
alertThreshold
'
:
float
(
test
.
alert_threshold
)
                                      
'
lowerIsBetter
'
:
test
.
subtest_lower_is_better
                                      
'
name
'
:
sub
                                      
'
replicates
'
:
[
]
}
                
_subtests
[
sub
]
[
'
replicates
'
]
.
extend
(
[
replicate
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
'
value
'
]
=
filters
.
median
(
_subtests
[
name
]
[
'
replicates
'
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
'
value
'
]
name
]
)
        
return
subtests
vals
    
def
parseSunspiderOutput
(
self
test
)
:
        
_subtests
=
{
}
        
data
=
test
.
measurements
[
'
sunspider
'
]
        
for
page_cycle
in
data
:
            
for
sub
replicates
in
page_cycle
[
0
]
.
iteritems
(
)
:
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
'
unit
'
:
test
.
subtest_unit
                                      
'
alertThreshold
'
:
float
(
test
.
alert_threshold
)
                                      
'
lowerIsBetter
'
:
test
.
subtest_lower_is_better
                                      
'
name
'
:
sub
                                      
'
replicates
'
:
[
]
}
                
_subtests
[
sub
]
[
'
replicates
'
]
.
extend
(
[
round
(
x
3
)
for
x
in
replicates
]
)
        
subtests
=
[
]
        
vals
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
'
value
'
]
=
filters
.
mean
(
_subtests
[
name
]
[
'
replicates
'
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
'
value
'
]
name
]
)
        
return
subtests
vals
    
def
parseUnityWebGLOutput
(
self
test
)
:
        
"
"
"
        
Example
output
(
this
is
one
page
cycle
)
:
        
{
'
name
'
:
'
raptor
-
unity
-
webgl
-
firefox
'
         
'
type
'
:
'
benchmark
'
         
'
measurements
'
:
{
            
'
unity
-
webgl
'
:
[
                
[
                    
'
[
{
"
benchmark
"
:
"
Mandelbrot
GPU
"
"
result
"
:
1035361
}
.
.
.
}
]
'
                
]
            
]
         
}
         
'
lower_is_better
'
:
False
         
'
unit
'
:
'
score
'
        
}
        
"
"
"
        
_subtests
=
{
}
        
data
=
test
.
measurements
[
'
unity
-
webgl
'
]
        
for
page_cycle
in
data
:
            
data
=
json
.
loads
(
page_cycle
[
0
]
)
            
for
item
in
data
:
                
sub
=
item
[
'
benchmark
'
]
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
'
unit
'
:
test
.
subtest_unit
                                      
'
alertThreshold
'
:
float
(
test
.
alert_threshold
)
                                      
'
lowerIsBetter
'
:
test
.
subtest_lower_is_better
                                      
'
name
'
:
sub
                                      
'
replicates
'
:
[
]
}
                
_subtests
[
sub
]
[
'
replicates
'
]
.
append
(
item
[
'
result
'
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
'
value
'
]
=
filters
.
median
(
_subtests
[
name
]
[
'
replicates
'
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
'
value
'
]
name
]
)
        
return
subtests
vals
    
def
parseAssortedDomOutput
(
self
test
)
:
        
_subtests
=
{
}
        
data
=
test
.
measurements
[
'
assorted
-
dom
'
]
        
for
pagecycle
in
data
:
            
for
_sub
_value
in
pagecycle
[
0
]
.
iteritems
(
)
:
                
if
_sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
_sub
]
=
{
'
unit
'
:
test
.
subtest_unit
                                       
'
alertThreshold
'
:
float
(
test
.
alert_threshold
)
                                       
'
lowerIsBetter
'
:
test
.
subtest_lower_is_better
                                       
'
name
'
:
_sub
                                       
'
replicates
'
:
[
]
}
                
_subtests
[
_sub
]
[
'
replicates
'
]
.
extend
(
[
_value
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
'
value
'
]
=
round
(
filters
.
median
(
_subtests
[
name
]
[
'
replicates
'
]
)
2
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
if
name
=
=
'
total
'
:
                
vals
.
append
(
[
_subtests
[
name
]
[
'
value
'
]
name
]
)
        
return
subtests
vals
    
def
parseYoutubePlaybackPerformanceOutput
(
self
test
)
:
        
"
"
"
Parse
the
metrics
for
the
Youtube
playback
performance
test
.
        
For
each
video
measured
values
for
dropped
and
decoded
frames
will
be
        
available
from
the
benchmark
site
.
        
{
u
'
PlaybackPerf
.
VP9
.
2160p60
2X
'
:
{
u
'
droppedFrames
'
:
1
u
'
decodedFrames
'
:
796
}
        
With
each
page
cycle
/
iteration
of
the
test
multiple
values
can
be
present
.
        
Raptor
will
calculate
the
percentage
of
dropped
frames
to
decoded
frames
.
        
All
those
three
values
will
then
be
emitted
as
separate
sub
tests
.
        
"
"
"
        
_subtests
=
{
}
        
data
=
test
.
measurements
[
'
youtube
-
playbackperf
-
test
'
]
        
def
create_subtest_entry
(
name
value
                                 
unit
=
test
.
subtest_unit
                                 
lower_is_better
=
test
.
subtest_lower_is_better
)
:
            
if
name
not
in
_subtests
.
keys
(
)
:
                
_subtests
[
name
]
=
{
                    
'
name
'
:
name
                    
'
unit
'
:
unit
                    
'
lowerIsBetter
'
:
lower_is_better
                    
'
replicates
'
:
[
]
                
}
            
_subtests
[
name
]
[
'
replicates
'
]
.
append
(
value
)
            
if
self
.
subtest_alert_on
is
not
None
:
                
if
name
in
self
.
subtest_alert_on
:
                    
LOG
.
info
(
"
turning
on
subtest
alerting
for
measurement
type
:
%
s
"
                             
%
name
)
                    
_subtests
[
name
]
[
'
shouldAlert
'
]
=
True
        
for
pagecycle
in
data
:
            
for
_sub
_value
in
pagecycle
[
0
]
.
iteritems
(
)
:
                
try
:
                    
percent_dropped
=
(
float
(
_value
[
'
droppedFrames
'
]
)
/
                                       
_value
[
'
decodedFrames
'
]
*
100
.
0
)
                
except
ZeroDivisionError
:
                    
percent_dropped
=
100
.
0
                
_sub
=
_sub
.
split
(
'
PlaybackPerf
.
'
1
)
[
-
1
]
                
create_subtest_entry
(
"
{
}
_decoded_frames
"
.
format
(
_sub
)
                                     
_value
[
'
decodedFrames
'
]
                                     
lower_is_better
=
False
                                     
)
                
create_subtest_entry
(
"
{
}
_dropped_frames
"
.
format
(
_sub
)
                                     
_value
[
'
droppedFrames
'
]
                                     
)
                
create_subtest_entry
(
"
{
}
_
%
_dropped_frames
"
.
format
(
_sub
)
                                     
percent_dropped
                                     
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
'
value
'
]
=
round
(
filters
.
median
(
_subtests
[
name
]
[
'
replicates
'
]
)
2
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
if
name
.
endswith
(
"
X_dropped_frames
"
)
:
                
vals
.
append
(
[
_subtests
[
name
]
[
'
value
'
]
name
]
)
        
return
subtests
vals
    
def
summarize_screenshots
(
self
screenshots
)
:
        
if
len
(
screenshots
)
=
=
0
:
            
return
        
self
.
summarized_screenshots
.
append
(
"
"
"
<
!
DOCTYPE
html
>
        
<
head
>
        
<
style
>
            
table
th
td
{
              
border
:
1px
solid
black
;
              
border
-
collapse
:
collapse
;
            
}
        
<
/
style
>
        
<
/
head
>
        
<
html
>
<
body
>
        
<
h1
>
Captured
screenshots
!
<
/
h1
>
        
<
table
style
=
"
width
:
100
%
"
>
          
<
tr
>
            
<
th
>
Test
Name
<
/
th
>
            
<
th
>
Pagecycle
<
/
th
>
            
<
th
>
Screenshot
<
/
th
>
          
<
/
tr
>
"
"
"
)
        
for
screenshot
in
screenshots
:
            
self
.
summarized_screenshots
.
append
(
"
"
"
<
tr
>
            
<
th
>
%
s
<
/
th
>
            
<
th
>
%
s
<
/
th
>
            
<
th
>
                
<
img
src
=
"
%
s
"
alt
=
"
%
s
%
s
"
width
=
"
320
"
height
=
"
240
"
>
            
<
/
th
>
            
<
/
tr
>
"
"
"
%
(
screenshot
[
'
test_name
'
]
                        
screenshot
[
'
page_cycle
'
]
                        
screenshot
[
'
screenshot
'
]
                        
screenshot
[
'
test_name
'
]
                        
screenshot
[
'
page_cycle
'
]
)
)
        
self
.
summarized_screenshots
.
append
(
"
"
"
<
/
table
>
<
/
body
>
<
/
html
>
"
"
"
)
class
BrowsertimeOutput
(
PerftestOutput
)
:
    
"
"
"
class
for
browsertime
output
"
"
"
    
def
parseSpeedometerOutput
(
self
test
)
:
        
_subtests
=
{
}
        
data
=
test
[
'
measurements
'
]
[
'
speedometer
'
]
        
for
page_cycle
in
data
:
            
for
sub
replicates
in
page_cycle
.
iteritems
(
)
:
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
'
unit
'
:
test
[
'
subtest_unit
'
]
                                      
'
alertThreshold
'
:
float
(
test
[
'
alert_threshold
'
]
)
                                      
'
lowerIsBetter
'
:
test
[
'
subtest_lower_is_better
'
]
                                      
'
name
'
:
sub
                                      
'
replicates
'
:
[
]
}
                
_subtests
[
sub
]
[
'
replicates
'
]
.
extend
(
[
round
(
x
3
)
for
x
in
replicates
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
'
value
'
]
=
filters
.
median
(
_subtests
[
name
]
[
'
replicates
'
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
'
value
'
]
name
]
)
        
return
subtests
vals
    
def
summarize
(
self
test_names
)
:
        
"
"
"
        
Summarize
the
parsed
browsertime
test
output
and
format
accordingly
so
the
output
can
        
be
ingested
by
Perfherder
.
        
At
this
point
each
entry
in
self
.
results
for
browsertime
-
pageload
tests
is
in
this
format
:
        
{
'
statistics
'
:
{
'
fcp
'
:
{
u
'
p99
'
:
932
u
'
mdev
'
:
10
.
0941
u
'
min
'
:
712
u
'
p90
'
:
810
u
'
max
'
:
        
932
u
'
median
'
:
758
u
'
p10
'
:
728
u
'
stddev
'
:
50
u
'
mean
'
:
769
}
'
dcf
'
:
{
u
'
p99
'
:
864
        
u
'
mdev
'
:
11
.
6768
u
'
min
'
:
614
u
'
p90
'
:
738
u
'
max
'
:
864
u
'
median
'
:
670
u
'
p10
'
:
632
        
u
'
stddev
'
:
58
u
'
mean
'
:
684
}
'
fnbpaint
'
:
{
u
'
p99
'
:
830
u
'
mdev
'
:
9
.
6851
u
'
min
'
:
616
        
u
'
p90
'
:
719
u
'
max
'
:
830
u
'
median
'
:
668
u
'
p10
'
:
642
u
'
stddev
'
:
48
u
'
mean
'
:
680
}
        
'
loadtime
'
:
{
u
'
p99
'
:
5818
u
'
mdev
'
:
111
.
7028
u
'
min
'
:
3220
u
'
p90
'
:
4450
u
'
max
'
:
5818
        
u
'
median
'
:
3476
u
'
p10
'
:
3241
u
'
stddev
'
:
559
u
'
mean
'
:
3642
}
}
'
name
'
:
        
'
raptor
-
tp6
-
guardian
-
firefox
'
'
url
'
:
'
https
:
/
/
www
.
theguardian
.
co
.
uk
'
'
lower_is_better
'
:
        
True
'
measurements
'
:
{
'
fcp
'
:
[
932
744
744
810
712
775
759
744
777
739
809
906
        
734
742
760
758
728
792
757
759
742
759
775
726
730
]
'
dcf
'
:
[
864
679
637
        
662
652
651
710
679
646
689
686
845
670
694
632
703
670
738
633
703
614
        
703
650
622
670
]
'
fnbpaint
'
:
[
830
648
666
704
616
683
678
650
685
651
719
        
820
634
664
681
664
642
703
668
670
669
668
681
652
642
]
'
loadtime
'
:
[
4450
        
3592
3770
3345
3453
3220
3434
3621
3511
3416
3430
5818
4729
3406
3506
3588
        
3245
3381
3707
3241
3595
3483
3236
3390
3476
]
}
'
subtest_unit
'
:
'
ms
'
'
bt_ver
'
:
        
'
4
.
9
.
2
-
android
'
'
alert_threshold
'
:
2
'
cold
'
:
True
'
type
'
:
'
browsertime
-
pageload
'
        
'
unit
'
:
'
ms
'
'
browser
'
:
"
{
u
'
userAgent
'
:
u
'
Mozilla
/
5
.
0
(
Macintosh
;
Intel
Mac
OS
X
10
.
13
;
        
rv
:
70
.
0
)
Gecko
/
20100101
Firefox
/
70
.
0
'
u
'
windowSize
'
:
u
'
1366x694
'
}
"
}
        
Now
we
must
process
this
further
and
prepare
the
result
for
output
suitable
for
perfherder
        
ingestion
.
        
Note
:
For
the
overall
subtest
values
/
results
(
i
.
e
.
for
each
measurement
type
)
we
will
use
        
the
Browsertime
-
provided
statistics
instead
of
calcuating
our
own
geomeans
from
the
        
replicates
.
        
"
"
"
        
def
_process
(
subtest
)
:
            
subtest
[
'
value
'
]
=
filters
.
median
(
filters
.
ignore_first
(
subtest
[
'
replicates
'
]
1
)
)
            
return
subtest
        
def
_process_suite
(
suite
)
:
            
suite
[
'
subtests
'
]
=
[
                
_process
(
subtest
)
for
subtest
in
suite
[
'
subtests
'
]
.
values
(
)
                
if
subtest
[
'
replicates
'
]
            
]
            
suite
[
'
subtests
'
]
.
sort
(
key
=
lambda
subtest
:
subtest
[
'
name
'
]
)
            
if
len
(
suite
[
'
subtests
'
]
)
>
1
:
                
vals
=
[
[
subtest
[
'
value
'
]
subtest
[
'
name
'
]
]
for
subtest
in
suite
[
'
subtests
'
]
]
                
suite
[
'
value
'
]
=
self
.
construct_summary
(
vals
                                                        
testname
=
test
[
'
name
'
]
)
            
return
suite
        
LOG
.
info
(
"
preparing
browsertime
results
for
output
"
)
        
if
len
(
self
.
results
)
=
=
0
:
            
LOG
.
error
(
"
no
browsertime
test
results
found
for
%
s
"
%
                      
'
'
.
join
(
test_names
)
)
            
return
        
test_results
=
{
            
'
framework
'
:
{
                
'
name
'
:
'
browsertime
'
            
}
        
}
        
suites
=
{
}
        
for
test
in
self
.
results
:
            
if
test
[
'
name
'
]
not
in
suites
:
                
suite
=
{
                    
'
name
'
:
test
[
'
name
'
]
                    
'
type
'
:
test
[
'
type
'
]
                    
'
extraOptions
'
:
test
[
'
extra_options
'
]
                    
'
lowerIsBetter
'
:
test
[
'
lower_is_better
'
]
                    
'
unit
'
:
test
[
'
unit
'
]
                    
'
alertThreshold
'
:
float
(
test
[
'
alert_threshold
'
]
)
                    
'
subtests
'
:
{
}
                
}
                
if
'
alert_change_type
'
in
test
:
                    
suite
[
'
alertChangeType
'
]
=
test
[
'
alert_change_type
'
]
                
suites
[
test
[
'
name
'
]
]
=
suite
            
else
:
                
suite
=
suites
[
test
[
'
name
'
]
]
            
if
(
"
pageload
"
or
"
scenario
"
)
in
test
[
'
type
'
]
:
                
for
measurement_name
replicates
in
test
[
'
measurements
'
]
.
iteritems
(
)
:
                    
if
measurement_name
not
in
suite
[
'
subtests
'
]
:
                        
subtest
=
{
}
                        
subtest
[
'
name
'
]
=
measurement_name
                        
subtest
[
'
lowerIsBetter
'
]
=
test
[
'
subtest_lower_is_better
'
]
                        
subtest
[
'
alertThreshold
'
]
=
float
(
test
[
'
alert_threshold
'
]
)
                        
subtest
[
'
unit
'
]
=
test
[
'
subtest_unit
'
]
                        
if
self
.
subtest_alert_on
is
not
None
:
                            
if
measurement_name
in
self
.
subtest_alert_on
:
                                
LOG
.
info
(
"
turning
on
subtest
alerting
for
measurement
type
:
%
s
"
                                         
%
measurement_name
)
                                
subtest
[
'
shouldAlert
'
]
=
True
                        
subtest
[
'
replicates
'
]
=
[
]
                        
suite
[
'
subtests
'
]
[
measurement_name
]
=
subtest
                    
else
:
                        
subtest
=
suite
[
'
subtests
'
]
[
measurement_name
]
                    
subtest
[
'
replicates
'
]
.
extend
(
replicates
)
            
elif
"
benchmark
"
in
test
[
'
type
'
]
:
                
if
'
speedometer
'
in
test
[
'
name
'
]
:
                    
subtests
vals
=
self
.
parseSpeedometerOutput
(
test
)
                
suite
[
'
subtests
'
]
=
subtests
                
if
len
(
subtests
)
>
1
:
                    
suite
[
'
value
'
]
=
self
.
construct_summary
(
vals
testname
=
test
[
'
name
'
]
)
                
subtests
.
sort
(
key
=
lambda
subtest
:
subtest
[
'
name
'
]
)
        
suites
=
[
s
if
"
benchmark
"
in
test
[
'
type
'
]
else
_process_suite
(
s
)
                  
for
s
in
suites
.
values
(
)
]
        
suites
.
sort
(
key
=
lambda
suite
:
suite
[
'
name
'
]
)
        
test_results
[
'
suites
'
]
=
suites
        
self
.
summarized_results
=
test_results
