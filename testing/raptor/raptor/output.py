"
"
"
output
raptor
test
results
"
"
"
from
__future__
import
absolute_import
print_function
import
filters
import
json
import
os
from
abc
import
ABCMeta
abstractmethod
from
logger
.
logger
import
RaptorLogger
LOG
=
RaptorLogger
(
component
=
"
perftest
-
output
"
)
class
PerftestOutput
(
object
)
:
    
"
"
"
Abstract
base
class
to
handle
output
of
perftest
results
"
"
"
    
__metaclass__
=
ABCMeta
    
def
__init__
(
self
results
supporting_data
subtest_alert_on
)
:
        
"
"
"
        
-
results
:
list
of
RaptorTestResult
instances
        
"
"
"
        
self
.
results
=
results
        
self
.
summarized_results
=
{
}
        
self
.
supporting_data
=
supporting_data
        
self
.
summarized_supporting_data
=
[
]
        
self
.
summarized_screenshots
=
[
]
        
self
.
subtest_alert_on
=
subtest_alert_on
        
self
.
mozproxy_data
=
False
        
self
.
browser_name
=
None
        
self
.
browser_version
=
None
    
abstractmethod
    
def
summarize
(
self
test_names
)
:
        
raise
NotImplementedError
(
)
    
def
set_browser_meta
(
self
browser_name
browser_version
)
:
        
self
.
browser_name
=
browser_name
        
self
.
browser_version
=
browser_version
    
def
summarize_supporting_data
(
self
)
:
        
"
"
"
        
Supporting
data
was
gathered
outside
of
the
main
raptor
test
;
it
will
be
kept
        
separate
from
the
main
raptor
test
results
.
Summarize
it
appropriately
.
        
supporting_data
=
{
            
'
type
'
:
'
data
-
type
'
            
'
test
'
:
'
raptor
-
test
-
ran
-
when
-
data
-
was
-
gathered
'
            
'
unit
'
:
'
unit
that
the
values
are
in
'
            
'
summarize
-
values
'
:
True
/
False
            
'
suite
-
suffix
-
type
'
:
True
/
False
            
'
values
'
:
{
                
'
name
'
:
value_dict
                
'
nameN
'
:
value_dictN
            
}
        
}
        
More
specifically
subtest
supporting
data
will
look
like
this
:
        
supporting_data
=
{
            
'
type
'
:
'
power
'
            
'
test
'
:
'
raptor
-
speedometer
-
geckoview
'
            
'
unit
'
:
'
mAh
'
            
'
values
'
:
{
                
'
cpu
'
:
{
                    
'
values
'
:
val
                    
'
lowerIsBetter
'
:
True
/
False
                    
'
alertThreshold
'
:
2
.
0
                    
'
subtest
-
prefix
-
type
'
:
True
/
False
                    
'
unit
'
:
'
mWh
'
                
}
                
'
wifi
'
:
.
.
.
            
}
        
}
        
We
want
to
treat
each
value
as
a
'
subtest
'
;
and
for
the
overall
aggregated
        
test
result
the
summary
value
is
dependent
on
the
unit
.
An
exception
is
        
raised
in
case
we
don
'
t
know
about
the
specified
unit
.
        
"
"
"
        
if
self
.
supporting_data
is
None
:
            
return
        
self
.
summarized_supporting_data
=
[
]
        
support_data_by_type
=
{
}
        
for
data_set
in
self
.
supporting_data
:
            
data_type
=
data_set
[
"
type
"
]
            
LOG
.
info
(
"
summarizing
%
s
data
"
%
data_type
)
            
if
"
mozproxy
"
in
data_type
:
                
self
.
mozproxy_data
=
True
                
LOG
.
info
(
"
data
:
{
}
"
.
format
(
self
.
supporting_data
)
)
            
if
data_type
not
in
support_data_by_type
:
                
support_data_by_type
[
data_type
]
=
{
                    
"
framework
"
:
{
"
name
"
:
"
raptor
"
}
                    
"
suites
"
:
[
]
                
}
            
vals
=
[
]
            
subtests
=
[
]
            
suite_name
=
data_set
[
"
test
"
]
            
if
data_set
.
get
(
"
suite
-
suffix
-
type
"
True
)
:
                
suite_name
=
"
%
s
-
%
s
"
%
(
data_set
[
"
test
"
]
data_set
[
"
type
"
]
)
            
suite
=
{
                
"
name
"
:
suite_name
                
"
type
"
:
data_set
[
"
type
"
]
                
"
subtests
"
:
subtests
            
}
            
if
data_set
.
get
(
"
summarize
-
values
"
True
)
:
                
suite
.
update
(
{
                    
"
lowerIsBetter
"
:
True
                    
"
unit
"
:
data_set
[
"
unit
"
]
                    
"
alertThreshold
"
:
2
.
0
                
}
)
            
for
result
in
self
.
results
:
                
if
result
[
"
name
"
]
=
=
data_set
[
"
test
"
]
:
                    
suite
[
"
extraOptions
"
]
=
result
[
"
extra_options
"
]
                    
break
            
support_data_by_type
[
data_type
]
[
"
suites
"
]
.
append
(
suite
)
            
for
measurement_name
value_info
in
data_set
[
"
values
"
]
.
items
(
)
:
                
if
not
isinstance
(
value_info
dict
)
:
                    
value_info
=
{
"
values
"
:
value_info
}
                
new_subtest
=
{
}
                
if
value_info
.
get
(
"
subtest
-
prefix
-
type
"
True
)
:
                    
new_subtest
[
"
name
"
]
=
data_type
+
"
-
"
+
measurement_name
                
else
:
                    
new_subtest
[
"
name
"
]
=
measurement_name
                
new_subtest
[
"
value
"
]
=
value_info
[
"
values
"
]
                
new_subtest
[
"
lowerIsBetter
"
]
=
value_info
.
get
(
"
lowerIsBetter
"
True
)
                
new_subtest
[
"
alertThreshold
"
]
=
value_info
.
get
(
"
alertThreshold
"
2
.
0
)
                
new_subtest
[
"
unit
"
]
=
value_info
.
get
(
"
unit
"
data_set
[
"
unit
"
]
)
                
if
"
shouldAlert
"
in
value_info
:
                    
new_subtest
[
"
shouldAlert
"
]
=
value_info
.
get
(
"
shouldAlert
"
)
                
subtests
.
append
(
new_subtest
)
                
vals
.
append
(
[
new_subtest
[
"
value
"
]
new_subtest
[
"
name
"
]
]
)
            
if
len
(
subtests
)
>
=
1
and
data_set
.
get
(
"
summarize
-
values
"
True
)
:
                
suite
[
"
value
"
]
=
self
.
construct_summary
(
                    
vals
testname
=
"
supporting_data
"
unit
=
data_set
[
"
unit
"
]
                
)
        
for
data_type
in
support_data_by_type
:
            
data
=
support_data_by_type
[
data_type
]
            
if
self
.
browser_name
:
                
data
[
"
application
"
]
=
{
"
name
"
:
self
.
browser_name
}
                
if
self
.
browser_version
:
                    
data
[
"
application
"
]
[
"
version
"
]
=
self
.
browser_version
            
self
.
summarized_supporting_data
.
append
(
data
)
        
return
    
def
output
(
self
test_names
)
:
        
"
"
"
output
to
file
and
perfherder
data
json
"
"
"
        
if
os
.
getenv
(
"
MOZ_UPLOAD_DIR
"
)
:
            
results_path
=
os
.
path
.
join
(
                
os
.
path
.
dirname
(
os
.
environ
[
"
MOZ_UPLOAD_DIR
"
]
)
"
raptor
.
json
"
            
)
            
screenshot_path
=
os
.
path
.
join
(
                
os
.
path
.
dirname
(
os
.
environ
[
"
MOZ_UPLOAD_DIR
"
]
)
"
screenshots
.
html
"
            
)
        
else
:
            
results_path
=
os
.
path
.
join
(
os
.
getcwd
(
)
"
raptor
.
json
"
)
            
screenshot_path
=
os
.
path
.
join
(
os
.
getcwd
(
)
"
screenshots
.
html
"
)
        
success
=
True
        
if
self
.
summarized_results
=
=
{
}
:
            
success
=
False
            
LOG
.
error
(
                
"
no
summarized
raptor
results
found
for
%
s
"
%
"
"
.
join
(
test_names
)
            
)
        
else
:
            
for
suite
in
self
.
summarized_results
[
"
suites
"
]
:
                
tname
=
suite
[
"
name
"
]
                
found
=
False
                
for
test
in
test_names
:
                    
if
tname
in
test
:
                        
found
=
True
                        
break
                
if
not
found
:
                    
success
=
False
                    
LOG
.
error
(
"
no
summarized
raptor
results
found
for
%
s
"
%
tname
)
            
with
open
(
results_path
"
w
"
)
as
f
:
                
for
result
in
self
.
summarized_results
:
                    
f
.
write
(
"
%
s
\
n
"
%
result
)
        
if
len
(
self
.
summarized_screenshots
)
>
0
:
            
with
open
(
screenshot_path
"
w
"
)
as
f
:
                
for
result
in
self
.
summarized_screenshots
:
                    
f
.
write
(
"
%
s
\
n
"
%
result
)
            
LOG
.
info
(
"
screen
captures
can
be
found
locally
at
:
%
s
"
%
screenshot_path
)
        
if
self
.
summarized_results
=
=
{
}
:
            
return
success
0
        
extra_opts
=
self
.
summarized_results
[
"
suites
"
]
[
0
]
.
get
(
"
extraOptions
"
[
]
)
        
test_type
=
self
.
summarized_results
[
"
suites
"
]
[
0
]
.
get
(
"
type
"
"
"
)
        
output_perf_data
=
True
        
not_posting
=
"
-
not
posting
regular
test
results
for
perfherder
"
        
if
"
gecko_profile
"
in
extra_opts
:
            
LOG
.
info
(
"
gecko
profiling
enabled
%
s
"
%
not_posting
)
            
output_perf_data
=
False
        
if
test_type
=
=
"
scenario
"
:
            
LOG
.
info
(
"
scenario
test
type
was
run
%
s
"
%
not_posting
)
            
output_perf_data
=
False
        
if
self
.
browser_name
:
            
self
.
summarized_results
[
"
application
"
]
=
{
"
name
"
:
self
.
browser_name
}
            
if
self
.
browser_version
:
                
self
.
summarized_results
[
"
application
"
]
[
"
version
"
]
=
self
.
browser_version
        
total_perfdata
=
0
        
if
output_perf_data
:
            
if
len
(
self
.
summarized_supporting_data
)
=
=
0
or
self
.
mozproxy_data
:
                
LOG
.
info
(
"
PERFHERDER_DATA
:
%
s
"
%
json
.
dumps
(
self
.
summarized_results
)
)
                
total_perfdata
=
1
            
else
:
                
LOG
.
info
(
                    
"
supporting
data
measurements
exist
-
only
posting
those
to
perfherder
"
                
)
        
json
.
dump
(
            
self
.
summarized_results
open
(
results_path
"
w
"
)
indent
=
2
sort_keys
=
True
        
)
        
LOG
.
info
(
"
results
can
also
be
found
locally
at
:
%
s
"
%
results_path
)
        
return
success
total_perfdata
    
def
output_supporting_data
(
self
test_names
)
:
        
"
"
"
        
Supporting
data
was
gathered
outside
of
the
main
raptor
test
;
it
has
already
        
been
summarized
now
output
it
appropriately
.
        
We
want
to
output
supporting
data
in
a
completely
separate
perfherder
json
blob
and
        
in
a
corresponding
file
artifact
.
This
way
supporting
data
can
be
ingested
as
its
own
        
test
suite
in
perfherder
and
alerted
upon
if
desired
;
kept
outside
of
the
test
results
        
from
the
actual
Raptor
test
which
was
run
when
the
supporting
data
was
gathered
.
        
"
"
"
        
if
len
(
self
.
summarized_supporting_data
)
=
=
0
:
            
LOG
.
error
(
                
"
no
summarized
supporting
data
found
for
%
s
"
%
"
"
.
join
(
test_names
)
            
)
            
return
False
0
        
total_perfdata
=
0
        
for
next_data_set
in
self
.
summarized_supporting_data
:
            
data_type
=
next_data_set
[
"
suites
"
]
[
0
]
[
"
type
"
]
            
if
os
.
environ
[
"
MOZ_UPLOAD_DIR
"
]
:
                
results_path
=
os
.
path
.
join
(
                    
os
.
path
.
dirname
(
os
.
environ
[
"
MOZ_UPLOAD_DIR
"
]
)
                    
"
raptor
-
%
s
.
json
"
%
data_type
                
)
            
else
:
                
results_path
=
os
.
path
.
join
(
os
.
getcwd
(
)
"
raptor
-
%
s
.
json
"
%
data_type
)
            
json
.
dump
(
next_data_set
open
(
results_path
"
w
"
)
indent
=
2
sort_keys
=
True
)
            
LOG
.
info
(
"
PERFHERDER_DATA
:
%
s
"
%
json
.
dumps
(
next_data_set
)
)
            
LOG
.
info
(
                
"
%
s
results
can
also
be
found
locally
at
:
%
s
"
                
%
(
data_type
results_path
)
            
)
            
total_perfdata
+
=
1
        
return
True
total_perfdata
    
def
construct_summary
(
self
vals
testname
unit
=
None
)
:
        
def
_filter
(
vals
value
=
None
)
:
            
if
value
is
None
:
                
return
[
i
for
i
j
in
vals
]
            
return
[
i
for
i
j
in
vals
if
j
=
=
value
]
        
if
testname
.
startswith
(
"
raptor
-
v8_7
"
)
:
            
return
100
*
filters
.
geometric_mean
(
_filter
(
vals
)
)
        
if
testname
.
startswith
(
"
raptor
-
speedometer
"
)
or
testname
.
startswith
(
"
speedometer
"
)
:
            
correctionFactor
=
3
            
results
=
_filter
(
vals
)
            
if
len
(
results
)
!
=
160
:
                
raise
Exception
(
                    
"
Speedometer
has
160
subtests
found
:
%
s
instead
"
%
len
(
results
)
                
)
            
results
=
results
[
9
:
:
10
]
            
score
=
60
*
1000
/
filters
.
geometric_mean
(
results
)
/
correctionFactor
            
return
score
        
if
testname
.
startswith
(
"
raptor
-
stylebench
"
)
:
            
correctionFactor
=
3
            
results
=
_filter
(
vals
)
            
if
len
(
results
)
!
=
380
:
                
raise
Exception
(
                    
"
StyleBench
requires
380
entries
found
:
%
s
instead
"
%
len
(
results
)
                
)
            
results
=
results
[
75
:
:
76
]
            
return
60
*
1000
/
filters
.
geometric_mean
(
results
)
/
correctionFactor
        
if
testname
.
startswith
(
(
"
raptor
-
kraken
"
"
raptor
-
sunspider
"
)
)
:
            
return
sum
(
_filter
(
vals
)
)
        
if
testname
.
startswith
(
(
"
raptor
-
unity
-
webgl
"
"
raptor
-
webaudio
"
)
)
:
            
return
filters
.
mean
(
_filter
(
vals
"
Geometric
Mean
"
)
)
        
if
testname
.
startswith
(
"
raptor
-
assorted
-
dom
"
)
:
            
return
round
(
filters
.
geometric_mean
(
_filter
(
vals
)
)
2
)
        
if
testname
.
startswith
(
"
raptor
-
wasm
-
misc
"
)
:
            
return
filters
.
mean
(
_filter
(
vals
"
__total__
"
)
)
        
if
testname
.
startswith
(
"
raptor
-
wasm
-
godot
"
)
:
            
return
filters
.
mean
(
_filter
(
vals
"
first
-
interactive
"
)
)
        
if
testname
.
startswith
(
"
raptor
-
youtube
-
playback
"
)
:
            
return
round
(
filters
.
mean
(
_filter
(
vals
)
)
2
)
        
if
testname
.
startswith
(
"
supporting_data
"
)
:
            
if
not
unit
:
                
return
sum
(
_filter
(
vals
)
)
            
if
unit
=
=
"
%
"
:
                
return
filters
.
mean
(
_filter
(
vals
)
)
            
if
unit
in
(
"
W
"
"
MHz
"
)
:
                
allavgs
=
[
]
                
for
val
subtest
in
vals
:
                    
if
"
avg
"
in
subtest
:
                        
allavgs
.
append
(
val
)
                
if
allavgs
:
                    
return
sum
(
allavgs
)
                
raise
Exception
(
                    
"
No
average
measurements
found
for
supporting
data
with
W
or
MHz
unit
.
"
)
            
if
unit
in
[
"
KB
"
"
mAh
"
"
mWh
"
]
:
                
return
sum
(
_filter
(
vals
)
)
            
raise
NotImplementedError
(
"
Unit
%
s
not
suported
"
%
unit
)
        
if
len
(
vals
)
>
1
:
            
return
round
(
filters
.
geometric_mean
(
_filter
(
vals
)
)
2
)
        
return
round
(
filters
.
mean
(
_filter
(
vals
)
)
2
)
    
def
parseSpeedometerOutput
(
self
test
)
:
        
_subtests
=
{
}
        
data
=
test
[
"
measurements
"
]
[
"
speedometer
"
]
        
for
page_cycle
in
data
:
            
for
sub
replicates
in
page_cycle
[
0
]
.
items
(
)
:
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
                        
"
unit
"
:
test
[
"
subtest_unit
"
]
                        
"
alertThreshold
"
:
float
(
test
[
"
alert_threshold
"
]
)
                        
"
lowerIsBetter
"
:
test
[
"
subtest_lower_is_better
"
]
                        
"
name
"
:
sub
                        
"
replicates
"
:
[
]
                    
}
                
_subtests
[
sub
]
[
"
replicates
"
]
.
extend
(
[
round
(
x
3
)
for
x
in
replicates
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
"
value
"
]
=
filters
.
median
(
_subtests
[
name
]
[
"
replicates
"
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
"
value
"
]
name
]
)
        
return
subtests
vals
    
def
parseAresSixOutput
(
self
test
)
:
        
"
"
"
        
https
:
/
/
browserbench
.
org
/
ARES
-
6
/
        
Every
pagecycle
will
perform
the
tests
from
the
index
page
        
We
have
4
main
tests
per
index
page
:
        
-
Air
Basic
Babylon
ML
        
-
and
from
these
4
above
ares6
generates
the
Overall
results
        
Each
test
has
3
subtests
(
firstIteration
steadyState
averageWorstCase
)
:
        
-
_steadyState
        
-
_firstIteration
        
-
_averageWorstCase
        
Each
index
page
will
run
5
cycles
this
is
set
in
glue
.
js
        
{
            
'
expected_browser_cycles
'
:
1
            
'
subtest_unit
'
:
'
ms
'
            
'
name
'
:
'
raptor
-
ares6
-
firefox
'
            
'
lower_is_better
'
:
False
            
'
browser_cycle
'
:
'
1
'
            
'
subtest_lower_is_better
'
:
True
            
'
cold
'
:
False
            
'
browser
'
:
'
Firefox
69
.
0a1
20190531035909
'
            
'
type
'
:
'
benchmark
'
            
'
page
'
:
'
http
:
/
/
127
.
0
.
0
.
1
:
35369
/
ARES
-
6
/
index
.
html
?
raptor
'
            
'
unit
'
:
'
ms
'
            
'
alert_threshold
'
:
2
            
'
measurements
'
:
{
                
'
ares6
'
:
[
[
{
                    
'
Babylon_firstIteration
'
:
[
                        
123
.
68
                        
168
.
21999999999997
                        
127
.
34000000000003
                        
113
.
56
                        
128
.
78
                        
169
.
44000000000003
                    
]
                    
'
Air_steadyState
'
:
[
                        
21
.
184723618090434
                        
22
.
906331658291457
                        
19
.
939396984924624
                        
20
.
572462311557775
                        
20
.
790452261306534
                        
18
.
378693467336696
                    
]
                    
etc
.
                
}
]
]
            
}
        
}
        
Details
on
how
/
ARES6
/
index
.
html
is
showing
the
mean
on
subsequent
test
results
:
        
I
selected
just
a
small
part
from
the
metrics
just
to
be
easier
to
explain
        
what
is
going
on
.
        
After
the
raptor
GeckoView
test
finishes
we
have
these
results
in
the
logs
:
        
Extracted
from
"
INFO
-
raptor
-
control
-
server
Info
:
received
webext_results
:
"
        
'
Air_firstIteration
'
:
[
660
.
8000000000002
626
.
4599999999999
655
.
6199999999999
        
635
.
9000000000001
636
.
4000000000001
]
        
Extracted
from
"
INFO
-
raptor
-
output
Info
:
PERFHERDER_DATA
:
"
        
{
"
name
"
:
"
Air_firstIteration
"
"
lowerIsBetter
"
:
true
"
alertThreshold
"
:
2
.
0
        
"
replicates
"
:
[
660
.
8
626
.
46
655
.
62
635
.
9
636
.
4
]
"
value
"
:
636
.
4
"
unit
"
:
"
ms
"
}
        
On
GeckoView
'
s
/
ARES6
/
index
.
html
this
is
what
we
see
for
Air
-
First
Iteration
:
        
-
on
1st
test
cycle
:
660
.
80
(
rounded
from
660
.
8000000000002
)
        
-
on
2nd
test
cycle
:
643
.
63
this
is
coming
from
          
(
660
.
8000000000002
+
626
.
4599999999999
)
/
2
          
then
rounded
up
to
a
precision
of
2
decimals
        
-
on
3rd
test
cycle
:
647
.
63
this
is
coming
from
          
(
660
.
8000000000002
+
626
.
4599999999999
+
655
.
6199999999999
)
/
3
          
then
rounded
up
to
a
precision
of
2
decimals
        
-
and
so
on
        
"
"
"
        
_subtests
=
{
}
        
data
=
test
[
"
measurements
"
]
[
"
ares6
"
]
        
for
page_cycle
in
data
:
            
for
sub
replicates
in
page_cycle
[
0
]
.
items
(
)
:
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
                        
"
unit
"
:
test
[
"
subtest_unit
"
]
                        
"
alertThreshold
"
:
float
(
test
[
"
alert_threshold
"
]
)
                        
"
lowerIsBetter
"
:
test
[
"
subtest_lower_is_better
"
]
                        
"
name
"
:
sub
                        
"
replicates
"
:
[
]
                    
}
                
_subtests
[
sub
]
[
"
replicates
"
]
.
extend
(
[
round
(
x
3
)
for
x
in
replicates
]
)
        
vals
=
[
]
        
for
name
test
in
_subtests
.
items
(
)
:
            
test
[
"
value
"
]
=
filters
.
mean
(
test
[
"
replicates
"
]
)
            
vals
.
append
(
[
test
[
"
value
"
]
name
]
)
        
return
_subtests
.
values
(
)
sorted
(
vals
reverse
=
True
)
    
def
parseYoutubePlaybackPerformanceOutput
(
self
test
)
:
        
"
"
"
Parse
the
metrics
for
the
Youtube
playback
performance
test
.
        
For
each
video
measured
values
for
dropped
and
decoded
frames
will
be
        
available
from
the
benchmark
site
.
        
{
u
'
PlaybackPerf
.
VP9
.
2160p60
2X
'
:
{
u
'
droppedFrames
'
:
1
u
'
decodedFrames
'
:
796
}
        
With
each
page
cycle
/
iteration
of
the
test
multiple
values
can
be
present
.
        
Raptor
will
calculate
the
percentage
of
dropped
frames
to
decoded
frames
.
        
All
those
three
values
will
then
be
emitted
as
separate
sub
tests
.
        
"
"
"
        
_subtests
=
{
}
        
test_name
=
[
measurement
                     
for
measurement
in
test
[
"
measurements
"
]
.
keys
(
)
                     
if
"
youtube
-
playback
"
in
measurement
                     
]
        
if
len
(
test_name
)
>
0
:
            
data
=
test
[
"
measurements
"
]
.
get
(
test_name
[
0
]
)
        
else
:
            
raise
Exception
(
"
No
measurements
found
for
youtube
test
!
"
)
        
def
create_subtest_entry
(
            
name
            
value
            
unit
=
test
[
"
subtest_unit
"
]
            
lower_is_better
=
test
[
"
subtest_lower_is_better
"
]
        
)
:
            
if
name
not
in
_subtests
.
keys
(
)
:
                
_subtests
[
name
]
=
{
                    
"
name
"
:
name
                    
"
unit
"
:
unit
                    
"
lowerIsBetter
"
:
lower_is_better
                    
"
replicates
"
:
[
]
                
}
            
_subtests
[
name
]
[
"
replicates
"
]
.
append
(
value
)
            
if
self
.
subtest_alert_on
is
not
None
:
                
if
name
in
self
.
subtest_alert_on
:
                    
LOG
.
info
(
                        
"
turning
on
subtest
alerting
for
measurement
type
:
%
s
"
%
name
                    
)
                    
_subtests
[
name
]
[
"
shouldAlert
"
]
=
True
        
failed_tests
=
[
]
        
for
pagecycle
in
data
:
            
for
_sub
_value
in
pagecycle
[
0
]
.
iteritems
(
)
:
                
if
_value
[
"
decodedFrames
"
]
=
=
0
:
                    
failed_tests
.
append
(
"
%
s
test
Failed
.
decodedFrames
%
s
droppedFrames
%
s
.
"
%
                                        
(
_sub
_value
[
"
decodedFrames
"
]
_value
[
"
droppedFrames
"
]
)
)
                
try
:
                    
percent_dropped
=
(
                        
float
(
_value
[
"
droppedFrames
"
]
)
/
_value
[
"
decodedFrames
"
]
*
100
.
0
                    
)
                
except
ZeroDivisionError
:
                    
percent_dropped
=
100
.
0
                
_sub
=
_sub
.
split
(
"
PlaybackPerf
"
1
)
[
-
1
]
                
if
_sub
.
startswith
(
"
.
"
)
:
                    
_sub
=
_sub
[
1
:
]
                
create_subtest_entry
(
                    
"
{
}
_decoded_frames
"
.
format
(
_sub
)
                    
_value
[
"
decodedFrames
"
]
                    
lower_is_better
=
False
                
)
                
create_subtest_entry
(
                    
"
{
}
_dropped_frames
"
.
format
(
_sub
)
_value
[
"
droppedFrames
"
]
                
)
                
create_subtest_entry
(
                    
"
{
}
_
%
_dropped_frames
"
.
format
(
_sub
)
percent_dropped
                
)
        
if
len
(
failed_tests
)
>
0
:
            
[
LOG
.
warning
(
"
Youtube
sub
-
test
FAILED
:
%
s
"
%
test
)
for
test
in
failed_tests
]
            
LOG
.
warning
(
"
Youtube
playback
sub
-
tests
failed
!
!
!
"
                        
"
Not
submitting
results
to
perfherder
!
"
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
"
value
"
]
=
round
(
                
filters
.
median
(
_subtests
[
name
]
[
"
replicates
"
]
)
2
            
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
if
name
.
endswith
(
"
X_dropped_frames
"
)
:
                
vals
.
append
(
[
_subtests
[
name
]
[
"
value
"
]
name
]
)
        
return
subtests
vals
class
RaptorOutput
(
PerftestOutput
)
:
    
"
"
"
class
for
raptor
output
"
"
"
    
def
summarize
(
self
test_names
)
:
        
suites
=
[
]
        
test_results
=
{
"
framework
"
:
{
"
name
"
:
"
raptor
"
}
"
suites
"
:
suites
}
        
if
len
(
self
.
results
)
=
=
0
:
            
LOG
.
error
(
"
no
raptor
test
results
found
for
%
s
"
%
"
"
.
join
(
test_names
)
)
            
return
        
for
test
in
self
.
results
:
            
vals
=
[
]
            
subtests
=
[
]
            
suite
=
{
                
"
name
"
:
test
[
"
name
"
]
                
"
type
"
:
test
[
"
type
"
]
                
"
tags
"
:
[
]
                
"
extraOptions
"
:
test
[
"
extra_options
"
]
                
"
subtests
"
:
subtests
                
"
lowerIsBetter
"
:
test
[
"
lower_is_better
"
]
                
"
unit
"
:
test
[
"
unit
"
]
                
"
alertThreshold
"
:
float
(
test
[
"
alert_threshold
"
]
)
            
}
            
if
hasattr
(
test
"
alert_change_type
"
)
:
                
suite
[
"
alertChangeType
"
]
=
test
[
"
alert_change_type
"
]
            
if
test
[
"
cold
"
]
is
True
:
                
suite
[
"
cold
"
]
=
True
                
suite
[
"
browser_cycle
"
]
=
int
(
test
[
"
browser_cycle
"
]
)
                
suite
[
"
expected_browser_cycles
"
]
=
int
(
test
[
"
expected_browser_cycles
"
]
)
                
suite
[
"
tags
"
]
.
append
(
"
cold
"
)
            
else
:
                
suite
[
"
tags
"
]
.
append
(
"
warm
"
)
            
suites
.
append
(
suite
)
            
if
test
[
"
type
"
]
in
(
"
pageload
"
"
scenario
"
)
:
                
for
measurement_name
replicates
in
test
[
"
measurements
"
]
.
items
(
)
:
                    
new_subtest
=
{
}
                    
new_subtest
[
"
name
"
]
=
measurement_name
                    
new_subtest
[
"
replicates
"
]
=
replicates
                    
new_subtest
[
"
lowerIsBetter
"
]
=
test
[
"
subtest_lower_is_better
"
]
                    
new_subtest
[
"
alertThreshold
"
]
=
float
(
test
[
"
alert_threshold
"
]
)
                    
new_subtest
[
"
value
"
]
=
0
                    
new_subtest
[
"
unit
"
]
=
test
[
"
subtest_unit
"
]
                    
if
test
[
"
cold
"
]
is
False
:
                        
LOG
.
info
(
                            
"
ignoring
the
first
%
s
value
due
to
initial
pageload
noise
"
                            
%
measurement_name
                        
)
                        
filtered_values
=
filters
.
ignore_first
(
                            
new_subtest
[
"
replicates
"
]
1
                        
)
                    
else
:
                        
filtered_values
=
new_subtest
[
"
replicates
"
]
                    
if
measurement_name
=
=
"
ttfi
"
:
                        
filtered_values
=
filters
.
ignore_negative
(
filtered_values
)
                        
if
len
(
filtered_values
)
<
1
:
                            
continue
                    
if
self
.
subtest_alert_on
is
not
None
:
                        
if
measurement_name
in
self
.
subtest_alert_on
:
                            
LOG
.
info
(
                                
"
turning
on
subtest
alerting
for
measurement
type
:
%
s
"
                                
%
measurement_name
                            
)
                            
new_subtest
[
"
shouldAlert
"
]
=
True
                    
new_subtest
[
"
value
"
]
=
filters
.
median
(
filtered_values
)
                    
vals
.
append
(
[
new_subtest
[
"
value
"
]
new_subtest
[
"
name
"
]
]
)
                    
subtests
.
append
(
new_subtest
)
            
elif
test
[
"
type
"
]
=
=
"
benchmark
"
:
                
if
any
(
[
"
youtube
-
playback
"
in
measurement
                        
for
measurement
in
test
[
"
measurements
"
]
.
keys
(
)
]
)
:
                    
subtests
vals
=
self
.
parseYoutubePlaybackPerformanceOutput
(
test
)
                
elif
"
assorted
-
dom
"
in
test
[
"
measurements
"
]
:
                    
subtests
vals
=
self
.
parseAssortedDomOutput
(
test
)
                
elif
"
ares6
"
in
test
[
"
measurements
"
]
:
                    
subtests
vals
=
self
.
parseAresSixOutput
(
test
)
                
elif
"
jetstream2
"
in
test
[
"
measurements
"
]
:
                    
subtests
vals
=
self
.
parseJetstreamTwoOutput
(
test
)
                
elif
"
motionmark
"
in
test
[
"
measurements
"
]
:
                    
subtests
vals
=
self
.
parseMotionmarkOutput
(
test
)
                
elif
"
speedometer
"
in
test
[
"
measurements
"
]
:
                    
subtests
vals
=
self
.
parseSpeedometerOutput
(
test
)
                
elif
"
sunspider
"
in
test
[
"
measurements
"
]
:
                    
subtests
vals
=
self
.
parseSunspiderOutput
(
test
)
                
elif
"
unity
-
webgl
"
in
test
[
"
measurements
"
]
:
                    
subtests
vals
=
self
.
parseUnityWebGLOutput
(
test
)
                
elif
"
wasm
-
godot
"
in
test
[
"
measurements
"
]
:
                    
subtests
vals
=
self
.
parseWASMGodotOutput
(
test
)
                
elif
"
wasm
-
misc
"
in
test
[
"
measurements
"
]
:
                    
subtests
vals
=
self
.
parseWASMMiscOutput
(
test
)
                
elif
"
webaudio
"
in
test
[
"
measurements
"
]
:
                    
subtests
vals
=
self
.
parseWebaudioOutput
(
test
)
                
suite
[
"
subtests
"
]
=
subtests
            
else
:
                
LOG
.
error
(
                    
"
output
.
summarize
received
unsupported
test
results
type
for
%
s
"
                    
%
test
[
"
name
"
]
                
)
                
return
            
suite
[
"
tags
"
]
.
append
(
test
[
"
type
"
]
)
            
if
len
(
subtests
)
>
1
:
                
suite
[
"
value
"
]
=
self
.
construct_summary
(
vals
testname
=
test
[
"
name
"
]
)
            
subtests
.
sort
(
key
=
lambda
subtest
:
subtest
[
"
name
"
]
)
            
suite
[
"
tags
"
]
.
sort
(
)
        
suites
.
sort
(
key
=
lambda
suite
:
suite
[
"
name
"
]
)
        
self
.
summarized_results
=
test_results
    
def
combine_browser_cycles
(
self
)
:
        
"
"
"
        
At
this
point
the
results
have
been
summarized
;
however
there
may
have
been
multiple
        
browser
cycles
(
i
.
e
.
cold
load
)
.
In
which
case
the
results
have
one
entry
for
each
        
test
for
each
browser
cycle
.
For
each
test
we
need
to
combine
the
results
for
all
        
browser
cycles
into
one
results
entry
.
        
For
example
this
is
what
the
summarized
results
suites
list
looks
like
from
a
test
that
        
was
run
with
multiple
(
two
)
browser
cycles
:
        
[
{
'
expected_browser_cycles
'
:
2
'
extraOptions
'
:
[
]
            
'
name
'
:
u
'
raptor
-
tp6m
-
amazon
-
geckoview
-
cold
'
'
lowerIsBetter
'
:
True
            
'
alertThreshold
'
:
2
.
0
'
value
'
:
1776
.
94
'
browser_cycle
'
:
1
            
'
subtests
'
:
[
{
'
name
'
:
u
'
dcf
'
'
lowerIsBetter
'
:
True
'
alertThreshold
'
:
2
.
0
                
'
value
'
:
818
'
replicates
'
:
[
818
]
'
unit
'
:
u
'
ms
'
}
{
'
name
'
:
u
'
fcp
'
                
'
lowerIsBetter
'
:
True
'
alertThreshold
'
:
2
.
0
'
value
'
:
1131
'
shouldAlert
'
:
True
                
'
replicates
'
:
[
1131
]
'
unit
'
:
u
'
ms
'
}
{
'
name
'
:
u
'
fnbpaint
'
'
lowerIsBetter
'
:
True
                
'
alertThreshold
'
:
2
.
0
'
value
'
:
1056
'
replicates
'
:
[
1056
]
'
unit
'
:
u
'
ms
'
}
                
{
'
name
'
:
u
'
ttfi
'
'
lowerIsBetter
'
:
True
'
alertThreshold
'
:
2
.
0
'
value
'
:
18074
                
'
replicates
'
:
[
18074
]
'
unit
'
:
u
'
ms
'
}
{
'
name
'
:
u
'
loadtime
'
'
lowerIsBetter
'
:
True
                
'
alertThreshold
'
:
2
.
0
'
value
'
:
1002
'
shouldAlert
'
:
True
'
replicates
'
:
[
1002
]
                
'
unit
'
:
u
'
ms
'
}
]
            
'
cold
'
:
True
'
type
'
:
u
'
pageload
'
'
unit
'
:
u
'
ms
'
}
        
{
'
expected_browser_cycles
'
:
2
'
extraOptions
'
:
[
]
            
'
name
'
:
u
'
raptor
-
tp6m
-
amazon
-
geckoview
-
cold
'
'
lowerIsBetter
'
:
True
            
'
alertThreshold
'
:
2
.
0
'
value
'
:
840
.
25
'
browser_cycle
'
:
2
            
'
subtests
'
:
[
{
'
name
'
:
u
'
dcf
'
'
lowerIsBetter
'
:
True
'
alertThreshold
'
:
2
.
0
                
'
value
'
:
462
'
replicates
'
:
[
462
]
'
unit
'
:
u
'
ms
'
}
{
'
name
'
:
u
'
fcp
'
                
'
lowerIsBetter
'
:
True
'
alertThreshold
'
:
2
.
0
'
value
'
:
718
'
shouldAlert
'
:
True
                
'
replicates
'
:
[
718
]
'
unit
'
:
u
'
ms
'
}
{
'
name
'
:
u
'
fnbpaint
'
'
lowerIsBetter
'
:
True
                
'
alertThreshold
'
:
2
.
0
'
value
'
:
676
'
replicates
'
:
[
676
]
'
unit
'
:
u
'
ms
'
}
                
{
'
name
'
:
u
'
ttfi
'
'
lowerIsBetter
'
:
True
'
alertThreshold
'
:
2
.
0
'
value
'
:
3084
                
'
replicates
'
:
[
3084
]
'
unit
'
:
u
'
ms
'
}
{
'
name
'
:
u
'
loadtime
'
'
lowerIsBetter
'
:
True
                
'
alertThreshold
'
:
2
.
0
'
value
'
:
605
'
shouldAlert
'
:
True
'
replicates
'
:
[
605
]
                
'
unit
'
:
u
'
ms
'
}
]
            
'
cold
'
:
True
'
type
'
:
u
'
pageload
'
'
unit
'
:
u
'
ms
'
}
]
        
Need
to
combine
those
into
a
single
entry
.
        
"
"
"
        
if
len
(
self
.
results
)
=
=
0
:
            
LOG
.
info
(
                
"
error
:
no
raptor
test
results
found
so
no
need
to
combine
browser
cycles
"
            
)
            
return
        
suites_to_be_combined
=
[
]
        
combined_suites
=
[
]
        
for
_index
suite
in
enumerate
(
self
.
summarized_results
.
get
(
"
suites
"
[
]
)
)
:
            
if
suite
.
get
(
"
cold
"
)
is
None
:
                
continue
            
if
suite
[
"
expected_browser_cycles
"
]
>
1
:
                
_name
=
suite
[
"
name
"
]
                
_details
=
suite
.
copy
(
)
                
suites_to_be_combined
.
append
(
{
"
name
"
:
_name
"
details
"
:
_details
}
)
                
suite
[
"
to_be_deleted
"
]
=
True
        
combined_suites
=
{
}
        
for
next_suite
in
suites_to_be_combined
:
            
suite_name
=
next_suite
[
"
details
"
]
[
"
name
"
]
            
browser_cycle
=
next_suite
[
"
details
"
]
[
"
browser_cycle
"
]
            
LOG
.
info
(
                
"
combining
results
from
browser
cycle
%
d
for
%
s
"
                
%
(
browser_cycle
suite_name
)
            
)
            
if
suite_name
not
in
combined_suites
:
                
combined_suites
[
suite_name
]
=
next_suite
[
"
details
"
]
                
LOG
.
info
(
"
created
new
combined
result
with
intial
cycle
replicates
"
)
                
del
combined_suites
[
suite_name
]
[
"
cold
"
]
                
del
combined_suites
[
suite_name
]
[
"
browser_cycle
"
]
                
del
combined_suites
[
suite_name
]
[
"
expected_browser_cycles
"
]
            
else
:
                
for
next_subtest
in
next_suite
[
"
details
"
]
[
"
subtests
"
]
:
                    
found_subtest
=
False
                    
for
combined_subtest
in
combined_suites
[
suite_name
]
[
"
subtests
"
]
:
                        
if
combined_subtest
[
"
name
"
]
=
=
next_subtest
[
"
name
"
]
:
                            
LOG
.
info
(
"
adding
replicates
for
%
s
"
%
next_subtest
[
"
name
"
]
)
                            
combined_subtest
[
"
replicates
"
]
.
extend
(
                                
next_subtest
[
"
replicates
"
]
                            
)
                            
found_subtest
=
True
                    
if
not
found_subtest
:
                        
LOG
.
info
(
"
adding
replicates
for
%
s
"
%
next_subtest
[
"
name
"
]
)
                        
combined_suites
[
next_suite
[
"
details
"
]
[
"
name
"
]
]
[
                            
"
subtests
"
                        
]
.
append
(
next_subtest
)
        
for
i
name
in
enumerate
(
combined_suites
)
:
            
vals
=
[
]
            
for
next_sub
in
combined_suites
[
name
]
[
"
subtests
"
]
:
                
next_sub
[
"
value
"
]
=
filters
.
median
(
next_sub
[
"
replicates
"
]
)
                
vals
.
append
(
[
next_sub
[
"
value
"
]
next_sub
[
"
name
"
]
]
)
            
if
len
(
combined_suites
[
name
]
[
"
subtests
"
]
)
>
1
:
                
combined_suites
[
name
]
[
"
value
"
]
=
self
.
construct_summary
(
                    
vals
testname
=
name
                
)
            
self
.
summarized_results
[
"
suites
"
]
.
append
(
combined_suites
[
name
]
)
        
self
.
summarized_results
[
"
suites
"
]
=
[
            
item
            
for
item
in
self
.
summarized_results
[
"
suites
"
]
            
if
item
.
get
(
"
to_be_deleted
"
)
is
not
True
        
]
    
def
parseJetstreamTwoOutput
(
self
test
)
:
        
_subtests
=
{
}
        
data
=
test
[
"
measurements
"
]
[
"
jetstream2
"
]
        
for
page_cycle
in
data
:
            
for
sub
replicates
in
page_cycle
[
0
]
.
items
(
)
:
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
                        
"
unit
"
:
test
[
"
subtest_unit
"
]
                        
"
alertThreshold
"
:
float
(
test
[
"
alert_threshold
"
]
)
                        
"
lowerIsBetter
"
:
test
[
"
subtest_lower_is_better
"
]
                        
"
name
"
:
sub
                        
"
replicates
"
:
[
]
                    
}
                
_subtests
[
sub
]
[
"
replicates
"
]
.
extend
(
[
round
(
x
3
)
for
x
in
replicates
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
"
value
"
]
=
filters
.
mean
(
_subtests
[
name
]
[
"
replicates
"
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
"
value
"
]
name
]
)
        
return
subtests
vals
    
def
parseWASMMiscOutput
(
self
test
)
:
        
"
"
"
          
{
u
'
wasm
-
misc
'
:
[
            
[
[
{
u
'
name
'
:
u
'
validate
'
u
'
time
'
:
163
.
44000000000005
}
              
.
.
.
              
{
u
'
name
'
:
u
'
__total__
'
u
'
time
'
:
63308
.
434904788155
}
]
]
            
.
.
.
            
[
[
{
u
'
name
'
:
u
'
validate
'
u
'
time
'
:
129
.
42000000000002
}
              
{
u
'
name
'
:
u
'
__total__
'
u
'
time
'
:
63181
.
24089257814
}
]
]
           
]
}
        
"
"
"
        
_subtests
=
{
}
        
data
=
test
[
"
measurements
"
]
[
"
wasm
-
misc
"
]
        
for
page_cycle
in
data
:
            
for
item
in
page_cycle
[
0
]
:
                
sub
=
item
[
"
name
"
]
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
                        
"
unit
"
:
test
[
"
subtest_unit
"
]
                        
"
alertThreshold
"
:
float
(
test
[
"
alert_threshold
"
]
)
                        
"
lowerIsBetter
"
:
test
[
"
subtest_lower_is_better
"
]
                        
"
name
"
:
sub
                        
"
replicates
"
:
[
]
                    
}
                
_subtests
[
sub
]
[
"
replicates
"
]
.
append
(
item
[
"
time
"
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
"
value
"
]
=
filters
.
median
(
_subtests
[
name
]
[
"
replicates
"
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
"
value
"
]
name
]
)
        
return
subtests
vals
    
def
parseWASMGodotOutput
(
self
test
)
:
        
"
"
"
            
{
u
'
wasm
-
godot
'
:
[
                
{
                  
"
name
"
:
"
wasm
-
instantiate
"
                  
"
time
"
:
349
                
}
{
                  
"
name
"
:
"
engine
-
instantiate
"
                  
"
time
"
:
1263
                
.
.
.
                
}
]
}
        
"
"
"
        
_subtests
=
{
}
        
data
=
test
[
"
measurements
"
]
[
"
wasm
-
godot
"
]
        
print
(
data
)
        
for
page_cycle
in
data
:
            
for
item
in
page_cycle
[
0
]
:
                
sub
=
item
[
"
name
"
]
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
                        
"
unit
"
:
test
[
"
subtest_unit
"
]
                        
"
alertThreshold
"
:
float
(
test
[
"
alert_threshold
"
]
)
                        
"
lowerIsBetter
"
:
test
[
"
subtest_lower_is_better
"
]
                        
"
name
"
:
sub
                        
"
replicates
"
:
[
]
                    
}
                
_subtests
[
sub
]
[
"
replicates
"
]
.
append
(
item
[
"
time
"
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
"
value
"
]
=
filters
.
median
(
_subtests
[
name
]
[
"
replicates
"
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
"
value
"
]
name
]
)
        
return
subtests
vals
    
def
parseWebaudioOutput
(
self
test
)
:
        
_subtests
=
{
}
        
data
=
test
[
"
measurements
"
]
[
"
webaudio
"
]
        
for
page_cycle
in
data
:
            
data
=
json
.
loads
(
page_cycle
[
0
]
)
            
for
item
in
data
:
                
sub
=
item
[
"
name
"
]
                
replicates
=
[
item
[
"
duration
"
]
]
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
                        
"
unit
"
:
test
[
"
subtest_unit
"
]
                        
"
alertThreshold
"
:
float
(
test
[
"
alert_threshold
"
]
)
                        
"
lowerIsBetter
"
:
test
[
"
subtest_lower_is_better
"
]
                        
"
name
"
:
sub
                        
"
replicates
"
:
[
]
                    
}
                
_subtests
[
sub
]
[
"
replicates
"
]
.
extend
(
[
round
(
x
3
)
for
x
in
replicates
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
"
value
"
]
=
filters
.
median
(
_subtests
[
name
]
[
"
replicates
"
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
"
value
"
]
name
]
)
        
print
(
subtests
)
        
return
subtests
vals
    
def
parseMotionmarkOutput
(
self
test
)
:
        
_subtests
=
{
}
        
data
=
test
[
"
measurements
"
]
[
"
motionmark
"
]
        
for
page_cycle
in
data
:
            
page_cycle_results
=
page_cycle
[
0
]
            
suite
=
page_cycle_results
.
keys
(
)
[
0
]
            
for
sub
in
page_cycle_results
[
suite
]
.
keys
(
)
:
                
try
:
                    
replicate
=
round
(
                        
float
(
page_cycle_results
[
suite
]
[
sub
]
[
"
frameLength
"
]
[
"
average
"
]
)
                        
3
                    
)
                
except
TypeError
as
e
:
                    
LOG
.
warning
(
                        
"
[
{
}
]
[
{
}
]
:
{
}
-
{
}
"
.
format
(
suite
sub
e
.
__class__
.
__name__
e
)
                    
)
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
                        
"
unit
"
:
test
[
"
subtest_unit
"
]
                        
"
alertThreshold
"
:
float
(
test
[
"
alert_threshold
"
]
)
                        
"
lowerIsBetter
"
:
test
[
"
subtest_lower_is_better
"
]
                        
"
name
"
:
sub
                        
"
replicates
"
:
[
]
                    
}
                
_subtests
[
sub
]
[
"
replicates
"
]
.
extend
(
[
replicate
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
"
value
"
]
=
filters
.
median
(
_subtests
[
name
]
[
"
replicates
"
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
"
value
"
]
name
]
)
        
return
subtests
vals
    
def
parseSunspiderOutput
(
self
test
)
:
        
_subtests
=
{
}
        
data
=
test
[
"
measurements
"
]
[
"
sunspider
"
]
        
for
page_cycle
in
data
:
            
for
sub
replicates
in
page_cycle
[
0
]
.
items
(
)
:
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
                        
"
unit
"
:
test
[
"
subtest_unit
"
]
                        
"
alertThreshold
"
:
float
(
test
[
"
alert_threshold
"
]
)
                        
"
lowerIsBetter
"
:
test
[
"
subtest_lower_is_better
"
]
                        
"
name
"
:
sub
                        
"
replicates
"
:
[
]
                    
}
                
_subtests
[
sub
]
[
"
replicates
"
]
.
extend
(
[
round
(
x
3
)
for
x
in
replicates
]
)
        
subtests
=
[
]
        
vals
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
"
value
"
]
=
filters
.
mean
(
_subtests
[
name
]
[
"
replicates
"
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
"
value
"
]
name
]
)
        
return
subtests
vals
    
def
parseUnityWebGLOutput
(
self
test
)
:
        
"
"
"
        
Example
output
(
this
is
one
page
cycle
)
:
        
{
'
name
'
:
'
raptor
-
unity
-
webgl
-
firefox
'
         
'
type
'
:
'
benchmark
'
         
'
measurements
'
:
{
            
'
unity
-
webgl
'
:
[
                
[
                    
'
[
{
"
benchmark
"
:
"
Mandelbrot
GPU
"
"
result
"
:
1035361
}
.
.
.
}
]
'
                
]
            
]
         
}
         
'
lower_is_better
'
:
False
         
'
unit
'
:
'
score
'
        
}
        
"
"
"
        
_subtests
=
{
}
        
data
=
test
[
"
measurements
"
]
[
"
unity
-
webgl
"
]
        
for
page_cycle
in
data
:
            
data
=
json
.
loads
(
page_cycle
[
0
]
)
            
for
item
in
data
:
                
sub
=
item
[
"
benchmark
"
]
                
if
sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
sub
]
=
{
                        
"
unit
"
:
test
[
"
subtest_unit
"
]
                        
"
alertThreshold
"
:
float
(
test
[
"
alert_threshold
"
]
)
                        
"
lowerIsBetter
"
:
test
[
"
subtest_lower_is_better
"
]
                        
"
name
"
:
sub
                        
"
replicates
"
:
[
]
                    
}
                
_subtests
[
sub
]
[
"
replicates
"
]
.
append
(
item
[
"
result
"
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
"
value
"
]
=
filters
.
median
(
_subtests
[
name
]
[
"
replicates
"
]
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
vals
.
append
(
[
_subtests
[
name
]
[
"
value
"
]
name
]
)
        
return
subtests
vals
    
def
parseAssortedDomOutput
(
self
test
)
:
        
_subtests
=
{
}
        
data
=
test
[
"
measurements
"
]
[
"
assorted
-
dom
"
]
        
for
pagecycle
in
data
:
            
for
_sub
_value
in
pagecycle
[
0
]
.
items
(
)
:
                
if
_sub
not
in
_subtests
.
keys
(
)
:
                    
_subtests
[
_sub
]
=
{
                        
"
unit
"
:
test
[
"
subtest_unit
"
]
                        
"
alertThreshold
"
:
float
(
test
[
"
alert_threshold
"
]
)
                        
"
lowerIsBetter
"
:
test
[
"
subtest_lower_is_better
"
]
                        
"
name
"
:
_sub
                        
"
replicates
"
:
[
]
                    
}
                
_subtests
[
_sub
]
[
"
replicates
"
]
.
extend
(
[
_value
]
)
        
vals
=
[
]
        
subtests
=
[
]
        
names
=
_subtests
.
keys
(
)
        
names
.
sort
(
reverse
=
True
)
        
for
name
in
names
:
            
_subtests
[
name
]
[
"
value
"
]
=
round
(
                
filters
.
median
(
_subtests
[
name
]
[
"
replicates
"
]
)
2
            
)
            
subtests
.
append
(
_subtests
[
name
]
)
            
if
name
=
=
"
total
"
:
                
vals
.
append
(
[
_subtests
[
name
]
[
"
value
"
]
name
]
)
        
return
subtests
vals
    
def
summarize_screenshots
(
self
screenshots
)
:
        
if
len
(
screenshots
)
=
=
0
:
            
return
        
self
.
summarized_screenshots
.
append
(
            
"
"
"
<
!
DOCTYPE
html
>
        
<
head
>
        
<
style
>
            
table
th
td
{
              
border
:
1px
solid
black
;
              
border
-
collapse
:
collapse
;
            
}
        
<
/
style
>
        
<
/
head
>
        
<
html
>
<
body
>
        
<
h1
>
Captured
screenshots
!
<
/
h1
>
        
<
table
style
=
"
width
:
100
%
"
>
          
<
tr
>
            
<
th
>
Test
Name
<
/
th
>
            
<
th
>
Pagecycle
<
/
th
>
            
<
th
>
Screenshot
<
/
th
>
          
<
/
tr
>
"
"
"
        
)
        
for
screenshot
in
screenshots
:
            
self
.
summarized_screenshots
.
append
(
                
"
"
"
<
tr
>
            
<
th
>
%
s
<
/
th
>
            
<
th
>
%
s
<
/
th
>
            
<
th
>
                
<
img
src
=
"
%
s
"
alt
=
"
%
s
%
s
"
width
=
"
320
"
height
=
"
240
"
>
            
<
/
th
>
            
<
/
tr
>
"
"
"
                
%
(
                    
screenshot
[
"
test_name
"
]
                    
screenshot
[
"
page_cycle
"
]
                    
screenshot
[
"
screenshot
"
]
                    
screenshot
[
"
test_name
"
]
                    
screenshot
[
"
page_cycle
"
]
                
)
            
)
        
self
.
summarized_screenshots
.
append
(
"
"
"
<
/
table
>
<
/
body
>
<
/
html
>
"
"
"
)
class
BrowsertimeOutput
(
PerftestOutput
)
:
    
"
"
"
class
for
browsertime
output
"
"
"
    
def
summarize
(
self
test_names
)
:
        
"
"
"
        
Summarize
the
parsed
browsertime
test
output
and
format
accordingly
so
the
output
can
        
be
ingested
by
Perfherder
.
        
At
this
point
each
entry
in
self
.
results
for
browsertime
-
pageload
tests
is
in
this
format
:
        
{
'
statistics
'
:
{
'
fcp
'
:
{
u
'
p99
'
:
932
u
'
mdev
'
:
10
.
0941
u
'
min
'
:
712
u
'
p90
'
:
810
u
'
max
'
:
        
932
u
'
median
'
:
758
u
'
p10
'
:
728
u
'
stddev
'
:
50
u
'
mean
'
:
769
}
'
dcf
'
:
{
u
'
p99
'
:
864
        
u
'
mdev
'
:
11
.
6768
u
'
min
'
:
614
u
'
p90
'
:
738
u
'
max
'
:
864
u
'
median
'
:
670
u
'
p10
'
:
632
        
u
'
stddev
'
:
58
u
'
mean
'
:
684
}
'
fnbpaint
'
:
{
u
'
p99
'
:
830
u
'
mdev
'
:
9
.
6851
u
'
min
'
:
616
        
u
'
p90
'
:
719
u
'
max
'
:
830
u
'
median
'
:
668
u
'
p10
'
:
642
u
'
stddev
'
:
48
u
'
mean
'
:
680
}
        
'
loadtime
'
:
{
u
'
p99
'
:
5818
u
'
mdev
'
:
111
.
7028
u
'
min
'
:
3220
u
'
p90
'
:
4450
u
'
max
'
:
5818
        
u
'
median
'
:
3476
u
'
p10
'
:
3241
u
'
stddev
'
:
559
u
'
mean
'
:
3642
}
}
'
name
'
:
        
'
raptor
-
tp6
-
guardian
-
firefox
'
'
url
'
:
'
https
:
/
/
www
.
theguardian
.
co
.
uk
'
'
lower_is_better
'
:
        
True
'
measurements
'
:
{
'
fcp
'
:
[
932
744
744
810
712
775
759
744
777
739
809
906
        
734
742
760
758
728
792
757
759
742
759
775
726
730
]
'
dcf
'
:
[
864
679
637
        
662
652
651
710
679
646
689
686
845
670
694
632
703
670
738
633
703
614
        
703
650
622
670
]
'
fnbpaint
'
:
[
830
648
666
704
616
683
678
650
685
651
719
        
820
634
664
681
664
642
703
668
670
669
668
681
652
642
]
'
loadtime
'
:
[
4450
        
3592
3770
3345
3453
3220
3434
3621
3511
3416
3430
5818
4729
3406
3506
3588
        
3245
3381
3707
3241
3595
3483
3236
3390
3476
]
}
'
subtest_unit
'
:
'
ms
'
'
bt_ver
'
:
        
'
4
.
9
.
2
-
android
'
'
alert_threshold
'
:
2
'
cold
'
:
True
'
type
'
:
'
browsertime
-
pageload
'
        
'
unit
'
:
'
ms
'
'
browser
'
:
"
{
u
'
userAgent
'
:
u
'
Mozilla
/
5
.
0
(
Macintosh
;
Intel
Mac
OS
X
10
.
13
;
        
rv
:
70
.
0
)
Gecko
/
20100101
Firefox
/
70
.
0
'
u
'
windowSize
'
:
u
'
1366x694
'
}
"
}
        
Now
we
must
process
this
further
and
prepare
the
result
for
output
suitable
for
perfherder
        
ingestion
.
        
Note
:
For
the
overall
subtest
values
/
results
(
i
.
e
.
for
each
measurement
type
)
we
will
use
        
the
Browsertime
-
provided
statistics
instead
of
calcuating
our
own
geomeans
from
the
        
replicates
.
        
"
"
"
        
def
_process
(
subtest
)
:
            
subtest
[
"
value
"
]
=
filters
.
median
(
                
filters
.
ignore_first
(
subtest
[
"
replicates
"
]
1
)
            
)
            
return
subtest
        
def
_process_suite
(
suite
)
:
            
suite
[
"
subtests
"
]
=
[
                
_process
(
subtest
)
                
for
subtest
in
suite
[
"
subtests
"
]
.
values
(
)
                
if
subtest
[
"
replicates
"
]
            
]
            
suite
[
"
subtests
"
]
.
sort
(
key
=
lambda
subtest
:
subtest
[
"
name
"
]
)
            
if
len
(
suite
[
"
subtests
"
]
)
>
1
:
                
vals
=
[
                    
[
subtest
[
"
value
"
]
subtest
[
"
name
"
]
]
for
subtest
in
suite
[
"
subtests
"
]
                
]
                
suite
[
"
value
"
]
=
self
.
construct_summary
(
vals
testname
=
test
[
"
name
"
]
)
            
return
suite
        
LOG
.
info
(
"
preparing
browsertime
results
for
output
"
)
        
if
len
(
self
.
results
)
=
=
0
:
            
LOG
.
error
(
                
"
no
browsertime
test
results
found
for
%
s
"
%
"
"
.
join
(
test_names
)
            
)
            
return
        
test_results
=
{
"
framework
"
:
{
"
name
"
:
"
browsertime
"
}
}
        
suites
=
{
}
        
for
test
in
self
.
results
:
            
test_name
=
test
[
"
name
"
]
            
extra_options
=
test
[
"
extra_options
"
]
            
if
test_name
in
suites
and
suites
[
test_name
]
[
"
extraOptions
"
]
!
=
extra_options
:
                
missing
=
set
(
extra_options
)
-
set
(
suites
[
test_name
]
[
"
extraOptions
"
]
)
                
test_name
=
test_name
+
"
-
"
.
join
(
list
(
missing
)
)
            
suite
=
suites
.
setdefault
(
                
test_name
                
{
                    
"
name
"
:
test
[
"
name
"
]
                    
"
type
"
:
test
[
"
type
"
]
                    
"
extraOptions
"
:
extra_options
                    
"
lowerIsBetter
"
:
test
[
"
lower_is_better
"
]
                    
"
unit
"
:
test
[
"
unit
"
]
                    
"
alertThreshold
"
:
float
(
test
[
"
alert_threshold
"
]
)
                    
"
subtests
"
:
{
}
                
}
            
)
            
if
"
alert_change_type
"
in
test
and
"
alertChangeType
"
not
in
suite
:
                
suite
[
"
alertChangeType
"
]
=
test
[
"
alert_change_type
"
]
            
if
(
"
pageload
"
or
"
scenario
"
)
in
test
[
"
type
"
]
:
                
for
measurement_name
replicates
in
test
[
"
measurements
"
]
.
items
(
)
:
                    
if
measurement_name
not
in
suite
[
"
subtests
"
]
:
                        
subtest
=
{
}
                        
subtest
[
"
name
"
]
=
measurement_name
                        
subtest
[
"
lowerIsBetter
"
]
=
test
[
"
subtest_lower_is_better
"
]
                        
subtest
[
"
alertThreshold
"
]
=
float
(
test
[
"
alert_threshold
"
]
)
                        
subtest
[
"
unit
"
]
=
test
[
"
subtest_unit
"
]
                        
if
self
.
subtest_alert_on
is
not
None
:
                            
if
measurement_name
in
self
.
subtest_alert_on
:
                                
LOG
.
info
(
                                    
"
turning
on
subtest
alerting
for
measurement
type
:
%
s
"
                                    
%
measurement_name
                                
)
                                
subtest
[
"
shouldAlert
"
]
=
True
                        
subtest
[
"
replicates
"
]
=
[
]
                        
suite
[
"
subtests
"
]
[
measurement_name
]
=
subtest
                    
else
:
                        
subtest
=
suite
[
"
subtests
"
]
[
measurement_name
]
                    
subtest
[
"
replicates
"
]
.
extend
(
replicates
)
            
elif
"
benchmark
"
in
test
[
"
type
"
]
:
                
if
"
speedometer
"
in
test
[
"
name
"
]
:
                    
subtests
vals
=
self
.
parseSpeedometerOutput
(
test
)
                
if
"
ares6
"
in
test
[
"
name
"
]
:
                    
subtests
vals
=
self
.
parseAresSixOutput
(
test
)
                
if
any
(
"
youtube
-
playback
"
in
key
for
key
in
test
[
"
measurements
"
]
.
keys
(
)
)
:
                    
subtests
vals
=
self
.
parseYoutubePlaybackPerformanceOutput
(
test
)
                
suite
[
"
subtests
"
]
=
subtests
                
if
len
(
subtests
)
>
1
:
                    
suite
[
"
value
"
]
=
self
.
construct_summary
(
vals
testname
=
test
[
"
name
"
]
)
                
subtests
.
sort
(
key
=
lambda
subtest
:
subtest
[
"
name
"
]
)
        
suites
=
[
            
s
if
"
benchmark
"
in
test
[
"
type
"
]
else
_process_suite
(
s
)
            
for
s
in
suites
.
values
(
)
        
]
        
suites
.
sort
(
key
=
lambda
suite
:
suite
[
"
name
"
]
)
        
test_results
[
"
suites
"
]
=
suites
        
self
.
summarized_results
=
test_results
