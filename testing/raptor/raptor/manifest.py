from
__future__
import
absolute_import
import
json
import
os
from
logger
.
logger
import
RaptorLogger
from
manifestparser
import
TestManifest
from
utils
import
transform_platform
here
=
os
.
path
.
abspath
(
os
.
path
.
dirname
(
__file__
)
)
raptor_ini
=
os
.
path
.
join
(
here
'
raptor
.
ini
'
)
tests_dir
=
os
.
path
.
join
(
here
'
tests
'
)
LOG
=
RaptorLogger
(
component
=
'
raptor
-
manifest
'
)
LIVE_SITE_TIMEOUT_MULTIPLIER
=
1
.
2
required_settings
=
[
    
'
alert_threshold
'
    
'
apps
'
    
'
lower_is_better
'
    
'
measure
'
    
'
page_cycles
'
    
'
test_url
'
    
'
scenario_time
'
    
'
type
'
    
'
unit
'
]
playback_settings
=
[
    
'
playback_pageset_manifest
'
    
'
playback_recordings
'
]
whitelist_live_site_tests
=
[
    
"
raptor
-
youtube
-
playback
"
]
def
filter_app
(
tests
values
)
:
    
for
test
in
tests
:
        
if
values
[
"
app
"
]
in
test
[
'
apps
'
]
:
            
yield
test
def
filter_live_sites
(
tests
values
)
:
    
for
test
in
tests
:
        
if
test
.
get
(
"
use_live_sites
"
"
false
"
)
=
=
"
true
"
:
            
if
values
[
"
run_local
"
]
is
True
:
                
yield
test
            
elif
"
hg
.
mozilla
.
org
/
try
"
in
os
.
environ
.
get
(
'
GECKO_HEAD_REPOSITORY
'
'
n
/
a
'
)
:
                
yield
test
            
elif
filter
(
lambda
name
:
test
[
'
name
'
]
.
startswith
(
name
)
whitelist_live_site_tests
)
:
                
yield
test
            
else
:
                
LOG
.
warning
(
'
%
s
is
not
allowed
to
run
with
use_live_sites
'
%
test
[
'
name
'
]
)
        
else
:
            
yield
test
def
get_browser_test_list
(
browser_app
run_local
)
:
    
LOG
.
info
(
raptor_ini
)
    
test_manifest
=
TestManifest
(
[
raptor_ini
]
strict
=
False
)
    
info
=
{
"
app
"
:
browser_app
"
run_local
"
:
run_local
}
    
return
test_manifest
.
active_tests
(
exists
=
False
                                      
disabled
=
False
                                      
filters
=
[
filter_app
filter_live_sites
]
                                      
*
*
info
)
def
validate_test_ini
(
test_details
)
:
    
valid_settings
=
True
    
for
setting
in
required_settings
:
        
if
setting
=
=
'
measure
'
and
test_details
[
'
type
'
]
=
=
'
benchmark
'
:
            
continue
        
if
setting
=
=
'
scenario_time
'
and
test_details
[
'
type
'
]
!
=
'
scenario
'
:
            
continue
        
if
test_details
.
get
(
setting
)
is
None
:
            
if
setting
=
=
"
page
-
cycles
"
and
test_details
.
get
(
'
browser_cycles
'
)
is
not
None
:
                
continue
            
valid_settings
=
False
            
LOG
.
error
(
"
setting
'
%
s
'
is
required
but
not
found
in
%
s
"
                      
%
(
setting
test_details
[
'
manifest
'
]
)
)
    
test_details
.
setdefault
(
"
page_timeout
"
30000
)
    
if
test_details
.
get
(
'
playback
'
)
is
not
None
:
        
for
setting
in
playback_settings
:
            
if
test_details
.
get
(
setting
)
is
None
:
                
valid_settings
=
False
                
LOG
.
error
(
"
setting
'
%
s
'
is
required
but
not
found
in
%
s
"
                          
%
(
setting
test_details
[
'
manifest
'
]
)
)
    
if
test_details
.
get
(
'
alert_on
'
)
is
not
None
:
        
test_details
[
'
alert_on
'
]
=
[
_item
.
strip
(
)
for
_item
in
test_details
[
'
alert_on
'
]
.
split
(
'
'
)
]
        
for
alert_on_value
in
test_details
[
'
alert_on
'
]
:
            
if
alert_on_value
not
in
test_details
[
'
measure
'
]
:
                
LOG
.
error
(
"
The
'
alert_on
'
value
of
'
%
s
'
is
not
valid
because
"
                          
"
it
doesn
'
t
exist
in
the
'
measure
'
test
setting
!
"
                          
%
alert_on_value
)
                
valid_settings
=
False
    
return
valid_settings
def
write_test_settings_json
(
args
test_details
oskey
)
:
    
test_url
=
transform_platform
(
test_details
[
'
test_url
'
]
oskey
)
    
test_settings
=
{
        
"
raptor
-
options
"
:
{
            
"
type
"
:
test_details
[
'
type
'
]
            
"
cold
"
:
test_details
[
'
cold
'
]
            
"
test_url
"
:
test_url
            
"
expected_browser_cycles
"
:
test_details
[
'
expected_browser_cycles
'
]
            
"
page_cycles
"
:
int
(
test_details
[
'
page_cycles
'
]
)
            
"
host
"
:
args
.
host
        
}
    
}
    
if
test_details
[
'
type
'
]
=
=
"
pageload
"
:
        
test_settings
[
'
raptor
-
options
'
]
[
'
measure
'
]
=
{
}
        
for
m
in
test_details
[
'
measure
'
]
:
            
test_settings
[
'
raptor
-
options
'
]
[
'
measure
'
]
[
m
]
=
True
            
if
m
=
=
'
hero
'
:
                
test_settings
[
'
raptor
-
options
'
]
[
'
measure
'
]
[
m
]
=
[
h
.
strip
(
)
for
h
in
                                                                 
test_details
[
'
hero
'
]
.
split
(
'
'
)
]
        
if
test_details
.
get
(
"
alert_on
"
None
)
is
not
None
:
            
test_settings
[
'
raptor
-
options
'
]
[
'
alert_on
'
]
=
test_details
[
'
alert_on
'
]
    
if
test_details
.
get
(
"
page_timeout
"
None
)
is
not
None
:
        
test_settings
[
'
raptor
-
options
'
]
[
'
page_timeout
'
]
=
int
(
test_details
[
'
page_timeout
'
]
)
    
test_settings
[
'
raptor
-
options
'
]
[
'
unit
'
]
=
test_details
.
get
(
"
unit
"
"
ms
"
)
    
test_settings
[
'
raptor
-
options
'
]
[
'
lower_is_better
'
]
=
bool_from_str
(
        
test_details
.
get
(
"
lower_is_better
"
"
true
"
)
)
    
val
=
test_details
.
get
(
'
subtest_unit
'
test_settings
[
'
raptor
-
options
'
]
[
'
unit
'
]
)
    
test_settings
[
'
raptor
-
options
'
]
[
'
subtest_unit
'
]
=
val
    
subtest_lower_is_better
=
test_details
.
get
(
'
subtest_lower_is_better
'
None
)
    
if
subtest_lower_is_better
is
None
:
        
test_settings
[
'
raptor
-
options
'
]
[
'
subtest_lower_is_better
'
]
=
(
            
test_settings
[
'
raptor
-
options
'
]
[
'
lower_is_better
'
]
)
    
else
:
        
test_settings
[
'
raptor
-
options
'
]
[
'
subtest_lower_is_better
'
]
=
bool_from_str
(
            
subtest_lower_is_better
)
    
if
test_details
.
get
(
"
alert_change_type
"
None
)
is
not
None
:
        
test_settings
[
'
raptor
-
options
'
]
[
'
alert_change_type
'
]
=
test_details
[
'
alert_change_type
'
]
    
if
test_details
.
get
(
"
alert_threshold
"
None
)
is
not
None
:
        
test_settings
[
'
raptor
-
options
'
]
[
'
alert_threshold
'
]
=
float
(
test_details
[
'
alert_threshold
'
]
)
    
if
test_details
.
get
(
"
screen_capture
"
None
)
is
not
None
:
        
test_settings
[
'
raptor
-
options
'
]
[
'
screen_capture
'
]
=
test_details
.
get
(
"
screen_capture
"
)
    
if
test_details
.
get
(
"
gecko_profile
"
False
)
:
        
threads
=
[
'
GeckoMain
'
'
Compositor
'
]
        
if
os
.
getenv
(
'
MOZ_WEBRENDER
'
)
=
=
'
1
'
:
            
threads
.
extend
(
[
'
Renderer
'
'
WR
'
]
)
        
if
test_details
.
get
(
'
gecko_profile_threads
'
)
:
            
test_threads
=
filter
(
None
test_details
[
'
gecko_profile_threads
'
]
.
split
(
'
'
)
)
            
threads
.
extend
(
test_threads
)
        
test_settings
[
'
raptor
-
options
'
]
.
update
(
{
            
'
gecko_profile
'
:
True
            
'
gecko_profile_entries
'
:
int
(
test_details
.
get
(
'
gecko_profile_entries
'
)
)
            
'
gecko_profile_interval
'
:
int
(
test_details
.
get
(
'
gecko_profile_interval
'
)
)
            
'
gecko_profile_threads
'
:
'
'
.
join
(
set
(
threads
)
)
        
}
)
    
if
test_details
.
get
(
"
newtab_per_cycle
"
None
)
is
not
None
:
        
test_settings
[
'
raptor
-
options
'
]
[
'
newtab_per_cycle
'
]
=
\
            
bool
(
test_details
[
'
newtab_per_cycle
'
]
)
    
if
test_details
[
'
type
'
]
=
=
"
scenario
"
:
        
test_settings
[
'
raptor
-
options
'
]
[
'
scenario_time
'
]
=
test_details
[
'
scenario_time
'
]
        
if
'
background_test
'
in
test_details
:
            
test_settings
[
'
raptor
-
options
'
]
[
'
background_test
'
]
=
\
                
bool
(
test_details
[
'
background_test
'
]
)
        
else
:
            
test_settings
[
'
raptor
-
options
'
]
[
'
background_test
'
]
=
False
    
jsons_dir
=
os
.
path
.
join
(
tests_dir
'
json
'
)
    
if
not
os
.
path
.
exists
(
jsons_dir
)
:
        
os
.
mkdir
(
os
.
path
.
join
(
tests_dir
'
json
'
)
)
    
settings_file
=
os
.
path
.
join
(
jsons_dir
test_details
[
'
name
'
]
+
'
.
json
'
)
    
try
:
        
with
open
(
settings_file
'
w
'
)
as
out_file
:
            
json
.
dump
(
test_settings
out_file
indent
=
4
ensure_ascii
=
False
)
            
out_file
.
close
(
)
    
except
IOError
:
        
LOG
.
info
(
"
abort
:
exception
writing
test
settings
json
!
"
)
def
get_raptor_test_list
(
args
oskey
)
:
    
'
'
'
    
A
test
ini
(
i
.
e
.
raptor
-
firefox
-
tp6
.
ini
)
will
have
one
or
more
subtests
inside
    
each
with
it
'
s
own
name
(
[
the
-
ini
-
file
-
test
-
section
]
)
.
    
We
want
the
ability
to
eiter
:
        
-
run
*
all
*
of
the
subtests
listed
inside
the
test
ini
;
-
or
-
        
-
just
run
a
single
one
of
those
subtests
that
are
inside
the
ini
    
A
test
name
is
received
on
the
command
line
.
This
will
either
match
the
name
    
of
a
single
subtest
(
within
an
ini
)
-
or
-
if
there
'
s
no
matching
single
    
subtest
with
that
name
then
the
test
name
provided
might
be
the
name
of
a
    
test
ini
itself
(
i
.
e
.
raptor
-
firefox
-
tp6
)
that
contains
multiple
subtests
.
    
First
look
for
a
single
matching
subtest
name
in
the
list
of
all
availble
tests
    
and
if
it
'
s
found
we
will
just
run
that
single
subtest
.
    
Then
look
at
the
list
of
all
available
tests
-
each
available
test
has
a
manifest
    
name
associated
to
it
-
and
pull
out
all
subtests
whose
manifest
name
matches
    
the
test
name
provided
on
the
command
line
i
.
e
.
run
all
subtests
in
a
specified
ini
.
    
If
no
tests
are
found
at
all
then
the
test
name
is
invalid
.
    
'
'
'
    
tests_to_run
=
[
]
    
available_tests
=
get_browser_test_list
(
args
.
app
args
.
run_local
)
    
for
next_test
in
available_tests
:
        
if
next_test
[
'
name
'
]
=
=
args
.
test
:
            
tests_to_run
.
append
(
next_test
)
            
break
    
if
len
(
tests_to_run
)
=
=
0
:
        
_ini
=
args
.
test
+
"
.
ini
"
        
for
next_test
in
available_tests
:
            
head
tail
=
os
.
path
.
split
(
next_test
[
'
manifest
'
]
)
            
if
tail
=
=
_ini
:
                
tests_to_run
.
append
(
next_test
)
    
for
next_test
in
tests_to_run
:
        
LOG
.
info
(
"
configuring
settings
for
test
%
s
"
%
next_test
[
'
name
'
]
)
        
max_page_cycles
=
next_test
.
get
(
'
page_cycles
'
1
)
        
if
args
.
gecko_profile
is
True
:
            
next_test
[
'
gecko_profile
'
]
=
True
            
LOG
.
info
(
'
gecko
-
profiling
enabled
'
)
            
max_page_cycles
=
3
            
if
'
gecko_profile_entries
'
in
args
and
args
.
gecko_profile_entries
is
not
None
:
                
next_test
[
'
gecko_profile_entries
'
]
=
str
(
args
.
gecko_profile_entries
)
                
LOG
.
info
(
'
gecko
-
profiling
entries
set
to
%
s
'
%
args
.
gecko_profile_entries
)
            
if
'
gecko_profile_interval
'
in
args
and
args
.
gecko_profile_interval
is
not
None
:
                
next_test
[
'
gecko_profile_interval
'
]
=
str
(
args
.
gecko_profile_interval
)
                
LOG
.
info
(
'
gecko
-
profiling
interval
set
to
%
s
'
%
args
.
gecko_profile_interval
)
            
if
'
gecko_profile_threads
'
in
args
and
args
.
gecko_profile_threads
is
not
None
:
                
threads
=
filter
(
None
next_test
.
get
(
'
gecko_profile_threads
'
'
'
)
.
split
(
'
'
)
)
                
threads
.
extend
(
args
.
gecko_profile_threads
)
                
next_test
[
'
gecko_profile_threads
'
]
=
'
'
.
join
(
threads
)
                
LOG
.
info
(
'
gecko
-
profiling
extra
threads
%
s
'
%
args
.
gecko_profile_threads
)
        
else
:
            
next_test
.
pop
(
'
gecko_profile_entries
'
None
)
            
next_test
.
pop
(
'
gecko_profile_interval
'
None
)
            
next_test
.
pop
(
'
gecko_profile_threads
'
None
)
        
if
args
.
debug_mode
is
True
:
            
next_test
[
'
debug_mode
'
]
=
True
            
LOG
.
info
(
"
debug
-
mode
enabled
"
)
            
max_page_cycles
=
2
        
if
args
.
page_cycles
is
not
None
:
            
next_test
[
'
page_cycles
'
]
=
args
.
page_cycles
            
LOG
.
info
(
"
set
page
-
cycles
to
%
d
as
specified
on
cmd
line
"
%
args
.
page_cycles
)
        
else
:
            
if
int
(
next_test
.
get
(
'
page_cycles
'
1
)
)
>
max_page_cycles
:
                
next_test
[
'
page_cycles
'
]
=
max_page_cycles
                
LOG
.
info
(
"
page
-
cycles
set
to
%
d
"
%
next_test
[
'
page_cycles
'
]
)
        
if
args
.
page_timeout
is
not
None
:
            
LOG
.
info
(
"
setting
page
-
timeout
to
%
d
as
specified
on
cmd
line
"
%
args
.
page_timeout
)
            
next_test
[
'
page_timeout
'
]
=
args
.
page_timeout
        
if
args
.
browser_cycles
is
not
None
:
            
LOG
.
info
(
"
setting
browser
-
cycles
to
%
d
as
specified
on
cmd
line
"
%
args
.
browser_cycles
)
            
next_test
[
'
browser_cycles
'
]
=
args
.
browser_cycles
        
if
next_test
.
get
(
"
cold
"
"
false
"
)
=
=
"
true
"
:
            
next_test
[
'
cold
'
]
=
True
            
next_test
[
'
expected_browser_cycles
'
]
=
int
(
next_test
[
'
browser_cycles
'
]
)
            
next_test
[
'
page_cycles
'
]
=
1
            
if
"
-
cold
"
not
in
next_test
[
'
name
'
]
:
                
next_test
[
'
name
'
]
+
=
"
-
cold
"
        
else
:
            
next_test
[
'
cold
'
]
=
False
            
next_test
[
'
expected_browser_cycles
'
]
=
1
        
next_test
[
'
browser_cycle
'
]
=
1
        
if
next_test
.
get
(
'
use_live_sites
'
"
false
"
)
=
=
"
true
"
:
            
LOG
.
info
(
"
using
live
sites
so
turning
playback
off
!
"
)
            
next_test
[
'
playback
'
]
=
None
            
LOG
.
info
(
"
using
live
sites
so
appending
'
-
live
'
to
the
test
name
"
)
            
next_test
[
'
name
'
]
=
next_test
[
'
name
'
]
+
"
-
live
"
            
next_test
[
'
page_timeout
'
]
=
int
(
                
next_test
[
'
page_timeout
'
]
)
*
LIVE_SITE_TIMEOUT_MULTIPLIER
            
LOG
.
info
(
"
using
live
sites
so
using
page
timeout
of
%
dms
"
%
next_test
[
'
page_timeout
'
]
)
        
if
next_test
.
get
(
'
measure
'
)
is
not
None
:
            
_measures
=
[
]
            
for
m
in
[
m
.
strip
(
)
for
m
in
next_test
[
'
measure
'
]
.
split
(
'
'
)
]
:
                
_measures
.
append
(
m
)
            
next_test
[
'
measure
'
]
=
_measures
            
if
'
hero
'
in
next_test
[
'
measure
'
]
and
\
               
next_test
.
get
(
'
use_live_sites
'
"
false
"
)
=
=
"
true
"
:
                
next_test
[
'
measure
'
]
.
remove
(
'
hero
'
)
                
del
next_test
[
'
hero
'
]
    
if
len
(
tests_to_run
)
!
=
0
:
        
for
test
in
tests_to_run
:
            
if
validate_test_ini
(
test
)
:
                
write_test_settings_json
(
args
test
oskey
)
            
else
:
                
LOG
.
info
(
"
test
%
s
is
not
valid
due
to
missing
settings
"
%
test
[
'
name
'
]
)
                
tests_to_run
.
remove
(
test
)
    
else
:
        
LOG
.
critical
(
"
abort
:
specified
test
name
doesn
'
t
exist
"
)
    
return
tests_to_run
def
bool_from_str
(
boolean_string
)
:
    
if
boolean_string
=
=
'
true
'
:
        
return
True
    
elif
boolean_string
=
=
'
false
'
:
        
return
False
    
else
:
        
raise
ValueError
(
"
Expected
either
'
true
'
or
'
false
'
"
)
