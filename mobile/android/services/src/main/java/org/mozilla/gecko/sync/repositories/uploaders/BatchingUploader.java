package
org
.
mozilla
.
gecko
.
sync
.
repositories
.
uploaders
;
import
android
.
net
.
Uri
;
import
android
.
support
.
annotation
.
VisibleForTesting
;
import
org
.
json
.
simple
.
JSONObject
;
import
org
.
mozilla
.
gecko
.
background
.
common
.
log
.
Logger
;
import
org
.
mozilla
.
gecko
.
sync
.
CryptoRecord
;
import
org
.
mozilla
.
gecko
.
sync
.
InfoConfiguration
;
import
org
.
mozilla
.
gecko
.
sync
.
Server15PreviousPostFailedException
;
import
org
.
mozilla
.
gecko
.
sync
.
net
.
AuthHeaderProvider
;
import
org
.
mozilla
.
gecko
.
sync
.
repositories
.
RepositorySession
;
import
org
.
mozilla
.
gecko
.
sync
.
repositories
.
delegates
.
RepositorySessionStoreDelegate
;
import
org
.
mozilla
.
gecko
.
sync
.
repositories
.
domain
.
Record
;
import
java
.
util
.
ArrayList
;
import
java
.
util
.
concurrent
.
ExecutorService
;
import
java
.
util
.
concurrent
.
atomic
.
AtomicLong
;
public
class
BatchingUploader
{
private
static
final
String
LOG_TAG
=
"
BatchingUploader
"
;
private
static
final
int
PER_RECORD_OVERHEAD_BYTE_COUNT
=
RecordUploadRunnable
.
RECORD_SEPARATOR
.
length
;
static
final
int
PER_PAYLOAD_OVERHEAD_BYTE_COUNT
=
RecordUploadRunnable
.
RECORDS_END
.
length
;
static
{
if
(
RecordUploadRunnable
.
RECORD_SEPARATOR
.
length
!
=
RecordUploadRunnable
.
RECORDS_START
.
length
)
{
throw
new
IllegalStateException
(
"
Separator
and
start
tokens
must
be
of
the
same
length
"
)
;
}
}
private
final
ExecutorService
executor
;
private
volatile
Payload
payload
;
final
Uri
collectionUri
;
final
RepositorySessionStoreDelegate
sessionStoreDelegate
;
VisibleForTesting
final
PayloadDispatcher
payloadDispatcher
;
final
AuthHeaderProvider
authHeaderProvider
;
private
final
RepositorySession
repositorySession
;
private
volatile
UploaderMeta
uploaderMeta
;
private
final
Object
payloadLock
=
new
Object
(
)
;
private
final
long
maxPayloadFieldBytes
;
public
BatchingUploader
(
final
RepositorySession
repositorySession
final
ExecutorService
workQueue
final
RepositorySessionStoreDelegate
sessionStoreDelegate
final
Uri
baseCollectionUri
final
Long
localCollectionLastModified
final
InfoConfiguration
infoConfiguration
final
AuthHeaderProvider
authHeaderProvider
)
{
this
.
repositorySession
=
repositorySession
;
this
.
sessionStoreDelegate
=
sessionStoreDelegate
;
this
.
collectionUri
=
baseCollectionUri
;
this
.
authHeaderProvider
=
authHeaderProvider
;
this
.
uploaderMeta
=
new
UploaderMeta
(
payloadLock
infoConfiguration
.
maxTotalBytes
infoConfiguration
.
maxTotalRecords
)
;
this
.
payload
=
new
Payload
(
payloadLock
infoConfiguration
.
maxPostBytes
infoConfiguration
.
maxPostRecords
)
;
this
.
payloadDispatcher
=
createPayloadDispatcher
(
workQueue
localCollectionLastModified
)
;
this
.
maxPayloadFieldBytes
=
infoConfiguration
.
maxPayloadBytes
;
this
.
executor
=
workQueue
;
}
public
void
process
(
final
Record
record
)
{
final
String
guid
=
record
.
guid
;
if
(
payloadDispatcher
.
storeFailed
)
{
return
;
}
if
(
payloadDispatcher
.
recordUploadFailed
)
{
sessionStoreDelegate
.
deferredStoreDelegate
(
executor
)
.
onRecordStoreFailed
(
new
Server15PreviousPostFailedException
(
)
guid
)
;
return
;
}
final
JSONObject
recordJSON
=
record
.
toJSONObject
(
)
;
final
String
payloadField
=
(
String
)
recordJSON
.
get
(
CryptoRecord
.
KEY_PAYLOAD
)
;
if
(
payloadField
=
=
null
)
{
sessionStoreDelegate
.
deferredStoreDelegate
(
executor
)
.
onRecordStoreFailed
(
new
IllegalRecordException
(
)
guid
)
;
return
;
}
if
(
payloadField
.
length
(
)
>
this
.
maxPayloadFieldBytes
)
{
sessionStoreDelegate
.
deferredStoreDelegate
(
executor
)
.
onRecordStoreFailed
(
new
PayloadTooLargeToUpload
(
)
guid
)
;
return
;
}
final
byte
[
]
recordBytes
=
Record
.
stringToJSONBytes
(
recordJSON
.
toJSONString
(
)
)
;
if
(
recordBytes
=
=
null
)
{
sessionStoreDelegate
.
deferredStoreDelegate
(
executor
)
.
onRecordStoreFailed
(
new
IllegalRecordException
(
)
guid
)
;
return
;
}
final
long
recordDeltaByteCount
=
recordBytes
.
length
+
PER_RECORD_OVERHEAD_BYTE_COUNT
;
Logger
.
debug
(
LOG_TAG
"
Processing
a
record
with
guid
:
"
+
guid
)
;
if
(
(
recordDeltaByteCount
+
PER_PAYLOAD_OVERHEAD_BYTE_COUNT
)
>
payload
.
maxBytes
)
{
sessionStoreDelegate
.
deferredStoreDelegate
(
executor
)
.
onRecordStoreFailed
(
new
RecordTooLargeToUpload
(
)
guid
)
;
return
;
}
synchronized
(
payloadLock
)
{
final
boolean
canFitRecordIntoBatch
=
uploaderMeta
.
canFit
(
recordDeltaByteCount
)
;
final
boolean
canFitRecordIntoPayload
=
payload
.
canFit
(
recordDeltaByteCount
)
;
if
(
canFitRecordIntoBatch
&
&
canFitRecordIntoPayload
)
{
Logger
.
debug
(
LOG_TAG
"
Record
fits
into
the
current
batch
and
payload
"
)
;
addAndFlushIfNecessary
(
recordDeltaByteCount
recordBytes
guid
)
;
}
else
if
(
canFitRecordIntoBatch
)
{
Logger
.
debug
(
LOG_TAG
"
Current
payload
won
'
t
fit
incoming
record
uploading
payload
.
"
)
;
flush
(
false
false
)
;
Logger
.
debug
(
LOG_TAG
"
Recording
the
incoming
record
into
a
new
payload
"
)
;
addAndFlushIfNecessary
(
recordDeltaByteCount
recordBytes
guid
)
;
}
else
{
Logger
.
debug
(
LOG_TAG
"
Current
batch
won
'
t
fit
incoming
record
committing
batch
.
"
)
;
flush
(
true
false
)
;
Logger
.
debug
(
LOG_TAG
"
Recording
the
incoming
record
into
a
new
batch
"
)
;
addAndFlushIfNecessary
(
recordDeltaByteCount
recordBytes
guid
)
;
}
}
}
private
void
addAndFlushIfNecessary
(
long
byteCount
byte
[
]
recordBytes
String
guid
)
{
boolean
isPayloadFull
=
payload
.
addAndEstimateIfFull
(
byteCount
recordBytes
guid
)
;
boolean
isBatchFull
=
uploaderMeta
.
addAndEstimateIfFull
(
byteCount
)
;
if
(
isBatchFull
)
{
flush
(
true
false
)
;
}
else
if
(
isPayloadFull
)
{
flush
(
false
false
)
;
}
}
public
void
noMoreRecordsToUpload
(
)
{
Logger
.
debug
(
LOG_TAG
"
Received
'
no
more
records
to
upload
'
signal
.
"
)
;
if
(
!
payload
.
isEmpty
(
)
)
{
flush
(
true
true
)
;
return
;
}
payloadDispatcher
.
finalizeQueue
(
uploaderMeta
.
needToCommit
(
)
new
Runnable
(
)
{
Override
public
void
run
(
)
{
flush
(
true
true
)
;
}
}
)
;
}
void
finished
(
AtomicLong
lastModifiedTimestamp
)
{
sessionStoreDelegate
.
deferredStoreDelegate
(
executor
)
.
onStoreCompleted
(
lastModifiedTimestamp
.
get
(
)
)
;
}
void
setUnlimitedMode
(
boolean
isUnlimited
)
{
this
.
uploaderMeta
.
setIsUnlimited
(
isUnlimited
)
;
}
private
void
flush
(
final
boolean
isCommit
final
boolean
isLastPayload
)
{
final
ArrayList
<
byte
[
]
>
outgoing
;
final
ArrayList
<
String
>
outgoingGuids
;
final
long
byteCount
;
synchronized
(
payloadLock
)
{
outgoing
=
payload
.
getRecordsBuffer
(
)
;
outgoingGuids
=
payload
.
getRecordGuidsBuffer
(
)
;
byteCount
=
payload
.
getByteCount
(
)
;
}
payload
=
payload
.
nextPayload
(
)
;
payloadDispatcher
.
queue
(
outgoing
outgoingGuids
byteCount
isCommit
isLastPayload
)
;
if
(
isCommit
&
&
!
isLastPayload
)
{
uploaderMeta
=
uploaderMeta
.
nextUploaderMeta
(
)
;
}
}
VisibleForTesting
PayloadDispatcher
createPayloadDispatcher
(
ExecutorService
workQueue
Long
localCollectionLastModified
)
{
return
new
PayloadDispatcher
(
workQueue
this
localCollectionLastModified
)
;
}
static
class
BatchingUploaderException
extends
Exception
{
private
static
final
long
serialVersionUID
=
1L
;
}
static
class
LastModifiedDidNotChange
extends
BatchingUploaderException
{
private
static
final
long
serialVersionUID
=
1L
;
}
static
class
LastModifiedChangedUnexpectedly
extends
BatchingUploaderException
{
private
static
final
long
serialVersionUID
=
1L
;
}
static
class
TokenModifiedException
extends
BatchingUploaderException
{
private
static
final
long
serialVersionUID
=
1L
;
}
private
static
class
RecordTooLargeToUpload
extends
BatchingUploaderException
{
private
static
final
long
serialVersionUID
=
1L
;
}
VisibleForTesting
static
class
PayloadTooLargeToUpload
extends
BatchingUploaderException
{
private
static
final
long
serialVersionUID
=
1L
;
}
private
static
class
IllegalRecordException
extends
BatchingUploaderException
{
private
static
final
long
serialVersionUID
=
1L
;
}
}
