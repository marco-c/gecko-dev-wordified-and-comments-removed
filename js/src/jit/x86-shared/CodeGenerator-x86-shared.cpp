#
include
"
jit
/
x86
-
shared
/
CodeGenerator
-
x86
-
shared
.
h
"
#
include
"
mozilla
/
DebugOnly
.
h
"
#
include
"
mozilla
/
MathAlgorithms
.
h
"
#
include
"
jsmath
.
h
"
#
include
"
jit
/
CodeGenerator
.
h
"
#
include
"
jit
/
JitFrames
.
h
"
#
include
"
jit
/
JitRealm
.
h
"
#
include
"
jit
/
Linker
.
h
"
#
include
"
jit
/
RangeAnalysis
.
h
"
#
include
"
vm
/
TraceLogging
.
h
"
#
include
"
jit
/
MacroAssembler
-
inl
.
h
"
#
include
"
jit
/
shared
/
CodeGenerator
-
shared
-
inl
.
h
"
using
namespace
js
;
using
namespace
js
:
:
jit
;
using
mozilla
:
:
Abs
;
using
mozilla
:
:
BitwiseCast
;
using
mozilla
:
:
DebugOnly
;
using
mozilla
:
:
FloatingPoint
;
using
mozilla
:
:
FloorLog2
;
using
mozilla
:
:
NegativeInfinity
;
using
mozilla
:
:
SpecificNaN
;
using
JS
:
:
GenericNaN
;
namespace
js
{
namespace
jit
{
CodeGeneratorX86Shared
:
:
CodeGeneratorX86Shared
(
MIRGenerator
*
gen
LIRGraph
*
graph
MacroAssembler
*
masm
)
:
CodeGeneratorShared
(
gen
graph
masm
)
{
}
#
ifdef
JS_PUNBOX64
Operand
CodeGeneratorX86Shared
:
:
ToOperandOrRegister64
(
const
LInt64Allocation
input
)
{
return
ToOperand
(
input
.
value
(
)
)
;
}
#
else
Register64
CodeGeneratorX86Shared
:
:
ToOperandOrRegister64
(
const
LInt64Allocation
input
)
{
return
ToRegister64
(
input
)
;
}
#
endif
void
OutOfLineBailout
:
:
accept
(
CodeGeneratorX86Shared
*
codegen
)
{
codegen
-
>
visitOutOfLineBailout
(
this
)
;
}
void
CodeGeneratorX86Shared
:
:
emitBranch
(
Assembler
:
:
Condition
cond
MBasicBlock
*
mirTrue
MBasicBlock
*
mirFalse
Assembler
:
:
NaNCond
ifNaN
)
{
if
(
ifNaN
=
=
Assembler
:
:
NaN_IsFalse
)
jumpToBlock
(
mirFalse
Assembler
:
:
Parity
)
;
else
if
(
ifNaN
=
=
Assembler
:
:
NaN_IsTrue
)
jumpToBlock
(
mirTrue
Assembler
:
:
Parity
)
;
if
(
isNextBlock
(
mirFalse
-
>
lir
(
)
)
)
{
jumpToBlock
(
mirTrue
cond
)
;
}
else
{
jumpToBlock
(
mirFalse
Assembler
:
:
InvertCondition
(
cond
)
)
;
jumpToBlock
(
mirTrue
)
;
}
}
void
CodeGenerator
:
:
visitDouble
(
LDouble
*
ins
)
{
const
LDefinition
*
out
=
ins
-
>
getDef
(
0
)
;
masm
.
loadConstantDouble
(
ins
-
>
getDouble
(
)
ToFloatRegister
(
out
)
)
;
}
void
CodeGenerator
:
:
visitFloat32
(
LFloat32
*
ins
)
{
const
LDefinition
*
out
=
ins
-
>
getDef
(
0
)
;
masm
.
loadConstantFloat32
(
ins
-
>
getFloat
(
)
ToFloatRegister
(
out
)
)
;
}
void
CodeGenerator
:
:
visitTestIAndBranch
(
LTestIAndBranch
*
test
)
{
Register
input
=
ToRegister
(
test
-
>
input
(
)
)
;
masm
.
test32
(
input
input
)
;
emitBranch
(
Assembler
:
:
NonZero
test
-
>
ifTrue
(
)
test
-
>
ifFalse
(
)
)
;
}
void
CodeGenerator
:
:
visitTestDAndBranch
(
LTestDAndBranch
*
test
)
{
const
LAllocation
*
opd
=
test
-
>
input
(
)
;
ScratchDoubleScope
scratch
(
masm
)
;
masm
.
zeroDouble
(
scratch
)
;
masm
.
vucomisd
(
scratch
ToFloatRegister
(
opd
)
)
;
emitBranch
(
Assembler
:
:
NotEqual
test
-
>
ifTrue
(
)
test
-
>
ifFalse
(
)
)
;
}
void
CodeGenerator
:
:
visitTestFAndBranch
(
LTestFAndBranch
*
test
)
{
const
LAllocation
*
opd
=
test
-
>
input
(
)
;
{
ScratchFloat32Scope
scratch
(
masm
)
;
masm
.
zeroFloat32
(
scratch
)
;
masm
.
vucomiss
(
scratch
ToFloatRegister
(
opd
)
)
;
}
emitBranch
(
Assembler
:
:
NotEqual
test
-
>
ifTrue
(
)
test
-
>
ifFalse
(
)
)
;
}
void
CodeGenerator
:
:
visitBitAndAndBranch
(
LBitAndAndBranch
*
baab
)
{
if
(
baab
-
>
right
(
)
-
>
isConstant
(
)
)
masm
.
test32
(
ToRegister
(
baab
-
>
left
(
)
)
Imm32
(
ToInt32
(
baab
-
>
right
(
)
)
)
)
;
else
masm
.
test32
(
ToRegister
(
baab
-
>
left
(
)
)
ToRegister
(
baab
-
>
right
(
)
)
)
;
emitBranch
(
baab
-
>
cond
(
)
baab
-
>
ifTrue
(
)
baab
-
>
ifFalse
(
)
)
;
}
void
CodeGeneratorX86Shared
:
:
emitCompare
(
MCompare
:
:
CompareType
type
const
LAllocation
*
left
const
LAllocation
*
right
)
{
#
ifdef
JS_CODEGEN_X64
if
(
type
=
=
MCompare
:
:
Compare_Object
|
|
type
=
=
MCompare
:
:
Compare_Symbol
)
{
masm
.
cmpPtr
(
ToRegister
(
left
)
ToOperand
(
right
)
)
;
return
;
}
#
endif
if
(
right
-
>
isConstant
(
)
)
masm
.
cmp32
(
ToRegister
(
left
)
Imm32
(
ToInt32
(
right
)
)
)
;
else
masm
.
cmp32
(
ToRegister
(
left
)
ToOperand
(
right
)
)
;
}
void
CodeGenerator
:
:
visitCompare
(
LCompare
*
comp
)
{
MCompare
*
mir
=
comp
-
>
mir
(
)
;
emitCompare
(
mir
-
>
compareType
(
)
comp
-
>
left
(
)
comp
-
>
right
(
)
)
;
masm
.
emitSet
(
JSOpToCondition
(
mir
-
>
compareType
(
)
comp
-
>
jsop
(
)
)
ToRegister
(
comp
-
>
output
(
)
)
)
;
}
void
CodeGenerator
:
:
visitCompareAndBranch
(
LCompareAndBranch
*
comp
)
{
MCompare
*
mir
=
comp
-
>
cmpMir
(
)
;
emitCompare
(
mir
-
>
compareType
(
)
comp
-
>
left
(
)
comp
-
>
right
(
)
)
;
Assembler
:
:
Condition
cond
=
JSOpToCondition
(
mir
-
>
compareType
(
)
comp
-
>
jsop
(
)
)
;
emitBranch
(
cond
comp
-
>
ifTrue
(
)
comp
-
>
ifFalse
(
)
)
;
}
void
CodeGenerator
:
:
visitCompareD
(
LCompareD
*
comp
)
{
FloatRegister
lhs
=
ToFloatRegister
(
comp
-
>
left
(
)
)
;
FloatRegister
rhs
=
ToFloatRegister
(
comp
-
>
right
(
)
)
;
Assembler
:
:
DoubleCondition
cond
=
JSOpToDoubleCondition
(
comp
-
>
mir
(
)
-
>
jsop
(
)
)
;
Assembler
:
:
NaNCond
nanCond
=
Assembler
:
:
NaNCondFromDoubleCondition
(
cond
)
;
if
(
comp
-
>
mir
(
)
-
>
operandsAreNeverNaN
(
)
)
nanCond
=
Assembler
:
:
NaN_HandledByCond
;
masm
.
compareDouble
(
cond
lhs
rhs
)
;
masm
.
emitSet
(
Assembler
:
:
ConditionFromDoubleCondition
(
cond
)
ToRegister
(
comp
-
>
output
(
)
)
nanCond
)
;
}
void
CodeGenerator
:
:
visitCompareF
(
LCompareF
*
comp
)
{
FloatRegister
lhs
=
ToFloatRegister
(
comp
-
>
left
(
)
)
;
FloatRegister
rhs
=
ToFloatRegister
(
comp
-
>
right
(
)
)
;
Assembler
:
:
DoubleCondition
cond
=
JSOpToDoubleCondition
(
comp
-
>
mir
(
)
-
>
jsop
(
)
)
;
Assembler
:
:
NaNCond
nanCond
=
Assembler
:
:
NaNCondFromDoubleCondition
(
cond
)
;
if
(
comp
-
>
mir
(
)
-
>
operandsAreNeverNaN
(
)
)
nanCond
=
Assembler
:
:
NaN_HandledByCond
;
masm
.
compareFloat
(
cond
lhs
rhs
)
;
masm
.
emitSet
(
Assembler
:
:
ConditionFromDoubleCondition
(
cond
)
ToRegister
(
comp
-
>
output
(
)
)
nanCond
)
;
}
void
CodeGenerator
:
:
visitNotI
(
LNotI
*
ins
)
{
masm
.
cmp32
(
ToRegister
(
ins
-
>
input
(
)
)
Imm32
(
0
)
)
;
masm
.
emitSet
(
Assembler
:
:
Equal
ToRegister
(
ins
-
>
output
(
)
)
)
;
}
void
CodeGenerator
:
:
visitNotD
(
LNotD
*
ins
)
{
FloatRegister
opd
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
Assembler
:
:
NaNCond
nanCond
=
Assembler
:
:
NaN_IsTrue
;
if
(
ins
-
>
mir
(
)
-
>
operandIsNeverNaN
(
)
)
nanCond
=
Assembler
:
:
NaN_HandledByCond
;
ScratchDoubleScope
scratch
(
masm
)
;
masm
.
zeroDouble
(
scratch
)
;
masm
.
compareDouble
(
Assembler
:
:
DoubleEqualOrUnordered
opd
scratch
)
;
masm
.
emitSet
(
Assembler
:
:
Equal
ToRegister
(
ins
-
>
output
(
)
)
nanCond
)
;
}
void
CodeGenerator
:
:
visitNotF
(
LNotF
*
ins
)
{
FloatRegister
opd
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
Assembler
:
:
NaNCond
nanCond
=
Assembler
:
:
NaN_IsTrue
;
if
(
ins
-
>
mir
(
)
-
>
operandIsNeverNaN
(
)
)
nanCond
=
Assembler
:
:
NaN_HandledByCond
;
ScratchFloat32Scope
scratch
(
masm
)
;
masm
.
zeroFloat32
(
scratch
)
;
masm
.
compareFloat
(
Assembler
:
:
DoubleEqualOrUnordered
opd
scratch
)
;
masm
.
emitSet
(
Assembler
:
:
Equal
ToRegister
(
ins
-
>
output
(
)
)
nanCond
)
;
}
void
CodeGenerator
:
:
visitCompareDAndBranch
(
LCompareDAndBranch
*
comp
)
{
FloatRegister
lhs
=
ToFloatRegister
(
comp
-
>
left
(
)
)
;
FloatRegister
rhs
=
ToFloatRegister
(
comp
-
>
right
(
)
)
;
Assembler
:
:
DoubleCondition
cond
=
JSOpToDoubleCondition
(
comp
-
>
cmpMir
(
)
-
>
jsop
(
)
)
;
Assembler
:
:
NaNCond
nanCond
=
Assembler
:
:
NaNCondFromDoubleCondition
(
cond
)
;
if
(
comp
-
>
cmpMir
(
)
-
>
operandsAreNeverNaN
(
)
)
nanCond
=
Assembler
:
:
NaN_HandledByCond
;
masm
.
compareDouble
(
cond
lhs
rhs
)
;
emitBranch
(
Assembler
:
:
ConditionFromDoubleCondition
(
cond
)
comp
-
>
ifTrue
(
)
comp
-
>
ifFalse
(
)
nanCond
)
;
}
void
CodeGenerator
:
:
visitCompareFAndBranch
(
LCompareFAndBranch
*
comp
)
{
FloatRegister
lhs
=
ToFloatRegister
(
comp
-
>
left
(
)
)
;
FloatRegister
rhs
=
ToFloatRegister
(
comp
-
>
right
(
)
)
;
Assembler
:
:
DoubleCondition
cond
=
JSOpToDoubleCondition
(
comp
-
>
cmpMir
(
)
-
>
jsop
(
)
)
;
Assembler
:
:
NaNCond
nanCond
=
Assembler
:
:
NaNCondFromDoubleCondition
(
cond
)
;
if
(
comp
-
>
cmpMir
(
)
-
>
operandsAreNeverNaN
(
)
)
nanCond
=
Assembler
:
:
NaN_HandledByCond
;
masm
.
compareFloat
(
cond
lhs
rhs
)
;
emitBranch
(
Assembler
:
:
ConditionFromDoubleCondition
(
cond
)
comp
-
>
ifTrue
(
)
comp
-
>
ifFalse
(
)
nanCond
)
;
}
void
CodeGenerator
:
:
visitWasmStackArg
(
LWasmStackArg
*
ins
)
{
const
MWasmStackArg
*
mir
=
ins
-
>
mir
(
)
;
Address
dst
(
StackPointer
mir
-
>
spOffset
(
)
)
;
if
(
ins
-
>
arg
(
)
-
>
isConstant
(
)
)
{
masm
.
storePtr
(
ImmWord
(
ToInt32
(
ins
-
>
arg
(
)
)
)
dst
)
;
}
else
if
(
ins
-
>
arg
(
)
-
>
isGeneralReg
(
)
)
{
masm
.
storePtr
(
ToRegister
(
ins
-
>
arg
(
)
)
dst
)
;
}
else
{
switch
(
mir
-
>
input
(
)
-
>
type
(
)
)
{
case
MIRType
:
:
Double
:
masm
.
storeDouble
(
ToFloatRegister
(
ins
-
>
arg
(
)
)
dst
)
;
return
;
case
MIRType
:
:
Float32
:
masm
.
storeFloat32
(
ToFloatRegister
(
ins
-
>
arg
(
)
)
dst
)
;
return
;
case
MIRType
:
:
Int32x4
:
case
MIRType
:
:
Bool32x4
:
masm
.
storeAlignedSimd128Int
(
ToFloatRegister
(
ins
-
>
arg
(
)
)
dst
)
;
return
;
case
MIRType
:
:
Float32x4
:
masm
.
storeAlignedSimd128Float
(
ToFloatRegister
(
ins
-
>
arg
(
)
)
dst
)
;
return
;
default
:
break
;
}
MOZ_MAKE_COMPILER_ASSUME_IS_UNREACHABLE
(
"
unexpected
mir
type
in
WasmStackArg
"
)
;
}
}
void
CodeGenerator
:
:
visitWasmStackArgI64
(
LWasmStackArgI64
*
ins
)
{
const
MWasmStackArg
*
mir
=
ins
-
>
mir
(
)
;
Address
dst
(
StackPointer
mir
-
>
spOffset
(
)
)
;
if
(
IsConstant
(
ins
-
>
arg
(
)
)
)
masm
.
store64
(
Imm64
(
ToInt64
(
ins
-
>
arg
(
)
)
)
dst
)
;
else
masm
.
store64
(
ToRegister64
(
ins
-
>
arg
(
)
)
dst
)
;
}
void
CodeGenerator
:
:
visitWasmSelect
(
LWasmSelect
*
ins
)
{
MIRType
mirType
=
ins
-
>
mir
(
)
-
>
type
(
)
;
Register
cond
=
ToRegister
(
ins
-
>
condExpr
(
)
)
;
Operand
falseExpr
=
ToOperand
(
ins
-
>
falseExpr
(
)
)
;
masm
.
test32
(
cond
cond
)
;
if
(
mirType
=
=
MIRType
:
:
Int32
)
{
Register
out
=
ToRegister
(
ins
-
>
output
(
)
)
;
MOZ_ASSERT
(
ToRegister
(
ins
-
>
trueExpr
(
)
)
=
=
out
"
true
expr
input
is
reused
for
output
"
)
;
masm
.
cmovzl
(
falseExpr
out
)
;
return
;
}
FloatRegister
out
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
MOZ_ASSERT
(
ToFloatRegister
(
ins
-
>
trueExpr
(
)
)
=
=
out
"
true
expr
input
is
reused
for
output
"
)
;
Label
done
;
masm
.
j
(
Assembler
:
:
NonZero
&
done
)
;
if
(
mirType
=
=
MIRType
:
:
Float32
)
{
if
(
falseExpr
.
kind
(
)
=
=
Operand
:
:
FPREG
)
masm
.
moveFloat32
(
ToFloatRegister
(
ins
-
>
falseExpr
(
)
)
out
)
;
else
masm
.
loadFloat32
(
falseExpr
out
)
;
}
else
if
(
mirType
=
=
MIRType
:
:
Double
)
{
if
(
falseExpr
.
kind
(
)
=
=
Operand
:
:
FPREG
)
masm
.
moveDouble
(
ToFloatRegister
(
ins
-
>
falseExpr
(
)
)
out
)
;
else
masm
.
loadDouble
(
falseExpr
out
)
;
}
else
{
MOZ_CRASH
(
"
unhandled
type
in
visitWasmSelect
!
"
)
;
}
masm
.
bind
(
&
done
)
;
}
void
CodeGenerator
:
:
visitWasmReinterpret
(
LWasmReinterpret
*
lir
)
{
MOZ_ASSERT
(
gen
-
>
compilingWasm
(
)
)
;
MWasmReinterpret
*
ins
=
lir
-
>
mir
(
)
;
MIRType
to
=
ins
-
>
type
(
)
;
#
ifdef
DEBUG
MIRType
from
=
ins
-
>
input
(
)
-
>
type
(
)
;
#
endif
switch
(
to
)
{
case
MIRType
:
:
Int32
:
MOZ_ASSERT
(
from
=
=
MIRType
:
:
Float32
)
;
masm
.
vmovd
(
ToFloatRegister
(
lir
-
>
input
(
)
)
ToRegister
(
lir
-
>
output
(
)
)
)
;
break
;
case
MIRType
:
:
Float32
:
MOZ_ASSERT
(
from
=
=
MIRType
:
:
Int32
)
;
masm
.
vmovd
(
ToRegister
(
lir
-
>
input
(
)
)
ToFloatRegister
(
lir
-
>
output
(
)
)
)
;
break
;
case
MIRType
:
:
Double
:
case
MIRType
:
:
Int64
:
MOZ_CRASH
(
"
not
handled
by
this
LIR
opcode
"
)
;
default
:
MOZ_CRASH
(
"
unexpected
WasmReinterpret
"
)
;
}
}
void
CodeGeneratorX86Shared
:
:
visitOutOfLineLoadTypedArrayOutOfBounds
(
OutOfLineLoadTypedArrayOutOfBounds
*
ool
)
{
switch
(
ool
-
>
viewType
(
)
)
{
case
Scalar
:
:
Int64
:
case
Scalar
:
:
Float32x4
:
case
Scalar
:
:
Int8x16
:
case
Scalar
:
:
Int16x8
:
case
Scalar
:
:
Int32x4
:
case
Scalar
:
:
MaxTypedArrayViewType
:
MOZ_CRASH
(
"
unexpected
array
type
"
)
;
case
Scalar
:
:
Float32
:
masm
.
loadConstantFloat32
(
float
(
GenericNaN
(
)
)
ool
-
>
dest
(
)
.
fpu
(
)
)
;
break
;
case
Scalar
:
:
Float64
:
masm
.
loadConstantDouble
(
GenericNaN
(
)
ool
-
>
dest
(
)
.
fpu
(
)
)
;
break
;
case
Scalar
:
:
Int8
:
case
Scalar
:
:
Uint8
:
case
Scalar
:
:
Int16
:
case
Scalar
:
:
Uint16
:
case
Scalar
:
:
Int32
:
case
Scalar
:
:
Uint32
:
case
Scalar
:
:
Uint8Clamped
:
Register
destReg
=
ool
-
>
dest
(
)
.
gpr
(
)
;
masm
.
mov
(
ImmWord
(
0
)
destReg
)
;
break
;
}
masm
.
jmp
(
ool
-
>
rejoin
(
)
)
;
}
void
CodeGenerator
:
:
visitWasmAddOffset
(
LWasmAddOffset
*
lir
)
{
MWasmAddOffset
*
mir
=
lir
-
>
mir
(
)
;
Register
base
=
ToRegister
(
lir
-
>
base
(
)
)
;
Register
out
=
ToRegister
(
lir
-
>
output
(
)
)
;
if
(
base
!
=
out
)
masm
.
move32
(
base
out
)
;
masm
.
add32
(
Imm32
(
mir
-
>
offset
(
)
)
out
)
;
Label
ok
;
masm
.
j
(
Assembler
:
:
CarryClear
&
ok
)
;
masm
.
wasmTrap
(
wasm
:
:
Trap
:
:
OutOfBounds
mir
-
>
bytecodeOffset
(
)
)
;
masm
.
bind
(
&
ok
)
;
}
void
CodeGenerator
:
:
visitWasmTruncateToInt32
(
LWasmTruncateToInt32
*
lir
)
{
FloatRegister
input
=
ToFloatRegister
(
lir
-
>
input
(
)
)
;
Register
output
=
ToRegister
(
lir
-
>
output
(
)
)
;
MWasmTruncateToInt32
*
mir
=
lir
-
>
mir
(
)
;
MIRType
inputType
=
mir
-
>
input
(
)
-
>
type
(
)
;
MOZ_ASSERT
(
inputType
=
=
MIRType
:
:
Double
|
|
inputType
=
=
MIRType
:
:
Float32
)
;
auto
*
ool
=
new
(
alloc
(
)
)
OutOfLineWasmTruncateCheck
(
mir
input
output
)
;
addOutOfLineCode
(
ool
mir
)
;
Label
*
oolEntry
=
ool
-
>
entry
(
)
;
if
(
mir
-
>
isUnsigned
(
)
)
{
if
(
inputType
=
=
MIRType
:
:
Double
)
masm
.
wasmTruncateDoubleToUInt32
(
input
output
mir
-
>
isSaturating
(
)
oolEntry
)
;
else
if
(
inputType
=
=
MIRType
:
:
Float32
)
masm
.
wasmTruncateFloat32ToUInt32
(
input
output
mir
-
>
isSaturating
(
)
oolEntry
)
;
else
MOZ_CRASH
(
"
unexpected
type
"
)
;
if
(
mir
-
>
isSaturating
(
)
)
masm
.
bind
(
ool
-
>
rejoin
(
)
)
;
return
;
}
if
(
inputType
=
=
MIRType
:
:
Double
)
masm
.
wasmTruncateDoubleToInt32
(
input
output
mir
-
>
isSaturating
(
)
oolEntry
)
;
else
if
(
inputType
=
=
MIRType
:
:
Float32
)
masm
.
wasmTruncateFloat32ToInt32
(
input
output
mir
-
>
isSaturating
(
)
oolEntry
)
;
else
MOZ_CRASH
(
"
unexpected
type
"
)
;
masm
.
bind
(
ool
-
>
rejoin
(
)
)
;
}
bool
CodeGeneratorX86Shared
:
:
generateOutOfLineCode
(
)
{
if
(
!
CodeGeneratorShared
:
:
generateOutOfLineCode
(
)
)
return
false
;
if
(
deoptLabel_
.
used
(
)
)
{
masm
.
bind
(
&
deoptLabel_
)
;
masm
.
push
(
Imm32
(
frameSize
(
)
)
)
;
TrampolinePtr
handler
=
gen
-
>
jitRuntime
(
)
-
>
getGenericBailoutHandler
(
)
;
masm
.
jump
(
handler
)
;
}
return
!
masm
.
oom
(
)
;
}
class
BailoutJump
{
Assembler
:
:
Condition
cond_
;
public
:
explicit
BailoutJump
(
Assembler
:
:
Condition
cond
)
:
cond_
(
cond
)
{
}
#
ifdef
JS_CODEGEN_X86
void
operator
(
)
(
MacroAssembler
&
masm
uint8_t
*
code
)
const
{
masm
.
j
(
cond_
ImmPtr
(
code
)
Relocation
:
:
HARDCODED
)
;
}
#
endif
void
operator
(
)
(
MacroAssembler
&
masm
Label
*
label
)
const
{
masm
.
j
(
cond_
label
)
;
}
}
;
class
BailoutLabel
{
Label
*
label_
;
public
:
explicit
BailoutLabel
(
Label
*
label
)
:
label_
(
label
)
{
}
#
ifdef
JS_CODEGEN_X86
void
operator
(
)
(
MacroAssembler
&
masm
uint8_t
*
code
)
const
{
masm
.
retarget
(
label_
ImmPtr
(
code
)
Relocation
:
:
HARDCODED
)
;
}
#
endif
void
operator
(
)
(
MacroAssembler
&
masm
Label
*
label
)
const
{
masm
.
retarget
(
label_
label
)
;
}
}
;
template
<
typename
T
>
void
CodeGeneratorX86Shared
:
:
bailout
(
const
T
&
binder
LSnapshot
*
snapshot
)
{
encode
(
snapshot
)
;
MOZ_ASSERT_IF
(
frameClass_
!
=
FrameSizeClass
:
:
None
(
)
&
&
deoptTable_
frameClass_
.
frameSize
(
)
=
=
masm
.
framePushed
(
)
)
;
#
ifdef
JS_CODEGEN_X86
if
(
assignBailoutId
(
snapshot
)
)
{
binder
(
masm
deoptTable_
-
>
value
+
snapshot
-
>
bailoutId
(
)
*
BAILOUT_TABLE_ENTRY_SIZE
)
;
return
;
}
#
endif
InlineScriptTree
*
tree
=
snapshot
-
>
mir
(
)
-
>
block
(
)
-
>
trackedTree
(
)
;
OutOfLineBailout
*
ool
=
new
(
alloc
(
)
)
OutOfLineBailout
(
snapshot
)
;
addOutOfLineCode
(
ool
new
(
alloc
(
)
)
BytecodeSite
(
tree
tree
-
>
script
(
)
-
>
code
(
)
)
)
;
binder
(
masm
ool
-
>
entry
(
)
)
;
}
void
CodeGeneratorX86Shared
:
:
bailoutIf
(
Assembler
:
:
Condition
condition
LSnapshot
*
snapshot
)
{
bailout
(
BailoutJump
(
condition
)
snapshot
)
;
}
void
CodeGeneratorX86Shared
:
:
bailoutIf
(
Assembler
:
:
DoubleCondition
condition
LSnapshot
*
snapshot
)
{
MOZ_ASSERT
(
Assembler
:
:
NaNCondFromDoubleCondition
(
condition
)
=
=
Assembler
:
:
NaN_HandledByCond
)
;
bailoutIf
(
Assembler
:
:
ConditionFromDoubleCondition
(
condition
)
snapshot
)
;
}
void
CodeGeneratorX86Shared
:
:
bailoutFrom
(
Label
*
label
LSnapshot
*
snapshot
)
{
MOZ_ASSERT_IF
(
!
masm
.
oom
(
)
label
-
>
used
(
)
&
&
!
label
-
>
bound
(
)
)
;
bailout
(
BailoutLabel
(
label
)
snapshot
)
;
}
void
CodeGeneratorX86Shared
:
:
bailout
(
LSnapshot
*
snapshot
)
{
Label
label
;
masm
.
jump
(
&
label
)
;
bailoutFrom
(
&
label
snapshot
)
;
}
void
CodeGeneratorX86Shared
:
:
visitOutOfLineBailout
(
OutOfLineBailout
*
ool
)
{
masm
.
push
(
Imm32
(
ool
-
>
snapshot
(
)
-
>
snapshotOffset
(
)
)
)
;
masm
.
jmp
(
&
deoptLabel_
)
;
}
void
CodeGenerator
:
:
visitMinMaxD
(
LMinMaxD
*
ins
)
{
FloatRegister
first
=
ToFloatRegister
(
ins
-
>
first
(
)
)
;
FloatRegister
second
=
ToFloatRegister
(
ins
-
>
second
(
)
)
;
#
ifdef
DEBUG
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
MOZ_ASSERT
(
first
=
=
output
)
;
#
endif
bool
handleNaN
=
!
ins
-
>
mir
(
)
-
>
range
(
)
|
|
ins
-
>
mir
(
)
-
>
range
(
)
-
>
canBeNaN
(
)
;
if
(
ins
-
>
mir
(
)
-
>
isMax
(
)
)
masm
.
maxDouble
(
second
first
handleNaN
)
;
else
masm
.
minDouble
(
second
first
handleNaN
)
;
}
void
CodeGenerator
:
:
visitMinMaxF
(
LMinMaxF
*
ins
)
{
FloatRegister
first
=
ToFloatRegister
(
ins
-
>
first
(
)
)
;
FloatRegister
second
=
ToFloatRegister
(
ins
-
>
second
(
)
)
;
#
ifdef
DEBUG
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
MOZ_ASSERT
(
first
=
=
output
)
;
#
endif
bool
handleNaN
=
!
ins
-
>
mir
(
)
-
>
range
(
)
|
|
ins
-
>
mir
(
)
-
>
range
(
)
-
>
canBeNaN
(
)
;
if
(
ins
-
>
mir
(
)
-
>
isMax
(
)
)
masm
.
maxFloat32
(
second
first
handleNaN
)
;
else
masm
.
minFloat32
(
second
first
handleNaN
)
;
}
void
CodeGenerator
:
:
visitAbsD
(
LAbsD
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
MOZ_ASSERT
(
input
=
=
ToFloatRegister
(
ins
-
>
output
(
)
)
)
;
ScratchDoubleScope
scratch
(
masm
)
;
masm
.
loadConstantDouble
(
SpecificNaN
<
double
>
(
0
FloatingPoint
<
double
>
:
:
kSignificandBits
)
scratch
)
;
masm
.
vandpd
(
scratch
input
input
)
;
}
void
CodeGenerator
:
:
visitAbsF
(
LAbsF
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
MOZ_ASSERT
(
input
=
=
ToFloatRegister
(
ins
-
>
output
(
)
)
)
;
ScratchFloat32Scope
scratch
(
masm
)
;
masm
.
loadConstantFloat32
(
SpecificNaN
<
float
>
(
0
FloatingPoint
<
float
>
:
:
kSignificandBits
)
scratch
)
;
masm
.
vandps
(
scratch
input
input
)
;
}
void
CodeGenerator
:
:
visitClzI
(
LClzI
*
ins
)
{
Register
input
=
ToRegister
(
ins
-
>
input
(
)
)
;
Register
output
=
ToRegister
(
ins
-
>
output
(
)
)
;
bool
knownNotZero
=
ins
-
>
mir
(
)
-
>
operandIsNeverZero
(
)
;
masm
.
clz32
(
input
output
knownNotZero
)
;
}
void
CodeGenerator
:
:
visitCtzI
(
LCtzI
*
ins
)
{
Register
input
=
ToRegister
(
ins
-
>
input
(
)
)
;
Register
output
=
ToRegister
(
ins
-
>
output
(
)
)
;
bool
knownNotZero
=
ins
-
>
mir
(
)
-
>
operandIsNeverZero
(
)
;
masm
.
ctz32
(
input
output
knownNotZero
)
;
}
void
CodeGenerator
:
:
visitPopcntI
(
LPopcntI
*
ins
)
{
Register
input
=
ToRegister
(
ins
-
>
input
(
)
)
;
Register
output
=
ToRegister
(
ins
-
>
output
(
)
)
;
Register
temp
=
ins
-
>
temp
(
)
-
>
isBogusTemp
(
)
?
InvalidReg
:
ToRegister
(
ins
-
>
temp
(
)
)
;
masm
.
popcnt32
(
input
output
temp
)
;
}
void
CodeGenerator
:
:
visitSqrtD
(
LSqrtD
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
masm
.
vsqrtsd
(
input
output
output
)
;
}
void
CodeGenerator
:
:
visitSqrtF
(
LSqrtF
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
masm
.
vsqrtss
(
input
output
output
)
;
}
void
CodeGenerator
:
:
visitPowHalfD
(
LPowHalfD
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
ScratchDoubleScope
scratch
(
masm
)
;
Label
done
sqrt
;
if
(
!
ins
-
>
mir
(
)
-
>
operandIsNeverNegativeInfinity
(
)
)
{
masm
.
loadConstantDouble
(
NegativeInfinity
<
double
>
(
)
scratch
)
;
Assembler
:
:
DoubleCondition
cond
=
Assembler
:
:
DoubleNotEqualOrUnordered
;
if
(
ins
-
>
mir
(
)
-
>
operandIsNeverNaN
(
)
)
cond
=
Assembler
:
:
DoubleNotEqual
;
masm
.
branchDouble
(
cond
input
scratch
&
sqrt
)
;
masm
.
zeroDouble
(
output
)
;
masm
.
subDouble
(
scratch
output
)
;
masm
.
jump
(
&
done
)
;
masm
.
bind
(
&
sqrt
)
;
}
if
(
!
ins
-
>
mir
(
)
-
>
operandIsNeverNegativeZero
(
)
)
{
masm
.
zeroDouble
(
scratch
)
;
masm
.
addDouble
(
input
scratch
)
;
masm
.
vsqrtsd
(
scratch
output
output
)
;
}
else
{
masm
.
vsqrtsd
(
input
output
output
)
;
}
masm
.
bind
(
&
done
)
;
}
class
OutOfLineUndoALUOperation
:
public
OutOfLineCodeBase
<
CodeGeneratorX86Shared
>
{
LInstruction
*
ins_
;
public
:
explicit
OutOfLineUndoALUOperation
(
LInstruction
*
ins
)
:
ins_
(
ins
)
{
}
virtual
void
accept
(
CodeGeneratorX86Shared
*
codegen
)
override
{
codegen
-
>
visitOutOfLineUndoALUOperation
(
this
)
;
}
LInstruction
*
ins
(
)
const
{
return
ins_
;
}
}
;
void
CodeGenerator
:
:
visitAddI
(
LAddI
*
ins
)
{
if
(
ins
-
>
rhs
(
)
-
>
isConstant
(
)
)
masm
.
addl
(
Imm32
(
ToInt32
(
ins
-
>
rhs
(
)
)
)
ToOperand
(
ins
-
>
lhs
(
)
)
)
;
else
masm
.
addl
(
ToOperand
(
ins
-
>
rhs
(
)
)
ToRegister
(
ins
-
>
lhs
(
)
)
)
;
if
(
ins
-
>
snapshot
(
)
)
{
if
(
ins
-
>
recoversInput
(
)
)
{
OutOfLineUndoALUOperation
*
ool
=
new
(
alloc
(
)
)
OutOfLineUndoALUOperation
(
ins
)
;
addOutOfLineCode
(
ool
ins
-
>
mir
(
)
)
;
masm
.
j
(
Assembler
:
:
Overflow
ool
-
>
entry
(
)
)
;
}
else
{
bailoutIf
(
Assembler
:
:
Overflow
ins
-
>
snapshot
(
)
)
;
}
}
}
void
CodeGenerator
:
:
visitAddI64
(
LAddI64
*
lir
)
{
const
LInt64Allocation
lhs
=
lir
-
>
getInt64Operand
(
LAddI64
:
:
Lhs
)
;
const
LInt64Allocation
rhs
=
lir
-
>
getInt64Operand
(
LAddI64
:
:
Rhs
)
;
MOZ_ASSERT
(
ToOutRegister64
(
lir
)
=
=
ToRegister64
(
lhs
)
)
;
if
(
IsConstant
(
rhs
)
)
{
masm
.
add64
(
Imm64
(
ToInt64
(
rhs
)
)
ToRegister64
(
lhs
)
)
;
return
;
}
masm
.
add64
(
ToOperandOrRegister64
(
rhs
)
ToRegister64
(
lhs
)
)
;
}
void
CodeGenerator
:
:
visitSubI
(
LSubI
*
ins
)
{
if
(
ins
-
>
rhs
(
)
-
>
isConstant
(
)
)
masm
.
subl
(
Imm32
(
ToInt32
(
ins
-
>
rhs
(
)
)
)
ToOperand
(
ins
-
>
lhs
(
)
)
)
;
else
masm
.
subl
(
ToOperand
(
ins
-
>
rhs
(
)
)
ToRegister
(
ins
-
>
lhs
(
)
)
)
;
if
(
ins
-
>
snapshot
(
)
)
{
if
(
ins
-
>
recoversInput
(
)
)
{
OutOfLineUndoALUOperation
*
ool
=
new
(
alloc
(
)
)
OutOfLineUndoALUOperation
(
ins
)
;
addOutOfLineCode
(
ool
ins
-
>
mir
(
)
)
;
masm
.
j
(
Assembler
:
:
Overflow
ool
-
>
entry
(
)
)
;
}
else
{
bailoutIf
(
Assembler
:
:
Overflow
ins
-
>
snapshot
(
)
)
;
}
}
}
void
CodeGenerator
:
:
visitSubI64
(
LSubI64
*
lir
)
{
const
LInt64Allocation
lhs
=
lir
-
>
getInt64Operand
(
LSubI64
:
:
Lhs
)
;
const
LInt64Allocation
rhs
=
lir
-
>
getInt64Operand
(
LSubI64
:
:
Rhs
)
;
MOZ_ASSERT
(
ToOutRegister64
(
lir
)
=
=
ToRegister64
(
lhs
)
)
;
if
(
IsConstant
(
rhs
)
)
{
masm
.
sub64
(
Imm64
(
ToInt64
(
rhs
)
)
ToRegister64
(
lhs
)
)
;
return
;
}
masm
.
sub64
(
ToOperandOrRegister64
(
rhs
)
ToRegister64
(
lhs
)
)
;
}
void
CodeGeneratorX86Shared
:
:
visitOutOfLineUndoALUOperation
(
OutOfLineUndoALUOperation
*
ool
)
{
LInstruction
*
ins
=
ool
-
>
ins
(
)
;
Register
reg
=
ToRegister
(
ins
-
>
getDef
(
0
)
)
;
DebugOnly
<
LAllocation
*
>
lhs
=
ins
-
>
getOperand
(
0
)
;
LAllocation
*
rhs
=
ins
-
>
getOperand
(
1
)
;
MOZ_ASSERT
(
reg
=
=
ToRegister
(
lhs
)
)
;
MOZ_ASSERT_IF
(
rhs
-
>
isGeneralReg
(
)
reg
!
=
ToRegister
(
rhs
)
)
;
if
(
rhs
-
>
isConstant
(
)
)
{
Imm32
constant
(
ToInt32
(
rhs
)
)
;
if
(
ins
-
>
isAddI
(
)
)
masm
.
subl
(
constant
reg
)
;
else
masm
.
addl
(
constant
reg
)
;
}
else
{
if
(
ins
-
>
isAddI
(
)
)
masm
.
subl
(
ToOperand
(
rhs
)
reg
)
;
else
masm
.
addl
(
ToOperand
(
rhs
)
reg
)
;
}
bailout
(
ool
-
>
ins
(
)
-
>
snapshot
(
)
)
;
}
class
MulNegativeZeroCheck
:
public
OutOfLineCodeBase
<
CodeGeneratorX86Shared
>
{
LMulI
*
ins_
;
public
:
explicit
MulNegativeZeroCheck
(
LMulI
*
ins
)
:
ins_
(
ins
)
{
}
virtual
void
accept
(
CodeGeneratorX86Shared
*
codegen
)
override
{
codegen
-
>
visitMulNegativeZeroCheck
(
this
)
;
}
LMulI
*
ins
(
)
const
{
return
ins_
;
}
}
;
void
CodeGenerator
:
:
visitMulI
(
LMulI
*
ins
)
{
const
LAllocation
*
lhs
=
ins
-
>
lhs
(
)
;
const
LAllocation
*
rhs
=
ins
-
>
rhs
(
)
;
MMul
*
mul
=
ins
-
>
mir
(
)
;
MOZ_ASSERT_IF
(
mul
-
>
mode
(
)
=
=
MMul
:
:
Integer
!
mul
-
>
canBeNegativeZero
(
)
&
&
!
mul
-
>
canOverflow
(
)
)
;
if
(
rhs
-
>
isConstant
(
)
)
{
int32_t
constant
=
ToInt32
(
rhs
)
;
if
(
mul
-
>
canBeNegativeZero
(
)
&
&
constant
<
=
0
)
{
Assembler
:
:
Condition
bailoutCond
=
(
constant
=
=
0
)
?
Assembler
:
:
Signed
:
Assembler
:
:
Equal
;
masm
.
test32
(
ToRegister
(
lhs
)
ToRegister
(
lhs
)
)
;
bailoutIf
(
bailoutCond
ins
-
>
snapshot
(
)
)
;
}
switch
(
constant
)
{
case
-
1
:
masm
.
negl
(
ToOperand
(
lhs
)
)
;
break
;
case
0
:
masm
.
xorl
(
ToOperand
(
lhs
)
ToRegister
(
lhs
)
)
;
return
;
case
1
:
return
;
case
2
:
masm
.
addl
(
ToOperand
(
lhs
)
ToRegister
(
lhs
)
)
;
break
;
default
:
if
(
!
mul
-
>
canOverflow
(
)
&
&
constant
>
0
)
{
int32_t
shift
=
FloorLog2
(
constant
)
;
if
(
(
1
<
<
shift
)
=
=
constant
)
{
masm
.
shll
(
Imm32
(
shift
)
ToRegister
(
lhs
)
)
;
return
;
}
}
masm
.
imull
(
Imm32
(
ToInt32
(
rhs
)
)
ToRegister
(
lhs
)
)
;
}
if
(
mul
-
>
canOverflow
(
)
)
bailoutIf
(
Assembler
:
:
Overflow
ins
-
>
snapshot
(
)
)
;
}
else
{
masm
.
imull
(
ToOperand
(
rhs
)
ToRegister
(
lhs
)
)
;
if
(
mul
-
>
canOverflow
(
)
)
bailoutIf
(
Assembler
:
:
Overflow
ins
-
>
snapshot
(
)
)
;
if
(
mul
-
>
canBeNegativeZero
(
)
)
{
MulNegativeZeroCheck
*
ool
=
new
(
alloc
(
)
)
MulNegativeZeroCheck
(
ins
)
;
addOutOfLineCode
(
ool
mul
)
;
masm
.
test32
(
ToRegister
(
lhs
)
ToRegister
(
lhs
)
)
;
masm
.
j
(
Assembler
:
:
Zero
ool
-
>
entry
(
)
)
;
masm
.
bind
(
ool
-
>
rejoin
(
)
)
;
}
}
}
void
CodeGenerator
:
:
visitMulI64
(
LMulI64
*
lir
)
{
const
LInt64Allocation
lhs
=
lir
-
>
getInt64Operand
(
LMulI64
:
:
Lhs
)
;
const
LInt64Allocation
rhs
=
lir
-
>
getInt64Operand
(
LMulI64
:
:
Rhs
)
;
MOZ_ASSERT
(
ToRegister64
(
lhs
)
=
=
ToOutRegister64
(
lir
)
)
;
if
(
IsConstant
(
rhs
)
)
{
int64_t
constant
=
ToInt64
(
rhs
)
;
switch
(
constant
)
{
case
-
1
:
masm
.
neg64
(
ToRegister64
(
lhs
)
)
;
return
;
case
0
:
masm
.
xor64
(
ToRegister64
(
lhs
)
ToRegister64
(
lhs
)
)
;
return
;
case
1
:
return
;
case
2
:
masm
.
add64
(
ToRegister64
(
lhs
)
ToRegister64
(
lhs
)
)
;
return
;
default
:
if
(
constant
>
0
)
{
int32_t
shift
=
mozilla
:
:
FloorLog2
(
constant
)
;
if
(
int64_t
(
1
)
<
<
shift
=
=
constant
)
{
masm
.
lshift64
(
Imm32
(
shift
)
ToRegister64
(
lhs
)
)
;
return
;
}
}
Register
temp
=
ToTempRegisterOrInvalid
(
lir
-
>
temp
(
)
)
;
masm
.
mul64
(
Imm64
(
constant
)
ToRegister64
(
lhs
)
temp
)
;
}
}
else
{
Register
temp
=
ToTempRegisterOrInvalid
(
lir
-
>
temp
(
)
)
;
masm
.
mul64
(
ToOperandOrRegister64
(
rhs
)
ToRegister64
(
lhs
)
temp
)
;
}
}
class
ReturnZero
:
public
OutOfLineCodeBase
<
CodeGeneratorX86Shared
>
{
Register
reg_
;
public
:
explicit
ReturnZero
(
Register
reg
)
:
reg_
(
reg
)
{
}
virtual
void
accept
(
CodeGeneratorX86Shared
*
codegen
)
override
{
codegen
-
>
visitReturnZero
(
this
)
;
}
Register
reg
(
)
const
{
return
reg_
;
}
}
;
void
CodeGeneratorX86Shared
:
:
visitReturnZero
(
ReturnZero
*
ool
)
{
masm
.
mov
(
ImmWord
(
0
)
ool
-
>
reg
(
)
)
;
masm
.
jmp
(
ool
-
>
rejoin
(
)
)
;
}
void
CodeGenerator
:
:
visitUDivOrMod
(
LUDivOrMod
*
ins
)
{
Register
lhs
=
ToRegister
(
ins
-
>
lhs
(
)
)
;
Register
rhs
=
ToRegister
(
ins
-
>
rhs
(
)
)
;
Register
output
=
ToRegister
(
ins
-
>
output
(
)
)
;
MOZ_ASSERT_IF
(
lhs
!
=
rhs
rhs
!
=
eax
)
;
MOZ_ASSERT
(
rhs
!
=
edx
)
;
MOZ_ASSERT_IF
(
output
=
=
eax
ToRegister
(
ins
-
>
remainder
(
)
)
=
=
edx
)
;
ReturnZero
*
ool
=
nullptr
;
if
(
lhs
!
=
eax
)
masm
.
mov
(
lhs
eax
)
;
if
(
ins
-
>
canBeDivideByZero
(
)
)
{
masm
.
test32
(
rhs
rhs
)
;
if
(
ins
-
>
mir
(
)
-
>
isTruncated
(
)
)
{
if
(
ins
-
>
trapOnError
(
)
)
{
Label
nonZero
;
masm
.
j
(
Assembler
:
:
NonZero
&
nonZero
)
;
masm
.
wasmTrap
(
wasm
:
:
Trap
:
:
IntegerDivideByZero
ins
-
>
bytecodeOffset
(
)
)
;
masm
.
bind
(
&
nonZero
)
;
}
else
{
ool
=
new
(
alloc
(
)
)
ReturnZero
(
output
)
;
masm
.
j
(
Assembler
:
:
Zero
ool
-
>
entry
(
)
)
;
}
}
else
{
bailoutIf
(
Assembler
:
:
Zero
ins
-
>
snapshot
(
)
)
;
}
}
masm
.
mov
(
ImmWord
(
0
)
edx
)
;
masm
.
udiv
(
rhs
)
;
if
(
ins
-
>
mir
(
)
-
>
isDiv
(
)
&
&
!
ins
-
>
mir
(
)
-
>
toDiv
(
)
-
>
canTruncateRemainder
(
)
)
{
Register
remainder
=
ToRegister
(
ins
-
>
remainder
(
)
)
;
masm
.
test32
(
remainder
remainder
)
;
bailoutIf
(
Assembler
:
:
NonZero
ins
-
>
snapshot
(
)
)
;
}
if
(
!
ins
-
>
mir
(
)
-
>
isTruncated
(
)
)
{
masm
.
test32
(
output
output
)
;
bailoutIf
(
Assembler
:
:
Signed
ins
-
>
snapshot
(
)
)
;
}
if
(
ool
)
{
addOutOfLineCode
(
ool
ins
-
>
mir
(
)
)
;
masm
.
bind
(
ool
-
>
rejoin
(
)
)
;
}
}
void
CodeGenerator
:
:
visitUDivOrModConstant
(
LUDivOrModConstant
*
ins
)
{
Register
lhs
=
ToRegister
(
ins
-
>
numerator
(
)
)
;
Register
output
=
ToRegister
(
ins
-
>
output
(
)
)
;
uint32_t
d
=
ins
-
>
denominator
(
)
;
MOZ_ASSERT
(
output
=
=
eax
|
|
output
=
=
edx
)
;
MOZ_ASSERT
(
lhs
!
=
eax
&
&
lhs
!
=
edx
)
;
bool
isDiv
=
(
output
=
=
edx
)
;
if
(
d
=
=
0
)
{
if
(
ins
-
>
mir
(
)
-
>
isTruncated
(
)
)
{
if
(
ins
-
>
trapOnError
(
)
)
masm
.
wasmTrap
(
wasm
:
:
Trap
:
:
IntegerDivideByZero
ins
-
>
bytecodeOffset
(
)
)
;
else
masm
.
xorl
(
output
output
)
;
}
else
{
bailout
(
ins
-
>
snapshot
(
)
)
;
}
return
;
}
MOZ_ASSERT
(
(
d
&
(
d
-
1
)
)
!
=
0
)
;
ReciprocalMulConstants
rmc
=
computeDivisionConstants
(
d
32
)
;
masm
.
movl
(
Imm32
(
rmc
.
multiplier
)
eax
)
;
masm
.
umull
(
lhs
)
;
if
(
rmc
.
multiplier
>
UINT32_MAX
)
{
MOZ_ASSERT
(
rmc
.
shiftAmount
>
0
)
;
MOZ_ASSERT
(
rmc
.
multiplier
<
(
int64_t
(
1
)
<
<
33
)
)
;
masm
.
movl
(
lhs
eax
)
;
masm
.
subl
(
edx
eax
)
;
masm
.
shrl
(
Imm32
(
1
)
eax
)
;
masm
.
addl
(
eax
edx
)
;
masm
.
shrl
(
Imm32
(
rmc
.
shiftAmount
-
1
)
edx
)
;
}
else
{
masm
.
shrl
(
Imm32
(
rmc
.
shiftAmount
)
edx
)
;
}
if
(
!
isDiv
)
{
masm
.
imull
(
Imm32
(
d
)
edx
edx
)
;
masm
.
movl
(
lhs
eax
)
;
masm
.
subl
(
edx
eax
)
;
if
(
!
ins
-
>
mir
(
)
-
>
isTruncated
(
)
)
bailoutIf
(
Assembler
:
:
Signed
ins
-
>
snapshot
(
)
)
;
}
else
if
(
!
ins
-
>
mir
(
)
-
>
isTruncated
(
)
)
{
masm
.
imull
(
Imm32
(
d
)
edx
eax
)
;
masm
.
cmpl
(
lhs
eax
)
;
bailoutIf
(
Assembler
:
:
NotEqual
ins
-
>
snapshot
(
)
)
;
}
}
void
CodeGeneratorX86Shared
:
:
visitMulNegativeZeroCheck
(
MulNegativeZeroCheck
*
ool
)
{
LMulI
*
ins
=
ool
-
>
ins
(
)
;
Register
result
=
ToRegister
(
ins
-
>
output
(
)
)
;
Operand
lhsCopy
=
ToOperand
(
ins
-
>
lhsCopy
(
)
)
;
Operand
rhs
=
ToOperand
(
ins
-
>
rhs
(
)
)
;
MOZ_ASSERT_IF
(
lhsCopy
.
kind
(
)
=
=
Operand
:
:
REG
lhsCopy
.
reg
(
)
!
=
result
.
code
(
)
)
;
masm
.
movl
(
lhsCopy
result
)
;
masm
.
orl
(
rhs
result
)
;
bailoutIf
(
Assembler
:
:
Signed
ins
-
>
snapshot
(
)
)
;
masm
.
mov
(
ImmWord
(
0
)
result
)
;
masm
.
jmp
(
ool
-
>
rejoin
(
)
)
;
}
void
CodeGenerator
:
:
visitDivPowTwoI
(
LDivPowTwoI
*
ins
)
{
Register
lhs
=
ToRegister
(
ins
-
>
numerator
(
)
)
;
DebugOnly
<
Register
>
output
=
ToRegister
(
ins
-
>
output
(
)
)
;
int32_t
shift
=
ins
-
>
shift
(
)
;
bool
negativeDivisor
=
ins
-
>
negativeDivisor
(
)
;
MDiv
*
mir
=
ins
-
>
mir
(
)
;
MOZ_ASSERT
(
lhs
=
=
output
)
;
if
(
!
mir
-
>
isTruncated
(
)
&
&
negativeDivisor
)
{
masm
.
test32
(
lhs
lhs
)
;
bailoutIf
(
Assembler
:
:
Zero
ins
-
>
snapshot
(
)
)
;
}
if
(
shift
)
{
if
(
!
mir
-
>
isTruncated
(
)
)
{
masm
.
test32
(
lhs
Imm32
(
UINT32_MAX
>
>
(
32
-
shift
)
)
)
;
bailoutIf
(
Assembler
:
:
NonZero
ins
-
>
snapshot
(
)
)
;
}
if
(
mir
-
>
isUnsigned
(
)
)
{
masm
.
shrl
(
Imm32
(
shift
)
lhs
)
;
}
else
{
if
(
mir
-
>
canBeNegativeDividend
(
)
)
{
Register
lhsCopy
=
ToRegister
(
ins
-
>
numeratorCopy
(
)
)
;
MOZ_ASSERT
(
lhsCopy
!
=
lhs
)
;
if
(
shift
>
1
)
masm
.
sarl
(
Imm32
(
31
)
lhs
)
;
masm
.
shrl
(
Imm32
(
32
-
shift
)
lhs
)
;
masm
.
addl
(
lhsCopy
lhs
)
;
}
masm
.
sarl
(
Imm32
(
shift
)
lhs
)
;
if
(
negativeDivisor
)
masm
.
negl
(
lhs
)
;
}
return
;
}
if
(
negativeDivisor
)
{
masm
.
negl
(
lhs
)
;
if
(
!
mir
-
>
isTruncated
(
)
)
{
bailoutIf
(
Assembler
:
:
Overflow
ins
-
>
snapshot
(
)
)
;
}
else
if
(
mir
-
>
trapOnError
(
)
)
{
Label
ok
;
masm
.
j
(
Assembler
:
:
NoOverflow
&
ok
)
;
masm
.
wasmTrap
(
wasm
:
:
Trap
:
:
IntegerOverflow
mir
-
>
bytecodeOffset
(
)
)
;
masm
.
bind
(
&
ok
)
;
}
}
else
if
(
mir
-
>
isUnsigned
(
)
&
&
!
mir
-
>
isTruncated
(
)
)
{
masm
.
test32
(
lhs
lhs
)
;
bailoutIf
(
Assembler
:
:
Signed
ins
-
>
snapshot
(
)
)
;
}
}
void
CodeGenerator
:
:
visitDivOrModConstantI
(
LDivOrModConstantI
*
ins
)
{
Register
lhs
=
ToRegister
(
ins
-
>
numerator
(
)
)
;
Register
output
=
ToRegister
(
ins
-
>
output
(
)
)
;
int32_t
d
=
ins
-
>
denominator
(
)
;
MOZ_ASSERT
(
output
=
=
eax
|
|
output
=
=
edx
)
;
MOZ_ASSERT
(
lhs
!
=
eax
&
&
lhs
!
=
edx
)
;
bool
isDiv
=
(
output
=
=
edx
)
;
MOZ_ASSERT
(
(
Abs
(
d
)
&
(
Abs
(
d
)
-
1
)
)
!
=
0
)
;
ReciprocalMulConstants
rmc
=
computeDivisionConstants
(
Abs
(
d
)
31
)
;
masm
.
movl
(
Imm32
(
rmc
.
multiplier
)
eax
)
;
masm
.
imull
(
lhs
)
;
if
(
rmc
.
multiplier
>
INT32_MAX
)
{
MOZ_ASSERT
(
rmc
.
multiplier
<
(
int64_t
(
1
)
<
<
32
)
)
;
masm
.
addl
(
lhs
edx
)
;
}
masm
.
sarl
(
Imm32
(
rmc
.
shiftAmount
)
edx
)
;
if
(
ins
-
>
canBeNegativeDividend
(
)
)
{
masm
.
movl
(
lhs
eax
)
;
masm
.
sarl
(
Imm32
(
31
)
eax
)
;
masm
.
subl
(
eax
edx
)
;
}
if
(
d
<
0
)
masm
.
negl
(
edx
)
;
if
(
!
isDiv
)
{
masm
.
imull
(
Imm32
(
-
d
)
edx
eax
)
;
masm
.
addl
(
lhs
eax
)
;
}
if
(
!
ins
-
>
mir
(
)
-
>
isTruncated
(
)
)
{
if
(
isDiv
)
{
masm
.
imull
(
Imm32
(
d
)
edx
eax
)
;
masm
.
cmp32
(
lhs
eax
)
;
bailoutIf
(
Assembler
:
:
NotEqual
ins
-
>
snapshot
(
)
)
;
if
(
d
<
0
)
{
masm
.
test32
(
lhs
lhs
)
;
bailoutIf
(
Assembler
:
:
Zero
ins
-
>
snapshot
(
)
)
;
}
}
else
if
(
ins
-
>
canBeNegativeDividend
(
)
)
{
Label
done
;
masm
.
cmp32
(
lhs
Imm32
(
0
)
)
;
masm
.
j
(
Assembler
:
:
GreaterThanOrEqual
&
done
)
;
masm
.
test32
(
eax
eax
)
;
bailoutIf
(
Assembler
:
:
Zero
ins
-
>
snapshot
(
)
)
;
masm
.
bind
(
&
done
)
;
}
}
}
void
CodeGenerator
:
:
visitDivI
(
LDivI
*
ins
)
{
Register
remainder
=
ToRegister
(
ins
-
>
remainder
(
)
)
;
Register
lhs
=
ToRegister
(
ins
-
>
lhs
(
)
)
;
Register
rhs
=
ToRegister
(
ins
-
>
rhs
(
)
)
;
Register
output
=
ToRegister
(
ins
-
>
output
(
)
)
;
MDiv
*
mir
=
ins
-
>
mir
(
)
;
MOZ_ASSERT_IF
(
lhs
!
=
rhs
rhs
!
=
eax
)
;
MOZ_ASSERT
(
rhs
!
=
edx
)
;
MOZ_ASSERT
(
remainder
=
=
edx
)
;
MOZ_ASSERT
(
output
=
=
eax
)
;
Label
done
;
ReturnZero
*
ool
=
nullptr
;
if
(
lhs
!
=
eax
)
masm
.
mov
(
lhs
eax
)
;
if
(
mir
-
>
canBeDivideByZero
(
)
)
{
masm
.
test32
(
rhs
rhs
)
;
if
(
mir
-
>
trapOnError
(
)
)
{
Label
nonZero
;
masm
.
j
(
Assembler
:
:
NonZero
&
nonZero
)
;
masm
.
wasmTrap
(
wasm
:
:
Trap
:
:
IntegerDivideByZero
mir
-
>
bytecodeOffset
(
)
)
;
masm
.
bind
(
&
nonZero
)
;
}
else
if
(
mir
-
>
canTruncateInfinities
(
)
)
{
if
(
!
ool
)
ool
=
new
(
alloc
(
)
)
ReturnZero
(
output
)
;
masm
.
j
(
Assembler
:
:
Zero
ool
-
>
entry
(
)
)
;
}
else
{
MOZ_ASSERT
(
mir
-
>
fallible
(
)
)
;
bailoutIf
(
Assembler
:
:
Zero
ins
-
>
snapshot
(
)
)
;
}
}
if
(
mir
-
>
canBeNegativeOverflow
(
)
)
{
Label
notOverflow
;
masm
.
cmp32
(
lhs
Imm32
(
INT32_MIN
)
)
;
masm
.
j
(
Assembler
:
:
NotEqual
&
notOverflow
)
;
masm
.
cmp32
(
rhs
Imm32
(
-
1
)
)
;
if
(
mir
-
>
trapOnError
(
)
)
{
masm
.
j
(
Assembler
:
:
NotEqual
&
notOverflow
)
;
masm
.
wasmTrap
(
wasm
:
:
Trap
:
:
IntegerOverflow
mir
-
>
bytecodeOffset
(
)
)
;
}
else
if
(
mir
-
>
canTruncateOverflow
(
)
)
{
masm
.
j
(
Assembler
:
:
Equal
&
done
)
;
}
else
{
MOZ_ASSERT
(
mir
-
>
fallible
(
)
)
;
bailoutIf
(
Assembler
:
:
Equal
ins
-
>
snapshot
(
)
)
;
}
masm
.
bind
(
&
notOverflow
)
;
}
if
(
!
mir
-
>
canTruncateNegativeZero
(
)
&
&
mir
-
>
canBeNegativeZero
(
)
)
{
Label
nonzero
;
masm
.
test32
(
lhs
lhs
)
;
masm
.
j
(
Assembler
:
:
NonZero
&
nonzero
)
;
masm
.
cmp32
(
rhs
Imm32
(
0
)
)
;
bailoutIf
(
Assembler
:
:
LessThan
ins
-
>
snapshot
(
)
)
;
masm
.
bind
(
&
nonzero
)
;
}
if
(
lhs
!
=
eax
)
masm
.
mov
(
lhs
eax
)
;
masm
.
cdq
(
)
;
masm
.
idiv
(
rhs
)
;
if
(
!
mir
-
>
canTruncateRemainder
(
)
)
{
masm
.
test32
(
remainder
remainder
)
;
bailoutIf
(
Assembler
:
:
NonZero
ins
-
>
snapshot
(
)
)
;
}
masm
.
bind
(
&
done
)
;
if
(
ool
)
{
addOutOfLineCode
(
ool
mir
)
;
masm
.
bind
(
ool
-
>
rejoin
(
)
)
;
}
}
void
CodeGenerator
:
:
visitModPowTwoI
(
LModPowTwoI
*
ins
)
{
Register
lhs
=
ToRegister
(
ins
-
>
getOperand
(
0
)
)
;
int32_t
shift
=
ins
-
>
shift
(
)
;
Label
negative
;
if
(
!
ins
-
>
mir
(
)
-
>
isUnsigned
(
)
&
&
ins
-
>
mir
(
)
-
>
canBeNegativeDividend
(
)
)
{
masm
.
branchTest32
(
Assembler
:
:
Signed
lhs
lhs
&
negative
)
;
}
masm
.
andl
(
Imm32
(
(
uint32_t
(
1
)
<
<
shift
)
-
1
)
lhs
)
;
if
(
!
ins
-
>
mir
(
)
-
>
isUnsigned
(
)
&
&
ins
-
>
mir
(
)
-
>
canBeNegativeDividend
(
)
)
{
Label
done
;
masm
.
jump
(
&
done
)
;
masm
.
bind
(
&
negative
)
;
masm
.
negl
(
lhs
)
;
masm
.
andl
(
Imm32
(
(
uint32_t
(
1
)
<
<
shift
)
-
1
)
lhs
)
;
masm
.
negl
(
lhs
)
;
if
(
!
ins
-
>
mir
(
)
-
>
isTruncated
(
)
)
bailoutIf
(
Assembler
:
:
Zero
ins
-
>
snapshot
(
)
)
;
masm
.
bind
(
&
done
)
;
}
}
class
ModOverflowCheck
:
public
OutOfLineCodeBase
<
CodeGeneratorX86Shared
>
{
Label
done_
;
LModI
*
ins_
;
Register
rhs_
;
public
:
explicit
ModOverflowCheck
(
LModI
*
ins
Register
rhs
)
:
ins_
(
ins
)
rhs_
(
rhs
)
{
}
virtual
void
accept
(
CodeGeneratorX86Shared
*
codegen
)
override
{
codegen
-
>
visitModOverflowCheck
(
this
)
;
}
Label
*
done
(
)
{
return
&
done_
;
}
LModI
*
ins
(
)
const
{
return
ins_
;
}
Register
rhs
(
)
const
{
return
rhs_
;
}
}
;
void
CodeGeneratorX86Shared
:
:
visitModOverflowCheck
(
ModOverflowCheck
*
ool
)
{
masm
.
cmp32
(
ool
-
>
rhs
(
)
Imm32
(
-
1
)
)
;
if
(
ool
-
>
ins
(
)
-
>
mir
(
)
-
>
isTruncated
(
)
)
{
masm
.
j
(
Assembler
:
:
NotEqual
ool
-
>
rejoin
(
)
)
;
masm
.
mov
(
ImmWord
(
0
)
edx
)
;
masm
.
jmp
(
ool
-
>
done
(
)
)
;
}
else
{
bailoutIf
(
Assembler
:
:
Equal
ool
-
>
ins
(
)
-
>
snapshot
(
)
)
;
masm
.
jmp
(
ool
-
>
rejoin
(
)
)
;
}
}
void
CodeGenerator
:
:
visitModI
(
LModI
*
ins
)
{
Register
remainder
=
ToRegister
(
ins
-
>
remainder
(
)
)
;
Register
lhs
=
ToRegister
(
ins
-
>
lhs
(
)
)
;
Register
rhs
=
ToRegister
(
ins
-
>
rhs
(
)
)
;
MOZ_ASSERT_IF
(
lhs
!
=
rhs
rhs
!
=
eax
)
;
MOZ_ASSERT
(
rhs
!
=
edx
)
;
MOZ_ASSERT
(
remainder
=
=
edx
)
;
MOZ_ASSERT
(
ToRegister
(
ins
-
>
getTemp
(
0
)
)
=
=
eax
)
;
Label
done
;
ReturnZero
*
ool
=
nullptr
;
ModOverflowCheck
*
overflow
=
nullptr
;
if
(
lhs
!
=
eax
)
masm
.
mov
(
lhs
eax
)
;
MMod
*
mir
=
ins
-
>
mir
(
)
;
if
(
mir
-
>
canBeDivideByZero
(
)
)
{
masm
.
test32
(
rhs
rhs
)
;
if
(
mir
-
>
isTruncated
(
)
)
{
if
(
mir
-
>
trapOnError
(
)
)
{
Label
nonZero
;
masm
.
j
(
Assembler
:
:
NonZero
&
nonZero
)
;
masm
.
wasmTrap
(
wasm
:
:
Trap
:
:
IntegerDivideByZero
mir
-
>
bytecodeOffset
(
)
)
;
masm
.
bind
(
&
nonZero
)
;
}
else
{
if
(
!
ool
)
ool
=
new
(
alloc
(
)
)
ReturnZero
(
edx
)
;
masm
.
j
(
Assembler
:
:
Zero
ool
-
>
entry
(
)
)
;
}
}
else
{
bailoutIf
(
Assembler
:
:
Zero
ins
-
>
snapshot
(
)
)
;
}
}
Label
negative
;
if
(
mir
-
>
canBeNegativeDividend
(
)
)
masm
.
branchTest32
(
Assembler
:
:
Signed
lhs
lhs
&
negative
)
;
{
if
(
mir
-
>
canBePowerOfTwoDivisor
(
)
)
{
MOZ_ASSERT
(
rhs
!
=
remainder
)
;
Label
notPowerOfTwo
;
masm
.
mov
(
rhs
remainder
)
;
masm
.
subl
(
Imm32
(
1
)
remainder
)
;
masm
.
branchTest32
(
Assembler
:
:
NonZero
remainder
rhs
&
notPowerOfTwo
)
;
{
masm
.
andl
(
lhs
remainder
)
;
masm
.
jmp
(
&
done
)
;
}
masm
.
bind
(
&
notPowerOfTwo
)
;
}
masm
.
mov
(
ImmWord
(
0
)
edx
)
;
masm
.
idiv
(
rhs
)
;
}
if
(
mir
-
>
canBeNegativeDividend
(
)
)
{
masm
.
jump
(
&
done
)
;
masm
.
bind
(
&
negative
)
;
Label
notmin
;
masm
.
cmp32
(
lhs
Imm32
(
INT32_MIN
)
)
;
overflow
=
new
(
alloc
(
)
)
ModOverflowCheck
(
ins
rhs
)
;
masm
.
j
(
Assembler
:
:
Equal
overflow
-
>
entry
(
)
)
;
masm
.
bind
(
overflow
-
>
rejoin
(
)
)
;
masm
.
cdq
(
)
;
masm
.
idiv
(
rhs
)
;
if
(
!
mir
-
>
isTruncated
(
)
)
{
masm
.
test32
(
remainder
remainder
)
;
bailoutIf
(
Assembler
:
:
Zero
ins
-
>
snapshot
(
)
)
;
}
}
masm
.
bind
(
&
done
)
;
if
(
overflow
)
{
addOutOfLineCode
(
overflow
mir
)
;
masm
.
bind
(
overflow
-
>
done
(
)
)
;
}
if
(
ool
)
{
addOutOfLineCode
(
ool
mir
)
;
masm
.
bind
(
ool
-
>
rejoin
(
)
)
;
}
}
void
CodeGenerator
:
:
visitBitNotI
(
LBitNotI
*
ins
)
{
const
LAllocation
*
input
=
ins
-
>
getOperand
(
0
)
;
MOZ_ASSERT
(
!
input
-
>
isConstant
(
)
)
;
masm
.
notl
(
ToOperand
(
input
)
)
;
}
void
CodeGenerator
:
:
visitBitOpI
(
LBitOpI
*
ins
)
{
const
LAllocation
*
lhs
=
ins
-
>
getOperand
(
0
)
;
const
LAllocation
*
rhs
=
ins
-
>
getOperand
(
1
)
;
switch
(
ins
-
>
bitop
(
)
)
{
case
JSOP_BITOR
:
if
(
rhs
-
>
isConstant
(
)
)
masm
.
orl
(
Imm32
(
ToInt32
(
rhs
)
)
ToOperand
(
lhs
)
)
;
else
masm
.
orl
(
ToOperand
(
rhs
)
ToRegister
(
lhs
)
)
;
break
;
case
JSOP_BITXOR
:
if
(
rhs
-
>
isConstant
(
)
)
masm
.
xorl
(
Imm32
(
ToInt32
(
rhs
)
)
ToOperand
(
lhs
)
)
;
else
masm
.
xorl
(
ToOperand
(
rhs
)
ToRegister
(
lhs
)
)
;
break
;
case
JSOP_BITAND
:
if
(
rhs
-
>
isConstant
(
)
)
masm
.
andl
(
Imm32
(
ToInt32
(
rhs
)
)
ToOperand
(
lhs
)
)
;
else
masm
.
andl
(
ToOperand
(
rhs
)
ToRegister
(
lhs
)
)
;
break
;
default
:
MOZ_CRASH
(
"
unexpected
binary
opcode
"
)
;
}
}
void
CodeGenerator
:
:
visitBitOpI64
(
LBitOpI64
*
lir
)
{
const
LInt64Allocation
lhs
=
lir
-
>
getInt64Operand
(
LBitOpI64
:
:
Lhs
)
;
const
LInt64Allocation
rhs
=
lir
-
>
getInt64Operand
(
LBitOpI64
:
:
Rhs
)
;
MOZ_ASSERT
(
ToOutRegister64
(
lir
)
=
=
ToRegister64
(
lhs
)
)
;
switch
(
lir
-
>
bitop
(
)
)
{
case
JSOP_BITOR
:
if
(
IsConstant
(
rhs
)
)
masm
.
or64
(
Imm64
(
ToInt64
(
rhs
)
)
ToRegister64
(
lhs
)
)
;
else
masm
.
or64
(
ToOperandOrRegister64
(
rhs
)
ToRegister64
(
lhs
)
)
;
break
;
case
JSOP_BITXOR
:
if
(
IsConstant
(
rhs
)
)
masm
.
xor64
(
Imm64
(
ToInt64
(
rhs
)
)
ToRegister64
(
lhs
)
)
;
else
masm
.
xor64
(
ToOperandOrRegister64
(
rhs
)
ToRegister64
(
lhs
)
)
;
break
;
case
JSOP_BITAND
:
if
(
IsConstant
(
rhs
)
)
masm
.
and64
(
Imm64
(
ToInt64
(
rhs
)
)
ToRegister64
(
lhs
)
)
;
else
masm
.
and64
(
ToOperandOrRegister64
(
rhs
)
ToRegister64
(
lhs
)
)
;
break
;
default
:
MOZ_CRASH
(
"
unexpected
binary
opcode
"
)
;
}
}
void
CodeGenerator
:
:
visitShiftI
(
LShiftI
*
ins
)
{
Register
lhs
=
ToRegister
(
ins
-
>
lhs
(
)
)
;
const
LAllocation
*
rhs
=
ins
-
>
rhs
(
)
;
if
(
rhs
-
>
isConstant
(
)
)
{
int32_t
shift
=
ToInt32
(
rhs
)
&
0x1F
;
switch
(
ins
-
>
bitop
(
)
)
{
case
JSOP_LSH
:
if
(
shift
)
masm
.
shll
(
Imm32
(
shift
)
lhs
)
;
break
;
case
JSOP_RSH
:
if
(
shift
)
masm
.
sarl
(
Imm32
(
shift
)
lhs
)
;
break
;
case
JSOP_URSH
:
if
(
shift
)
{
masm
.
shrl
(
Imm32
(
shift
)
lhs
)
;
}
else
if
(
ins
-
>
mir
(
)
-
>
toUrsh
(
)
-
>
fallible
(
)
)
{
masm
.
test32
(
lhs
lhs
)
;
bailoutIf
(
Assembler
:
:
Signed
ins
-
>
snapshot
(
)
)
;
}
break
;
default
:
MOZ_CRASH
(
"
Unexpected
shift
op
"
)
;
}
}
else
{
MOZ_ASSERT
(
ToRegister
(
rhs
)
=
=
ecx
)
;
switch
(
ins
-
>
bitop
(
)
)
{
case
JSOP_LSH
:
masm
.
shll_cl
(
lhs
)
;
break
;
case
JSOP_RSH
:
masm
.
sarl_cl
(
lhs
)
;
break
;
case
JSOP_URSH
:
masm
.
shrl_cl
(
lhs
)
;
if
(
ins
-
>
mir
(
)
-
>
toUrsh
(
)
-
>
fallible
(
)
)
{
masm
.
test32
(
lhs
lhs
)
;
bailoutIf
(
Assembler
:
:
Signed
ins
-
>
snapshot
(
)
)
;
}
break
;
default
:
MOZ_CRASH
(
"
Unexpected
shift
op
"
)
;
}
}
}
void
CodeGenerator
:
:
visitShiftI64
(
LShiftI64
*
lir
)
{
const
LInt64Allocation
lhs
=
lir
-
>
getInt64Operand
(
LShiftI64
:
:
Lhs
)
;
LAllocation
*
rhs
=
lir
-
>
getOperand
(
LShiftI64
:
:
Rhs
)
;
MOZ_ASSERT
(
ToOutRegister64
(
lir
)
=
=
ToRegister64
(
lhs
)
)
;
if
(
rhs
-
>
isConstant
(
)
)
{
int32_t
shift
=
int32_t
(
rhs
-
>
toConstant
(
)
-
>
toInt64
(
)
&
0x3F
)
;
switch
(
lir
-
>
bitop
(
)
)
{
case
JSOP_LSH
:
if
(
shift
)
masm
.
lshift64
(
Imm32
(
shift
)
ToRegister64
(
lhs
)
)
;
break
;
case
JSOP_RSH
:
if
(
shift
)
masm
.
rshift64Arithmetic
(
Imm32
(
shift
)
ToRegister64
(
lhs
)
)
;
break
;
case
JSOP_URSH
:
if
(
shift
)
masm
.
rshift64
(
Imm32
(
shift
)
ToRegister64
(
lhs
)
)
;
break
;
default
:
MOZ_CRASH
(
"
Unexpected
shift
op
"
)
;
}
return
;
}
MOZ_ASSERT
(
ToRegister
(
rhs
)
=
=
ecx
)
;
switch
(
lir
-
>
bitop
(
)
)
{
case
JSOP_LSH
:
masm
.
lshift64
(
ecx
ToRegister64
(
lhs
)
)
;
break
;
case
JSOP_RSH
:
masm
.
rshift64Arithmetic
(
ecx
ToRegister64
(
lhs
)
)
;
break
;
case
JSOP_URSH
:
masm
.
rshift64
(
ecx
ToRegister64
(
lhs
)
)
;
break
;
default
:
MOZ_CRASH
(
"
Unexpected
shift
op
"
)
;
}
}
void
CodeGenerator
:
:
visitUrshD
(
LUrshD
*
ins
)
{
Register
lhs
=
ToRegister
(
ins
-
>
lhs
(
)
)
;
MOZ_ASSERT
(
ToRegister
(
ins
-
>
temp
(
)
)
=
=
lhs
)
;
const
LAllocation
*
rhs
=
ins
-
>
rhs
(
)
;
FloatRegister
out
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
if
(
rhs
-
>
isConstant
(
)
)
{
int32_t
shift
=
ToInt32
(
rhs
)
&
0x1F
;
if
(
shift
)
masm
.
shrl
(
Imm32
(
shift
)
lhs
)
;
}
else
{
MOZ_ASSERT
(
ToRegister
(
rhs
)
=
=
ecx
)
;
masm
.
shrl_cl
(
lhs
)
;
}
masm
.
convertUInt32ToDouble
(
lhs
out
)
;
}
Operand
CodeGeneratorX86Shared
:
:
ToOperand
(
const
LAllocation
&
a
)
{
if
(
a
.
isGeneralReg
(
)
)
return
Operand
(
a
.
toGeneralReg
(
)
-
>
reg
(
)
)
;
if
(
a
.
isFloatReg
(
)
)
return
Operand
(
a
.
toFloatReg
(
)
-
>
reg
(
)
)
;
return
Operand
(
masm
.
getStackPointer
(
)
ToStackOffset
(
&
a
)
)
;
}
Operand
CodeGeneratorX86Shared
:
:
ToOperand
(
const
LAllocation
*
a
)
{
return
ToOperand
(
*
a
)
;
}
Operand
CodeGeneratorX86Shared
:
:
ToOperand
(
const
LDefinition
*
def
)
{
return
ToOperand
(
def
-
>
output
(
)
)
;
}
MoveOperand
CodeGeneratorX86Shared
:
:
toMoveOperand
(
LAllocation
a
)
const
{
if
(
a
.
isGeneralReg
(
)
)
return
MoveOperand
(
ToRegister
(
a
)
)
;
if
(
a
.
isFloatReg
(
)
)
return
MoveOperand
(
ToFloatRegister
(
a
)
)
;
return
MoveOperand
(
StackPointer
ToStackOffset
(
a
)
)
;
}
class
OutOfLineTableSwitch
:
public
OutOfLineCodeBase
<
CodeGeneratorX86Shared
>
{
MTableSwitch
*
mir_
;
CodeLabel
jumpLabel_
;
void
accept
(
CodeGeneratorX86Shared
*
codegen
)
override
{
codegen
-
>
visitOutOfLineTableSwitch
(
this
)
;
}
public
:
explicit
OutOfLineTableSwitch
(
MTableSwitch
*
mir
)
:
mir_
(
mir
)
{
}
MTableSwitch
*
mir
(
)
const
{
return
mir_
;
}
CodeLabel
*
jumpLabel
(
)
{
return
&
jumpLabel_
;
}
}
;
void
CodeGeneratorX86Shared
:
:
visitOutOfLineTableSwitch
(
OutOfLineTableSwitch
*
ool
)
{
MTableSwitch
*
mir
=
ool
-
>
mir
(
)
;
masm
.
haltingAlign
(
sizeof
(
void
*
)
)
;
masm
.
bind
(
ool
-
>
jumpLabel
(
)
)
;
masm
.
addCodeLabel
(
*
ool
-
>
jumpLabel
(
)
)
;
for
(
size_t
i
=
0
;
i
<
mir
-
>
numCases
(
)
;
i
+
+
)
{
LBlock
*
caseblock
=
skipTrivialBlocks
(
mir
-
>
getCase
(
i
)
)
-
>
lir
(
)
;
Label
*
caseheader
=
caseblock
-
>
label
(
)
;
uint32_t
caseoffset
=
caseheader
-
>
offset
(
)
;
CodeLabel
cl
;
masm
.
writeCodePointer
(
&
cl
)
;
cl
.
target
(
)
-
>
bind
(
caseoffset
)
;
masm
.
addCodeLabel
(
cl
)
;
}
}
void
CodeGeneratorX86Shared
:
:
emitTableSwitchDispatch
(
MTableSwitch
*
mir
Register
index
Register
base
)
{
Label
*
defaultcase
=
skipTrivialBlocks
(
mir
-
>
getDefault
(
)
)
-
>
lir
(
)
-
>
label
(
)
;
if
(
mir
-
>
low
(
)
!
=
0
)
masm
.
subl
(
Imm32
(
mir
-
>
low
(
)
)
index
)
;
int32_t
cases
=
mir
-
>
numCases
(
)
;
masm
.
cmp32
(
index
Imm32
(
cases
)
)
;
masm
.
j
(
AssemblerX86Shared
:
:
AboveOrEqual
defaultcase
)
;
OutOfLineTableSwitch
*
ool
=
new
(
alloc
(
)
)
OutOfLineTableSwitch
(
mir
)
;
addOutOfLineCode
(
ool
mir
)
;
masm
.
mov
(
ool
-
>
jumpLabel
(
)
base
)
;
BaseIndex
pointer
(
base
index
ScalePointer
)
;
masm
.
branchToComputedAddress
(
pointer
)
;
}
void
CodeGenerator
:
:
visitMathD
(
LMathD
*
math
)
{
FloatRegister
lhs
=
ToFloatRegister
(
math
-
>
lhs
(
)
)
;
Operand
rhs
=
ToOperand
(
math
-
>
rhs
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
math
-
>
output
(
)
)
;
switch
(
math
-
>
jsop
(
)
)
{
case
JSOP_ADD
:
masm
.
vaddsd
(
rhs
lhs
output
)
;
break
;
case
JSOP_SUB
:
masm
.
vsubsd
(
rhs
lhs
output
)
;
break
;
case
JSOP_MUL
:
masm
.
vmulsd
(
rhs
lhs
output
)
;
break
;
case
JSOP_DIV
:
masm
.
vdivsd
(
rhs
lhs
output
)
;
break
;
default
:
MOZ_CRASH
(
"
unexpected
opcode
"
)
;
}
}
void
CodeGenerator
:
:
visitMathF
(
LMathF
*
math
)
{
FloatRegister
lhs
=
ToFloatRegister
(
math
-
>
lhs
(
)
)
;
Operand
rhs
=
ToOperand
(
math
-
>
rhs
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
math
-
>
output
(
)
)
;
switch
(
math
-
>
jsop
(
)
)
{
case
JSOP_ADD
:
masm
.
vaddss
(
rhs
lhs
output
)
;
break
;
case
JSOP_SUB
:
masm
.
vsubss
(
rhs
lhs
output
)
;
break
;
case
JSOP_MUL
:
masm
.
vmulss
(
rhs
lhs
output
)
;
break
;
case
JSOP_DIV
:
masm
.
vdivss
(
rhs
lhs
output
)
;
break
;
default
:
MOZ_CRASH
(
"
unexpected
opcode
"
)
;
}
}
void
CodeGenerator
:
:
visitFloor
(
LFloor
*
lir
)
{
FloatRegister
input
=
ToFloatRegister
(
lir
-
>
input
(
)
)
;
Register
output
=
ToRegister
(
lir
-
>
output
(
)
)
;
Label
bailout
;
if
(
AssemblerX86Shared
:
:
HasSSE41
(
)
)
{
masm
.
branchNegativeZero
(
input
output
&
bailout
)
;
bailoutFrom
(
&
bailout
lir
-
>
snapshot
(
)
)
;
{
ScratchDoubleScope
scratch
(
masm
)
;
masm
.
vroundsd
(
X86Encoding
:
:
RoundDown
input
scratch
scratch
)
;
bailoutCvttsd2si
(
scratch
output
lir
-
>
snapshot
(
)
)
;
}
}
else
{
Label
negative
end
;
{
ScratchDoubleScope
scratch
(
masm
)
;
masm
.
zeroDouble
(
scratch
)
;
masm
.
branchDouble
(
Assembler
:
:
DoubleLessThan
input
scratch
&
negative
)
;
}
masm
.
branchNegativeZero
(
input
output
&
bailout
)
;
bailoutFrom
(
&
bailout
lir
-
>
snapshot
(
)
)
;
bailoutCvttsd2si
(
input
output
lir
-
>
snapshot
(
)
)
;
masm
.
jump
(
&
end
)
;
masm
.
bind
(
&
negative
)
;
{
bailoutCvttsd2si
(
input
output
lir
-
>
snapshot
(
)
)
;
{
ScratchDoubleScope
scratch
(
masm
)
;
masm
.
convertInt32ToDouble
(
output
scratch
)
;
masm
.
branchDouble
(
Assembler
:
:
DoubleEqualOrUnordered
input
scratch
&
end
)
;
}
masm
.
subl
(
Imm32
(
1
)
output
)
;
}
masm
.
bind
(
&
end
)
;
}
}
void
CodeGenerator
:
:
visitFloorF
(
LFloorF
*
lir
)
{
FloatRegister
input
=
ToFloatRegister
(
lir
-
>
input
(
)
)
;
Register
output
=
ToRegister
(
lir
-
>
output
(
)
)
;
Label
bailout
;
if
(
AssemblerX86Shared
:
:
HasSSE41
(
)
)
{
masm
.
branchNegativeZeroFloat32
(
input
output
&
bailout
)
;
bailoutFrom
(
&
bailout
lir
-
>
snapshot
(
)
)
;
{
ScratchFloat32Scope
scratch
(
masm
)
;
masm
.
vroundss
(
X86Encoding
:
:
RoundDown
input
scratch
scratch
)
;
bailoutCvttss2si
(
scratch
output
lir
-
>
snapshot
(
)
)
;
}
}
else
{
Label
negative
end
;
{
ScratchFloat32Scope
scratch
(
masm
)
;
masm
.
zeroFloat32
(
scratch
)
;
masm
.
branchFloat
(
Assembler
:
:
DoubleLessThan
input
scratch
&
negative
)
;
}
masm
.
branchNegativeZeroFloat32
(
input
output
&
bailout
)
;
bailoutFrom
(
&
bailout
lir
-
>
snapshot
(
)
)
;
bailoutCvttss2si
(
input
output
lir
-
>
snapshot
(
)
)
;
masm
.
jump
(
&
end
)
;
masm
.
bind
(
&
negative
)
;
{
bailoutCvttss2si
(
input
output
lir
-
>
snapshot
(
)
)
;
{
ScratchFloat32Scope
scratch
(
masm
)
;
masm
.
convertInt32ToFloat32
(
output
scratch
)
;
masm
.
branchFloat
(
Assembler
:
:
DoubleEqualOrUnordered
input
scratch
&
end
)
;
}
masm
.
subl
(
Imm32
(
1
)
output
)
;
}
masm
.
bind
(
&
end
)
;
}
}
void
CodeGenerator
:
:
visitCeil
(
LCeil
*
lir
)
{
FloatRegister
input
=
ToFloatRegister
(
lir
-
>
input
(
)
)
;
ScratchDoubleScope
scratch
(
masm
)
;
Register
output
=
ToRegister
(
lir
-
>
output
(
)
)
;
Label
bailout
lessThanMinusOne
;
masm
.
loadConstantDouble
(
-
1
scratch
)
;
masm
.
branchDouble
(
Assembler
:
:
DoubleLessThanOrEqualOrUnordered
input
scratch
&
lessThanMinusOne
)
;
masm
.
vmovmskpd
(
input
output
)
;
masm
.
branchTest32
(
Assembler
:
:
NonZero
output
Imm32
(
1
)
&
bailout
)
;
bailoutFrom
(
&
bailout
lir
-
>
snapshot
(
)
)
;
if
(
AssemblerX86Shared
:
:
HasSSE41
(
)
)
{
masm
.
bind
(
&
lessThanMinusOne
)
;
masm
.
vroundsd
(
X86Encoding
:
:
RoundUp
input
scratch
scratch
)
;
bailoutCvttsd2si
(
scratch
output
lir
-
>
snapshot
(
)
)
;
return
;
}
Label
end
;
bailoutCvttsd2si
(
input
output
lir
-
>
snapshot
(
)
)
;
masm
.
convertInt32ToDouble
(
output
scratch
)
;
masm
.
branchDouble
(
Assembler
:
:
DoubleEqualOrUnordered
input
scratch
&
end
)
;
masm
.
addl
(
Imm32
(
1
)
output
)
;
bailoutIf
(
Assembler
:
:
Overflow
lir
-
>
snapshot
(
)
)
;
masm
.
jump
(
&
end
)
;
masm
.
bind
(
&
lessThanMinusOne
)
;
bailoutCvttsd2si
(
input
output
lir
-
>
snapshot
(
)
)
;
masm
.
bind
(
&
end
)
;
}
void
CodeGenerator
:
:
visitCeilF
(
LCeilF
*
lir
)
{
FloatRegister
input
=
ToFloatRegister
(
lir
-
>
input
(
)
)
;
ScratchFloat32Scope
scratch
(
masm
)
;
Register
output
=
ToRegister
(
lir
-
>
output
(
)
)
;
Label
bailout
lessThanMinusOne
;
masm
.
loadConstantFloat32
(
-
1
.
f
scratch
)
;
masm
.
branchFloat
(
Assembler
:
:
DoubleLessThanOrEqualOrUnordered
input
scratch
&
lessThanMinusOne
)
;
masm
.
vmovmskps
(
input
output
)
;
masm
.
branchTest32
(
Assembler
:
:
NonZero
output
Imm32
(
1
)
&
bailout
)
;
bailoutFrom
(
&
bailout
lir
-
>
snapshot
(
)
)
;
if
(
AssemblerX86Shared
:
:
HasSSE41
(
)
)
{
masm
.
bind
(
&
lessThanMinusOne
)
;
masm
.
vroundss
(
X86Encoding
:
:
RoundUp
input
scratch
scratch
)
;
bailoutCvttss2si
(
scratch
output
lir
-
>
snapshot
(
)
)
;
return
;
}
Label
end
;
bailoutCvttss2si
(
input
output
lir
-
>
snapshot
(
)
)
;
masm
.
convertInt32ToFloat32
(
output
scratch
)
;
masm
.
branchFloat
(
Assembler
:
:
DoubleEqualOrUnordered
input
scratch
&
end
)
;
masm
.
addl
(
Imm32
(
1
)
output
)
;
bailoutIf
(
Assembler
:
:
Overflow
lir
-
>
snapshot
(
)
)
;
masm
.
jump
(
&
end
)
;
masm
.
bind
(
&
lessThanMinusOne
)
;
bailoutCvttss2si
(
input
output
lir
-
>
snapshot
(
)
)
;
masm
.
bind
(
&
end
)
;
}
void
CodeGenerator
:
:
visitRound
(
LRound
*
lir
)
{
FloatRegister
input
=
ToFloatRegister
(
lir
-
>
input
(
)
)
;
FloatRegister
temp
=
ToFloatRegister
(
lir
-
>
temp
(
)
)
;
ScratchDoubleScope
scratch
(
masm
)
;
Register
output
=
ToRegister
(
lir
-
>
output
(
)
)
;
Label
negativeOrZero
negative
end
bailout
;
masm
.
zeroDouble
(
scratch
)
;
masm
.
loadConstantDouble
(
GetBiggestNumberLessThan
(
0
.
5
)
temp
)
;
masm
.
branchDouble
(
Assembler
:
:
DoubleLessThanOrEqual
input
scratch
&
negativeOrZero
)
;
masm
.
addDouble
(
input
temp
)
;
bailoutCvttsd2si
(
temp
output
lir
-
>
snapshot
(
)
)
;
masm
.
jump
(
&
end
)
;
masm
.
bind
(
&
negativeOrZero
)
;
masm
.
j
(
Assembler
:
:
NotEqual
&
negative
)
;
masm
.
branchNegativeZero
(
input
output
&
bailout
false
)
;
bailoutFrom
(
&
bailout
lir
-
>
snapshot
(
)
)
;
masm
.
xor32
(
output
output
)
;
masm
.
jump
(
&
end
)
;
masm
.
bind
(
&
negative
)
;
Label
loadJoin
;
masm
.
loadConstantDouble
(
-
0
.
5
scratch
)
;
masm
.
branchDouble
(
Assembler
:
:
DoubleLessThan
input
scratch
&
loadJoin
)
;
masm
.
loadConstantDouble
(
0
.
5
temp
)
;
masm
.
bind
(
&
loadJoin
)
;
if
(
AssemblerX86Shared
:
:
HasSSE41
(
)
)
{
masm
.
addDouble
(
input
temp
)
;
masm
.
vroundsd
(
X86Encoding
:
:
RoundDown
temp
scratch
scratch
)
;
bailoutCvttsd2si
(
scratch
output
lir
-
>
snapshot
(
)
)
;
masm
.
test32
(
output
output
)
;
bailoutIf
(
Assembler
:
:
Zero
lir
-
>
snapshot
(
)
)
;
}
else
{
masm
.
addDouble
(
input
temp
)
;
{
masm
.
compareDouble
(
Assembler
:
:
DoubleGreaterThanOrEqual
temp
scratch
)
;
bailoutIf
(
Assembler
:
:
DoubleGreaterThanOrEqual
lir
-
>
snapshot
(
)
)
;
bailoutCvttsd2si
(
temp
output
lir
-
>
snapshot
(
)
)
;
masm
.
convertInt32ToDouble
(
output
scratch
)
;
masm
.
branchDouble
(
Assembler
:
:
DoubleEqualOrUnordered
temp
scratch
&
end
)
;
masm
.
subl
(
Imm32
(
1
)
output
)
;
}
}
masm
.
bind
(
&
end
)
;
}
void
CodeGenerator
:
:
visitRoundF
(
LRoundF
*
lir
)
{
FloatRegister
input
=
ToFloatRegister
(
lir
-
>
input
(
)
)
;
FloatRegister
temp
=
ToFloatRegister
(
lir
-
>
temp
(
)
)
;
ScratchFloat32Scope
scratch
(
masm
)
;
Register
output
=
ToRegister
(
lir
-
>
output
(
)
)
;
Label
negativeOrZero
negative
end
bailout
;
masm
.
zeroFloat32
(
scratch
)
;
masm
.
loadConstantFloat32
(
GetBiggestNumberLessThan
(
0
.
5f
)
temp
)
;
masm
.
branchFloat
(
Assembler
:
:
DoubleLessThanOrEqual
input
scratch
&
negativeOrZero
)
;
masm
.
addFloat32
(
input
temp
)
;
bailoutCvttss2si
(
temp
output
lir
-
>
snapshot
(
)
)
;
masm
.
jump
(
&
end
)
;
masm
.
bind
(
&
negativeOrZero
)
;
masm
.
j
(
Assembler
:
:
NotEqual
&
negative
)
;
masm
.
branchNegativeZeroFloat32
(
input
output
&
bailout
)
;
bailoutFrom
(
&
bailout
lir
-
>
snapshot
(
)
)
;
masm
.
xor32
(
output
output
)
;
masm
.
jump
(
&
end
)
;
masm
.
bind
(
&
negative
)
;
Label
loadJoin
;
masm
.
loadConstantFloat32
(
-
0
.
5f
scratch
)
;
masm
.
branchFloat
(
Assembler
:
:
DoubleLessThan
input
scratch
&
loadJoin
)
;
masm
.
loadConstantFloat32
(
0
.
5f
temp
)
;
masm
.
bind
(
&
loadJoin
)
;
if
(
AssemblerX86Shared
:
:
HasSSE41
(
)
)
{
masm
.
addFloat32
(
input
temp
)
;
masm
.
vroundss
(
X86Encoding
:
:
RoundDown
temp
scratch
scratch
)
;
bailoutCvttss2si
(
scratch
output
lir
-
>
snapshot
(
)
)
;
masm
.
test32
(
output
output
)
;
bailoutIf
(
Assembler
:
:
Zero
lir
-
>
snapshot
(
)
)
;
}
else
{
masm
.
addFloat32
(
input
temp
)
;
{
masm
.
compareFloat
(
Assembler
:
:
DoubleGreaterThanOrEqual
temp
scratch
)
;
bailoutIf
(
Assembler
:
:
DoubleGreaterThanOrEqual
lir
-
>
snapshot
(
)
)
;
bailoutCvttss2si
(
temp
output
lir
-
>
snapshot
(
)
)
;
masm
.
convertInt32ToFloat32
(
output
scratch
)
;
masm
.
branchFloat
(
Assembler
:
:
DoubleEqualOrUnordered
temp
scratch
&
end
)
;
masm
.
subl
(
Imm32
(
1
)
output
)
;
}
}
masm
.
bind
(
&
end
)
;
}
void
CodeGenerator
:
:
visitTrunc
(
LTrunc
*
lir
)
{
FloatRegister
input
=
ToFloatRegister
(
lir
-
>
input
(
)
)
;
Register
output
=
ToRegister
(
lir
-
>
output
(
)
)
;
Label
bailout
lessThanMinusOne
;
{
ScratchDoubleScope
scratch
(
masm
)
;
masm
.
loadConstantDouble
(
-
1
scratch
)
;
masm
.
branchDouble
(
Assembler
:
:
DoubleLessThanOrEqualOrUnordered
input
scratch
&
lessThanMinusOne
)
;
}
masm
.
vmovmskpd
(
input
output
)
;
masm
.
branchTest32
(
Assembler
:
:
NonZero
output
Imm32
(
1
)
&
bailout
)
;
bailoutFrom
(
&
bailout
lir
-
>
snapshot
(
)
)
;
masm
.
bind
(
&
lessThanMinusOne
)
;
bailoutCvttsd2si
(
input
output
lir
-
>
snapshot
(
)
)
;
}
void
CodeGenerator
:
:
visitTruncF
(
LTruncF
*
lir
)
{
FloatRegister
input
=
ToFloatRegister
(
lir
-
>
input
(
)
)
;
Register
output
=
ToRegister
(
lir
-
>
output
(
)
)
;
Label
bailout
lessThanMinusOne
;
{
ScratchFloat32Scope
scratch
(
masm
)
;
masm
.
loadConstantFloat32
(
-
1
.
f
scratch
)
;
masm
.
branchFloat
(
Assembler
:
:
DoubleLessThanOrEqualOrUnordered
input
scratch
&
lessThanMinusOne
)
;
}
masm
.
vmovmskps
(
input
output
)
;
masm
.
branchTest32
(
Assembler
:
:
NonZero
output
Imm32
(
1
)
&
bailout
)
;
bailoutFrom
(
&
bailout
lir
-
>
snapshot
(
)
)
;
masm
.
bind
(
&
lessThanMinusOne
)
;
bailoutCvttss2si
(
input
output
lir
-
>
snapshot
(
)
)
;
}
void
CodeGenerator
:
:
visitNearbyInt
(
LNearbyInt
*
lir
)
{
FloatRegister
input
=
ToFloatRegister
(
lir
-
>
input
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
lir
-
>
output
(
)
)
;
RoundingMode
roundingMode
=
lir
-
>
mir
(
)
-
>
roundingMode
(
)
;
masm
.
vroundsd
(
Assembler
:
:
ToX86RoundingMode
(
roundingMode
)
input
output
output
)
;
}
void
CodeGenerator
:
:
visitNearbyIntF
(
LNearbyIntF
*
lir
)
{
FloatRegister
input
=
ToFloatRegister
(
lir
-
>
input
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
lir
-
>
output
(
)
)
;
RoundingMode
roundingMode
=
lir
-
>
mir
(
)
-
>
roundingMode
(
)
;
masm
.
vroundss
(
Assembler
:
:
ToX86RoundingMode
(
roundingMode
)
input
output
output
)
;
}
void
CodeGenerator
:
:
visitEffectiveAddress
(
LEffectiveAddress
*
ins
)
{
const
MEffectiveAddress
*
mir
=
ins
-
>
mir
(
)
;
Register
base
=
ToRegister
(
ins
-
>
base
(
)
)
;
Register
index
=
ToRegister
(
ins
-
>
index
(
)
)
;
Register
output
=
ToRegister
(
ins
-
>
output
(
)
)
;
masm
.
leal
(
Operand
(
base
index
mir
-
>
scale
(
)
mir
-
>
displacement
(
)
)
output
)
;
}
void
CodeGeneratorX86Shared
:
:
generateInvalidateEpilogue
(
)
{
for
(
size_t
i
=
0
;
i
<
sizeof
(
void
*
)
;
i
+
=
Assembler
:
:
NopSize
(
)
)
masm
.
nop
(
)
;
masm
.
bind
(
&
invalidate_
)
;
invalidateEpilogueData_
=
masm
.
pushWithPatch
(
ImmWord
(
uintptr_t
(
-
1
)
)
)
;
TrampolinePtr
thunk
=
gen
-
>
jitRuntime
(
)
-
>
getInvalidationThunk
(
)
;
masm
.
call
(
thunk
)
;
masm
.
assumeUnreachable
(
"
Should
have
returned
directly
to
its
caller
instead
of
here
.
"
)
;
}
void
CodeGenerator
:
:
visitNegI
(
LNegI
*
ins
)
{
Register
input
=
ToRegister
(
ins
-
>
input
(
)
)
;
MOZ_ASSERT
(
input
=
=
ToRegister
(
ins
-
>
output
(
)
)
)
;
masm
.
neg32
(
input
)
;
}
void
CodeGenerator
:
:
visitNegD
(
LNegD
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
MOZ_ASSERT
(
input
=
=
ToFloatRegister
(
ins
-
>
output
(
)
)
)
;
masm
.
negateDouble
(
input
)
;
}
void
CodeGenerator
:
:
visitNegF
(
LNegF
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
MOZ_ASSERT
(
input
=
=
ToFloatRegister
(
ins
-
>
output
(
)
)
)
;
masm
.
negateFloat
(
input
)
;
}
void
CodeGenerator
:
:
visitSimd128Int
(
LSimd128Int
*
ins
)
{
const
LDefinition
*
out
=
ins
-
>
getDef
(
0
)
;
masm
.
loadConstantSimd128Int
(
ins
-
>
getValue
(
)
ToFloatRegister
(
out
)
)
;
}
void
CodeGenerator
:
:
visitSimd128Float
(
LSimd128Float
*
ins
)
{
const
LDefinition
*
out
=
ins
-
>
getDef
(
0
)
;
masm
.
loadConstantSimd128Float
(
ins
-
>
getValue
(
)
ToFloatRegister
(
out
)
)
;
}
void
CodeGenerator
:
:
visitInt32x4ToFloat32x4
(
LInt32x4ToFloat32x4
*
ins
)
{
FloatRegister
in
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
FloatRegister
out
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
masm
.
convertInt32x4ToFloat32x4
(
in
out
)
;
}
void
CodeGenerator
:
:
visitFloat32x4ToInt32x4
(
LFloat32x4ToInt32x4
*
ins
)
{
FloatRegister
in
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
FloatRegister
out
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
Register
temp
=
ToRegister
(
ins
-
>
temp
(
)
)
;
masm
.
convertFloat32x4ToInt32x4
(
in
out
)
;
auto
*
ool
=
new
(
alloc
(
)
)
OutOfLineSimdFloatToIntCheck
(
temp
in
ins
ins
-
>
mir
(
)
-
>
bytecodeOffset
(
)
)
;
addOutOfLineCode
(
ool
ins
-
>
mir
(
)
)
;
static
const
SimdConstant
InvalidResult
=
SimdConstant
:
:
SplatX4
(
int32_t
(
-
2147483648
)
)
;
ScratchSimd128Scope
scratch
(
masm
)
;
masm
.
loadConstantSimd128Int
(
InvalidResult
scratch
)
;
masm
.
packedEqualInt32x4
(
Operand
(
out
)
scratch
)
;
masm
.
vmovmskps
(
scratch
temp
)
;
masm
.
cmp32
(
temp
Imm32
(
0
)
)
;
masm
.
j
(
Assembler
:
:
NotEqual
ool
-
>
entry
(
)
)
;
masm
.
bind
(
ool
-
>
rejoin
(
)
)
;
}
void
CodeGeneratorX86Shared
:
:
visitOutOfLineSimdFloatToIntCheck
(
OutOfLineSimdFloatToIntCheck
*
ool
)
{
static
const
SimdConstant
Int32MaxX4
=
SimdConstant
:
:
SplatX4
(
2147483647
.
f
)
;
static
const
SimdConstant
Int32MinX4
=
SimdConstant
:
:
SplatX4
(
-
2147483648
.
f
)
;
Label
onConversionError
;
FloatRegister
input
=
ool
-
>
input
(
)
;
Register
temp
=
ool
-
>
temp
(
)
;
ScratchSimd128Scope
scratch
(
masm
)
;
masm
.
loadConstantSimd128Float
(
Int32MinX4
scratch
)
;
masm
.
vcmpleps
(
Operand
(
input
)
scratch
scratch
)
;
masm
.
vmovmskps
(
scratch
temp
)
;
masm
.
cmp32
(
temp
Imm32
(
15
)
)
;
masm
.
j
(
Assembler
:
:
NotEqual
&
onConversionError
)
;
masm
.
loadConstantSimd128Float
(
Int32MaxX4
scratch
)
;
masm
.
vcmpleps
(
Operand
(
input
)
scratch
scratch
)
;
masm
.
vmovmskps
(
scratch
temp
)
;
masm
.
cmp32
(
temp
Imm32
(
0
)
)
;
masm
.
j
(
Assembler
:
:
NotEqual
&
onConversionError
)
;
masm
.
jump
(
ool
-
>
rejoin
(
)
)
;
masm
.
bind
(
&
onConversionError
)
;
if
(
gen
-
>
compilingWasm
(
)
)
masm
.
wasmTrap
(
wasm
:
:
Trap
:
:
ImpreciseSimdConversion
ool
-
>
bytecodeOffset
(
)
)
;
else
bailout
(
ool
-
>
ins
(
)
-
>
snapshot
(
)
)
;
}
void
CodeGenerator
:
:
visitFloat32x4ToUint32x4
(
LFloat32x4ToUint32x4
*
ins
)
{
const
MSimdConvert
*
mir
=
ins
-
>
mir
(
)
;
FloatRegister
in
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
FloatRegister
out
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
Register
temp
=
ToRegister
(
ins
-
>
tempR
(
)
)
;
FloatRegister
tempF
=
ToFloatRegister
(
ins
-
>
tempF
(
)
)
;
ScratchSimd128Scope
scratch
(
masm
)
;
static
const
float
Adjust
=
0x80000000
;
static
const
SimdConstant
Bias
=
SimdConstant
:
:
SplatX4
(
-
Adjust
)
;
masm
.
loadConstantSimd128Float
(
Bias
scratch
)
;
masm
.
packedAddFloat32
(
Operand
(
in
)
scratch
)
;
masm
.
convertFloat32x4ToInt32x4
(
scratch
scratch
)
;
masm
.
convertFloat32x4ToInt32x4
(
in
out
)
;
masm
.
zeroSimd128Float
(
tempF
)
;
masm
.
packedGreaterThanInt32x4
(
Operand
(
out
)
tempF
)
;
masm
.
bitwiseAndSimd128
(
Operand
(
tempF
)
scratch
)
;
masm
.
bitwiseOrSimd128
(
Operand
(
scratch
)
out
)
;
masm
.
vmovmskps
(
scratch
temp
)
;
masm
.
cmp32
(
temp
Imm32
(
0
)
)
;
if
(
gen
-
>
compilingWasm
(
)
)
{
Label
ok
;
masm
.
j
(
Assembler
:
:
Equal
&
ok
)
;
masm
.
wasmTrap
(
wasm
:
:
Trap
:
:
ImpreciseSimdConversion
mir
-
>
bytecodeOffset
(
)
)
;
masm
.
bind
(
&
ok
)
;
}
else
{
bailoutIf
(
Assembler
:
:
NotEqual
ins
-
>
snapshot
(
)
)
;
}
}
void
CodeGenerator
:
:
visitSimdValueInt32x4
(
LSimdValueInt32x4
*
ins
)
{
MOZ_ASSERT
(
ins
-
>
mir
(
)
-
>
type
(
)
=
=
MIRType
:
:
Int32x4
|
|
ins
-
>
mir
(
)
-
>
type
(
)
=
=
MIRType
:
:
Bool32x4
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
if
(
AssemblerX86Shared
:
:
HasSSE41
(
)
)
{
masm
.
vmovd
(
ToRegister
(
ins
-
>
getOperand
(
0
)
)
output
)
;
for
(
size_t
i
=
1
;
i
<
4
;
+
+
i
)
{
Register
r
=
ToRegister
(
ins
-
>
getOperand
(
i
)
)
;
masm
.
vpinsrd
(
i
r
output
output
)
;
}
return
;
}
masm
.
reserveStack
(
Simd128DataSize
)
;
for
(
size_t
i
=
0
;
i
<
4
;
+
+
i
)
{
Register
r
=
ToRegister
(
ins
-
>
getOperand
(
i
)
)
;
masm
.
store32
(
r
Address
(
StackPointer
i
*
sizeof
(
int32_t
)
)
)
;
}
masm
.
loadAlignedSimd128Int
(
Address
(
StackPointer
0
)
output
)
;
masm
.
freeStack
(
Simd128DataSize
)
;
}
void
CodeGenerator
:
:
visitSimdValueFloat32x4
(
LSimdValueFloat32x4
*
ins
)
{
MOZ_ASSERT
(
ins
-
>
mir
(
)
-
>
type
(
)
=
=
MIRType
:
:
Float32x4
)
;
FloatRegister
r0
=
ToFloatRegister
(
ins
-
>
getOperand
(
0
)
)
;
FloatRegister
r1
=
ToFloatRegister
(
ins
-
>
getOperand
(
1
)
)
;
FloatRegister
r2
=
ToFloatRegister
(
ins
-
>
getOperand
(
2
)
)
;
FloatRegister
r3
=
ToFloatRegister
(
ins
-
>
getOperand
(
3
)
)
;
FloatRegister
tmp
=
ToFloatRegister
(
ins
-
>
getTemp
(
0
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
FloatRegister
r0Copy
=
masm
.
reusedInputFloat32x4
(
r0
output
)
;
FloatRegister
r1Copy
=
masm
.
reusedInputFloat32x4
(
r1
tmp
)
;
masm
.
vunpcklps
(
r3
r1Copy
tmp
)
;
masm
.
vunpcklps
(
r2
r0Copy
output
)
;
masm
.
vunpcklps
(
tmp
output
output
)
;
}
void
CodeGenerator
:
:
visitSimdSplatX16
(
LSimdSplatX16
*
ins
)
{
MOZ_ASSERT
(
SimdTypeToLength
(
ins
-
>
mir
(
)
-
>
type
(
)
)
=
=
16
)
;
Register
input
=
ToRegister
(
ins
-
>
getOperand
(
0
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
masm
.
vmovd
(
input
output
)
;
if
(
AssemblerX86Shared
:
:
HasSSSE3
(
)
)
{
masm
.
zeroSimd128Int
(
ScratchSimd128Reg
)
;
masm
.
vpshufb
(
ScratchSimd128Reg
output
output
)
;
}
else
{
masm
.
vpsllw
(
Imm32
(
8
)
output
output
)
;
masm
.
vmovdqa
(
output
ScratchSimd128Reg
)
;
masm
.
vpsrlw
(
Imm32
(
8
)
ScratchSimd128Reg
ScratchSimd128Reg
)
;
masm
.
vpor
(
ScratchSimd128Reg
output
output
)
;
masm
.
vpshuflw
(
0
output
output
)
;
masm
.
vpshufd
(
0
output
output
)
;
}
}
void
CodeGenerator
:
:
visitSimdSplatX8
(
LSimdSplatX8
*
ins
)
{
MOZ_ASSERT
(
SimdTypeToLength
(
ins
-
>
mir
(
)
-
>
type
(
)
)
=
=
8
)
;
Register
input
=
ToRegister
(
ins
-
>
getOperand
(
0
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
masm
.
vmovd
(
input
output
)
;
masm
.
vpshuflw
(
0
output
output
)
;
masm
.
vpshufd
(
0
output
output
)
;
}
void
CodeGenerator
:
:
visitSimdSplatX4
(
LSimdSplatX4
*
ins
)
{
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
MSimdSplat
*
mir
=
ins
-
>
mir
(
)
;
MOZ_ASSERT
(
IsSimdType
(
mir
-
>
type
(
)
)
)
;
JS_STATIC_ASSERT
(
sizeof
(
float
)
=
=
sizeof
(
int32_t
)
)
;
if
(
mir
-
>
type
(
)
=
=
MIRType
:
:
Float32x4
)
{
FloatRegister
r
=
ToFloatRegister
(
ins
-
>
getOperand
(
0
)
)
;
FloatRegister
rCopy
=
masm
.
reusedInputFloat32x4
(
r
output
)
;
masm
.
vshufps
(
0
rCopy
rCopy
output
)
;
}
else
{
Register
r
=
ToRegister
(
ins
-
>
getOperand
(
0
)
)
;
masm
.
vmovd
(
r
output
)
;
masm
.
vpshufd
(
0
output
output
)
;
}
}
void
CodeGenerator
:
:
visitSimdReinterpretCast
(
LSimdReinterpretCast
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
if
(
input
.
aliases
(
output
)
)
return
;
if
(
IsIntegerSimdType
(
ins
-
>
mir
(
)
-
>
type
(
)
)
)
masm
.
vmovdqa
(
input
output
)
;
else
masm
.
vmovaps
(
input
output
)
;
}
void
CodeGeneratorX86Shared
:
:
emitSimdExtractLane32x4
(
FloatRegister
input
Register
output
unsigned
lane
)
{
if
(
lane
=
=
0
)
{
masm
.
moveLowInt32
(
input
output
)
;
}
else
if
(
AssemblerX86Shared
:
:
HasSSE41
(
)
)
{
masm
.
vpextrd
(
lane
input
output
)
;
}
else
{
uint32_t
mask
=
MacroAssembler
:
:
ComputeShuffleMask
(
lane
)
;
masm
.
shuffleInt32
(
mask
input
ScratchSimd128Reg
)
;
masm
.
moveLowInt32
(
ScratchSimd128Reg
output
)
;
}
}
void
CodeGeneratorX86Shared
:
:
emitSimdExtractLane16x8
(
FloatRegister
input
Register
output
unsigned
lane
SimdSign
signedness
)
{
masm
.
vpextrw
(
lane
input
output
)
;
if
(
signedness
=
=
SimdSign
:
:
Signed
)
masm
.
movswl
(
output
output
)
;
}
void
CodeGeneratorX86Shared
:
:
emitSimdExtractLane8x16
(
FloatRegister
input
Register
output
unsigned
lane
SimdSign
signedness
)
{
if
(
AssemblerX86Shared
:
:
HasSSE41
(
)
)
{
masm
.
vpextrb
(
lane
input
output
)
;
if
(
signedness
=
=
SimdSign
:
:
Unsigned
)
signedness
=
SimdSign
:
:
NotApplicable
;
}
else
{
emitSimdExtractLane16x8
(
input
output
lane
/
2
SimdSign
:
:
Unsigned
)
;
if
(
lane
%
2
)
{
masm
.
shrl
(
Imm32
(
8
)
output
)
;
if
(
signedness
=
=
SimdSign
:
:
Unsigned
)
signedness
=
SimdSign
:
:
NotApplicable
;
}
}
switch
(
signedness
)
{
case
SimdSign
:
:
Signed
:
masm
.
movsbl
(
output
output
)
;
break
;
case
SimdSign
:
:
Unsigned
:
masm
.
movzbl
(
output
output
)
;
break
;
case
SimdSign
:
:
NotApplicable
:
break
;
}
}
void
CodeGenerator
:
:
visitSimdExtractElementB
(
LSimdExtractElementB
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
Register
output
=
ToRegister
(
ins
-
>
output
(
)
)
;
MSimdExtractElement
*
mir
=
ins
-
>
mir
(
)
;
unsigned
length
=
SimdTypeToLength
(
mir
-
>
specialization
(
)
)
;
switch
(
length
)
{
case
4
:
emitSimdExtractLane32x4
(
input
output
mir
-
>
lane
(
)
)
;
break
;
case
8
:
emitSimdExtractLane16x8
(
input
output
mir
-
>
lane
(
)
SimdSign
:
:
NotApplicable
)
;
break
;
case
16
:
emitSimdExtractLane8x16
(
input
output
mir
-
>
lane
(
)
SimdSign
:
:
NotApplicable
)
;
break
;
default
:
MOZ_CRASH
(
"
Unhandled
SIMD
length
"
)
;
}
masm
.
and32
(
Imm32
(
1
)
output
)
;
}
void
CodeGenerator
:
:
visitSimdExtractElementI
(
LSimdExtractElementI
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
Register
output
=
ToRegister
(
ins
-
>
output
(
)
)
;
MSimdExtractElement
*
mir
=
ins
-
>
mir
(
)
;
unsigned
length
=
SimdTypeToLength
(
mir
-
>
specialization
(
)
)
;
switch
(
length
)
{
case
4
:
emitSimdExtractLane32x4
(
input
output
mir
-
>
lane
(
)
)
;
break
;
case
8
:
emitSimdExtractLane16x8
(
input
output
mir
-
>
lane
(
)
mir
-
>
signedness
(
)
)
;
break
;
case
16
:
emitSimdExtractLane8x16
(
input
output
mir
-
>
lane
(
)
mir
-
>
signedness
(
)
)
;
break
;
default
:
MOZ_CRASH
(
"
Unhandled
SIMD
length
"
)
;
}
}
void
CodeGenerator
:
:
visitSimdExtractElementU2D
(
LSimdExtractElementU2D
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
Register
temp
=
ToRegister
(
ins
-
>
temp
(
)
)
;
MSimdExtractElement
*
mir
=
ins
-
>
mir
(
)
;
MOZ_ASSERT
(
mir
-
>
specialization
(
)
=
=
MIRType
:
:
Int32x4
)
;
emitSimdExtractLane32x4
(
input
temp
mir
-
>
lane
(
)
)
;
masm
.
convertUInt32ToDouble
(
temp
output
)
;
}
void
CodeGenerator
:
:
visitSimdExtractElementF
(
LSimdExtractElementF
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
unsigned
lane
=
ins
-
>
mir
(
)
-
>
lane
(
)
;
if
(
lane
=
=
0
)
{
if
(
input
!
=
output
)
masm
.
moveFloat32
(
input
output
)
;
}
else
if
(
lane
=
=
2
)
{
masm
.
moveHighPairToLowPairFloat32
(
input
output
)
;
}
else
{
uint32_t
mask
=
MacroAssembler
:
:
ComputeShuffleMask
(
lane
)
;
masm
.
shuffleFloat32
(
mask
input
output
)
;
}
if
(
!
gen
-
>
compilingWasm
(
)
)
masm
.
canonicalizeFloat
(
output
)
;
}
void
CodeGenerator
:
:
visitSimdInsertElementI
(
LSimdInsertElementI
*
ins
)
{
FloatRegister
vector
=
ToFloatRegister
(
ins
-
>
vector
(
)
)
;
Register
value
=
ToRegister
(
ins
-
>
value
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
MOZ_ASSERT
(
vector
=
=
output
)
;
unsigned
lane
=
ins
-
>
lane
(
)
;
unsigned
length
=
ins
-
>
length
(
)
;
if
(
length
=
=
8
)
{
masm
.
vpinsrw
(
lane
value
vector
output
)
;
return
;
}
if
(
AssemblerX86Shared
:
:
HasSSE41
(
)
)
{
switch
(
length
)
{
case
4
:
masm
.
vpinsrd
(
lane
value
vector
output
)
;
return
;
case
16
:
masm
.
vpinsrb
(
lane
value
vector
output
)
;
return
;
}
}
masm
.
reserveStack
(
Simd128DataSize
)
;
masm
.
storeAlignedSimd128Int
(
vector
Address
(
StackPointer
0
)
)
;
switch
(
length
)
{
case
4
:
masm
.
store32
(
value
Address
(
StackPointer
lane
*
sizeof
(
int32_t
)
)
)
;
break
;
case
16
:
masm
.
store8
(
value
Address
(
StackPointer
lane
*
sizeof
(
int8_t
)
)
)
;
break
;
default
:
MOZ_CRASH
(
"
Unsupported
SIMD
length
"
)
;
}
masm
.
loadAlignedSimd128Int
(
Address
(
StackPointer
0
)
output
)
;
masm
.
freeStack
(
Simd128DataSize
)
;
}
void
CodeGenerator
:
:
visitSimdInsertElementF
(
LSimdInsertElementF
*
ins
)
{
FloatRegister
vector
=
ToFloatRegister
(
ins
-
>
vector
(
)
)
;
FloatRegister
value
=
ToFloatRegister
(
ins
-
>
value
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
MOZ_ASSERT
(
vector
=
=
output
)
;
if
(
ins
-
>
lane
(
)
=
=
0
)
{
if
(
value
!
=
output
)
masm
.
vmovss
(
value
vector
output
)
;
return
;
}
if
(
AssemblerX86Shared
:
:
HasSSE41
(
)
)
{
masm
.
vinsertps
(
masm
.
vinsertpsMask
(
0
ins
-
>
lane
(
)
)
value
output
output
)
;
return
;
}
unsigned
component
=
unsigned
(
ins
-
>
lane
(
)
)
;
masm
.
reserveStack
(
Simd128DataSize
)
;
masm
.
storeAlignedSimd128Float
(
vector
Address
(
StackPointer
0
)
)
;
masm
.
storeFloat32
(
value
Address
(
StackPointer
component
*
sizeof
(
int32_t
)
)
)
;
masm
.
loadAlignedSimd128Float
(
Address
(
StackPointer
0
)
output
)
;
masm
.
freeStack
(
Simd128DataSize
)
;
}
void
CodeGenerator
:
:
visitSimdAllTrue
(
LSimdAllTrue
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
Register
output
=
ToRegister
(
ins
-
>
output
(
)
)
;
masm
.
vpmovmskb
(
input
output
)
;
masm
.
cmp32
(
output
Imm32
(
0xffff
)
)
;
masm
.
emitSet
(
Assembler
:
:
Zero
output
)
;
}
void
CodeGenerator
:
:
visitSimdAnyTrue
(
LSimdAnyTrue
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
Register
output
=
ToRegister
(
ins
-
>
output
(
)
)
;
masm
.
vpmovmskb
(
input
output
)
;
masm
.
cmp32
(
output
Imm32
(
0x0
)
)
;
masm
.
emitSet
(
Assembler
:
:
NonZero
output
)
;
}
template
<
class
T
class
Reg
>
void
CodeGeneratorX86Shared
:
:
visitSimdGeneralShuffle
(
LSimdGeneralShuffleBase
*
ins
Reg
tempRegister
)
{
MSimdGeneralShuffle
*
mir
=
ins
-
>
mir
(
)
;
unsigned
numVectors
=
mir
-
>
numVectors
(
)
;
Register
laneTemp
=
ToRegister
(
ins
-
>
temp
(
)
)
;
unsigned
stackSpace
=
Simd128DataSize
*
(
numVectors
+
1
)
;
masm
.
reserveStack
(
stackSpace
)
;
for
(
unsigned
i
=
0
;
i
<
numVectors
;
i
+
+
)
{
masm
.
storeAlignedVector
<
T
>
(
ToFloatRegister
(
ins
-
>
vector
(
i
)
)
Address
(
StackPointer
Simd128DataSize
*
(
1
+
i
)
)
)
;
}
Label
bail
;
const
Scale
laneScale
=
ScaleFromElemWidth
(
sizeof
(
T
)
)
;
for
(
size_t
i
=
0
;
i
<
mir
-
>
numLanes
(
)
;
i
+
+
)
{
Operand
lane
=
ToOperand
(
ins
-
>
lane
(
i
)
)
;
masm
.
cmp32
(
lane
Imm32
(
numVectors
*
mir
-
>
numLanes
(
)
-
1
)
)
;
masm
.
j
(
Assembler
:
:
Above
&
bail
)
;
if
(
lane
.
kind
(
)
=
=
Operand
:
:
REG
)
{
masm
.
loadScalar
<
T
>
(
Operand
(
StackPointer
ToRegister
(
ins
-
>
lane
(
i
)
)
laneScale
Simd128DataSize
)
tempRegister
)
;
}
else
{
masm
.
load32
(
lane
laneTemp
)
;
masm
.
loadScalar
<
T
>
(
Operand
(
StackPointer
laneTemp
laneScale
Simd128DataSize
)
tempRegister
)
;
}
masm
.
storeScalar
<
T
>
(
tempRegister
Address
(
StackPointer
i
*
sizeof
(
T
)
)
)
;
}
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
masm
.
loadAlignedVector
<
T
>
(
Address
(
StackPointer
0
)
output
)
;
Label
join
;
masm
.
jump
(
&
join
)
;
{
masm
.
bind
(
&
bail
)
;
masm
.
freeStack
(
stackSpace
)
;
bailout
(
ins
-
>
snapshot
(
)
)
;
}
masm
.
bind
(
&
join
)
;
masm
.
setFramePushed
(
masm
.
framePushed
(
)
+
stackSpace
)
;
masm
.
freeStack
(
stackSpace
)
;
}
void
CodeGenerator
:
:
visitSimdGeneralShuffleI
(
LSimdGeneralShuffleI
*
ins
)
{
switch
(
ins
-
>
mir
(
)
-
>
type
(
)
)
{
case
MIRType
:
:
Int8x16
:
return
visitSimdGeneralShuffle
<
int8_t
Register
>
(
ins
ToRegister
(
ins
-
>
temp
(
)
)
)
;
case
MIRType
:
:
Int16x8
:
return
visitSimdGeneralShuffle
<
int16_t
Register
>
(
ins
ToRegister
(
ins
-
>
temp
(
)
)
)
;
case
MIRType
:
:
Int32x4
:
return
visitSimdGeneralShuffle
<
int32_t
Register
>
(
ins
ToRegister
(
ins
-
>
temp
(
)
)
)
;
default
:
MOZ_CRASH
(
"
unsupported
type
for
general
shuffle
"
)
;
}
}
void
CodeGenerator
:
:
visitSimdGeneralShuffleF
(
LSimdGeneralShuffleF
*
ins
)
{
ScratchFloat32Scope
scratch
(
masm
)
;
visitSimdGeneralShuffle
<
float
FloatRegister
>
(
ins
scratch
)
;
}
void
CodeGenerator
:
:
visitSimdSwizzleI
(
LSimdSwizzleI
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
const
unsigned
numLanes
=
ins
-
>
numLanes
(
)
;
switch
(
numLanes
)
{
case
4
:
{
uint32_t
x
=
ins
-
>
lane
(
0
)
;
uint32_t
y
=
ins
-
>
lane
(
1
)
;
uint32_t
z
=
ins
-
>
lane
(
2
)
;
uint32_t
w
=
ins
-
>
lane
(
3
)
;
uint32_t
mask
=
MacroAssembler
:
:
ComputeShuffleMask
(
x
y
z
w
)
;
masm
.
shuffleInt32
(
mask
input
output
)
;
return
;
}
}
const
unsigned
bytesPerLane
=
16
/
numLanes
;
int8_t
bLane
[
16
]
;
for
(
unsigned
i
=
0
;
i
<
numLanes
;
i
+
+
)
{
for
(
unsigned
b
=
0
;
b
<
bytesPerLane
;
b
+
+
)
{
bLane
[
i
*
bytesPerLane
+
b
]
=
ins
-
>
lane
(
i
)
*
bytesPerLane
+
b
;
}
}
if
(
AssemblerX86Shared
:
:
HasSSSE3
(
)
)
{
ScratchSimd128Scope
scratch
(
masm
)
;
masm
.
loadConstantSimd128Int
(
SimdConstant
:
:
CreateX16
(
bLane
)
scratch
)
;
FloatRegister
inputCopy
=
masm
.
reusedInputInt32x4
(
input
output
)
;
masm
.
vpshufb
(
scratch
inputCopy
output
)
;
return
;
}
Register
temp
=
ToRegister
(
ins
-
>
getTemp
(
0
)
)
;
masm
.
reserveStack
(
2
*
Simd128DataSize
)
;
masm
.
storeAlignedSimd128Int
(
input
Address
(
StackPointer
Simd128DataSize
)
)
;
for
(
unsigned
i
=
0
;
i
<
16
;
i
+
+
)
{
masm
.
load8ZeroExtend
(
Address
(
StackPointer
Simd128DataSize
+
bLane
[
i
]
)
temp
)
;
masm
.
store8
(
temp
Address
(
StackPointer
i
)
)
;
}
masm
.
loadAlignedSimd128Int
(
Address
(
StackPointer
0
)
output
)
;
masm
.
freeStack
(
2
*
Simd128DataSize
)
;
}
void
CodeGenerator
:
:
visitSimdSwizzleF
(
LSimdSwizzleF
*
ins
)
{
FloatRegister
input
=
ToFloatRegister
(
ins
-
>
input
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
MOZ_ASSERT
(
ins
-
>
numLanes
(
)
=
=
4
)
;
uint32_t
x
=
ins
-
>
lane
(
0
)
;
uint32_t
y
=
ins
-
>
lane
(
1
)
;
uint32_t
z
=
ins
-
>
lane
(
2
)
;
uint32_t
w
=
ins
-
>
lane
(
3
)
;
if
(
AssemblerX86Shared
:
:
HasSSE3
(
)
)
{
if
(
ins
-
>
lanesMatch
(
0
0
2
2
)
)
{
masm
.
vmovsldup
(
input
output
)
;
return
;
}
if
(
ins
-
>
lanesMatch
(
1
1
3
3
)
)
{
masm
.
vmovshdup
(
input
output
)
;
return
;
}
}
if
(
ins
-
>
lanesMatch
(
2
3
2
3
)
)
{
FloatRegister
inputCopy
=
masm
.
reusedInputFloat32x4
(
input
output
)
;
masm
.
vmovhlps
(
input
inputCopy
output
)
;
return
;
}
if
(
ins
-
>
lanesMatch
(
0
1
0
1
)
)
{
if
(
AssemblerX86Shared
:
:
HasSSE3
(
)
&
&
!
AssemblerX86Shared
:
:
HasAVX
(
)
)
{
masm
.
vmovddup
(
input
output
)
;
return
;
}
FloatRegister
inputCopy
=
masm
.
reusedInputFloat32x4
(
input
output
)
;
masm
.
vmovlhps
(
input
inputCopy
output
)
;
return
;
}
if
(
ins
-
>
lanesMatch
(
0
0
1
1
)
)
{
FloatRegister
inputCopy
=
masm
.
reusedInputFloat32x4
(
input
output
)
;
masm
.
vunpcklps
(
input
inputCopy
output
)
;
return
;
}
if
(
ins
-
>
lanesMatch
(
2
2
3
3
)
)
{
FloatRegister
inputCopy
=
masm
.
reusedInputFloat32x4
(
input
output
)
;
masm
.
vunpckhps
(
input
inputCopy
output
)
;
return
;
}
uint32_t
mask
=
MacroAssembler
:
:
ComputeShuffleMask
(
x
y
z
w
)
;
masm
.
shuffleFloat32
(
mask
input
output
)
;
}
void
CodeGenerator
:
:
visitSimdShuffle
(
LSimdShuffle
*
ins
)
{
FloatRegister
lhs
=
ToFloatRegister
(
ins
-
>
lhs
(
)
)
;
FloatRegister
rhs
=
ToFloatRegister
(
ins
-
>
rhs
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
const
unsigned
numLanes
=
ins
-
>
numLanes
(
)
;
const
unsigned
bytesPerLane
=
16
/
numLanes
;
uint8_t
bLane
[
16
]
;
for
(
unsigned
i
=
0
;
i
<
numLanes
;
i
+
+
)
{
for
(
unsigned
b
=
0
;
b
<
bytesPerLane
;
b
+
+
)
{
bLane
[
i
*
bytesPerLane
+
b
]
=
ins
-
>
lane
(
i
)
*
bytesPerLane
+
b
;
}
}
if
(
AssemblerX86Shared
:
:
HasSSSE3
(
)
)
{
FloatRegister
scratch1
=
ToFloatRegister
(
ins
-
>
temp
(
)
)
;
ScratchSimd128Scope
scratch2
(
masm
)
;
int8_t
idx
[
16
]
;
for
(
unsigned
i
=
0
;
i
<
16
;
i
+
+
)
idx
[
i
]
=
bLane
[
i
]
<
16
?
bLane
[
i
]
:
-
1
;
masm
.
loadConstantSimd128Int
(
SimdConstant
:
:
CreateX16
(
idx
)
scratch1
)
;
FloatRegister
lhsCopy
=
masm
.
reusedInputInt32x4
(
lhs
scratch2
)
;
masm
.
vpshufb
(
scratch1
lhsCopy
scratch2
)
;
for
(
unsigned
i
=
0
;
i
<
16
;
i
+
+
)
idx
[
i
]
=
bLane
[
i
]
>
=
16
?
bLane
[
i
]
-
16
:
-
1
;
masm
.
loadConstantSimd128Int
(
SimdConstant
:
:
CreateX16
(
idx
)
scratch1
)
;
FloatRegister
rhsCopy
=
masm
.
reusedInputInt32x4
(
rhs
output
)
;
masm
.
vpshufb
(
scratch1
rhsCopy
output
)
;
masm
.
vpor
(
scratch2
output
output
)
;
return
;
}
Register
temp
=
ToRegister
(
ins
-
>
getTemp
(
0
)
)
;
masm
.
reserveStack
(
3
*
Simd128DataSize
)
;
masm
.
storeAlignedSimd128Int
(
lhs
Address
(
StackPointer
Simd128DataSize
)
)
;
masm
.
storeAlignedSimd128Int
(
rhs
Address
(
StackPointer
2
*
Simd128DataSize
)
)
;
for
(
unsigned
i
=
0
;
i
<
16
;
i
+
+
)
{
masm
.
load8ZeroExtend
(
Address
(
StackPointer
Simd128DataSize
+
bLane
[
i
]
)
temp
)
;
masm
.
store8
(
temp
Address
(
StackPointer
i
)
)
;
}
masm
.
loadAlignedSimd128Int
(
Address
(
StackPointer
0
)
output
)
;
masm
.
freeStack
(
3
*
Simd128DataSize
)
;
}
void
CodeGenerator
:
:
visitSimdShuffleX4
(
LSimdShuffleX4
*
ins
)
{
FloatRegister
lhs
=
ToFloatRegister
(
ins
-
>
lhs
(
)
)
;
Operand
rhs
=
ToOperand
(
ins
-
>
rhs
(
)
)
;
FloatRegister
out
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
uint32_t
x
=
ins
-
>
lane
(
0
)
;
uint32_t
y
=
ins
-
>
lane
(
1
)
;
uint32_t
z
=
ins
-
>
lane
(
2
)
;
uint32_t
w
=
ins
-
>
lane
(
3
)
;
unsigned
numLanesFromLHS
=
(
x
<
4
)
+
(
y
<
4
)
+
(
z
<
4
)
+
(
w
<
4
)
;
MOZ_ASSERT
(
numLanesFromLHS
>
=
2
)
;
uint32_t
mask
;
MOZ_ASSERT
(
numLanesFromLHS
<
4
)
;
if
(
AssemblerX86Shared
:
:
HasSSE41
(
)
)
{
if
(
x
%
4
=
=
0
&
&
y
%
4
=
=
1
&
&
z
%
4
=
=
2
&
&
w
%
4
=
=
3
)
{
masm
.
vblendps
(
masm
.
blendpsMask
(
x
>
=
4
y
>
=
4
z
>
=
4
w
>
=
4
)
rhs
lhs
out
)
;
return
;
}
}
if
(
numLanesFromLHS
=
=
3
)
{
unsigned
firstMask
=
-
1
secondMask
=
-
1
;
if
(
ins
-
>
lanesMatch
(
4
1
2
3
)
&
&
rhs
.
kind
(
)
=
=
Operand
:
:
FPREG
)
{
masm
.
vmovss
(
FloatRegister
:
:
FromCode
(
rhs
.
fpu
(
)
)
lhs
out
)
;
return
;
}
unsigned
numLanesUnchanged
=
(
x
=
=
0
)
+
(
y
=
=
1
)
+
(
z
=
=
2
)
+
(
w
=
=
3
)
;
if
(
AssemblerX86Shared
:
:
HasSSE41
(
)
&
&
numLanesUnchanged
=
=
3
)
{
unsigned
srcLane
;
unsigned
dstLane
;
if
(
x
>
=
4
)
{
srcLane
=
x
-
4
;
dstLane
=
0
;
}
else
if
(
y
>
=
4
)
{
srcLane
=
y
-
4
;
dstLane
=
1
;
}
else
if
(
z
>
=
4
)
{
srcLane
=
z
-
4
;
dstLane
=
2
;
}
else
{
MOZ_ASSERT
(
w
>
=
4
)
;
srcLane
=
w
-
4
;
dstLane
=
3
;
}
masm
.
vinsertps
(
masm
.
vinsertpsMask
(
srcLane
dstLane
)
rhs
lhs
out
)
;
return
;
}
FloatRegister
rhsCopy
=
ToFloatRegister
(
ins
-
>
temp
(
)
)
;
if
(
x
<
4
&
&
y
<
4
)
{
if
(
w
>
=
4
)
{
w
%
=
4
;
firstMask
=
MacroAssembler
:
:
ComputeShuffleMask
(
w
w
z
z
)
;
secondMask
=
MacroAssembler
:
:
ComputeShuffleMask
(
x
y
2
0
)
;
}
else
{
MOZ_ASSERT
(
z
>
=
4
)
;
z
%
=
4
;
firstMask
=
MacroAssembler
:
:
ComputeShuffleMask
(
z
z
w
w
)
;
secondMask
=
MacroAssembler
:
:
ComputeShuffleMask
(
x
y
0
2
)
;
}
masm
.
vshufps
(
firstMask
lhs
rhsCopy
rhsCopy
)
;
masm
.
vshufps
(
secondMask
rhsCopy
lhs
out
)
;
return
;
}
MOZ_ASSERT
(
z
<
4
&
&
w
<
4
)
;
if
(
y
>
=
4
)
{
y
%
=
4
;
firstMask
=
MacroAssembler
:
:
ComputeShuffleMask
(
y
y
x
x
)
;
secondMask
=
MacroAssembler
:
:
ComputeShuffleMask
(
2
0
z
w
)
;
}
else
{
MOZ_ASSERT
(
x
>
=
4
)
;
x
%
=
4
;
firstMask
=
MacroAssembler
:
:
ComputeShuffleMask
(
x
x
y
y
)
;
secondMask
=
MacroAssembler
:
:
ComputeShuffleMask
(
0
2
z
w
)
;
}
masm
.
vshufps
(
firstMask
lhs
rhsCopy
rhsCopy
)
;
if
(
AssemblerX86Shared
:
:
HasAVX
(
)
)
{
masm
.
vshufps
(
secondMask
lhs
rhsCopy
out
)
;
}
else
{
masm
.
vshufps
(
secondMask
lhs
rhsCopy
rhsCopy
)
;
masm
.
moveSimd128Float
(
rhsCopy
out
)
;
}
return
;
}
MOZ_ASSERT
(
numLanesFromLHS
=
=
2
)
;
if
(
ins
-
>
lanesMatch
(
2
3
6
7
)
)
{
ScratchSimd128Scope
scratch
(
masm
)
;
if
(
AssemblerX86Shared
:
:
HasAVX
(
)
)
{
FloatRegister
rhsCopy
=
masm
.
reusedInputAlignedFloat32x4
(
rhs
scratch
)
;
masm
.
vmovhlps
(
lhs
rhsCopy
out
)
;
}
else
{
masm
.
loadAlignedSimd128Float
(
rhs
scratch
)
;
masm
.
vmovhlps
(
lhs
scratch
scratch
)
;
masm
.
moveSimd128Float
(
scratch
out
)
;
}
return
;
}
if
(
ins
-
>
lanesMatch
(
0
1
4
5
)
)
{
FloatRegister
rhsCopy
;
ScratchSimd128Scope
scratch
(
masm
)
;
if
(
rhs
.
kind
(
)
=
=
Operand
:
:
FPREG
)
{
rhsCopy
=
FloatRegister
:
:
FromCode
(
rhs
.
fpu
(
)
)
;
}
else
{
masm
.
loadAlignedSimd128Float
(
rhs
scratch
)
;
rhsCopy
=
scratch
;
}
masm
.
vmovlhps
(
rhsCopy
lhs
out
)
;
return
;
}
if
(
ins
-
>
lanesMatch
(
0
4
1
5
)
)
{
masm
.
vunpcklps
(
rhs
lhs
out
)
;
return
;
}
if
(
ins
-
>
lanesMatch
(
4
0
5
1
)
)
{
ScratchSimd128Scope
scratch
(
masm
)
;
if
(
AssemblerX86Shared
:
:
HasAVX
(
)
)
{
FloatRegister
rhsCopy
=
masm
.
reusedInputAlignedFloat32x4
(
rhs
scratch
)
;
masm
.
vunpcklps
(
lhs
rhsCopy
out
)
;
}
else
{
masm
.
loadAlignedSimd128Float
(
rhs
scratch
)
;
masm
.
vunpcklps
(
lhs
scratch
scratch
)
;
masm
.
moveSimd128Float
(
scratch
out
)
;
}
return
;
}
if
(
ins
-
>
lanesMatch
(
2
6
3
7
)
)
{
masm
.
vunpckhps
(
rhs
lhs
out
)
;
return
;
}
if
(
ins
-
>
lanesMatch
(
6
2
7
3
)
)
{
ScratchSimd128Scope
scratch
(
masm
)
;
if
(
AssemblerX86Shared
:
:
HasAVX
(
)
)
{
FloatRegister
rhsCopy
=
masm
.
reusedInputAlignedFloat32x4
(
rhs
scratch
)
;
masm
.
vunpckhps
(
lhs
rhsCopy
out
)
;
}
else
{
masm
.
loadAlignedSimd128Float
(
rhs
scratch
)
;
masm
.
vunpckhps
(
lhs
scratch
scratch
)
;
masm
.
moveSimd128Float
(
scratch
out
)
;
}
return
;
}
if
(
x
<
4
&
&
y
<
4
)
{
mask
=
MacroAssembler
:
:
ComputeShuffleMask
(
x
y
z
%
4
w
%
4
)
;
masm
.
vshufps
(
mask
rhs
lhs
out
)
;
return
;
}
MOZ_ASSERT
(
!
(
z
>
=
4
&
&
w
>
=
4
)
)
;
uint32_t
firstMask
[
4
]
secondMask
[
4
]
;
unsigned
i
=
0
j
=
2
k
=
0
;
#
define
COMPUTE_MASK
(
lane
)
\
if
(
lane
>
=
4
)
{
\
firstMask
[
j
]
=
lane
%
4
;
\
secondMask
[
k
+
+
]
=
j
+
+
;
\
}
else
{
\
firstMask
[
i
]
=
lane
;
\
secondMask
[
k
+
+
]
=
i
+
+
;
\
}
COMPUTE_MASK
(
x
)
COMPUTE_MASK
(
y
)
COMPUTE_MASK
(
z
)
COMPUTE_MASK
(
w
)
#
undef
COMPUTE_MASK
MOZ_ASSERT
(
i
=
=
2
&
&
j
=
=
4
&
&
k
=
=
4
)
;
mask
=
MacroAssembler
:
:
ComputeShuffleMask
(
firstMask
[
0
]
firstMask
[
1
]
firstMask
[
2
]
firstMask
[
3
]
)
;
masm
.
vshufps
(
mask
rhs
lhs
lhs
)
;
mask
=
MacroAssembler
:
:
ComputeShuffleMask
(
secondMask
[
0
]
secondMask
[
1
]
secondMask
[
2
]
secondMask
[
3
]
)
;
masm
.
vshufps
(
mask
lhs
lhs
lhs
)
;
}
void
CodeGenerator
:
:
visitSimdBinaryCompIx16
(
LSimdBinaryCompIx16
*
ins
)
{
static
const
SimdConstant
allOnes
=
SimdConstant
:
:
SplatX16
(
-
1
)
;
FloatRegister
lhs
=
ToFloatRegister
(
ins
-
>
lhs
(
)
)
;
Operand
rhs
=
ToOperand
(
ins
-
>
rhs
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
MOZ_ASSERT_IF
(
!
Assembler
:
:
HasAVX
(
)
output
=
=
lhs
)
;
ScratchSimd128Scope
scratch
(
masm
)
;
MSimdBinaryComp
:
:
Operation
op
=
ins
-
>
operation
(
)
;
switch
(
op
)
{
case
MSimdBinaryComp
:
:
greaterThan
:
masm
.
vpcmpgtb
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryComp
:
:
equal
:
masm
.
vpcmpeqb
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryComp
:
:
lessThan
:
if
(
rhs
.
kind
(
)
=
=
Operand
:
:
FPREG
)
masm
.
moveSimd128Int
(
ToFloatRegister
(
ins
-
>
rhs
(
)
)
scratch
)
;
else
masm
.
loadAlignedSimd128Int
(
rhs
scratch
)
;
masm
.
vpcmpgtb
(
ToOperand
(
ins
-
>
lhs
(
)
)
scratch
scratch
)
;
masm
.
moveSimd128Int
(
scratch
output
)
;
return
;
case
MSimdBinaryComp
:
:
notEqual
:
masm
.
loadConstantSimd128Int
(
allOnes
scratch
)
;
masm
.
vpcmpeqb
(
rhs
lhs
output
)
;
masm
.
bitwiseXorSimd128
(
Operand
(
scratch
)
output
)
;
return
;
case
MSimdBinaryComp
:
:
greaterThanOrEqual
:
if
(
rhs
.
kind
(
)
=
=
Operand
:
:
FPREG
)
masm
.
moveSimd128Int
(
ToFloatRegister
(
ins
-
>
rhs
(
)
)
scratch
)
;
else
masm
.
loadAlignedSimd128Int
(
rhs
scratch
)
;
masm
.
vpcmpgtb
(
ToOperand
(
ins
-
>
lhs
(
)
)
scratch
scratch
)
;
masm
.
loadConstantSimd128Int
(
allOnes
output
)
;
masm
.
bitwiseXorSimd128
(
Operand
(
scratch
)
output
)
;
return
;
case
MSimdBinaryComp
:
:
lessThanOrEqual
:
masm
.
loadConstantSimd128Int
(
allOnes
scratch
)
;
masm
.
vpcmpgtb
(
rhs
lhs
output
)
;
masm
.
bitwiseXorSimd128
(
Operand
(
scratch
)
output
)
;
return
;
}
MOZ_CRASH
(
"
unexpected
SIMD
op
"
)
;
}
void
CodeGenerator
:
:
visitSimdBinaryCompIx8
(
LSimdBinaryCompIx8
*
ins
)
{
static
const
SimdConstant
allOnes
=
SimdConstant
:
:
SplatX8
(
-
1
)
;
FloatRegister
lhs
=
ToFloatRegister
(
ins
-
>
lhs
(
)
)
;
Operand
rhs
=
ToOperand
(
ins
-
>
rhs
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
MOZ_ASSERT_IF
(
!
Assembler
:
:
HasAVX
(
)
output
=
=
lhs
)
;
ScratchSimd128Scope
scratch
(
masm
)
;
MSimdBinaryComp
:
:
Operation
op
=
ins
-
>
operation
(
)
;
switch
(
op
)
{
case
MSimdBinaryComp
:
:
greaterThan
:
masm
.
vpcmpgtw
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryComp
:
:
equal
:
masm
.
vpcmpeqw
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryComp
:
:
lessThan
:
if
(
rhs
.
kind
(
)
=
=
Operand
:
:
FPREG
)
masm
.
moveSimd128Int
(
ToFloatRegister
(
ins
-
>
rhs
(
)
)
scratch
)
;
else
masm
.
loadAlignedSimd128Int
(
rhs
scratch
)
;
masm
.
vpcmpgtw
(
ToOperand
(
ins
-
>
lhs
(
)
)
scratch
scratch
)
;
masm
.
moveSimd128Int
(
scratch
output
)
;
return
;
case
MSimdBinaryComp
:
:
notEqual
:
masm
.
loadConstantSimd128Int
(
allOnes
scratch
)
;
masm
.
vpcmpeqw
(
rhs
lhs
output
)
;
masm
.
bitwiseXorSimd128
(
Operand
(
scratch
)
output
)
;
return
;
case
MSimdBinaryComp
:
:
greaterThanOrEqual
:
if
(
rhs
.
kind
(
)
=
=
Operand
:
:
FPREG
)
masm
.
moveSimd128Int
(
ToFloatRegister
(
ins
-
>
rhs
(
)
)
scratch
)
;
else
masm
.
loadAlignedSimd128Int
(
rhs
scratch
)
;
masm
.
vpcmpgtw
(
ToOperand
(
ins
-
>
lhs
(
)
)
scratch
scratch
)
;
masm
.
loadConstantSimd128Int
(
allOnes
output
)
;
masm
.
bitwiseXorSimd128
(
Operand
(
scratch
)
output
)
;
return
;
case
MSimdBinaryComp
:
:
lessThanOrEqual
:
masm
.
loadConstantSimd128Int
(
allOnes
scratch
)
;
masm
.
vpcmpgtw
(
rhs
lhs
output
)
;
masm
.
bitwiseXorSimd128
(
Operand
(
scratch
)
output
)
;
return
;
}
MOZ_CRASH
(
"
unexpected
SIMD
op
"
)
;
}
void
CodeGenerator
:
:
visitSimdBinaryCompIx4
(
LSimdBinaryCompIx4
*
ins
)
{
static
const
SimdConstant
allOnes
=
SimdConstant
:
:
SplatX4
(
-
1
)
;
FloatRegister
lhs
=
ToFloatRegister
(
ins
-
>
lhs
(
)
)
;
Operand
rhs
=
ToOperand
(
ins
-
>
rhs
(
)
)
;
MOZ_ASSERT
(
ToFloatRegister
(
ins
-
>
output
(
)
)
=
=
lhs
)
;
ScratchSimd128Scope
scratch
(
masm
)
;
MSimdBinaryComp
:
:
Operation
op
=
ins
-
>
operation
(
)
;
switch
(
op
)
{
case
MSimdBinaryComp
:
:
greaterThan
:
masm
.
packedGreaterThanInt32x4
(
rhs
lhs
)
;
return
;
case
MSimdBinaryComp
:
:
equal
:
masm
.
packedEqualInt32x4
(
rhs
lhs
)
;
return
;
case
MSimdBinaryComp
:
:
lessThan
:
if
(
rhs
.
kind
(
)
=
=
Operand
:
:
FPREG
)
masm
.
moveSimd128Int
(
ToFloatRegister
(
ins
-
>
rhs
(
)
)
scratch
)
;
else
masm
.
loadAlignedSimd128Int
(
rhs
scratch
)
;
masm
.
packedGreaterThanInt32x4
(
ToOperand
(
ins
-
>
lhs
(
)
)
scratch
)
;
masm
.
moveSimd128Int
(
scratch
lhs
)
;
return
;
case
MSimdBinaryComp
:
:
notEqual
:
masm
.
loadConstantSimd128Int
(
allOnes
scratch
)
;
masm
.
packedEqualInt32x4
(
rhs
lhs
)
;
masm
.
bitwiseXorSimd128
(
Operand
(
scratch
)
lhs
)
;
return
;
case
MSimdBinaryComp
:
:
greaterThanOrEqual
:
if
(
rhs
.
kind
(
)
=
=
Operand
:
:
FPREG
)
masm
.
moveSimd128Int
(
ToFloatRegister
(
ins
-
>
rhs
(
)
)
scratch
)
;
else
masm
.
loadAlignedSimd128Int
(
rhs
scratch
)
;
masm
.
packedGreaterThanInt32x4
(
ToOperand
(
ins
-
>
lhs
(
)
)
scratch
)
;
masm
.
loadConstantSimd128Int
(
allOnes
lhs
)
;
masm
.
bitwiseXorSimd128
(
Operand
(
scratch
)
lhs
)
;
return
;
case
MSimdBinaryComp
:
:
lessThanOrEqual
:
masm
.
loadConstantSimd128Int
(
allOnes
scratch
)
;
masm
.
packedGreaterThanInt32x4
(
rhs
lhs
)
;
masm
.
bitwiseXorSimd128
(
Operand
(
scratch
)
lhs
)
;
return
;
}
MOZ_CRASH
(
"
unexpected
SIMD
op
"
)
;
}
void
CodeGenerator
:
:
visitSimdBinaryCompFx4
(
LSimdBinaryCompFx4
*
ins
)
{
FloatRegister
lhs
=
ToFloatRegister
(
ins
-
>
lhs
(
)
)
;
Operand
rhs
=
ToOperand
(
ins
-
>
rhs
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
MSimdBinaryComp
:
:
Operation
op
=
ins
-
>
operation
(
)
;
switch
(
op
)
{
case
MSimdBinaryComp
:
:
equal
:
masm
.
vcmpeqps
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryComp
:
:
lessThan
:
masm
.
vcmpltps
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryComp
:
:
lessThanOrEqual
:
masm
.
vcmpleps
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryComp
:
:
notEqual
:
masm
.
vcmpneqps
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryComp
:
:
greaterThanOrEqual
:
case
MSimdBinaryComp
:
:
greaterThan
:
MOZ_CRASH
(
"
lowering
should
have
reversed
this
"
)
;
}
MOZ_CRASH
(
"
unexpected
SIMD
op
"
)
;
}
void
CodeGenerator
:
:
visitSimdBinaryArithIx16
(
LSimdBinaryArithIx16
*
ins
)
{
FloatRegister
lhs
=
ToFloatRegister
(
ins
-
>
lhs
(
)
)
;
Operand
rhs
=
ToOperand
(
ins
-
>
rhs
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
MSimdBinaryArith
:
:
Operation
op
=
ins
-
>
operation
(
)
;
switch
(
op
)
{
case
MSimdBinaryArith
:
:
Op_add
:
masm
.
vpaddb
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryArith
:
:
Op_sub
:
masm
.
vpsubb
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryArith
:
:
Op_mul
:
break
;
case
MSimdBinaryArith
:
:
Op_div
:
case
MSimdBinaryArith
:
:
Op_max
:
case
MSimdBinaryArith
:
:
Op_min
:
case
MSimdBinaryArith
:
:
Op_minNum
:
case
MSimdBinaryArith
:
:
Op_maxNum
:
break
;
}
MOZ_CRASH
(
"
unexpected
SIMD
op
"
)
;
}
void
CodeGenerator
:
:
visitSimdBinaryArithIx8
(
LSimdBinaryArithIx8
*
ins
)
{
FloatRegister
lhs
=
ToFloatRegister
(
ins
-
>
lhs
(
)
)
;
Operand
rhs
=
ToOperand
(
ins
-
>
rhs
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
MSimdBinaryArith
:
:
Operation
op
=
ins
-
>
operation
(
)
;
switch
(
op
)
{
case
MSimdBinaryArith
:
:
Op_add
:
masm
.
vpaddw
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryArith
:
:
Op_sub
:
masm
.
vpsubw
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryArith
:
:
Op_mul
:
masm
.
vpmullw
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryArith
:
:
Op_div
:
case
MSimdBinaryArith
:
:
Op_max
:
case
MSimdBinaryArith
:
:
Op_min
:
case
MSimdBinaryArith
:
:
Op_minNum
:
case
MSimdBinaryArith
:
:
Op_maxNum
:
break
;
}
MOZ_CRASH
(
"
unexpected
SIMD
op
"
)
;
}
void
CodeGenerator
:
:
visitSimdBinaryArithIx4
(
LSimdBinaryArithIx4
*
ins
)
{
FloatRegister
lhs
=
ToFloatRegister
(
ins
-
>
lhs
(
)
)
;
Operand
rhs
=
ToOperand
(
ins
-
>
rhs
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
ScratchSimd128Scope
scratch
(
masm
)
;
MSimdBinaryArith
:
:
Operation
op
=
ins
-
>
operation
(
)
;
switch
(
op
)
{
case
MSimdBinaryArith
:
:
Op_add
:
masm
.
vpaddd
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryArith
:
:
Op_sub
:
masm
.
vpsubd
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryArith
:
:
Op_mul
:
{
if
(
AssemblerX86Shared
:
:
HasSSE41
(
)
)
{
masm
.
vpmulld
(
rhs
lhs
output
)
;
return
;
}
masm
.
loadAlignedSimd128Int
(
rhs
scratch
)
;
masm
.
vpmuludq
(
lhs
scratch
scratch
)
;
FloatRegister
temp
=
ToFloatRegister
(
ins
-
>
temp
(
)
)
;
masm
.
vpshufd
(
MacroAssembler
:
:
ComputeShuffleMask
(
1
1
3
3
)
lhs
lhs
)
;
masm
.
vpshufd
(
MacroAssembler
:
:
ComputeShuffleMask
(
1
1
3
3
)
rhs
temp
)
;
masm
.
vpmuludq
(
temp
lhs
lhs
)
;
masm
.
vshufps
(
MacroAssembler
:
:
ComputeShuffleMask
(
0
2
0
2
)
scratch
lhs
lhs
)
;
masm
.
vshufps
(
MacroAssembler
:
:
ComputeShuffleMask
(
2
0
3
1
)
lhs
lhs
lhs
)
;
return
;
}
case
MSimdBinaryArith
:
:
Op_div
:
break
;
case
MSimdBinaryArith
:
:
Op_max
:
break
;
case
MSimdBinaryArith
:
:
Op_min
:
break
;
case
MSimdBinaryArith
:
:
Op_minNum
:
case
MSimdBinaryArith
:
:
Op_maxNum
:
break
;
}
MOZ_CRASH
(
"
unexpected
SIMD
op
"
)
;
}
void
CodeGenerator
:
:
visitSimdBinaryArithFx4
(
LSimdBinaryArithFx4
*
ins
)
{
FloatRegister
lhs
=
ToFloatRegister
(
ins
-
>
lhs
(
)
)
;
Operand
rhs
=
ToOperand
(
ins
-
>
rhs
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
ScratchSimd128Scope
scratch
(
masm
)
;
MSimdBinaryArith
:
:
Operation
op
=
ins
-
>
operation
(
)
;
switch
(
op
)
{
case
MSimdBinaryArith
:
:
Op_add
:
masm
.
vaddps
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryArith
:
:
Op_sub
:
masm
.
vsubps
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryArith
:
:
Op_mul
:
masm
.
vmulps
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryArith
:
:
Op_div
:
masm
.
vdivps
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryArith
:
:
Op_max
:
{
FloatRegister
lhsCopy
=
masm
.
reusedInputFloat32x4
(
lhs
scratch
)
;
masm
.
vcmpunordps
(
rhs
lhsCopy
scratch
)
;
FloatRegister
tmp
=
ToFloatRegister
(
ins
-
>
temp
(
)
)
;
FloatRegister
rhsCopy
=
masm
.
reusedInputAlignedFloat32x4
(
rhs
tmp
)
;
masm
.
vmaxps
(
Operand
(
lhs
)
rhsCopy
tmp
)
;
masm
.
vmaxps
(
rhs
lhs
output
)
;
masm
.
vandps
(
tmp
output
output
)
;
masm
.
vorps
(
scratch
output
output
)
;
return
;
}
case
MSimdBinaryArith
:
:
Op_min
:
{
FloatRegister
rhsCopy
=
masm
.
reusedInputAlignedFloat32x4
(
rhs
scratch
)
;
masm
.
vminps
(
Operand
(
lhs
)
rhsCopy
scratch
)
;
masm
.
vminps
(
rhs
lhs
output
)
;
masm
.
vorps
(
scratch
output
output
)
;
return
;
}
case
MSimdBinaryArith
:
:
Op_minNum
:
{
FloatRegister
tmp
=
ToFloatRegister
(
ins
-
>
temp
(
)
)
;
masm
.
loadConstantSimd128Int
(
SimdConstant
:
:
SplatX4
(
int32_t
(
0x80000000
)
)
tmp
)
;
FloatRegister
mask
=
scratch
;
FloatRegister
tmpCopy
=
masm
.
reusedInputFloat32x4
(
tmp
scratch
)
;
masm
.
vpcmpeqd
(
Operand
(
lhs
)
tmpCopy
mask
)
;
masm
.
vandps
(
tmp
mask
mask
)
;
FloatRegister
lhsCopy
=
masm
.
reusedInputFloat32x4
(
lhs
tmp
)
;
masm
.
vminps
(
rhs
lhsCopy
tmp
)
;
masm
.
vorps
(
mask
tmp
tmp
)
;
FloatRegister
rhsCopy
=
masm
.
reusedInputAlignedFloat32x4
(
rhs
mask
)
;
masm
.
vcmpneqps
(
rhs
rhsCopy
mask
)
;
if
(
AssemblerX86Shared
:
:
HasAVX
(
)
)
{
masm
.
vblendvps
(
mask
lhs
tmp
output
)
;
}
else
{
if
(
lhs
!
=
output
)
masm
.
moveSimd128Float
(
lhs
output
)
;
masm
.
vandps
(
Operand
(
mask
)
output
output
)
;
masm
.
vandnps
(
Operand
(
tmp
)
mask
mask
)
;
masm
.
vorps
(
Operand
(
mask
)
output
output
)
;
}
return
;
}
case
MSimdBinaryArith
:
:
Op_maxNum
:
{
FloatRegister
mask
=
scratch
;
masm
.
loadConstantSimd128Int
(
SimdConstant
:
:
SplatX4
(
0
)
mask
)
;
masm
.
vpcmpeqd
(
Operand
(
lhs
)
mask
mask
)
;
FloatRegister
tmp
=
ToFloatRegister
(
ins
-
>
temp
(
)
)
;
masm
.
loadConstantSimd128Int
(
SimdConstant
:
:
SplatX4
(
int32_t
(
0x80000000
)
)
tmp
)
;
masm
.
vandps
(
tmp
mask
mask
)
;
FloatRegister
lhsCopy
=
masm
.
reusedInputFloat32x4
(
lhs
tmp
)
;
masm
.
vmaxps
(
rhs
lhsCopy
tmp
)
;
masm
.
vandnps
(
Operand
(
tmp
)
mask
mask
)
;
mask
=
tmp
;
tmp
=
scratch
;
FloatRegister
rhsCopy
=
masm
.
reusedInputAlignedFloat32x4
(
rhs
mask
)
;
masm
.
vcmpneqps
(
rhs
rhsCopy
mask
)
;
if
(
AssemblerX86Shared
:
:
HasAVX
(
)
)
{
masm
.
vblendvps
(
mask
lhs
tmp
output
)
;
}
else
{
if
(
lhs
!
=
output
)
masm
.
moveSimd128Float
(
lhs
output
)
;
masm
.
vandps
(
Operand
(
mask
)
output
output
)
;
masm
.
vandnps
(
Operand
(
tmp
)
mask
mask
)
;
masm
.
vorps
(
Operand
(
mask
)
output
output
)
;
}
return
;
}
}
MOZ_CRASH
(
"
unexpected
SIMD
op
"
)
;
}
void
CodeGenerator
:
:
visitSimdBinarySaturating
(
LSimdBinarySaturating
*
ins
)
{
FloatRegister
lhs
=
ToFloatRegister
(
ins
-
>
lhs
(
)
)
;
Operand
rhs
=
ToOperand
(
ins
-
>
rhs
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
SimdSign
sign
=
ins
-
>
signedness
(
)
;
MOZ_ASSERT
(
sign
!
=
SimdSign
:
:
NotApplicable
)
;
switch
(
ins
-
>
type
(
)
)
{
case
MIRType
:
:
Int8x16
:
switch
(
ins
-
>
operation
(
)
)
{
case
MSimdBinarySaturating
:
:
add
:
if
(
sign
=
=
SimdSign
:
:
Signed
)
masm
.
vpaddsb
(
rhs
lhs
output
)
;
else
masm
.
vpaddusb
(
rhs
lhs
output
)
;
return
;
case
MSimdBinarySaturating
:
:
sub
:
if
(
sign
=
=
SimdSign
:
:
Signed
)
masm
.
vpsubsb
(
rhs
lhs
output
)
;
else
masm
.
vpsubusb
(
rhs
lhs
output
)
;
return
;
}
break
;
case
MIRType
:
:
Int16x8
:
switch
(
ins
-
>
operation
(
)
)
{
case
MSimdBinarySaturating
:
:
add
:
if
(
sign
=
=
SimdSign
:
:
Signed
)
masm
.
vpaddsw
(
rhs
lhs
output
)
;
else
masm
.
vpaddusw
(
rhs
lhs
output
)
;
return
;
case
MSimdBinarySaturating
:
:
sub
:
if
(
sign
=
=
SimdSign
:
:
Signed
)
masm
.
vpsubsw
(
rhs
lhs
output
)
;
else
masm
.
vpsubusw
(
rhs
lhs
output
)
;
return
;
}
break
;
default
:
break
;
}
MOZ_CRASH
(
"
unsupported
type
for
SIMD
saturating
arithmetic
"
)
;
}
void
CodeGenerator
:
:
visitSimdUnaryArithIx16
(
LSimdUnaryArithIx16
*
ins
)
{
Operand
in
=
ToOperand
(
ins
-
>
input
(
)
)
;
FloatRegister
out
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
static
const
SimdConstant
allOnes
=
SimdConstant
:
:
SplatX16
(
-
1
)
;
switch
(
ins
-
>
operation
(
)
)
{
case
MSimdUnaryArith
:
:
neg
:
masm
.
zeroSimd128Int
(
out
)
;
masm
.
packedSubInt8
(
in
out
)
;
return
;
case
MSimdUnaryArith
:
:
not_
:
masm
.
loadConstantSimd128Int
(
allOnes
out
)
;
masm
.
bitwiseXorSimd128
(
in
out
)
;
return
;
case
MSimdUnaryArith
:
:
abs
:
case
MSimdUnaryArith
:
:
reciprocalApproximation
:
case
MSimdUnaryArith
:
:
reciprocalSqrtApproximation
:
case
MSimdUnaryArith
:
:
sqrt
:
break
;
}
MOZ_CRASH
(
"
unexpected
SIMD
op
"
)
;
}
void
CodeGenerator
:
:
visitSimdUnaryArithIx8
(
LSimdUnaryArithIx8
*
ins
)
{
Operand
in
=
ToOperand
(
ins
-
>
input
(
)
)
;
FloatRegister
out
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
static
const
SimdConstant
allOnes
=
SimdConstant
:
:
SplatX8
(
-
1
)
;
switch
(
ins
-
>
operation
(
)
)
{
case
MSimdUnaryArith
:
:
neg
:
masm
.
zeroSimd128Int
(
out
)
;
masm
.
packedSubInt16
(
in
out
)
;
return
;
case
MSimdUnaryArith
:
:
not_
:
masm
.
loadConstantSimd128Int
(
allOnes
out
)
;
masm
.
bitwiseXorSimd128
(
in
out
)
;
return
;
case
MSimdUnaryArith
:
:
abs
:
case
MSimdUnaryArith
:
:
reciprocalApproximation
:
case
MSimdUnaryArith
:
:
reciprocalSqrtApproximation
:
case
MSimdUnaryArith
:
:
sqrt
:
break
;
}
MOZ_CRASH
(
"
unexpected
SIMD
op
"
)
;
}
void
CodeGenerator
:
:
visitSimdUnaryArithIx4
(
LSimdUnaryArithIx4
*
ins
)
{
Operand
in
=
ToOperand
(
ins
-
>
input
(
)
)
;
FloatRegister
out
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
static
const
SimdConstant
allOnes
=
SimdConstant
:
:
SplatX4
(
-
1
)
;
switch
(
ins
-
>
operation
(
)
)
{
case
MSimdUnaryArith
:
:
neg
:
masm
.
zeroSimd128Int
(
out
)
;
masm
.
packedSubInt32
(
in
out
)
;
return
;
case
MSimdUnaryArith
:
:
not_
:
masm
.
loadConstantSimd128Int
(
allOnes
out
)
;
masm
.
bitwiseXorSimd128
(
in
out
)
;
return
;
case
MSimdUnaryArith
:
:
abs
:
case
MSimdUnaryArith
:
:
reciprocalApproximation
:
case
MSimdUnaryArith
:
:
reciprocalSqrtApproximation
:
case
MSimdUnaryArith
:
:
sqrt
:
break
;
}
MOZ_CRASH
(
"
unexpected
SIMD
op
"
)
;
}
void
CodeGenerator
:
:
visitSimdUnaryArithFx4
(
LSimdUnaryArithFx4
*
ins
)
{
Operand
in
=
ToOperand
(
ins
-
>
input
(
)
)
;
FloatRegister
out
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
float
signMask
=
SpecificNaN
<
float
>
(
0
FloatingPoint
<
float
>
:
:
kSignificandBits
)
;
static
const
SimdConstant
signMasks
=
SimdConstant
:
:
SplatX4
(
signMask
)
;
float
ones
=
SpecificNaN
<
float
>
(
1
FloatingPoint
<
float
>
:
:
kSignificandBits
)
;
static
const
SimdConstant
allOnes
=
SimdConstant
:
:
SplatX4
(
ones
)
;
static
const
SimdConstant
minusZero
=
SimdConstant
:
:
SplatX4
(
-
0
.
f
)
;
switch
(
ins
-
>
operation
(
)
)
{
case
MSimdUnaryArith
:
:
abs
:
masm
.
loadConstantSimd128Float
(
signMasks
out
)
;
masm
.
bitwiseAndSimd128
(
in
out
)
;
return
;
case
MSimdUnaryArith
:
:
neg
:
masm
.
loadConstantSimd128Float
(
minusZero
out
)
;
masm
.
bitwiseXorSimd128
(
in
out
)
;
return
;
case
MSimdUnaryArith
:
:
not_
:
masm
.
loadConstantSimd128Float
(
allOnes
out
)
;
masm
.
bitwiseXorSimd128
(
in
out
)
;
return
;
case
MSimdUnaryArith
:
:
reciprocalApproximation
:
masm
.
packedRcpApproximationFloat32x4
(
in
out
)
;
return
;
case
MSimdUnaryArith
:
:
reciprocalSqrtApproximation
:
masm
.
packedRcpSqrtApproximationFloat32x4
(
in
out
)
;
return
;
case
MSimdUnaryArith
:
:
sqrt
:
masm
.
packedSqrtFloat32x4
(
in
out
)
;
return
;
}
MOZ_CRASH
(
"
unexpected
SIMD
op
"
)
;
}
void
CodeGenerator
:
:
visitSimdBinaryBitwise
(
LSimdBinaryBitwise
*
ins
)
{
FloatRegister
lhs
=
ToFloatRegister
(
ins
-
>
lhs
(
)
)
;
Operand
rhs
=
ToOperand
(
ins
-
>
rhs
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
MSimdBinaryBitwise
:
:
Operation
op
=
ins
-
>
operation
(
)
;
switch
(
op
)
{
case
MSimdBinaryBitwise
:
:
and_
:
if
(
ins
-
>
type
(
)
=
=
MIRType
:
:
Float32x4
)
masm
.
vandps
(
rhs
lhs
output
)
;
else
masm
.
vpand
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryBitwise
:
:
or_
:
if
(
ins
-
>
type
(
)
=
=
MIRType
:
:
Float32x4
)
masm
.
vorps
(
rhs
lhs
output
)
;
else
masm
.
vpor
(
rhs
lhs
output
)
;
return
;
case
MSimdBinaryBitwise
:
:
xor_
:
if
(
ins
-
>
type
(
)
=
=
MIRType
:
:
Float32x4
)
masm
.
vxorps
(
rhs
lhs
output
)
;
else
masm
.
vpxor
(
rhs
lhs
output
)
;
return
;
}
MOZ_CRASH
(
"
unexpected
SIMD
bitwise
op
"
)
;
}
void
CodeGenerator
:
:
visitSimdShift
(
LSimdShift
*
ins
)
{
FloatRegister
out
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
MOZ_ASSERT
(
ToFloatRegister
(
ins
-
>
vector
(
)
)
=
=
out
)
;
uint32_t
shiftmask
=
(
128u
/
SimdTypeToLength
(
ins
-
>
type
(
)
)
)
-
1
;
const
LAllocation
*
val
=
ins
-
>
value
(
)
;
if
(
val
-
>
isConstant
(
)
)
{
MOZ_ASSERT
(
ins
-
>
temp
(
)
-
>
isBogusTemp
(
)
)
;
Imm32
count
(
uint32_t
(
ToInt32
(
val
)
)
&
shiftmask
)
;
switch
(
ins
-
>
type
(
)
)
{
case
MIRType
:
:
Int16x8
:
switch
(
ins
-
>
operation
(
)
)
{
case
MSimdShift
:
:
lsh
:
masm
.
packedLeftShiftByScalarInt16x8
(
count
out
)
;
return
;
case
MSimdShift
:
:
rsh
:
masm
.
packedRightShiftByScalarInt16x8
(
count
out
)
;
return
;
case
MSimdShift
:
:
ursh
:
masm
.
packedUnsignedRightShiftByScalarInt16x8
(
count
out
)
;
return
;
}
break
;
case
MIRType
:
:
Int32x4
:
switch
(
ins
-
>
operation
(
)
)
{
case
MSimdShift
:
:
lsh
:
masm
.
packedLeftShiftByScalarInt32x4
(
count
out
)
;
return
;
case
MSimdShift
:
:
rsh
:
masm
.
packedRightShiftByScalarInt32x4
(
count
out
)
;
return
;
case
MSimdShift
:
:
ursh
:
masm
.
packedUnsignedRightShiftByScalarInt32x4
(
count
out
)
;
return
;
}
break
;
default
:
MOZ_CRASH
(
"
unsupported
type
for
SIMD
shifts
"
)
;
}
MOZ_CRASH
(
"
unexpected
SIMD
bitwise
op
"
)
;
}
MOZ_ASSERT
(
val
-
>
isRegister
(
)
)
;
Register
count
=
ToRegister
(
ins
-
>
temp
(
)
)
;
masm
.
mov
(
ToRegister
(
val
)
count
)
;
masm
.
andl
(
Imm32
(
shiftmask
)
count
)
;
ScratchFloat32Scope
scratch
(
masm
)
;
masm
.
vmovd
(
count
scratch
)
;
switch
(
ins
-
>
type
(
)
)
{
case
MIRType
:
:
Int16x8
:
switch
(
ins
-
>
operation
(
)
)
{
case
MSimdShift
:
:
lsh
:
masm
.
packedLeftShiftByScalarInt16x8
(
scratch
out
)
;
return
;
case
MSimdShift
:
:
rsh
:
masm
.
packedRightShiftByScalarInt16x8
(
scratch
out
)
;
return
;
case
MSimdShift
:
:
ursh
:
masm
.
packedUnsignedRightShiftByScalarInt16x8
(
scratch
out
)
;
return
;
}
break
;
case
MIRType
:
:
Int32x4
:
switch
(
ins
-
>
operation
(
)
)
{
case
MSimdShift
:
:
lsh
:
masm
.
packedLeftShiftByScalarInt32x4
(
scratch
out
)
;
return
;
case
MSimdShift
:
:
rsh
:
masm
.
packedRightShiftByScalarInt32x4
(
scratch
out
)
;
return
;
case
MSimdShift
:
:
ursh
:
masm
.
packedUnsignedRightShiftByScalarInt32x4
(
scratch
out
)
;
return
;
}
break
;
default
:
MOZ_CRASH
(
"
unsupported
type
for
SIMD
shifts
"
)
;
}
MOZ_CRASH
(
"
unexpected
SIMD
bitwise
op
"
)
;
}
void
CodeGenerator
:
:
visitSimdSelect
(
LSimdSelect
*
ins
)
{
FloatRegister
mask
=
ToFloatRegister
(
ins
-
>
mask
(
)
)
;
FloatRegister
onTrue
=
ToFloatRegister
(
ins
-
>
lhs
(
)
)
;
FloatRegister
onFalse
=
ToFloatRegister
(
ins
-
>
rhs
(
)
)
;
FloatRegister
output
=
ToFloatRegister
(
ins
-
>
output
(
)
)
;
FloatRegister
temp
=
ToFloatRegister
(
ins
-
>
temp
(
)
)
;
if
(
onTrue
!
=
output
)
masm
.
vmovaps
(
onTrue
output
)
;
if
(
mask
!
=
temp
)
masm
.
vmovaps
(
mask
temp
)
;
MSimdSelect
*
mir
=
ins
-
>
mir
(
)
;
unsigned
lanes
=
SimdTypeToLength
(
mir
-
>
type
(
)
)
;
if
(
AssemblerX86Shared
:
:
HasAVX
(
)
&
&
lanes
=
=
4
)
{
masm
.
vblendvps
(
mask
onTrue
onFalse
output
)
;
return
;
}
masm
.
bitwiseAndSimd128
(
Operand
(
temp
)
output
)
;
masm
.
bitwiseAndNotSimd128
(
Operand
(
onFalse
)
temp
)
;
masm
.
bitwiseOrSimd128
(
Operand
(
temp
)
output
)
;
}
void
CodeGenerator
:
:
visitCompareExchangeTypedArrayElement
(
LCompareExchangeTypedArrayElement
*
lir
)
{
Register
elements
=
ToRegister
(
lir
-
>
elements
(
)
)
;
AnyRegister
output
=
ToAnyRegister
(
lir
-
>
output
(
)
)
;
Register
temp
=
lir
-
>
temp
(
)
-
>
isBogusTemp
(
)
?
InvalidReg
:
ToRegister
(
lir
-
>
temp
(
)
)
;
Register
oldval
=
ToRegister
(
lir
-
>
oldval
(
)
)
;
Register
newval
=
ToRegister
(
lir
-
>
newval
(
)
)
;
Scalar
:
:
Type
arrayType
=
lir
-
>
mir
(
)
-
>
arrayType
(
)
;
int
width
=
Scalar
:
:
byteSize
(
arrayType
)
;
if
(
lir
-
>
index
(
)
-
>
isConstant
(
)
)
{
Address
dest
(
elements
ToInt32
(
lir
-
>
index
(
)
)
*
width
)
;
masm
.
compareExchangeJS
(
arrayType
Synchronization
:
:
Full
(
)
dest
oldval
newval
temp
output
)
;
}
else
{
BaseIndex
dest
(
elements
ToRegister
(
lir
-
>
index
(
)
)
ScaleFromElemWidth
(
width
)
)
;
masm
.
compareExchangeJS
(
arrayType
Synchronization
:
:
Full
(
)
dest
oldval
newval
temp
output
)
;
}
}
void
CodeGenerator
:
:
visitAtomicExchangeTypedArrayElement
(
LAtomicExchangeTypedArrayElement
*
lir
)
{
Register
elements
=
ToRegister
(
lir
-
>
elements
(
)
)
;
AnyRegister
output
=
ToAnyRegister
(
lir
-
>
output
(
)
)
;
Register
temp
=
lir
-
>
temp
(
)
-
>
isBogusTemp
(
)
?
InvalidReg
:
ToRegister
(
lir
-
>
temp
(
)
)
;
Register
value
=
ToRegister
(
lir
-
>
value
(
)
)
;
Scalar
:
:
Type
arrayType
=
lir
-
>
mir
(
)
-
>
arrayType
(
)
;
int
width
=
Scalar
:
:
byteSize
(
arrayType
)
;
if
(
lir
-
>
index
(
)
-
>
isConstant
(
)
)
{
Address
dest
(
elements
ToInt32
(
lir
-
>
index
(
)
)
*
width
)
;
masm
.
atomicExchangeJS
(
arrayType
Synchronization
:
:
Full
(
)
dest
value
temp
output
)
;
}
else
{
BaseIndex
dest
(
elements
ToRegister
(
lir
-
>
index
(
)
)
ScaleFromElemWidth
(
width
)
)
;
masm
.
atomicExchangeJS
(
arrayType
Synchronization
:
:
Full
(
)
dest
value
temp
output
)
;
}
}
template
<
typename
T
>
static
inline
void
AtomicBinopToTypedArray
(
MacroAssembler
&
masm
AtomicOp
op
Scalar
:
:
Type
arrayType
const
LAllocation
*
value
const
T
&
mem
Register
temp1
Register
temp2
AnyRegister
output
)
{
if
(
value
-
>
isConstant
(
)
)
{
masm
.
atomicFetchOpJS
(
arrayType
Synchronization
:
:
Full
(
)
op
Imm32
(
ToInt32
(
value
)
)
mem
temp1
temp2
output
)
;
}
else
{
masm
.
atomicFetchOpJS
(
arrayType
Synchronization
:
:
Full
(
)
op
ToRegister
(
value
)
mem
temp1
temp2
output
)
;
}
}
void
CodeGenerator
:
:
visitAtomicTypedArrayElementBinop
(
LAtomicTypedArrayElementBinop
*
lir
)
{
MOZ_ASSERT
(
lir
-
>
mir
(
)
-
>
hasUses
(
)
)
;
AnyRegister
output
=
ToAnyRegister
(
lir
-
>
output
(
)
)
;
Register
elements
=
ToRegister
(
lir
-
>
elements
(
)
)
;
Register
temp1
=
lir
-
>
temp1
(
)
-
>
isBogusTemp
(
)
?
InvalidReg
:
ToRegister
(
lir
-
>
temp1
(
)
)
;
Register
temp2
=
lir
-
>
temp2
(
)
-
>
isBogusTemp
(
)
?
InvalidReg
:
ToRegister
(
lir
-
>
temp2
(
)
)
;
const
LAllocation
*
value
=
lir
-
>
value
(
)
;
Scalar
:
:
Type
arrayType
=
lir
-
>
mir
(
)
-
>
arrayType
(
)
;
int
width
=
Scalar
:
:
byteSize
(
arrayType
)
;
if
(
lir
-
>
index
(
)
-
>
isConstant
(
)
)
{
Address
mem
(
elements
ToInt32
(
lir
-
>
index
(
)
)
*
width
)
;
AtomicBinopToTypedArray
(
masm
lir
-
>
mir
(
)
-
>
operation
(
)
arrayType
value
mem
temp1
temp2
output
)
;
}
else
{
BaseIndex
mem
(
elements
ToRegister
(
lir
-
>
index
(
)
)
ScaleFromElemWidth
(
width
)
)
;
AtomicBinopToTypedArray
(
masm
lir
-
>
mir
(
)
-
>
operation
(
)
arrayType
value
mem
temp1
temp2
output
)
;
}
}
template
<
typename
T
>
static
inline
void
AtomicBinopToTypedArray
(
MacroAssembler
&
masm
Scalar
:
:
Type
arrayType
AtomicOp
op
const
LAllocation
*
value
const
T
&
mem
)
{
if
(
value
-
>
isConstant
(
)
)
{
masm
.
atomicEffectOpJS
(
arrayType
Synchronization
:
:
Full
(
)
op
Imm32
(
ToInt32
(
value
)
)
mem
InvalidReg
)
;
}
else
{
masm
.
atomicEffectOpJS
(
arrayType
Synchronization
:
:
Full
(
)
op
ToRegister
(
value
)
mem
InvalidReg
)
;
}
}
void
CodeGenerator
:
:
visitAtomicTypedArrayElementBinopForEffect
(
LAtomicTypedArrayElementBinopForEffect
*
lir
)
{
MOZ_ASSERT
(
!
lir
-
>
mir
(
)
-
>
hasUses
(
)
)
;
Register
elements
=
ToRegister
(
lir
-
>
elements
(
)
)
;
const
LAllocation
*
value
=
lir
-
>
value
(
)
;
Scalar
:
:
Type
arrayType
=
lir
-
>
mir
(
)
-
>
arrayType
(
)
;
int
width
=
Scalar
:
:
byteSize
(
arrayType
)
;
if
(
lir
-
>
index
(
)
-
>
isConstant
(
)
)
{
Address
mem
(
elements
ToInt32
(
lir
-
>
index
(
)
)
*
width
)
;
AtomicBinopToTypedArray
(
masm
arrayType
lir
-
>
mir
(
)
-
>
operation
(
)
value
mem
)
;
}
else
{
BaseIndex
mem
(
elements
ToRegister
(
lir
-
>
index
(
)
)
ScaleFromElemWidth
(
width
)
)
;
AtomicBinopToTypedArray
(
masm
arrayType
lir
-
>
mir
(
)
-
>
operation
(
)
value
mem
)
;
}
}
void
CodeGenerator
:
:
visitMemoryBarrier
(
LMemoryBarrier
*
ins
)
{
if
(
ins
-
>
type
(
)
&
MembarStoreLoad
)
masm
.
storeLoadFence
(
)
;
}
void
CodeGeneratorX86Shared
:
:
visitOutOfLineWasmTruncateCheck
(
OutOfLineWasmTruncateCheck
*
ool
)
{
FloatRegister
input
=
ool
-
>
input
(
)
;
Register
output
=
ool
-
>
output
(
)
;
Register64
output64
=
ool
-
>
output64
(
)
;
MIRType
fromType
=
ool
-
>
fromType
(
)
;
MIRType
toType
=
ool
-
>
toType
(
)
;
Label
*
oolRejoin
=
ool
-
>
rejoin
(
)
;
TruncFlags
flags
=
ool
-
>
flags
(
)
;
wasm
:
:
BytecodeOffset
off
=
ool
-
>
bytecodeOffset
(
)
;
if
(
fromType
=
=
MIRType
:
:
Float32
)
{
if
(
toType
=
=
MIRType
:
:
Int32
)
masm
.
oolWasmTruncateCheckF32ToI32
(
input
output
flags
off
oolRejoin
)
;
else
if
(
toType
=
=
MIRType
:
:
Int64
)
masm
.
oolWasmTruncateCheckF32ToI64
(
input
output64
flags
off
oolRejoin
)
;
else
MOZ_CRASH
(
"
unexpected
type
"
)
;
}
else
if
(
fromType
=
=
MIRType
:
:
Double
)
{
if
(
toType
=
=
MIRType
:
:
Int32
)
masm
.
oolWasmTruncateCheckF64ToI32
(
input
output
flags
off
oolRejoin
)
;
else
if
(
toType
=
=
MIRType
:
:
Int64
)
masm
.
oolWasmTruncateCheckF64ToI64
(
input
output64
flags
off
oolRejoin
)
;
else
MOZ_CRASH
(
"
unexpected
type
"
)
;
}
else
{
MOZ_CRASH
(
"
unexpected
type
"
)
;
}
}
void
CodeGeneratorX86Shared
:
:
canonicalizeIfDeterministic
(
Scalar
:
:
Type
type
const
LAllocation
*
value
)
{
#
ifdef
JS_MORE_DETERMINISTIC
switch
(
type
)
{
case
Scalar
:
:
Float32
:
{
FloatRegister
in
=
ToFloatRegister
(
value
)
;
masm
.
canonicalizeFloatIfDeterministic
(
in
)
;
break
;
}
case
Scalar
:
:
Float64
:
{
FloatRegister
in
=
ToFloatRegister
(
value
)
;
masm
.
canonicalizeDoubleIfDeterministic
(
in
)
;
break
;
}
case
Scalar
:
:
Float32x4
:
{
FloatRegister
in
=
ToFloatRegister
(
value
)
;
MOZ_ASSERT
(
in
.
isSimd128
(
)
)
;
FloatRegister
scratch
=
in
!
=
xmm0
.
asSimd128
(
)
?
xmm0
:
xmm1
;
masm
.
push
(
scratch
)
;
masm
.
canonicalizeFloat32x4
(
in
scratch
)
;
masm
.
pop
(
scratch
)
;
break
;
}
default
:
{
break
;
}
}
#
endif
}
void
CodeGenerator
:
:
visitCopySignF
(
LCopySignF
*
lir
)
{
FloatRegister
lhs
=
ToFloatRegister
(
lir
-
>
getOperand
(
0
)
)
;
FloatRegister
rhs
=
ToFloatRegister
(
lir
-
>
getOperand
(
1
)
)
;
FloatRegister
out
=
ToFloatRegister
(
lir
-
>
output
(
)
)
;
if
(
lhs
=
=
rhs
)
{
if
(
lhs
!
=
out
)
masm
.
moveFloat32
(
lhs
out
)
;
return
;
}
ScratchFloat32Scope
scratch
(
masm
)
;
float
clearSignMask
=
BitwiseCast
<
float
>
(
INT32_MAX
)
;
masm
.
loadConstantFloat32
(
clearSignMask
scratch
)
;
masm
.
vandps
(
scratch
lhs
out
)
;
float
keepSignMask
=
BitwiseCast
<
float
>
(
INT32_MIN
)
;
masm
.
loadConstantFloat32
(
keepSignMask
scratch
)
;
masm
.
vandps
(
rhs
scratch
scratch
)
;
masm
.
vorps
(
scratch
out
out
)
;
}
void
CodeGenerator
:
:
visitCopySignD
(
LCopySignD
*
lir
)
{
FloatRegister
lhs
=
ToFloatRegister
(
lir
-
>
getOperand
(
0
)
)
;
FloatRegister
rhs
=
ToFloatRegister
(
lir
-
>
getOperand
(
1
)
)
;
FloatRegister
out
=
ToFloatRegister
(
lir
-
>
output
(
)
)
;
if
(
lhs
=
=
rhs
)
{
if
(
lhs
!
=
out
)
masm
.
moveDouble
(
lhs
out
)
;
return
;
}
ScratchDoubleScope
scratch
(
masm
)
;
double
clearSignMask
=
BitwiseCast
<
double
>
(
INT64_MAX
)
;
masm
.
loadConstantDouble
(
clearSignMask
scratch
)
;
masm
.
vandpd
(
scratch
lhs
out
)
;
double
keepSignMask
=
BitwiseCast
<
double
>
(
INT64_MIN
)
;
masm
.
loadConstantDouble
(
keepSignMask
scratch
)
;
masm
.
vandpd
(
rhs
scratch
scratch
)
;
masm
.
vorpd
(
scratch
out
out
)
;
}
void
CodeGenerator
:
:
visitRotateI64
(
LRotateI64
*
lir
)
{
MRotate
*
mir
=
lir
-
>
mir
(
)
;
LAllocation
*
count
=
lir
-
>
count
(
)
;
Register64
input
=
ToRegister64
(
lir
-
>
input
(
)
)
;
Register64
output
=
ToOutRegister64
(
lir
)
;
Register
temp
=
ToTempRegisterOrInvalid
(
lir
-
>
temp
(
)
)
;
MOZ_ASSERT
(
input
=
=
output
)
;
if
(
count
-
>
isConstant
(
)
)
{
int32_t
c
=
int32_t
(
count
-
>
toConstant
(
)
-
>
toInt64
(
)
&
0x3F
)
;
if
(
!
c
)
return
;
if
(
mir
-
>
isLeftRotate
(
)
)
masm
.
rotateLeft64
(
Imm32
(
c
)
input
output
temp
)
;
else
masm
.
rotateRight64
(
Imm32
(
c
)
input
output
temp
)
;
}
else
{
if
(
mir
-
>
isLeftRotate
(
)
)
masm
.
rotateLeft64
(
ToRegister
(
count
)
input
output
temp
)
;
else
masm
.
rotateRight64
(
ToRegister
(
count
)
input
output
temp
)
;
}
}
void
CodeGenerator
:
:
visitPopcntI64
(
LPopcntI64
*
lir
)
{
Register64
input
=
ToRegister64
(
lir
-
>
getInt64Operand
(
0
)
)
;
Register64
output
=
ToOutRegister64
(
lir
)
;
Register
temp
=
InvalidReg
;
if
(
!
AssemblerX86Shared
:
:
HasPOPCNT
(
)
)
temp
=
ToRegister
(
lir
-
>
getTemp
(
0
)
)
;
masm
.
popcnt64
(
input
output
temp
)
;
}
}
}
