#
ifndef
jit_AtomicOperations_h
#
define
jit_AtomicOperations_h
#
include
"
mozilla
/
Types
.
h
"
#
include
"
vm
/
SharedMem
.
h
"
namespace
js
{
namespace
jit
{
class
RegionLock
;
class
AtomicOperations
{
friend
class
RegionLock
;
private
:
template
<
typename
T
>
static
inline
T
loadSeqCst
(
T
*
addr
)
;
template
<
typename
T
>
static
inline
void
storeSeqCst
(
T
*
addr
T
val
)
;
template
<
typename
T
>
static
inline
T
exchangeSeqCst
(
T
*
addr
T
val
)
;
template
<
typename
T
>
static
inline
T
compareExchangeSeqCst
(
T
*
addr
T
oldval
T
newval
)
;
template
<
typename
T
>
static
inline
T
fetchAddSeqCst
(
T
*
addr
T
val
)
;
template
<
typename
T
>
static
inline
T
fetchSubSeqCst
(
T
*
addr
T
val
)
;
template
<
typename
T
>
static
inline
T
fetchAndSeqCst
(
T
*
addr
T
val
)
;
template
<
typename
T
>
static
inline
T
fetchOrSeqCst
(
T
*
addr
T
val
)
;
template
<
typename
T
>
static
inline
T
fetchXorSeqCst
(
T
*
addr
T
val
)
;
template
<
typename
T
>
static
inline
T
loadSafeWhenRacy
(
T
*
addr
)
;
template
<
typename
T
>
static
inline
void
storeSafeWhenRacy
(
T
*
addr
T
val
)
;
static
inline
void
memcpySafeWhenRacy
(
void
*
dest
const
void
*
src
size_t
nbytes
)
;
static
inline
void
memmoveSafeWhenRacy
(
void
*
dest
const
void
*
src
size_t
nbytes
)
;
public
:
static
inline
bool
isLockfree
(
int32_t
n
)
;
static
inline
bool
isLockfree8
(
)
;
static
inline
void
fenceSeqCst
(
)
;
template
<
typename
T
>
static
T
loadSeqCst
(
SharedMem
<
T
*
>
addr
)
{
return
loadSeqCst
(
addr
.
unwrap
(
)
)
;
}
template
<
typename
T
>
static
void
storeSeqCst
(
SharedMem
<
T
*
>
addr
T
val
)
{
return
storeSeqCst
(
addr
.
unwrap
(
)
val
)
;
}
template
<
typename
T
>
static
T
exchangeSeqCst
(
SharedMem
<
T
*
>
addr
T
val
)
{
return
exchangeSeqCst
(
addr
.
unwrap
(
)
val
)
;
}
template
<
typename
T
>
static
T
compareExchangeSeqCst
(
SharedMem
<
T
*
>
addr
T
oldval
T
newval
)
{
return
compareExchangeSeqCst
(
addr
.
unwrap
(
)
oldval
newval
)
;
}
template
<
typename
T
>
static
T
fetchAddSeqCst
(
SharedMem
<
T
*
>
addr
T
val
)
{
return
fetchAddSeqCst
(
addr
.
unwrap
(
)
val
)
;
}
template
<
typename
T
>
static
T
fetchSubSeqCst
(
SharedMem
<
T
*
>
addr
T
val
)
{
return
fetchSubSeqCst
(
addr
.
unwrap
(
)
val
)
;
}
template
<
typename
T
>
static
T
fetchAndSeqCst
(
SharedMem
<
T
*
>
addr
T
val
)
{
return
fetchAndSeqCst
(
addr
.
unwrap
(
)
val
)
;
}
template
<
typename
T
>
static
T
fetchOrSeqCst
(
SharedMem
<
T
*
>
addr
T
val
)
{
return
fetchOrSeqCst
(
addr
.
unwrap
(
)
val
)
;
}
template
<
typename
T
>
static
T
fetchXorSeqCst
(
SharedMem
<
T
*
>
addr
T
val
)
{
return
fetchXorSeqCst
(
addr
.
unwrap
(
)
val
)
;
}
template
<
typename
T
>
static
T
loadSafeWhenRacy
(
SharedMem
<
T
*
>
addr
)
{
return
loadSafeWhenRacy
(
addr
.
unwrap
(
)
)
;
}
template
<
typename
T
>
static
void
storeSafeWhenRacy
(
SharedMem
<
T
*
>
addr
T
val
)
{
return
storeSafeWhenRacy
(
addr
.
unwrap
(
)
val
)
;
}
template
<
typename
T
>
static
void
memcpySafeWhenRacy
(
SharedMem
<
T
*
>
dest
SharedMem
<
T
*
>
src
size_t
nbytes
)
{
memcpySafeWhenRacy
(
dest
.
template
cast
<
void
*
>
(
)
.
unwrap
(
)
src
.
template
cast
<
void
*
>
(
)
.
unwrap
(
)
nbytes
)
;
}
template
<
typename
T
>
static
void
memcpySafeWhenRacy
(
SharedMem
<
T
*
>
dest
T
*
src
size_t
nbytes
)
{
memcpySafeWhenRacy
(
dest
.
template
cast
<
void
*
>
(
)
.
unwrap
(
)
static_cast
<
void
*
>
(
src
)
nbytes
)
;
}
template
<
typename
T
>
static
void
memcpySafeWhenRacy
(
T
*
dest
SharedMem
<
T
*
>
src
size_t
nbytes
)
{
memcpySafeWhenRacy
(
static_cast
<
void
*
>
(
dest
)
src
.
template
cast
<
void
*
>
(
)
.
unwrap
(
)
nbytes
)
;
}
template
<
typename
T
>
static
void
memmoveSafeWhenRacy
(
SharedMem
<
T
*
>
dest
SharedMem
<
T
*
>
src
size_t
nbytes
)
{
memmoveSafeWhenRacy
(
dest
.
template
cast
<
void
*
>
(
)
.
unwrap
(
)
src
.
template
cast
<
void
*
>
(
)
.
unwrap
(
)
nbytes
)
;
}
template
<
typename
T
>
static
void
podCopySafeWhenRacy
(
SharedMem
<
T
*
>
dest
SharedMem
<
T
*
>
src
size_t
nelem
)
{
memcpySafeWhenRacy
(
dest
src
nelem
*
sizeof
(
T
)
)
;
}
template
<
typename
T
>
static
void
podMoveSafeWhenRacy
(
SharedMem
<
T
*
>
dest
SharedMem
<
T
*
>
src
size_t
nelem
)
{
memmoveSafeWhenRacy
(
dest
src
nelem
*
sizeof
(
T
)
)
;
}
}
;
class
RegionLock
{
public
:
RegionLock
(
)
:
spinlock
(
0
)
{
}
template
<
size_t
nbytes
>
void
acquire
(
void
*
addr
)
;
template
<
size_t
nbytes
>
void
release
(
void
*
addr
)
;
private
:
uint32_t
spinlock
;
}
;
inline
bool
AtomicOperations
:
:
isLockfree
(
int32_t
size
)
{
switch
(
size
)
{
case
1
:
case
2
:
case
4
:
return
true
;
case
8
:
return
false
;
default
:
return
false
;
}
}
}
}
#
if
defined
(
JS_CODEGEN_ARM
)
#
include
"
jit
/
arm
/
AtomicOperations
-
arm
.
h
"
#
elif
defined
(
JS_CODEGEN_ARM64
)
#
include
"
jit
/
arm64
/
AtomicOperations
-
arm64
.
h
"
#
elif
defined
(
JS_CODEGEN_MIPS32
)
|
|
defined
(
JS_CODEGEN_MIPS64
)
#
include
"
jit
/
mips
-
shared
/
AtomicOperations
-
mips
-
shared
.
h
"
#
elif
defined
(
__ppc64__
)
|
|
defined
(
__PPC64_
)
\
|
|
defined
(
__ppc64le__
)
|
|
defined
(
__PPC64LE__
)
\
|
|
defined
(
__ppc__
)
|
|
defined
(
__PPC__
)
#
include
"
jit
/
none
/
AtomicOperations
-
ppc
.
h
"
#
elif
defined
(
JS_CODEGEN_NONE
)
#
include
"
jit
/
none
/
AtomicOperations
-
none
.
h
"
#
elif
defined
(
JS_CODEGEN_X86
)
|
|
defined
(
JS_CODEGEN_X64
)
#
include
"
jit
/
x86
-
shared
/
AtomicOperations
-
x86
-
shared
.
h
"
#
else
#
error
"
Atomic
operations
must
be
defined
for
this
platform
"
#
endif
#
endif
