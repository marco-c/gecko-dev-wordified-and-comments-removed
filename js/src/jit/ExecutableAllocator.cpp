#
include
"
jit
/
ExecutableAllocator
.
h
"
#
include
"
mozilla
/
Atomics
.
h
"
#
include
"
jit
/
JitCompartment
.
h
"
#
include
"
js
/
MemoryMetrics
.
h
"
#
ifdef
__APPLE__
#
include
<
TargetConditionals
.
h
>
#
endif
using
namespace
js
:
:
jit
;
size_t
ExecutableAllocator
:
:
pageSize
=
0
;
size_t
ExecutableAllocator
:
:
largeAllocSize
=
0
;
ExecutablePool
:
:
~
ExecutablePool
(
)
{
MOZ_ASSERT
(
m_ionCodeBytes
=
=
0
)
;
MOZ_ASSERT
(
m_baselineCodeBytes
=
=
0
)
;
MOZ_ASSERT
(
m_regexpCodeBytes
=
=
0
)
;
MOZ_ASSERT
(
m_otherCodeBytes
=
=
0
)
;
MOZ_ASSERT
(
!
isMarked
(
)
)
;
m_allocator
-
>
releasePoolPages
(
this
)
;
}
void
ExecutablePool
:
:
release
(
bool
willDestroy
)
{
MOZ_ASSERT
(
m_refCount
!
=
0
)
;
MOZ_ASSERT_IF
(
willDestroy
m_refCount
=
=
1
)
;
if
(
-
-
m_refCount
=
=
0
)
js_delete
(
this
)
;
}
void
ExecutablePool
:
:
release
(
size_t
n
CodeKind
kind
)
{
switch
(
kind
)
{
case
ION_CODE
:
m_ionCodeBytes
-
=
n
;
MOZ_ASSERT
(
m_ionCodeBytes
<
m_allocation
.
size
)
;
break
;
case
BASELINE_CODE
:
m_baselineCodeBytes
-
=
n
;
MOZ_ASSERT
(
m_baselineCodeBytes
<
m_allocation
.
size
)
;
break
;
case
REGEXP_CODE
:
m_regexpCodeBytes
-
=
n
;
MOZ_ASSERT
(
m_regexpCodeBytes
<
m_allocation
.
size
)
;
break
;
case
OTHER_CODE
:
m_otherCodeBytes
-
=
n
;
MOZ_ASSERT
(
m_otherCodeBytes
<
m_allocation
.
size
)
;
break
;
default
:
MOZ_CRASH
(
"
bad
code
kind
"
)
;
}
release
(
)
;
}
void
ExecutablePool
:
:
addRef
(
)
{
MOZ_ASSERT
(
m_refCount
)
;
+
+
m_refCount
;
MOZ_ASSERT
(
m_refCount
"
refcount
overflow
"
)
;
}
void
*
ExecutablePool
:
:
alloc
(
size_t
n
CodeKind
kind
)
{
MOZ_ASSERT
(
n
<
=
available
(
)
)
;
void
*
result
=
m_freePtr
;
m_freePtr
+
=
n
;
switch
(
kind
)
{
case
ION_CODE
:
m_ionCodeBytes
+
=
n
;
break
;
case
BASELINE_CODE
:
m_baselineCodeBytes
+
=
n
;
break
;
case
REGEXP_CODE
:
m_regexpCodeBytes
+
=
n
;
break
;
case
OTHER_CODE
:
m_otherCodeBytes
+
=
n
;
break
;
default
:
MOZ_CRASH
(
"
bad
code
kind
"
)
;
}
return
result
;
}
size_t
ExecutablePool
:
:
available
(
)
const
{
MOZ_ASSERT
(
m_end
>
=
m_freePtr
)
;
return
m_end
-
m_freePtr
;
}
ExecutableAllocator
:
:
ExecutableAllocator
(
JSRuntime
*
rt
)
:
rt_
(
rt
)
{
MOZ_ASSERT
(
m_smallPools
.
empty
(
)
)
;
}
ExecutableAllocator
:
:
~
ExecutableAllocator
(
)
{
for
(
size_t
i
=
0
;
i
<
m_smallPools
.
length
(
)
;
i
+
+
)
m_smallPools
[
i
]
-
>
release
(
true
)
;
MOZ_ASSERT_IF
(
m_pools
.
initialized
(
)
m_pools
.
empty
(
)
)
;
}
ExecutablePool
*
ExecutableAllocator
:
:
poolForSize
(
size_t
n
)
{
ExecutablePool
*
minPool
=
nullptr
;
for
(
size_t
i
=
0
;
i
<
m_smallPools
.
length
(
)
;
i
+
+
)
{
ExecutablePool
*
pool
=
m_smallPools
[
i
]
;
if
(
n
<
=
pool
-
>
available
(
)
&
&
(
!
minPool
|
|
pool
-
>
available
(
)
<
minPool
-
>
available
(
)
)
)
minPool
=
pool
;
}
if
(
minPool
)
{
minPool
-
>
addRef
(
)
;
return
minPool
;
}
if
(
n
>
largeAllocSize
)
return
createPool
(
n
)
;
ExecutablePool
*
pool
=
createPool
(
largeAllocSize
)
;
if
(
!
pool
)
return
nullptr
;
if
(
m_smallPools
.
length
(
)
<
maxSmallPools
)
{
if
(
m_smallPools
.
append
(
pool
)
)
pool
-
>
addRef
(
)
;
}
else
{
int
iMin
=
0
;
for
(
size_t
i
=
1
;
i
<
m_smallPools
.
length
(
)
;
i
+
+
)
{
if
(
m_smallPools
[
i
]
-
>
available
(
)
<
m_smallPools
[
iMin
]
-
>
available
(
)
)
iMin
=
i
;
}
ExecutablePool
*
minPool
=
m_smallPools
[
iMin
]
;
if
(
(
pool
-
>
available
(
)
-
n
)
>
minPool
-
>
available
(
)
)
{
minPool
-
>
release
(
)
;
m_smallPools
[
iMin
]
=
pool
;
pool
-
>
addRef
(
)
;
}
}
return
pool
;
}
size_t
ExecutableAllocator
:
:
roundUpAllocationSize
(
size_t
request
size_t
granularity
)
{
#
ifdef
_MSC_VER
#
undef
max
#
endif
if
(
(
std
:
:
numeric_limits
<
size_t
>
:
:
max
(
)
-
granularity
)
<
=
request
)
return
OVERSIZE_ALLOCATION
;
size_t
size
=
request
+
(
granularity
-
1
)
;
size
=
size
&
~
(
granularity
-
1
)
;
MOZ_ASSERT
(
size
>
=
request
)
;
return
size
;
}
ExecutablePool
*
ExecutableAllocator
:
:
createPool
(
size_t
n
)
{
MOZ_ASSERT
(
rt_
-
>
jitRuntime
(
)
-
>
preventBackedgePatching
(
)
)
;
size_t
allocSize
=
roundUpAllocationSize
(
n
pageSize
)
;
if
(
allocSize
=
=
OVERSIZE_ALLOCATION
)
return
nullptr
;
if
(
!
m_pools
.
initialized
(
)
&
&
!
m_pools
.
init
(
)
)
return
nullptr
;
ExecutablePool
:
:
Allocation
a
=
systemAlloc
(
allocSize
)
;
if
(
!
a
.
pages
)
return
nullptr
;
ExecutablePool
*
pool
=
js_new
<
ExecutablePool
>
(
this
a
)
;
if
(
!
pool
)
{
systemRelease
(
a
)
;
return
nullptr
;
}
if
(
!
m_pools
.
put
(
pool
)
)
{
js_delete
(
pool
)
;
return
nullptr
;
}
return
pool
;
}
void
*
ExecutableAllocator
:
:
alloc
(
size_t
n
ExecutablePool
*
*
poolp
CodeKind
type
)
{
JitRuntime
:
:
AutoPreventBackedgePatching
apbp
(
rt_
)
;
MOZ_ASSERT
(
roundUpAllocationSize
(
n
sizeof
(
void
*
)
)
=
=
n
)
;
if
(
n
=
=
OVERSIZE_ALLOCATION
)
{
*
poolp
=
nullptr
;
return
nullptr
;
}
*
poolp
=
poolForSize
(
n
)
;
if
(
!
*
poolp
)
return
nullptr
;
void
*
result
=
(
*
poolp
)
-
>
alloc
(
n
type
)
;
MOZ_ASSERT
(
result
)
;
return
result
;
}
void
ExecutableAllocator
:
:
releasePoolPages
(
ExecutablePool
*
pool
)
{
JitRuntime
:
:
AutoPreventBackedgePatching
apbp
(
rt_
)
;
MOZ_ASSERT
(
pool
-
>
m_allocation
.
pages
)
;
systemRelease
(
pool
-
>
m_allocation
)
;
MOZ_ASSERT
(
m_pools
.
initialized
(
)
)
;
if
(
auto
ptr
=
m_pools
.
lookup
(
pool
)
)
m_pools
.
remove
(
ptr
)
;
}
void
ExecutableAllocator
:
:
purge
(
)
{
JitRuntime
:
:
AutoPreventBackedgePatching
apbp
(
rt_
)
;
for
(
size_t
i
=
0
;
i
<
m_smallPools
.
length
(
)
;
i
+
+
)
m_smallPools
[
i
]
-
>
release
(
)
;
m_smallPools
.
clear
(
)
;
}
void
ExecutableAllocator
:
:
initStatic
(
)
{
if
(
!
pageSize
)
{
pageSize
=
determinePageSize
(
)
;
#
if
defined
(
XP_WIN
)
&
&
(
defined
(
_M_X64
)
|
|
defined
(
__x86_64__
)
)
largeAllocSize
=
pageSize
*
15
;
#
else
largeAllocSize
=
pageSize
*
16
;
#
endif
}
}
void
ExecutableAllocator
:
:
addSizeOfCode
(
JS
:
:
CodeSizes
*
sizes
)
const
{
if
(
m_pools
.
initialized
(
)
)
{
for
(
ExecPoolHashSet
:
:
Range
r
=
m_pools
.
all
(
)
;
!
r
.
empty
(
)
;
r
.
popFront
(
)
)
{
ExecutablePool
*
pool
=
r
.
front
(
)
;
sizes
-
>
ion
+
=
pool
-
>
m_ionCodeBytes
;
sizes
-
>
baseline
+
=
pool
-
>
m_baselineCodeBytes
;
sizes
-
>
regexp
+
=
pool
-
>
m_regexpCodeBytes
;
sizes
-
>
other
+
=
pool
-
>
m_otherCodeBytes
;
sizes
-
>
unused
+
=
pool
-
>
m_allocation
.
size
-
pool
-
>
m_ionCodeBytes
-
pool
-
>
m_baselineCodeBytes
-
pool
-
>
m_regexpCodeBytes
-
pool
-
>
m_otherCodeBytes
;
}
}
}
void
ExecutableAllocator
:
:
reprotectAll
(
ProtectionSetting
protection
)
{
#
ifdef
NON_WRITABLE_JIT_CODE
if
(
!
m_pools
.
initialized
(
)
)
return
;
for
(
ExecPoolHashSet
:
:
Range
r
=
m_pools
.
all
(
)
;
!
r
.
empty
(
)
;
r
.
popFront
(
)
)
reprotectPool
(
rt_
r
.
front
(
)
protection
)
;
#
endif
}
void
ExecutableAllocator
:
:
reprotectPool
(
JSRuntime
*
rt
ExecutablePool
*
pool
ProtectionSetting
protection
)
{
#
ifdef
NON_WRITABLE_JIT_CODE
MOZ_ASSERT
(
rt
-
>
jitRuntime
(
)
-
>
preventBackedgePatching
(
)
|
|
rt
-
>
unsafeContextFromAnyThread
(
)
-
>
handlingJitInterrupt
(
)
)
;
char
*
start
=
pool
-
>
m_allocation
.
pages
;
if
(
!
reprotectRegion
(
start
pool
-
>
m_freePtr
-
start
protection
)
)
MOZ_CRASH
(
)
;
#
endif
}
void
ExecutableAllocator
:
:
poisonCode
(
JSRuntime
*
rt
JitPoisonRangeVector
&
ranges
)
{
MOZ_ASSERT
(
CurrentThreadCanAccessRuntime
(
rt
)
)
;
JitRuntime
:
:
AutoPreventBackedgePatching
apbp
(
rt
)
;
#
ifdef
DEBUG
for
(
size_t
i
=
0
;
i
<
ranges
.
length
(
)
;
i
+
+
)
MOZ_ASSERT
(
!
ranges
[
i
]
.
pool
-
>
isMarked
(
)
)
;
#
endif
for
(
size_t
i
=
0
;
i
<
ranges
.
length
(
)
;
i
+
+
)
{
ExecutablePool
*
pool
=
ranges
[
i
]
.
pool
;
if
(
pool
-
>
m_refCount
=
=
1
)
{
continue
;
}
MOZ_ASSERT
(
pool
-
>
m_refCount
>
1
)
;
if
(
!
pool
-
>
isMarked
(
)
)
{
reprotectPool
(
rt
pool
Writable
)
;
pool
-
>
mark
(
)
;
}
memset
(
ranges
[
i
]
.
start
JS_SWEPT_CODE_PATTERN
ranges
[
i
]
.
size
)
;
}
for
(
size_t
i
=
0
;
i
<
ranges
.
length
(
)
;
i
+
+
)
{
ExecutablePool
*
pool
=
ranges
[
i
]
.
pool
;
if
(
pool
-
>
isMarked
(
)
)
{
reprotectPool
(
rt
pool
Executable
)
;
pool
-
>
unmark
(
)
;
}
pool
-
>
release
(
)
;
}
}
#
if
JS_BITS_PER_WORD
=
=
32
static
const
size_t
MaxCodeBytesPerProcess
=
160
*
1024
*
1024
;
#
else
static
const
size_t
MaxCodeBytesPerProcess
=
640
*
1024
*
1024
;
#
endif
static
mozilla
:
:
Atomic
<
size_t
>
allocatedExecutableBytes
(
0
)
;
bool
js
:
:
jit
:
:
AddAllocatedExecutableBytes
(
size_t
bytes
)
{
MOZ_ASSERT
(
allocatedExecutableBytes
<
=
MaxCodeBytesPerProcess
)
;
while
(
true
)
{
size_t
bytesOld
=
allocatedExecutableBytes
;
size_t
bytesNew
=
bytesOld
+
bytes
;
if
(
bytesNew
>
MaxCodeBytesPerProcess
)
return
false
;
if
(
allocatedExecutableBytes
.
compareExchange
(
bytesOld
bytesNew
)
)
return
true
;
}
MOZ_CRASH
(
)
;
}
void
js
:
:
jit
:
:
SubAllocatedExecutableBytes
(
size_t
bytes
)
{
MOZ_ASSERT
(
bytes
<
=
allocatedExecutableBytes
)
;
allocatedExecutableBytes
-
=
bytes
;
}
void
js
:
:
jit
:
:
AssertAllocatedExecutableBytesIsZero
(
)
{
MOZ_ASSERT
(
allocatedExecutableBytes
=
=
0
)
;
}
bool
js
:
:
jit
:
:
CanLikelyAllocateMoreExecutableMemory
(
)
{
static
const
size_t
BufferSize
=
16
*
1024
*
1024
;
MOZ_ASSERT
(
allocatedExecutableBytes
<
=
MaxCodeBytesPerProcess
)
;
return
allocatedExecutableBytes
+
BufferSize
<
=
MaxCodeBytesPerProcess
;
}
