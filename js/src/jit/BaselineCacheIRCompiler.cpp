#
include
"
jit
/
BaselineCacheIRCompiler
.
h
"
#
include
"
jit
/
CacheIR
.
h
"
#
include
"
jit
/
Linker
.
h
"
#
include
"
jit
/
SharedICHelpers
.
h
"
#
include
"
jit
/
VMFunctions
.
h
"
#
include
"
proxy
/
DeadObjectProxy
.
h
"
#
include
"
proxy
/
Proxy
.
h
"
#
include
"
jit
/
MacroAssembler
-
inl
.
h
"
#
include
"
jit
/
SharedICHelpers
-
inl
.
h
"
#
include
"
jit
/
VMFunctionList
-
inl
.
h
"
#
include
"
vm
/
JSContext
-
inl
.
h
"
#
include
"
vm
/
Realm
-
inl
.
h
"
using
namespace
js
;
using
namespace
js
:
:
jit
;
using
mozilla
:
:
Maybe
;
namespace
js
{
namespace
jit
{
class
AutoStubFrame
;
Address
CacheRegisterAllocator
:
:
addressOf
(
MacroAssembler
&
masm
BaselineFrameSlot
slot
)
const
{
uint32_t
offset
=
stackPushed_
+
ICStackValueOffset
+
slot
.
slot
(
)
*
sizeof
(
JS
:
:
Value
)
;
return
Address
(
masm
.
getStackPointer
(
)
offset
)
;
}
BaseValueIndex
CacheRegisterAllocator
:
:
addressOf
(
MacroAssembler
&
masm
Register
argcReg
BaselineFrameSlot
slot
)
const
{
uint32_t
offset
=
stackPushed_
+
ICStackValueOffset
+
slot
.
slot
(
)
*
sizeof
(
JS
:
:
Value
)
;
return
BaseValueIndex
(
masm
.
getStackPointer
(
)
argcReg
offset
)
;
}
BaselineCacheIRCompiler
:
:
BaselineCacheIRCompiler
(
JSContext
*
cx
const
CacheIRWriter
&
writer
uint32_t
stubDataOffset
BaselineCacheIRStubKind
stubKind
)
:
CacheIRCompiler
(
cx
writer
stubDataOffset
Mode
:
:
Baseline
StubFieldPolicy
:
:
Address
)
makesGCCalls_
(
false
)
kind_
(
stubKind
)
{
}
#
define
DEFINE_SHARED_OP
(
op
)
\
bool
BaselineCacheIRCompiler
:
:
emit
#
#
op
(
)
{
\
return
CacheIRCompiler
:
:
emit
#
#
op
(
)
;
\
}
CACHE_IR_SHARED_OPS
(
DEFINE_SHARED_OP
)
#
undef
DEFINE_SHARED_OP
AutoStubFrame
:
:
AutoStubFrame
(
BaselineCacheIRCompiler
&
compiler
)
:
compiler
(
compiler
)
#
ifdef
DEBUG
framePushedAtEnterStubFrame_
(
0
)
#
endif
{
}
void
AutoStubFrame
:
:
enter
(
MacroAssembler
&
masm
Register
scratch
CallCanGC
canGC
)
{
MOZ_ASSERT
(
compiler
.
allocator
.
stackPushed
(
)
=
=
0
)
;
EmitBaselineEnterStubFrame
(
masm
scratch
)
;
#
ifdef
DEBUG
framePushedAtEnterStubFrame_
=
masm
.
framePushed
(
)
;
#
endif
MOZ_ASSERT
(
!
compiler
.
preparedForVMCall_
)
;
compiler
.
preparedForVMCall_
=
true
;
if
(
canGC
=
=
CallCanGC
:
:
CanGC
)
{
compiler
.
makesGCCalls_
=
true
;
}
}
void
AutoStubFrame
:
:
leave
(
MacroAssembler
&
masm
bool
calledIntoIon
)
{
MOZ_ASSERT
(
compiler
.
preparedForVMCall_
)
;
compiler
.
preparedForVMCall_
=
false
;
#
ifdef
DEBUG
masm
.
setFramePushed
(
framePushedAtEnterStubFrame_
)
;
if
(
calledIntoIon
)
{
masm
.
adjustFrame
(
sizeof
(
intptr_t
)
)
;
}
#
endif
EmitBaselineLeaveStubFrame
(
masm
calledIntoIon
)
;
}
#
ifdef
DEBUG
AutoStubFrame
:
:
~
AutoStubFrame
(
)
{
MOZ_ASSERT
(
!
compiler
.
preparedForVMCall_
)
;
}
#
endif
}
}
bool
BaselineCacheIRCompiler
:
:
makesGCCalls
(
)
const
{
return
makesGCCalls_
;
}
Address
BaselineCacheIRCompiler
:
:
stubAddress
(
uint32_t
offset
)
const
{
return
Address
(
ICStubReg
stubDataOffset_
+
offset
)
;
}
template
<
typename
Fn
Fn
fn
>
void
BaselineCacheIRCompiler
:
:
callVM
(
MacroAssembler
&
masm
)
{
VMFunctionId
id
=
VMFunctionToId
<
Fn
fn
>
:
:
id
;
callVMInternal
(
masm
id
)
;
}
template
<
typename
Fn
Fn
fn
>
void
BaselineCacheIRCompiler
:
:
tailCallVM
(
MacroAssembler
&
masm
)
{
TailCallVMFunctionId
id
=
TailCallVMFunctionToId
<
Fn
fn
>
:
:
id
;
tailCallVMInternal
(
masm
id
)
;
}
void
BaselineCacheIRCompiler
:
:
tailCallVMInternal
(
MacroAssembler
&
masm
TailCallVMFunctionId
id
)
{
MOZ_ASSERT
(
!
preparedForVMCall_
)
;
TrampolinePtr
code
=
cx_
-
>
runtime
(
)
-
>
jitRuntime
(
)
-
>
getVMWrapper
(
id
)
;
const
VMFunctionData
&
fun
=
GetVMFunction
(
id
)
;
MOZ_ASSERT
(
fun
.
expectTailCall
=
=
TailCall
)
;
size_t
argSize
=
fun
.
explicitStackSlots
(
)
*
sizeof
(
void
*
)
;
EmitBaselineTailCallVM
(
code
masm
argSize
)
;
}
static
size_t
GetEnteredOffset
(
BaselineCacheIRStubKind
kind
)
{
switch
(
kind
)
{
case
BaselineCacheIRStubKind
:
:
Regular
:
return
ICCacheIR_Regular
:
:
offsetOfEnteredCount
(
)
;
case
BaselineCacheIRStubKind
:
:
Updated
:
return
ICCacheIR_Updated
:
:
offsetOfEnteredCount
(
)
;
case
BaselineCacheIRStubKind
:
:
Monitored
:
return
ICCacheIR_Monitored
:
:
offsetOfEnteredCount
(
)
;
}
MOZ_CRASH
(
"
unhandled
BaselineCacheIRStubKind
"
)
;
}
JitCode
*
BaselineCacheIRCompiler
:
:
compile
(
)
{
#
ifndef
JS_USE_LINK_REGISTER
masm
.
adjustFrame
(
sizeof
(
intptr_t
)
)
;
#
endif
#
ifdef
JS_CODEGEN_ARM
masm
.
setSecondScratchReg
(
BaselineSecondScratchReg
)
;
#
endif
Address
enteredCount
(
ICStubReg
GetEnteredOffset
(
kind_
)
)
;
masm
.
add32
(
Imm32
(
1
)
enteredCount
)
;
do
{
switch
(
reader
.
readOp
(
)
)
{
#
define
DEFINE_OP
(
op
.
.
.
)
\
case
CacheOp
:
:
op
:
\
if
(
!
emit
#
#
op
(
)
)
return
nullptr
;
\
break
;
CACHE_IR_OPS
(
DEFINE_OP
)
#
undef
DEFINE_OP
default
:
MOZ_CRASH
(
"
Invalid
op
"
)
;
}
#
ifdef
DEBUG
assertAllArgumentsConsumed
(
)
;
#
endif
allocator
.
nextOp
(
)
;
}
while
(
reader
.
more
(
)
)
;
MOZ_ASSERT
(
!
preparedForVMCall_
)
;
masm
.
assumeUnreachable
(
"
Should
have
returned
from
IC
"
)
;
for
(
size_t
i
=
0
;
i
<
failurePaths
.
length
(
)
;
i
+
+
)
{
if
(
!
emitFailurePath
(
i
)
)
{
return
nullptr
;
}
EmitStubGuardFailure
(
masm
)
;
}
Linker
linker
(
masm
)
;
Rooted
<
JitCode
*
>
newStubCode
(
cx_
linker
.
newCode
(
cx_
CodeKind
:
:
Baseline
)
)
;
if
(
!
newStubCode
)
{
cx_
-
>
recoverFromOutOfMemory
(
)
;
return
nullptr
;
}
return
newStubCode
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardShape
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ObjOperandId
objId
=
reader
.
objOperandId
(
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch1
(
allocator
masm
)
;
bool
needSpectreMitigations
=
objectGuardNeedsSpectreMitigations
(
objId
)
;
Maybe
<
AutoScratchRegister
>
maybeScratch2
;
if
(
needSpectreMitigations
)
{
maybeScratch2
.
emplace
(
allocator
masm
)
;
}
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
addr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
masm
.
loadPtr
(
addr
scratch1
)
;
if
(
needSpectreMitigations
)
{
masm
.
branchTestObjShape
(
Assembler
:
:
NotEqual
obj
scratch1
*
maybeScratch2
obj
failure
-
>
label
(
)
)
;
}
else
{
masm
.
branchTestObjShapeNoSpectreMitigations
(
Assembler
:
:
NotEqual
obj
scratch1
failure
-
>
label
(
)
)
;
}
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardGroup
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ObjOperandId
objId
=
reader
.
objOperandId
(
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch1
(
allocator
masm
)
;
bool
needSpectreMitigations
=
objectGuardNeedsSpectreMitigations
(
objId
)
;
Maybe
<
AutoScratchRegister
>
maybeScratch2
;
if
(
needSpectreMitigations
)
{
maybeScratch2
.
emplace
(
allocator
masm
)
;
}
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
addr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
masm
.
loadPtr
(
addr
scratch1
)
;
if
(
needSpectreMitigations
)
{
masm
.
branchTestObjGroup
(
Assembler
:
:
NotEqual
obj
scratch1
*
maybeScratch2
obj
failure
-
>
label
(
)
)
;
}
else
{
masm
.
branchTestObjGroupNoSpectreMitigations
(
Assembler
:
:
NotEqual
obj
scratch1
failure
-
>
label
(
)
)
;
}
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardProto
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
addr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
masm
.
loadObjProto
(
obj
scratch
)
;
masm
.
branchPtr
(
Assembler
:
:
NotEqual
addr
scratch
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardCompartment
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
globalWrapper
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
masm
.
loadPtr
(
globalWrapper
scratch
)
;
Address
handlerAddr
(
scratch
ProxyObject
:
:
offsetOfHandler
(
)
)
;
masm
.
branchPtr
(
Assembler
:
:
Equal
handlerAddr
ImmPtr
(
&
DeadObjectProxy
:
:
singleton
)
failure
-
>
label
(
)
)
;
Address
addr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
masm
.
branchTestObjCompartment
(
Assembler
:
:
NotEqual
obj
addr
scratch
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardAnyClass
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ObjOperandId
objId
=
reader
.
objOperandId
(
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
testAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
if
(
objectGuardNeedsSpectreMitigations
(
objId
)
)
{
masm
.
branchTestObjClass
(
Assembler
:
:
NotEqual
obj
testAddr
scratch
obj
failure
-
>
label
(
)
)
;
}
else
{
masm
.
branchTestObjClassNoSpectreMitigations
(
Assembler
:
:
NotEqual
obj
testAddr
scratch
failure
-
>
label
(
)
)
;
}
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardHasProxyHandler
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
testAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
masm
.
loadPtr
(
testAddr
scratch
)
;
Address
handlerAddr
(
obj
ProxyObject
:
:
offsetOfHandler
(
)
)
;
masm
.
branchPtr
(
Assembler
:
:
NotEqual
handlerAddr
scratch
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardSpecificObject
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
addr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
masm
.
branchPtr
(
Assembler
:
:
NotEqual
addr
obj
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardSpecificAtom
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
str
=
allocator
.
useRegister
(
masm
reader
.
stringOperandId
(
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
atomAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
Label
done
;
masm
.
branchPtr
(
Assembler
:
:
Equal
atomAddr
str
&
done
)
;
masm
.
branchTest32
(
Assembler
:
:
Zero
Address
(
str
JSString
:
:
offsetOfFlags
(
)
)
Imm32
(
JSString
:
:
NON_ATOM_BIT
)
failure
-
>
label
(
)
)
;
masm
.
loadPtr
(
atomAddr
scratch
)
;
masm
.
loadStringLength
(
scratch
scratch
)
;
masm
.
branch32
(
Assembler
:
:
NotEqual
Address
(
str
JSString
:
:
offsetOfLength
(
)
)
scratch
failure
-
>
label
(
)
)
;
LiveRegisterSet
volatileRegs
(
GeneralRegisterSet
:
:
Volatile
(
)
liveVolatileFloatRegs
(
)
)
;
masm
.
PushRegsInMask
(
volatileRegs
)
;
masm
.
setupUnalignedABICall
(
scratch
)
;
masm
.
loadPtr
(
atomAddr
scratch
)
;
masm
.
passABIArg
(
scratch
)
;
masm
.
passABIArg
(
str
)
;
masm
.
callWithABI
(
JS_FUNC_TO_DATA_PTR
(
void
*
EqualStringsHelperPure
)
)
;
masm
.
mov
(
ReturnReg
scratch
)
;
LiveRegisterSet
ignore
;
ignore
.
add
(
scratch
)
;
masm
.
PopRegsInMaskIgnore
(
volatileRegs
ignore
)
;
masm
.
branchIfFalseBool
(
scratch
failure
-
>
label
(
)
)
;
masm
.
bind
(
&
done
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardSpecificSymbol
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
sym
=
allocator
.
useRegister
(
masm
reader
.
symbolOperandId
(
)
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
addr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
masm
.
branchPtr
(
Assembler
:
:
NotEqual
addr
sym
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadValueResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
masm
.
loadValue
(
stubAddress
(
reader
.
stubOffset
(
)
)
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadFixedSlotResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
masm
.
load32
(
stubAddress
(
reader
.
stubOffset
(
)
)
scratch
)
;
masm
.
loadValue
(
BaseIndex
(
obj
scratch
TimesOne
)
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadDynamicSlotResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
masm
.
load32
(
stubAddress
(
reader
.
stubOffset
(
)
)
scratch
)
;
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfSlots
(
)
)
scratch2
)
;
masm
.
loadValue
(
BaseIndex
(
scratch2
scratch
TimesOne
)
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardHasGetterSetter
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
Address
shapeAddr
=
stubAddress
(
reader
.
stubOffset
(
)
)
;
AutoScratchRegister
scratch1
(
allocator
masm
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
LiveRegisterSet
volatileRegs
(
GeneralRegisterSet
:
:
Volatile
(
)
liveVolatileFloatRegs
(
)
)
;
volatileRegs
.
takeUnchecked
(
scratch1
)
;
volatileRegs
.
takeUnchecked
(
scratch2
)
;
masm
.
PushRegsInMask
(
volatileRegs
)
;
masm
.
setupUnalignedABICall
(
scratch1
)
;
masm
.
loadJSContext
(
scratch1
)
;
masm
.
passABIArg
(
scratch1
)
;
masm
.
passABIArg
(
obj
)
;
masm
.
loadPtr
(
shapeAddr
scratch2
)
;
masm
.
passABIArg
(
scratch2
)
;
masm
.
callWithABI
(
JS_FUNC_TO_DATA_PTR
(
void
*
ObjectHasGetterSetterPure
)
)
;
masm
.
mov
(
ReturnReg
scratch1
)
;
masm
.
PopRegsInMask
(
volatileRegs
)
;
masm
.
branchIfFalseBool
(
scratch1
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallScriptedGetterResultShared
(
TypedOrValueRegister
receiver
)
{
Address
getterAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
bool
isSameRealm
=
reader
.
readBool
(
)
;
AutoScratchRegister
code
(
allocator
masm
)
;
AutoScratchRegister
callee
(
allocator
masm
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
masm
.
loadPtr
(
getterAddr
callee
)
;
masm
.
loadJitCodeRaw
(
callee
code
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
if
(
!
isSameRealm
)
{
masm
.
switchToObjectRealm
(
callee
scratch
)
;
}
masm
.
alignJitStackBasedOnNArgs
(
0
)
;
masm
.
Push
(
receiver
)
;
EmitBaselineCreateStubFrameDescriptor
(
masm
scratch
JitFrameLayout
:
:
Size
(
)
)
;
masm
.
Push
(
Imm32
(
0
)
)
;
masm
.
Push
(
callee
)
;
masm
.
Push
(
scratch
)
;
Label
noUnderflow
;
masm
.
load16ZeroExtend
(
Address
(
callee
JSFunction
:
:
offsetOfNargs
(
)
)
callee
)
;
masm
.
branch32
(
Assembler
:
:
Equal
callee
Imm32
(
0
)
&
noUnderflow
)
;
{
TrampolinePtr
argumentsRectifier
=
cx_
-
>
runtime
(
)
-
>
jitRuntime
(
)
-
>
getArgumentsRectifier
(
)
;
masm
.
movePtr
(
argumentsRectifier
code
)
;
}
masm
.
bind
(
&
noUnderflow
)
;
masm
.
callJit
(
code
)
;
stubFrame
.
leave
(
masm
true
)
;
if
(
!
isSameRealm
)
{
masm
.
switchToBaselineFrameRealm
(
R1
.
scratchReg
(
)
)
;
}
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallScriptedGetterResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
return
emitCallScriptedGetterResultShared
(
TypedOrValueRegister
(
MIRType
:
:
Object
AnyRegister
(
obj
)
)
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallScriptedGetterByValueResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
return
emitCallScriptedGetterResultShared
(
val
)
;
}
template
<
typename
T
typename
CallVM
>
bool
BaselineCacheIRCompiler
:
:
emitCallNativeGetterResultShared
(
T
receiver
const
CallVM
&
emitCallVM
)
{
Address
getterAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
loadPtr
(
getterAddr
scratch
)
;
masm
.
Push
(
receiver
)
;
masm
.
Push
(
scratch
)
;
emitCallVM
(
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallNativeGetterResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
return
emitCallNativeGetterResultShared
(
obj
[
this
]
(
)
{
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleFunction
HandleObject
MutableHandleValue
)
;
callVM
<
Fn
CallNativeGetter
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallNativeGetterByValueResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
return
emitCallNativeGetterResultShared
(
val
[
this
]
(
)
{
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleFunction
HandleValue
MutableHandleValue
)
;
callVM
<
Fn
CallNativeGetterByValue
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallProxyGetResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
Address
idAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
loadPtr
(
idAddr
scratch
)
;
masm
.
Push
(
scratch
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleObject
HandleId
MutableHandleValue
)
;
callVM
<
Fn
ProxyGetProperty
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardFrameHasNoArgumentsObject
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
branchTest32
(
Assembler
:
:
NonZero
Address
(
BaselineFrameReg
BaselineFrame
:
:
reverseOffsetOfFlags
(
)
)
Imm32
(
BaselineFrame
:
:
HAS_ARGS_OBJ
)
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadFrameCalleeResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
Address
callee
(
BaselineFrameReg
BaselineFrame
:
:
offsetOfCalleeToken
(
)
)
;
masm
.
loadFunctionFromCalleeToken
(
callee
scratch
)
;
masm
.
tagValue
(
JSVAL_TYPE_OBJECT
scratch
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadFrameNumActualArgsResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
Address
actualArgs
(
BaselineFrameReg
BaselineFrame
:
:
offsetOfNumActualArgs
(
)
)
;
masm
.
loadPtr
(
actualArgs
scratch
)
;
masm
.
tagValue
(
JSVAL_TYPE_INT32
scratch
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadFrameArgumentResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
index
=
allocator
.
useRegister
(
masm
reader
.
int32OperandId
(
)
)
;
AutoScratchRegister
scratch1
(
allocator
masm
)
;
AutoScratchRegisterMaybeOutput
scratch2
(
allocator
masm
output
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
BaselineFrameReg
BaselineFrame
:
:
offsetOfNumActualArgs
(
)
)
scratch1
)
;
masm
.
spectreBoundsCheck32
(
index
scratch1
scratch2
failure
-
>
label
(
)
)
;
masm
.
loadValue
(
BaseValueIndex
(
BaselineFrameReg
index
BaselineFrame
:
:
offsetOfArg
(
0
)
)
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadEnvironmentFixedSlotResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
load32
(
stubAddress
(
reader
.
stubOffset
(
)
)
scratch
)
;
BaseIndex
slot
(
obj
scratch
TimesOne
)
;
masm
.
branchTestMagic
(
Assembler
:
:
Equal
slot
failure
-
>
label
(
)
)
;
masm
.
loadValue
(
slot
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadEnvironmentDynamicSlotResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
AutoScratchRegisterMaybeOutput
scratch2
(
allocator
masm
output
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
load32
(
stubAddress
(
reader
.
stubOffset
(
)
)
scratch
)
;
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfSlots
(
)
)
scratch2
)
;
BaseIndex
slot
(
scratch2
scratch
TimesOne
)
;
masm
.
branchTestMagic
(
Assembler
:
:
Equal
slot
failure
-
>
label
(
)
)
;
masm
.
loadValue
(
slot
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadStringResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
masm
.
loadPtr
(
stubAddress
(
reader
.
stubOffset
(
)
)
scratch
)
;
masm
.
tagValue
(
JSVAL_TYPE_STRING
scratch
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCompareStringResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
left
=
allocator
.
useRegister
(
masm
reader
.
stringOperandId
(
)
)
;
Register
right
=
allocator
.
useRegister
(
masm
reader
.
stringOperandId
(
)
)
;
JSOp
op
=
reader
.
jsop
(
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
allocator
.
discardStack
(
masm
)
;
Label
slow
done
;
masm
.
compareStrings
(
op
left
right
scratch
&
slow
)
;
masm
.
jump
(
&
done
)
;
masm
.
bind
(
&
slow
)
;
{
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
if
(
op
=
=
JSOP_LE
|
|
op
=
=
JSOP_GT
)
{
masm
.
Push
(
left
)
;
masm
.
Push
(
right
)
;
}
else
{
masm
.
Push
(
right
)
;
masm
.
Push
(
left
)
;
}
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleString
HandleString
bool
*
)
;
if
(
op
=
=
JSOP_EQ
|
|
op
=
=
JSOP_STRICTEQ
)
{
callVM
<
Fn
jit
:
:
StringsEqual
<
EqualityKind
:
:
Equal
>
>
(
masm
)
;
}
else
if
(
op
=
=
JSOP_NE
|
|
op
=
=
JSOP_STRICTNE
)
{
callVM
<
Fn
jit
:
:
StringsEqual
<
EqualityKind
:
:
NotEqual
>
>
(
masm
)
;
}
else
if
(
op
=
=
JSOP_LT
|
|
op
=
=
JSOP_GT
)
{
callVM
<
Fn
jit
:
:
StringsCompare
<
ComparisonKind
:
:
LessThan
>
>
(
masm
)
;
}
else
{
MOZ_ASSERT
(
op
=
=
JSOP_LE
|
|
op
=
=
JSOP_GE
)
;
callVM
<
Fn
jit
:
:
StringsCompare
<
ComparisonKind
:
:
GreaterThanOrEqual
>
>
(
masm
)
;
}
stubFrame
.
leave
(
masm
)
;
masm
.
mov
(
ReturnReg
scratch
)
;
}
masm
.
bind
(
&
done
)
;
masm
.
tagValue
(
JSVAL_TYPE_BOOLEAN
scratch
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
callTypeUpdateIC
(
Register
obj
ValueOperand
val
Register
scratch
LiveGeneralRegisterSet
saveRegs
)
{
allocator
.
discardStack
(
masm
)
;
MOZ_ASSERT
(
val
=
=
R0
)
;
MOZ_ASSERT
(
scratch
=
=
R1
.
scratchReg
(
)
)
;
#
if
defined
(
JS_CODEGEN_X86
)
|
|
defined
(
JS_CODEGEN_X64
)
static
const
bool
CallClobbersTailReg
=
false
;
#
else
static
const
bool
CallClobbersTailReg
=
true
;
#
endif
if
(
CallClobbersTailReg
)
{
masm
.
push
(
ICTailCallReg
)
;
}
masm
.
push
(
ICStubReg
)
;
masm
.
loadPtr
(
Address
(
ICStubReg
ICCacheIR_Updated
:
:
offsetOfFirstUpdateStub
(
)
)
ICStubReg
)
;
masm
.
call
(
Address
(
ICStubReg
ICStub
:
:
offsetOfStubCode
(
)
)
)
;
masm
.
pop
(
ICStubReg
)
;
if
(
CallClobbersTailReg
)
{
masm
.
pop
(
ICTailCallReg
)
;
}
Label
done
;
masm
.
branch32
(
Assembler
:
:
Equal
scratch
Imm32
(
1
)
&
done
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
CallCanGC
:
:
CanNotGC
)
;
masm
.
PushRegsInMask
(
saveRegs
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
TypedOrValueRegister
(
MIRType
:
:
Object
AnyRegister
(
obj
)
)
)
;
masm
.
Push
(
ICStubReg
)
;
masm
.
loadPtr
(
Address
(
BaselineFrameReg
0
)
scratch
)
;
masm
.
pushBaselineFramePtr
(
scratch
scratch
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
BaselineFrame
*
ICCacheIR_Updated
*
HandleValue
HandleValue
)
;
callVM
<
Fn
DoTypeUpdateFallback
>
(
masm
)
;
masm
.
PopRegsInMask
(
saveRegs
)
;
stubFrame
.
leave
(
masm
)
;
masm
.
bind
(
&
done
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitStoreSlotShared
(
bool
isFixed
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ObjOperandId
objId
=
reader
.
objOperandId
(
)
;
Address
offsetAddr
=
stubAddress
(
reader
.
stubOffset
(
)
)
;
AutoScratchRegister
scratch1
(
allocator
masm
R1
.
scratchReg
(
)
)
;
ValueOperand
val
=
allocator
.
useFixedValueRegister
(
masm
reader
.
valOperandId
(
)
R0
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
Maybe
<
AutoScratchRegister
>
scratch2
;
if
(
!
isFixed
)
{
scratch2
.
emplace
(
allocator
masm
)
;
}
LiveGeneralRegisterSet
saveRegs
;
saveRegs
.
add
(
obj
)
;
saveRegs
.
add
(
val
)
;
if
(
!
callTypeUpdateIC
(
obj
val
scratch1
saveRegs
)
)
{
return
false
;
}
masm
.
load32
(
offsetAddr
scratch1
)
;
if
(
isFixed
)
{
BaseIndex
slot
(
obj
scratch1
TimesOne
)
;
EmitPreBarrier
(
masm
slot
MIRType
:
:
Value
)
;
masm
.
storeValue
(
val
slot
)
;
}
else
{
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfSlots
(
)
)
scratch2
.
ref
(
)
)
;
BaseIndex
slot
(
scratch2
.
ref
(
)
scratch1
TimesOne
)
;
EmitPreBarrier
(
masm
slot
MIRType
:
:
Value
)
;
masm
.
storeValue
(
val
slot
)
;
}
emitPostBarrierSlot
(
obj
val
scratch1
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitStoreFixedSlot
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitStoreSlotShared
(
true
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitStoreDynamicSlot
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitStoreSlotShared
(
false
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitAddAndStoreSlotShared
(
CacheOp
op
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ObjOperandId
objId
=
reader
.
objOperandId
(
)
;
Address
offsetAddr
=
stubAddress
(
reader
.
stubOffset
(
)
)
;
AutoScratchRegister
scratch1
(
allocator
masm
R1
.
scratchReg
(
)
)
;
ValueOperand
val
=
allocator
.
useFixedValueRegister
(
masm
reader
.
valOperandId
(
)
R0
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
bool
changeGroup
=
reader
.
readBool
(
)
;
Address
newGroupAddr
=
stubAddress
(
reader
.
stubOffset
(
)
)
;
Address
newShapeAddr
=
stubAddress
(
reader
.
stubOffset
(
)
)
;
if
(
op
=
=
CacheOp
:
:
AllocateAndStoreDynamicSlot
)
{
Address
numNewSlotsAddr
=
stubAddress
(
reader
.
stubOffset
(
)
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
LiveRegisterSet
save
(
GeneralRegisterSet
:
:
Volatile
(
)
liveVolatileFloatRegs
(
)
)
;
masm
.
PushRegsInMask
(
save
)
;
masm
.
setupUnalignedABICall
(
scratch1
)
;
masm
.
loadJSContext
(
scratch1
)
;
masm
.
passABIArg
(
scratch1
)
;
masm
.
passABIArg
(
obj
)
;
masm
.
load32
(
numNewSlotsAddr
scratch2
)
;
masm
.
passABIArg
(
scratch2
)
;
masm
.
callWithABI
(
JS_FUNC_TO_DATA_PTR
(
void
*
NativeObject
:
:
growSlotsPure
)
)
;
masm
.
mov
(
ReturnReg
scratch1
)
;
LiveRegisterSet
ignore
;
ignore
.
add
(
scratch1
)
;
masm
.
PopRegsInMaskIgnore
(
save
ignore
)
;
masm
.
branchIfFalseBool
(
scratch1
failure
-
>
label
(
)
)
;
}
LiveGeneralRegisterSet
saveRegs
;
saveRegs
.
add
(
obj
)
;
saveRegs
.
add
(
val
)
;
if
(
!
callTypeUpdateIC
(
obj
val
scratch1
saveRegs
)
)
{
return
false
;
}
if
(
changeGroup
)
{
Label
noGroupChange
;
masm
.
branchIfObjGroupHasNoAddendum
(
obj
scratch1
&
noGroupChange
)
;
masm
.
loadPtr
(
newGroupAddr
scratch1
)
;
masm
.
storeObjGroup
(
scratch1
obj
[
]
(
MacroAssembler
&
masm
const
Address
&
addr
)
{
EmitPreBarrier
(
masm
addr
MIRType
:
:
ObjectGroup
)
;
}
)
;
masm
.
bind
(
&
noGroupChange
)
;
}
masm
.
loadPtr
(
newShapeAddr
scratch1
)
;
masm
.
storeObjShape
(
scratch1
obj
[
]
(
MacroAssembler
&
masm
const
Address
&
addr
)
{
EmitPreBarrier
(
masm
addr
MIRType
:
:
Shape
)
;
}
)
;
masm
.
load32
(
offsetAddr
scratch1
)
;
if
(
op
=
=
CacheOp
:
:
AddAndStoreFixedSlot
)
{
BaseIndex
slot
(
obj
scratch1
TimesOne
)
;
masm
.
storeValue
(
val
slot
)
;
}
else
{
MOZ_ASSERT
(
op
=
=
CacheOp
:
:
AddAndStoreDynamicSlot
|
|
op
=
=
CacheOp
:
:
AllocateAndStoreDynamicSlot
)
;
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfSlots
(
)
)
scratch2
)
;
BaseIndex
slot
(
scratch2
scratch1
TimesOne
)
;
masm
.
storeValue
(
val
slot
)
;
}
emitPostBarrierSlot
(
obj
val
scratch1
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitAddAndStoreFixedSlot
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitAddAndStoreSlotShared
(
CacheOp
:
:
AddAndStoreFixedSlot
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitAddAndStoreDynamicSlot
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitAddAndStoreSlotShared
(
CacheOp
:
:
AddAndStoreDynamicSlot
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitAllocateAndStoreDynamicSlot
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitAddAndStoreSlotShared
(
CacheOp
:
:
AllocateAndStoreDynamicSlot
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitStoreTypedObjectReferenceProperty
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ObjOperandId
objId
=
reader
.
objOperandId
(
)
;
Address
offsetAddr
=
stubAddress
(
reader
.
stubOffset
(
)
)
;
TypedThingLayout
layout
=
reader
.
typedThingLayout
(
)
;
ReferenceType
type
=
reader
.
referenceTypeDescrType
(
)
;
AutoScratchRegister
scratch1
(
allocator
masm
R1
.
scratchReg
(
)
)
;
ValueOperand
val
=
allocator
.
useFixedValueRegister
(
masm
reader
.
valOperandId
(
)
R0
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
if
(
type
!
=
ReferenceType
:
:
TYPE_STRING
)
{
LiveGeneralRegisterSet
saveRegs
;
saveRegs
.
add
(
obj
)
;
saveRegs
.
add
(
val
)
;
if
(
!
callTypeUpdateIC
(
obj
val
scratch1
saveRegs
)
)
{
return
false
;
}
}
LoadTypedThingData
(
masm
layout
obj
scratch1
)
;
masm
.
addPtr
(
offsetAddr
scratch1
)
;
Address
dest
(
scratch1
0
)
;
emitStoreTypedObjectReferenceProp
(
val
type
dest
scratch2
)
;
emitPostBarrierSlot
(
obj
val
scratch1
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitStoreDenseElement
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ObjOperandId
objId
=
reader
.
objOperandId
(
)
;
Int32OperandId
indexId
=
reader
.
int32OperandId
(
)
;
AutoScratchRegister
scratch
(
allocator
masm
R1
.
scratchReg
(
)
)
;
ValueOperand
val
=
allocator
.
useFixedValueRegister
(
masm
reader
.
valOperandId
(
)
R0
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
Register
index
=
allocator
.
useRegister
(
masm
indexId
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
Register
spectreTemp
=
InvalidReg
;
Address
initLength
(
scratch
ObjectElements
:
:
offsetOfInitializedLength
(
)
)
;
masm
.
spectreBoundsCheck32
(
index
initLength
spectreTemp
failure
-
>
label
(
)
)
;
BaseObjectElementIndex
element
(
scratch
index
)
;
masm
.
branchTestMagic
(
Assembler
:
:
Equal
element
failure
-
>
label
(
)
)
;
Label
noSpecialHandling
;
Address
elementsFlags
(
scratch
ObjectElements
:
:
offsetOfFlags
(
)
)
;
masm
.
branchTest32
(
Assembler
:
:
Zero
elementsFlags
Imm32
(
ObjectElements
:
:
CONVERT_DOUBLE_ELEMENTS
|
ObjectElements
:
:
COPY_ON_WRITE
|
ObjectElements
:
:
FROZEN
)
&
noSpecialHandling
)
;
masm
.
branchTest32
(
Assembler
:
:
NonZero
elementsFlags
Imm32
(
ObjectElements
:
:
COPY_ON_WRITE
|
ObjectElements
:
:
FROZEN
)
failure
-
>
label
(
)
)
;
masm
.
convertInt32ValueToDouble
(
val
)
;
masm
.
bind
(
&
noSpecialHandling
)
;
LiveGeneralRegisterSet
saveRegs
;
saveRegs
.
add
(
obj
)
;
saveRegs
.
add
(
index
)
;
saveRegs
.
add
(
val
)
;
if
(
!
callTypeUpdateIC
(
obj
val
scratch
saveRegs
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
EmitPreBarrier
(
masm
element
MIRType
:
:
Value
)
;
masm
.
storeValue
(
val
element
)
;
emitPostBarrierElement
(
obj
val
scratch
index
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitStoreDenseElementHole
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ObjOperandId
objId
=
reader
.
objOperandId
(
)
;
Int32OperandId
indexId
=
reader
.
int32OperandId
(
)
;
AutoScratchRegister
scratch
(
allocator
masm
R1
.
scratchReg
(
)
)
;
ValueOperand
val
=
allocator
.
useFixedValueRegister
(
masm
reader
.
valOperandId
(
)
R0
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
Register
index
=
allocator
.
useRegister
(
masm
indexId
)
;
bool
handleAdd
=
reader
.
readBool
(
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
BaseObjectElementIndex
element
(
scratch
index
)
;
Address
initLength
(
scratch
ObjectElements
:
:
offsetOfInitializedLength
(
)
)
;
Address
elementsFlags
(
scratch
ObjectElements
:
:
offsetOfFlags
(
)
)
;
masm
.
branchTest32
(
Assembler
:
:
NonZero
elementsFlags
Imm32
(
ObjectElements
:
:
COPY_ON_WRITE
)
failure
-
>
label
(
)
)
;
Register
spectreTemp
=
InvalidReg
;
if
(
handleAdd
)
{
Label
capacityOk
outOfBounds
;
masm
.
spectreBoundsCheck32
(
index
initLength
spectreTemp
&
outOfBounds
)
;
masm
.
jump
(
&
capacityOk
)
;
masm
.
bind
(
&
outOfBounds
)
;
masm
.
branch32
(
Assembler
:
:
NotEqual
initLength
index
failure
-
>
label
(
)
)
;
Label
allocElement
;
Address
capacity
(
scratch
ObjectElements
:
:
offsetOfCapacity
(
)
)
;
masm
.
spectreBoundsCheck32
(
index
capacity
spectreTemp
&
allocElement
)
;
masm
.
jump
(
&
capacityOk
)
;
masm
.
bind
(
&
allocElement
)
;
masm
.
branchTest32
(
Assembler
:
:
NonZero
elementsFlags
Imm32
(
ObjectElements
:
:
NONWRITABLE_ARRAY_LENGTH
)
failure
-
>
label
(
)
)
;
LiveRegisterSet
save
(
GeneralRegisterSet
:
:
Volatile
(
)
liveVolatileFloatRegs
(
)
)
;
save
.
takeUnchecked
(
scratch
)
;
masm
.
PushRegsInMask
(
save
)
;
masm
.
setupUnalignedABICall
(
scratch
)
;
masm
.
loadJSContext
(
scratch
)
;
masm
.
passABIArg
(
scratch
)
;
masm
.
passABIArg
(
obj
)
;
masm
.
callWithABI
(
JS_FUNC_TO_DATA_PTR
(
void
*
NativeObject
:
:
addDenseElementPure
)
)
;
masm
.
mov
(
ReturnReg
scratch
)
;
masm
.
PopRegsInMask
(
save
)
;
masm
.
branchIfFalseBool
(
scratch
failure
-
>
label
(
)
)
;
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
masm
.
bind
(
&
capacityOk
)
;
}
else
{
masm
.
spectreBoundsCheck32
(
index
initLength
spectreTemp
failure
-
>
label
(
)
)
;
}
Label
noConversion
;
masm
.
branchTest32
(
Assembler
:
:
Zero
elementsFlags
Imm32
(
ObjectElements
:
:
CONVERT_DOUBLE_ELEMENTS
)
&
noConversion
)
;
masm
.
convertInt32ValueToDouble
(
val
)
;
masm
.
bind
(
&
noConversion
)
;
LiveGeneralRegisterSet
saveRegs
;
saveRegs
.
add
(
obj
)
;
saveRegs
.
add
(
index
)
;
saveRegs
.
add
(
val
)
;
if
(
!
callTypeUpdateIC
(
obj
val
scratch
saveRegs
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
Label
doStore
;
if
(
handleAdd
)
{
Label
inBounds
;
masm
.
branch32
(
Assembler
:
:
NotEqual
initLength
index
&
inBounds
)
;
masm
.
add32
(
Imm32
(
1
)
initLength
)
;
Label
skipIncrementLength
;
Address
length
(
scratch
ObjectElements
:
:
offsetOfLength
(
)
)
;
masm
.
branch32
(
Assembler
:
:
Above
length
index
&
skipIncrementLength
)
;
masm
.
add32
(
Imm32
(
1
)
length
)
;
masm
.
bind
(
&
skipIncrementLength
)
;
masm
.
jump
(
&
doStore
)
;
masm
.
bind
(
&
inBounds
)
;
}
EmitPreBarrier
(
masm
element
MIRType
:
:
Value
)
;
masm
.
bind
(
&
doStore
)
;
masm
.
storeValue
(
val
element
)
;
emitPostBarrierElement
(
obj
val
scratch
index
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitArrayPush
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ObjOperandId
objId
=
reader
.
objOperandId
(
)
;
ValOperandId
rhsId
=
reader
.
valOperandId
(
)
;
AutoScratchRegister
scratch
(
allocator
masm
R1
.
scratchReg
(
)
)
;
ValueOperand
val
=
allocator
.
useFixedValueRegister
(
masm
rhsId
R0
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratchLength
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
Address
elementsInitLength
(
scratch
ObjectElements
:
:
offsetOfInitializedLength
(
)
)
;
Address
elementsLength
(
scratch
ObjectElements
:
:
offsetOfLength
(
)
)
;
Address
elementsFlags
(
scratch
ObjectElements
:
:
offsetOfFlags
(
)
)
;
masm
.
branchTest32
(
Assembler
:
:
NonZero
elementsFlags
Imm32
(
ObjectElements
:
:
COPY_ON_WRITE
)
failure
-
>
label
(
)
)
;
masm
.
load32
(
elementsInitLength
scratchLength
)
;
masm
.
branch32
(
Assembler
:
:
NotEqual
elementsLength
scratchLength
failure
-
>
label
(
)
)
;
Label
capacityOk
allocElement
;
Address
capacity
(
scratch
ObjectElements
:
:
offsetOfCapacity
(
)
)
;
masm
.
spectreBoundsCheck32
(
scratchLength
capacity
InvalidReg
&
allocElement
)
;
masm
.
jump
(
&
capacityOk
)
;
masm
.
bind
(
&
allocElement
)
;
masm
.
branchTest32
(
Assembler
:
:
NonZero
elementsFlags
Imm32
(
ObjectElements
:
:
NONWRITABLE_ARRAY_LENGTH
)
failure
-
>
label
(
)
)
;
LiveRegisterSet
save
(
GeneralRegisterSet
:
:
Volatile
(
)
liveVolatileFloatRegs
(
)
)
;
save
.
takeUnchecked
(
scratch
)
;
masm
.
PushRegsInMask
(
save
)
;
masm
.
setupUnalignedABICall
(
scratch
)
;
masm
.
loadJSContext
(
scratch
)
;
masm
.
passABIArg
(
scratch
)
;
masm
.
passABIArg
(
obj
)
;
masm
.
callWithABI
(
JS_FUNC_TO_DATA_PTR
(
void
*
NativeObject
:
:
addDenseElementPure
)
)
;
masm
.
mov
(
ReturnReg
scratch
)
;
masm
.
PopRegsInMask
(
save
)
;
masm
.
branchIfFalseBool
(
scratch
failure
-
>
label
(
)
)
;
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
masm
.
bind
(
&
capacityOk
)
;
Label
noConversion
;
masm
.
branchTest32
(
Assembler
:
:
Zero
elementsFlags
Imm32
(
ObjectElements
:
:
CONVERT_DOUBLE_ELEMENTS
)
&
noConversion
)
;
masm
.
convertInt32ValueToDouble
(
val
)
;
masm
.
bind
(
&
noConversion
)
;
LiveGeneralRegisterSet
saveRegs
;
saveRegs
.
add
(
obj
)
;
saveRegs
.
add
(
val
)
;
if
(
!
callTypeUpdateIC
(
obj
val
scratch
saveRegs
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
masm
.
add32
(
Imm32
(
1
)
elementsInitLength
)
;
masm
.
load32
(
elementsLength
scratchLength
)
;
masm
.
add32
(
Imm32
(
1
)
elementsLength
)
;
BaseObjectElementIndex
element
(
scratch
scratchLength
)
;
masm
.
storeValue
(
val
element
)
;
emitPostBarrierElement
(
obj
val
scratch
scratchLength
)
;
masm
.
add32
(
Imm32
(
1
)
scratchLength
)
;
masm
.
tagValue
(
JSVAL_TYPE_INT32
scratchLength
val
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallNativeSetter
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
Address
setterAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
loadPtr
(
setterAddr
scratch
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
obj
)
;
masm
.
Push
(
scratch
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleFunction
HandleObject
HandleValue
)
;
callVM
<
Fn
CallNativeSetter
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallScriptedSetter
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoScratchRegister
scratch1
(
allocator
masm
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
Address
setterAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
bool
isSameRealm
=
reader
.
readBool
(
)
;
masm
.
loadPtr
(
setterAddr
scratch1
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch2
)
;
if
(
!
isSameRealm
)
{
masm
.
switchToObjectRealm
(
scratch1
scratch2
)
;
}
masm
.
alignJitStackBasedOnNArgs
(
1
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
TypedOrValueRegister
(
MIRType
:
:
Object
AnyRegister
(
obj
)
)
)
;
EmitBaselineCreateStubFrameDescriptor
(
masm
scratch2
JitFrameLayout
:
:
Size
(
)
)
;
masm
.
Push
(
Imm32
(
1
)
)
;
masm
.
Push
(
scratch1
)
;
masm
.
Push
(
scratch2
)
;
Label
noUnderflow
;
masm
.
load16ZeroExtend
(
Address
(
scratch1
JSFunction
:
:
offsetOfNargs
(
)
)
scratch2
)
;
masm
.
loadJitCodeRaw
(
scratch1
scratch1
)
;
masm
.
branch32
(
Assembler
:
:
BelowOrEqual
scratch2
Imm32
(
1
)
&
noUnderflow
)
;
{
TrampolinePtr
argumentsRectifier
=
cx_
-
>
runtime
(
)
-
>
jitRuntime
(
)
-
>
getArgumentsRectifier
(
)
;
masm
.
movePtr
(
argumentsRectifier
scratch1
)
;
}
masm
.
bind
(
&
noUnderflow
)
;
masm
.
callJit
(
scratch1
)
;
stubFrame
.
leave
(
masm
true
)
;
if
(
!
isSameRealm
)
{
masm
.
switchToBaselineFrameRealm
(
R1
.
scratchReg
(
)
)
;
}
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallSetArrayLength
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
bool
strict
=
reader
.
readBool
(
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
Push
(
Imm32
(
strict
)
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleObject
HandleValue
bool
)
;
callVM
<
Fn
jit
:
:
SetArrayLength
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallProxySet
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
Address
idAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
bool
strict
=
reader
.
readBool
(
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
loadPtr
(
idAddr
scratch
)
;
masm
.
Push
(
Imm32
(
strict
)
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
scratch
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleObject
HandleId
HandleValue
bool
)
;
callVM
<
Fn
ProxySetProperty
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallProxySetByValue
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
ValueOperand
idVal
=
allocator
.
useValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
bool
strict
=
reader
.
readBool
(
)
;
allocator
.
discardStack
(
masm
)
;
int
scratchOffset
=
BaselineFrame
:
:
reverseOffsetOfScratchValue
(
)
;
masm
.
storePtr
(
obj
Address
(
BaselineFrameReg
scratchOffset
)
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
obj
)
;
masm
.
loadPtr
(
Address
(
BaselineFrameReg
0
)
obj
)
;
masm
.
loadPtr
(
Address
(
obj
scratchOffset
)
obj
)
;
masm
.
Push
(
Imm32
(
strict
)
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
idVal
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleObject
HandleValue
HandleValue
bool
)
;
callVM
<
Fn
ProxySetPropertyByValue
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallAddOrUpdateSparseElementHelper
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
Register
id
=
allocator
.
useRegister
(
masm
reader
.
int32OperandId
(
)
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
bool
strict
=
reader
.
readBool
(
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
Push
(
Imm32
(
strict
)
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
id
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
cx
HandleArrayObject
obj
int32_t
int_id
HandleValue
v
bool
strict
)
;
callVM
<
Fn
AddOrUpdateSparseElementHelper
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitMegamorphicSetElement
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
ValueOperand
idVal
=
allocator
.
useValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
bool
strict
=
reader
.
readBool
(
)
;
allocator
.
discardStack
(
masm
)
;
int
scratchOffset
=
BaselineFrame
:
:
reverseOffsetOfScratchValue
(
)
;
masm
.
storePtr
(
obj
Address
(
BaselineFrameReg
scratchOffset
)
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
obj
)
;
masm
.
loadPtr
(
Address
(
BaselineFrameReg
0
)
obj
)
;
masm
.
loadPtr
(
Address
(
obj
scratchOffset
)
obj
)
;
masm
.
Push
(
Imm32
(
strict
)
)
;
masm
.
Push
(
TypedOrValueRegister
(
MIRType
:
:
Object
AnyRegister
(
obj
)
)
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
idVal
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleObject
HandleValue
HandleValue
HandleValue
bool
)
;
callVM
<
Fn
SetObjectElementWithReceiver
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitTypeMonitorResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
allocator
.
discardStack
(
masm
)
;
EmitEnterTypeMonitorIC
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitReturnFromIC
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
allocator
.
discardStack
(
masm
)
;
EmitReturnFromIC
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadArgumentFixedSlot
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ValueOperand
resultReg
=
allocator
.
defineValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
Address
addr
=
allocator
.
addressOf
(
masm
BaselineFrameSlot
(
reader
.
readByte
(
)
)
)
;
masm
.
loadValue
(
addr
resultReg
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadArgumentDynamicSlot
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ValueOperand
resultReg
=
allocator
.
defineValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
Register
argcReg
=
allocator
.
useRegister
(
masm
reader
.
int32OperandId
(
)
)
;
BaseValueIndex
addr
=
allocator
.
addressOf
(
masm
argcReg
BaselineFrameSlot
(
reader
.
readByte
(
)
)
)
;
masm
.
loadValue
(
addr
resultReg
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardAndGetIterator
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
AutoScratchRegister
scratch1
(
allocator
masm
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
AutoScratchRegister
niScratch
(
allocator
masm
)
;
Address
iterAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
Address
enumeratorsAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
Register
output
=
allocator
.
defineRegister
(
masm
reader
.
objOperandId
(
)
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
loadPtr
(
iterAddr
output
)
;
masm
.
loadObjPrivate
(
output
PropertyIteratorObject
:
:
NUM_FIXED_SLOTS
niScratch
)
;
masm
.
branchIfNativeIteratorNotReusable
(
niScratch
failure
-
>
label
(
)
)
;
Address
iterObjAddr
(
niScratch
NativeIterator
:
:
offsetOfObjectBeingIterated
(
)
)
;
EmitPreBarrier
(
masm
iterObjAddr
MIRType
:
:
Object
)
;
Address
iterFlagsAddr
(
niScratch
NativeIterator
:
:
offsetOfFlagsAndCount
(
)
)
;
masm
.
storePtr
(
obj
iterObjAddr
)
;
masm
.
or32
(
Imm32
(
NativeIterator
:
:
Flags
:
:
Active
)
iterFlagsAddr
)
;
emitPostBarrierSlot
(
output
TypedOrValueRegister
(
MIRType
:
:
Object
AnyRegister
(
obj
)
)
scratch1
)
;
masm
.
loadPtr
(
enumeratorsAddr
scratch1
)
;
masm
.
loadPtr
(
Address
(
scratch1
0
)
scratch1
)
;
emitRegisterEnumerator
(
scratch1
niScratch
scratch2
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardDOMExpandoMissingOrGuardShape
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
AutoScratchRegister
shapeScratch
(
allocator
masm
)
;
AutoScratchRegister
objScratch
(
allocator
masm
)
;
Address
shapeAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Label
done
;
masm
.
branchTestUndefined
(
Assembler
:
:
Equal
val
&
done
)
;
masm
.
debugAssertIsObject
(
val
)
;
masm
.
loadPtr
(
shapeAddr
shapeScratch
)
;
masm
.
unboxObject
(
val
objScratch
)
;
masm
.
branchTestObjShapeNoSpectreMitigations
(
Assembler
:
:
NotEqual
objScratch
shapeScratch
failure
-
>
label
(
)
)
;
masm
.
bind
(
&
done
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadDOMExpandoValueGuardGeneration
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
Address
expandoAndGenerationAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
Address
generationAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
ValueOperand
output
=
allocator
.
defineValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
obj
ProxyObject
:
:
offsetOfReservedSlots
(
)
)
scratch
)
;
Address
expandoAddr
(
scratch
js
:
:
detail
:
:
ProxyReservedSlots
:
:
offsetOfPrivateSlot
(
)
)
;
masm
.
loadPtr
(
expandoAndGenerationAddr
output
.
scratchReg
(
)
)
;
masm
.
branchPrivatePtr
(
Assembler
:
:
NotEqual
expandoAddr
output
.
scratchReg
(
)
failure
-
>
label
(
)
)
;
masm
.
branch64
(
Assembler
:
:
NotEqual
Address
(
output
.
scratchReg
(
)
ExpandoAndGeneration
:
:
offsetOfGeneration
(
)
)
generationAddr
scratch
failure
-
>
label
(
)
)
;
masm
.
loadValue
(
Address
(
output
.
scratchReg
(
)
ExpandoAndGeneration
:
:
offsetOfExpando
(
)
)
output
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
init
(
CacheKind
kind
)
{
if
(
!
allocator
.
init
(
)
)
{
return
false
;
}
allowDoubleResult_
.
emplace
(
true
)
;
size_t
numInputs
=
writer_
.
numInputOperands
(
)
;
size_t
numInputsInRegs
=
std
:
:
min
(
numInputs
size_t
(
2
)
)
;
AllocatableGeneralRegisterSet
available
(
ICStubCompiler
:
:
availableGeneralRegs
(
numInputsInRegs
)
)
;
switch
(
kind
)
{
case
CacheKind
:
:
NewObject
:
case
CacheKind
:
:
GetIntrinsic
:
MOZ_ASSERT
(
numInputs
=
=
0
)
;
break
;
case
CacheKind
:
:
GetProp
:
case
CacheKind
:
:
TypeOf
:
case
CacheKind
:
:
GetIterator
:
case
CacheKind
:
:
ToBool
:
case
CacheKind
:
:
UnaryArith
:
MOZ_ASSERT
(
numInputs
=
=
1
)
;
allocator
.
initInputLocation
(
0
R0
)
;
break
;
case
CacheKind
:
:
Compare
:
case
CacheKind
:
:
GetElem
:
case
CacheKind
:
:
GetPropSuper
:
case
CacheKind
:
:
SetProp
:
case
CacheKind
:
:
In
:
case
CacheKind
:
:
HasOwn
:
case
CacheKind
:
:
InstanceOf
:
case
CacheKind
:
:
BinaryArith
:
MOZ_ASSERT
(
numInputs
=
=
2
)
;
allocator
.
initInputLocation
(
0
R0
)
;
allocator
.
initInputLocation
(
1
R1
)
;
break
;
case
CacheKind
:
:
GetElemSuper
:
MOZ_ASSERT
(
numInputs
=
=
3
)
;
allocator
.
initInputLocation
(
0
BaselineFrameSlot
(
0
)
)
;
allocator
.
initInputLocation
(
1
R0
)
;
allocator
.
initInputLocation
(
2
R1
)
;
break
;
case
CacheKind
:
:
SetElem
:
MOZ_ASSERT
(
numInputs
=
=
3
)
;
allocator
.
initInputLocation
(
0
R0
)
;
allocator
.
initInputLocation
(
1
R1
)
;
allocator
.
initInputLocation
(
2
BaselineFrameSlot
(
0
)
)
;
break
;
case
CacheKind
:
:
GetName
:
case
CacheKind
:
:
BindName
:
MOZ_ASSERT
(
numInputs
=
=
1
)
;
allocator
.
initInputLocation
(
0
R0
.
scratchReg
(
)
JSVAL_TYPE_OBJECT
)
;
#
if
defined
(
JS_NUNBOX32
)
available
.
add
(
R0
.
typeReg
(
)
)
;
#
endif
break
;
case
CacheKind
:
:
Call
:
MOZ_ASSERT
(
numInputs
=
=
1
)
;
allocator
.
initInputLocation
(
0
R0
.
scratchReg
(
)
JSVAL_TYPE_INT32
)
;
#
if
defined
(
JS_NUNBOX32
)
available
.
add
(
R0
.
typeReg
(
)
)
;
#
endif
break
;
}
liveFloatRegs_
=
LiveFloatRegisterSet
(
FloatRegisterSet
(
)
)
;
allocator
.
initAvailableRegs
(
available
)
;
outputUnchecked_
.
emplace
(
R0
)
;
return
true
;
}
static
void
ResetEnteredCounts
(
ICFallbackStub
*
stub
)
{
for
(
ICStubIterator
iter
=
stub
-
>
beginChain
(
)
;
!
iter
.
atEnd
(
)
;
iter
+
+
)
{
switch
(
iter
-
>
kind
(
)
)
{
case
ICStub
:
:
CacheIR_Regular
:
iter
-
>
toCacheIR_Regular
(
)
-
>
resetEnteredCount
(
)
;
break
;
case
ICStub
:
:
CacheIR_Updated
:
iter
-
>
toCacheIR_Updated
(
)
-
>
resetEnteredCount
(
)
;
break
;
case
ICStub
:
:
CacheIR_Monitored
:
iter
-
>
toCacheIR_Monitored
(
)
-
>
resetEnteredCount
(
)
;
break
;
default
:
break
;
}
}
stub
-
>
resetEnteredCount
(
)
;
}
ICStub
*
js
:
:
jit
:
:
AttachBaselineCacheIRStub
(
JSContext
*
cx
const
CacheIRWriter
&
writer
CacheKind
kind
BaselineCacheIRStubKind
stubKind
JSScript
*
outerScript
ICFallbackStub
*
stub
bool
*
attached
)
{
AutoAssertNoPendingException
aanpe
(
cx
)
;
JS
:
:
AutoCheckCannotGC
nogc
;
MOZ_ASSERT
(
!
*
attached
)
;
if
(
writer
.
failed
(
)
)
{
return
nullptr
;
}
#
ifdef
DEBUG
static
const
size_t
MaxOptimizedCacheIRStubs
=
16
;
MOZ_ASSERT
(
stub
-
>
numOptimizedStubs
(
)
<
MaxOptimizedCacheIRStubs
)
;
#
endif
uint32_t
stubDataOffset
=
0
;
switch
(
stubKind
)
{
case
BaselineCacheIRStubKind
:
:
Monitored
:
stubDataOffset
=
sizeof
(
ICCacheIR_Monitored
)
;
break
;
case
BaselineCacheIRStubKind
:
:
Regular
:
stubDataOffset
=
sizeof
(
ICCacheIR_Regular
)
;
break
;
case
BaselineCacheIRStubKind
:
:
Updated
:
stubDataOffset
=
sizeof
(
ICCacheIR_Updated
)
;
break
;
}
JitZone
*
jitZone
=
cx
-
>
zone
(
)
-
>
jitZone
(
)
;
CacheIRStubInfo
*
stubInfo
;
CacheIRStubKey
:
:
Lookup
lookup
(
kind
ICStubEngine
:
:
Baseline
writer
.
codeStart
(
)
writer
.
codeLength
(
)
)
;
JitCode
*
code
=
jitZone
-
>
getBaselineCacheIRStubCode
(
lookup
&
stubInfo
)
;
if
(
!
code
)
{
JitContext
jctx
(
cx
nullptr
)
;
BaselineCacheIRCompiler
comp
(
cx
writer
stubDataOffset
stubKind
)
;
if
(
!
comp
.
init
(
kind
)
)
{
return
nullptr
;
}
code
=
comp
.
compile
(
)
;
if
(
!
code
)
{
return
nullptr
;
}
MOZ_ASSERT
(
!
stubInfo
)
;
stubInfo
=
CacheIRStubInfo
:
:
New
(
kind
ICStubEngine
:
:
Baseline
comp
.
makesGCCalls
(
)
stubDataOffset
writer
)
;
if
(
!
stubInfo
)
{
return
nullptr
;
}
CacheIRStubKey
key
(
stubInfo
)
;
if
(
!
jitZone
-
>
putBaselineCacheIRStubCode
(
lookup
key
code
)
)
{
return
nullptr
;
}
}
MOZ_ASSERT
(
code
)
;
MOZ_ASSERT
(
stubInfo
)
;
MOZ_ASSERT
(
stubInfo
-
>
stubDataSize
(
)
=
=
writer
.
stubDataSize
(
)
)
;
for
(
ICStubConstIterator
iter
=
stub
-
>
beginChainConst
(
)
;
!
iter
.
atEnd
(
)
;
iter
+
+
)
{
bool
updated
=
false
;
switch
(
stubKind
)
{
case
BaselineCacheIRStubKind
:
:
Regular
:
{
if
(
!
iter
-
>
isCacheIR_Regular
(
)
)
{
continue
;
}
auto
otherStub
=
iter
-
>
toCacheIR_Regular
(
)
;
if
(
otherStub
-
>
stubInfo
(
)
!
=
stubInfo
)
{
continue
;
}
if
(
!
writer
.
stubDataEqualsMaybeUpdate
(
otherStub
-
>
stubDataStart
(
)
&
updated
)
)
{
continue
;
}
break
;
}
case
BaselineCacheIRStubKind
:
:
Monitored
:
{
if
(
!
iter
-
>
isCacheIR_Monitored
(
)
)
{
continue
;
}
auto
otherStub
=
iter
-
>
toCacheIR_Monitored
(
)
;
if
(
otherStub
-
>
stubInfo
(
)
!
=
stubInfo
)
{
continue
;
}
if
(
!
writer
.
stubDataEqualsMaybeUpdate
(
otherStub
-
>
stubDataStart
(
)
&
updated
)
)
{
continue
;
}
break
;
}
case
BaselineCacheIRStubKind
:
:
Updated
:
{
if
(
!
iter
-
>
isCacheIR_Updated
(
)
)
{
continue
;
}
auto
otherStub
=
iter
-
>
toCacheIR_Updated
(
)
;
if
(
otherStub
-
>
stubInfo
(
)
!
=
stubInfo
)
{
continue
;
}
if
(
!
writer
.
stubDataEqualsMaybeUpdate
(
otherStub
-
>
stubDataStart
(
)
&
updated
)
)
{
continue
;
}
break
;
}
}
if
(
updated
)
{
*
attached
=
true
;
}
else
{
JitSpew
(
JitSpew_BaselineICFallback
"
Tried
attaching
identical
stub
for
(
%
s
:
%
u
:
%
u
)
"
outerScript
-
>
filename
(
)
outerScript
-
>
lineno
(
)
outerScript
-
>
column
(
)
)
;
}
return
nullptr
;
}
size_t
bytesNeeded
=
stubInfo
-
>
stubDataOffset
(
)
+
stubInfo
-
>
stubDataSize
(
)
;
ICStubSpace
*
stubSpace
=
ICStubCompiler
:
:
StubSpaceForStub
(
stubInfo
-
>
makesGCCalls
(
)
outerScript
)
;
void
*
newStubMem
=
stubSpace
-
>
alloc
(
bytesNeeded
)
;
if
(
!
newStubMem
)
{
return
nullptr
;
}
ResetEnteredCounts
(
stub
)
;
switch
(
stubKind
)
{
case
BaselineCacheIRStubKind
:
:
Regular
:
{
auto
newStub
=
new
(
newStubMem
)
ICCacheIR_Regular
(
code
stubInfo
)
;
writer
.
copyStubData
(
newStub
-
>
stubDataStart
(
)
)
;
stub
-
>
addNewStub
(
newStub
)
;
*
attached
=
true
;
return
newStub
;
}
case
BaselineCacheIRStubKind
:
:
Monitored
:
{
ICTypeMonitor_Fallback
*
typeMonitorFallback
=
stub
-
>
toMonitoredFallbackStub
(
)
-
>
getFallbackMonitorStub
(
cx
outerScript
)
;
if
(
!
typeMonitorFallback
)
{
cx
-
>
recoverFromOutOfMemory
(
)
;
return
nullptr
;
}
ICStub
*
monitorStub
=
typeMonitorFallback
-
>
firstMonitorStub
(
)
;
auto
newStub
=
new
(
newStubMem
)
ICCacheIR_Monitored
(
code
monitorStub
stubInfo
)
;
writer
.
copyStubData
(
newStub
-
>
stubDataStart
(
)
)
;
stub
-
>
addNewStub
(
newStub
)
;
*
attached
=
true
;
return
newStub
;
}
case
BaselineCacheIRStubKind
:
:
Updated
:
{
auto
newStub
=
new
(
newStubMem
)
ICCacheIR_Updated
(
code
stubInfo
)
;
if
(
!
newStub
-
>
initUpdatingChain
(
cx
stubSpace
)
)
{
cx
-
>
recoverFromOutOfMemory
(
)
;
return
nullptr
;
}
writer
.
copyStubData
(
newStub
-
>
stubDataStart
(
)
)
;
stub
-
>
addNewStub
(
newStub
)
;
*
attached
=
true
;
return
newStub
;
}
}
MOZ_CRASH
(
"
Invalid
kind
"
)
;
}
uint8_t
*
ICCacheIR_Regular
:
:
stubDataStart
(
)
{
return
reinterpret_cast
<
uint8_t
*
>
(
this
)
+
stubInfo_
-
>
stubDataOffset
(
)
;
}
uint8_t
*
ICCacheIR_Monitored
:
:
stubDataStart
(
)
{
return
reinterpret_cast
<
uint8_t
*
>
(
this
)
+
stubInfo_
-
>
stubDataOffset
(
)
;
}
uint8_t
*
ICCacheIR_Updated
:
:
stubDataStart
(
)
{
return
reinterpret_cast
<
uint8_t
*
>
(
this
)
+
stubInfo_
-
>
stubDataOffset
(
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallStringConcatResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
lhs
=
allocator
.
useRegister
(
masm
reader
.
stringOperandId
(
)
)
;
Register
rhs
=
allocator
.
useRegister
(
masm
reader
.
stringOperandId
(
)
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
push
(
rhs
)
;
masm
.
push
(
lhs
)
;
using
Fn
=
JSString
*
(
*
)
(
JSContext
*
HandleString
HandleString
)
;
callVM
<
Fn
ConcatStrings
<
CanGC
>
>
(
masm
)
;
masm
.
tagValue
(
JSVAL_TYPE_STRING
ReturnReg
output
.
valueReg
(
)
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallStringObjectConcatResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
ValueOperand
lhs
=
allocator
.
useValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
ValueOperand
rhs
=
allocator
.
useValueRegister
(
masm
reader
.
valOperandId
(
)
)
;
allocator
.
discardStack
(
masm
)
;
EmitRestoreTailCallReg
(
masm
)
;
masm
.
pushValue
(
lhs
)
;
masm
.
pushValue
(
rhs
)
;
masm
.
pushValue
(
rhs
)
;
masm
.
pushValue
(
lhs
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleValue
HandleValue
MutableHandleValue
)
;
tailCallVM
<
Fn
DoConcatStringObject
>
(
masm
)
;
return
true
;
}
template
<
typename
CallVM
>
bool
BaselineCacheIRCompiler
:
:
emitBigIntBinaryOperationShared
(
const
CallVM
&
emitCallVM
)
{
AutoOutputRegister
output
(
*
this
)
;
Register
lhs
=
allocator
.
useRegister
(
masm
reader
.
bigIntOperandId
(
)
)
;
Register
rhs
=
allocator
.
useRegister
(
masm
reader
.
bigIntOperandId
(
)
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
push
(
rhs
)
;
masm
.
push
(
lhs
)
;
emitCallVM
(
)
;
masm
.
tagValue
(
JSVAL_TYPE_BIGINT
ReturnReg
output
.
valueReg
(
)
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntAddResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntBinaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntAdd
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntSubResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntBinaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntSub
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntMulResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntBinaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntMul
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntDivResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntBinaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntDiv
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntModResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntBinaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntMod
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntPowResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntBinaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntPow
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntBitAndResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntBinaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntBitAnd
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntBitOrResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntBinaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntBitOr
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntBitXorResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntBinaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntBitXor
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntLeftShiftResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntBinaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntLeftShift
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntRightShiftResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntBinaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntRightShift
>
(
masm
)
;
}
)
;
}
template
<
typename
CallVM
>
bool
BaselineCacheIRCompiler
:
:
emitBigIntUnaryOperationShared
(
const
CallVM
&
emitCallVM
)
{
AutoOutputRegister
output
(
*
this
)
;
Register
val
=
allocator
.
useRegister
(
masm
reader
.
bigIntOperandId
(
)
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
push
(
val
)
;
emitCallVM
(
)
;
masm
.
tagValue
(
JSVAL_TYPE_BIGINT
ReturnReg
output
.
valueReg
(
)
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntNotResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntUnaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntBitNot
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntNegationResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntUnaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntNeg
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntIncResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntUnaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntInc
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitBigIntDecResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitBigIntUnaryOperationShared
(
[
&
]
(
)
{
using
Fn
=
BigInt
*
(
*
)
(
JSContext
*
HandleBigInt
)
;
callVM
<
Fn
jit
:
:
BigIntDec
>
(
masm
)
;
}
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitCompareBigIntStringResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
lhs
=
allocator
.
useRegister
(
masm
reader
.
bigIntOperandId
(
)
)
;
Register
rhs
=
allocator
.
useRegister
(
masm
reader
.
stringOperandId
(
)
)
;
JSOp
op
=
reader
.
jsop
(
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
if
(
op
=
=
JSOP_LE
|
|
op
=
=
JSOP_GT
)
{
masm
.
Push
(
lhs
)
;
masm
.
Push
(
rhs
)
;
}
else
{
masm
.
Push
(
rhs
)
;
masm
.
Push
(
lhs
)
;
}
using
FnBigIntString
=
bool
(
*
)
(
JSContext
*
HandleBigInt
HandleString
bool
*
)
;
using
FnStringBigInt
=
bool
(
*
)
(
JSContext
*
HandleString
HandleBigInt
bool
*
)
;
if
(
op
=
=
JSOP_EQ
)
{
callVM
<
FnBigIntString
jit
:
:
BigIntStringEqual
<
EqualityKind
:
:
Equal
>
>
(
masm
)
;
}
else
if
(
op
=
=
JSOP_NE
)
{
callVM
<
FnBigIntString
jit
:
:
BigIntStringEqual
<
EqualityKind
:
:
NotEqual
>
>
(
masm
)
;
}
else
if
(
op
=
=
JSOP_LT
)
{
callVM
<
FnBigIntString
jit
:
:
BigIntStringCompare
<
ComparisonKind
:
:
LessThan
>
>
(
masm
)
;
}
else
if
(
op
=
=
JSOP_GT
)
{
callVM
<
FnStringBigInt
jit
:
:
StringBigIntCompare
<
ComparisonKind
:
:
LessThan
>
>
(
masm
)
;
}
else
if
(
op
=
=
JSOP_LE
)
{
callVM
<
FnStringBigInt
jit
:
:
StringBigIntCompare
<
ComparisonKind
:
:
GreaterThanOrEqual
>
>
(
masm
)
;
}
else
{
MOZ_ASSERT
(
op
=
=
JSOP_GE
)
;
callVM
<
FnBigIntString
jit
:
:
BigIntStringCompare
<
ComparisonKind
:
:
GreaterThanOrEqual
>
>
(
masm
)
;
}
stubFrame
.
leave
(
masm
)
;
masm
.
tagValue
(
JSVAL_TYPE_BOOLEAN
ReturnReg
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCompareStringBigIntResult
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
lhs
=
allocator
.
useRegister
(
masm
reader
.
stringOperandId
(
)
)
;
Register
rhs
=
allocator
.
useRegister
(
masm
reader
.
bigIntOperandId
(
)
)
;
JSOp
op
=
reader
.
jsop
(
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
if
(
op
=
=
JSOP_LT
|
|
op
=
=
JSOP_GE
)
{
masm
.
Push
(
rhs
)
;
masm
.
Push
(
lhs
)
;
}
else
{
masm
.
Push
(
lhs
)
;
masm
.
Push
(
rhs
)
;
}
using
FnBigIntString
=
bool
(
*
)
(
JSContext
*
HandleBigInt
HandleString
bool
*
)
;
using
FnStringBigInt
=
bool
(
*
)
(
JSContext
*
HandleString
HandleBigInt
bool
*
)
;
if
(
op
=
=
JSOP_EQ
)
{
callVM
<
FnBigIntString
jit
:
:
BigIntStringEqual
<
EqualityKind
:
:
Equal
>
>
(
masm
)
;
}
else
if
(
op
=
=
JSOP_NE
)
{
callVM
<
FnBigIntString
jit
:
:
BigIntStringEqual
<
EqualityKind
:
:
NotEqual
>
>
(
masm
)
;
}
else
if
(
op
=
=
JSOP_LT
)
{
callVM
<
FnStringBigInt
jit
:
:
StringBigIntCompare
<
ComparisonKind
:
:
LessThan
>
>
(
masm
)
;
}
else
if
(
op
=
=
JSOP_GT
)
{
callVM
<
FnBigIntString
jit
:
:
BigIntStringCompare
<
ComparisonKind
:
:
LessThan
>
>
(
masm
)
;
}
else
if
(
op
=
=
JSOP_LE
)
{
callVM
<
FnBigIntString
jit
:
:
BigIntStringCompare
<
ComparisonKind
:
:
GreaterThanOrEqual
>
>
(
masm
)
;
}
else
{
MOZ_ASSERT
(
op
=
=
JSOP_GE
)
;
callVM
<
FnStringBigInt
jit
:
:
StringBigIntCompare
<
ComparisonKind
:
:
GreaterThanOrEqual
>
>
(
masm
)
;
}
stubFrame
.
leave
(
masm
)
;
masm
.
tagValue
(
JSVAL_TYPE_BOOLEAN
ReturnReg
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
updateArgc
(
CallFlags
flags
Register
argcReg
Register
scratch
)
{
static_assert
(
CacheIRCompiler
:
:
MAX_ARGS_ARRAY_LENGTH
<
=
ARGS_LENGTH_MAX
"
maximum
arguments
length
for
optimized
stub
should
be
<
=
"
"
ARGS_LENGTH_MAX
"
)
;
CallFlags
:
:
ArgFormat
format
=
flags
.
getArgFormat
(
)
;
switch
(
format
)
{
case
CallFlags
:
:
Standard
:
return
true
;
case
CallFlags
:
:
FunCall
:
return
true
;
case
CallFlags
:
:
FunApplyArray
:
{
BaselineFrameSlot
slot
(
0
)
;
masm
.
unboxObject
(
allocator
.
addressOf
(
masm
slot
)
argcReg
)
;
masm
.
loadPtr
(
Address
(
argcReg
NativeObject
:
:
offsetOfElements
(
)
)
argcReg
)
;
masm
.
load32
(
Address
(
argcReg
ObjectElements
:
:
offsetOfLength
(
)
)
argcReg
)
;
return
true
;
}
default
:
break
;
}
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
switch
(
flags
.
getArgFormat
(
)
)
{
case
CallFlags
:
:
Spread
:
{
BaselineFrameSlot
slot
(
flags
.
isConstructing
(
)
)
;
masm
.
unboxObject
(
allocator
.
addressOf
(
masm
slot
)
scratch
)
;
masm
.
loadPtr
(
Address
(
scratch
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
masm
.
load32
(
Address
(
scratch
ObjectElements
:
:
offsetOfLength
(
)
)
scratch
)
;
}
break
;
case
CallFlags
:
:
FunApplyArgs
:
{
Address
numActualArgsAddr
(
BaselineFrameReg
BaselineFrame
:
:
offsetOfNumActualArgs
(
)
)
;
masm
.
load32
(
numActualArgsAddr
scratch
)
;
}
break
;
default
:
MOZ_CRASH
(
"
Unknown
arg
format
"
)
;
}
masm
.
branch32
(
Assembler
:
:
Above
scratch
Imm32
(
CacheIRCompiler
:
:
MAX_ARGS_ARRAY_LENGTH
)
failure
-
>
label
(
)
)
;
masm
.
move32
(
scratch
argcReg
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardFunApply
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
Register
argcReg
=
allocator
.
useRegister
(
masm
reader
.
int32OperandId
(
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
CallFlags
flags
=
reader
.
callFlags
(
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
branch32
(
Assembler
:
:
NotEqual
argcReg
Imm32
(
2
)
failure
-
>
label
(
)
)
;
Address
argsAddr
=
allocator
.
addressOf
(
masm
BaselineFrameSlot
(
0
)
)
;
switch
(
flags
.
getArgFormat
(
)
)
{
case
CallFlags
:
:
FunApplyArgs
:
{
masm
.
branchTestMagic
(
Assembler
:
:
NotEqual
argsAddr
failure
-
>
label
(
)
)
;
Address
flagAddr
(
BaselineFrameReg
BaselineFrame
:
:
reverseOffsetOfFlags
(
)
)
;
masm
.
branchTest32
(
Assembler
:
:
NonZero
flagAddr
Imm32
(
BaselineFrame
:
:
HAS_ARGS_OBJ
)
failure
-
>
label
(
)
)
;
}
break
;
case
CallFlags
:
:
FunApplyArray
:
{
masm
.
branchTestObject
(
Assembler
:
:
NotEqual
argsAddr
failure
-
>
label
(
)
)
;
masm
.
unboxObject
(
argsAddr
scratch
)
;
const
JSClass
*
clasp
=
&
ArrayObject
:
:
class_
;
masm
.
branchTestObjClass
(
Assembler
:
:
NotEqual
scratch
clasp
scratch2
scratch
failure
-
>
label
(
)
)
;
Register
elementsReg
=
scratch
;
masm
.
loadPtr
(
Address
(
scratch
NativeObject
:
:
offsetOfElements
(
)
)
elementsReg
)
;
Register
calleeArgcReg
=
scratch2
;
masm
.
load32
(
Address
(
elementsReg
ObjectElements
:
:
offsetOfLength
(
)
)
calleeArgcReg
)
;
masm
.
branch32
(
Assembler
:
:
Above
calleeArgcReg
Imm32
(
CacheIRCompiler
:
:
MAX_ARGS_ARRAY_LENGTH
)
failure
-
>
label
(
)
)
;
Address
initLenAddr
(
elementsReg
ObjectElements
:
:
offsetOfInitializedLength
(
)
)
;
masm
.
branch32
(
Assembler
:
:
NotEqual
initLenAddr
calleeArgcReg
failure
-
>
label
(
)
)
;
Register
start
=
elementsReg
;
Register
end
=
scratch2
;
BaseValueIndex
endAddr
(
elementsReg
calleeArgcReg
)
;
masm
.
computeEffectiveAddress
(
endAddr
end
)
;
Label
loop
;
Label
endLoop
;
masm
.
bind
(
&
loop
)
;
masm
.
branchPtr
(
Assembler
:
:
AboveOrEqual
start
end
&
endLoop
)
;
masm
.
branchTestMagic
(
Assembler
:
:
Equal
Address
(
start
0
)
failure
-
>
label
(
)
)
;
masm
.
addPtr
(
Imm32
(
sizeof
(
Value
)
)
start
)
;
masm
.
jump
(
&
loop
)
;
masm
.
bind
(
&
endLoop
)
;
}
break
;
default
:
MOZ_CRASH
(
"
Invalid
arg
format
"
)
;
break
;
}
return
true
;
}
void
BaselineCacheIRCompiler
:
:
pushArguments
(
Register
argcReg
Register
calleeReg
Register
scratch
Register
scratch2
CallFlags
flags
bool
isJitCall
)
{
switch
(
flags
.
getArgFormat
(
)
)
{
case
CallFlags
:
:
Standard
:
pushStandardArguments
(
argcReg
scratch
scratch2
isJitCall
flags
.
isConstructing
(
)
)
;
break
;
case
CallFlags
:
:
Spread
:
pushArrayArguments
(
argcReg
scratch
scratch2
isJitCall
flags
.
isConstructing
(
)
)
;
break
;
case
CallFlags
:
:
FunCall
:
pushFunCallArguments
(
argcReg
calleeReg
scratch
scratch2
isJitCall
)
;
break
;
case
CallFlags
:
:
FunApplyArgs
:
pushFunApplyArgs
(
argcReg
calleeReg
scratch
scratch2
isJitCall
)
;
break
;
case
CallFlags
:
:
FunApplyArray
:
pushArrayArguments
(
argcReg
scratch
scratch2
isJitCall
false
)
;
break
;
default
:
MOZ_CRASH
(
"
Invalid
arg
format
"
)
;
}
}
void
BaselineCacheIRCompiler
:
:
pushStandardArguments
(
Register
argcReg
Register
scratch
Register
scratch2
bool
isJitCall
bool
isConstructing
)
{
Register
countReg
=
scratch
;
masm
.
move32
(
argcReg
countReg
)
;
masm
.
add32
(
Imm32
(
1
+
!
isJitCall
+
isConstructing
)
countReg
)
;
Register
argPtr
=
scratch2
;
Address
argAddress
(
masm
.
getStackPointer
(
)
STUB_FRAME_SIZE
)
;
masm
.
computeEffectiveAddress
(
argAddress
argPtr
)
;
if
(
isJitCall
)
{
masm
.
alignJitStackBasedOnNArgs
(
countReg
true
)
;
}
Label
loop
done
;
masm
.
branchTest32
(
Assembler
:
:
Zero
countReg
countReg
&
done
)
;
masm
.
bind
(
&
loop
)
;
{
masm
.
pushValue
(
Address
(
argPtr
0
)
)
;
masm
.
addPtr
(
Imm32
(
sizeof
(
Value
)
)
argPtr
)
;
masm
.
branchSub32
(
Assembler
:
:
NonZero
Imm32
(
1
)
countReg
&
loop
)
;
}
masm
.
bind
(
&
done
)
;
}
void
BaselineCacheIRCompiler
:
:
pushArrayArguments
(
Register
argcReg
Register
scratch
Register
scratch2
bool
isJitCall
bool
isConstructing
)
{
Register
startReg
=
scratch
;
masm
.
unboxObject
(
Address
(
masm
.
getStackPointer
(
)
(
isConstructing
*
sizeof
(
Value
)
)
+
STUB_FRAME_SIZE
)
startReg
)
;
masm
.
loadPtr
(
Address
(
startReg
NativeObject
:
:
offsetOfElements
(
)
)
startReg
)
;
if
(
isJitCall
)
{
Register
alignReg
=
argcReg
;
if
(
isConstructing
)
{
alignReg
=
scratch2
;
masm
.
computeEffectiveAddress
(
Address
(
argcReg
1
)
alignReg
)
;
}
masm
.
alignJitStackBasedOnNArgs
(
alignReg
false
)
;
}
if
(
isConstructing
)
{
masm
.
pushValue
(
Address
(
BaselineFrameReg
STUB_FRAME_SIZE
)
)
;
}
Register
endReg
=
scratch2
;
BaseValueIndex
endAddr
(
startReg
argcReg
)
;
masm
.
computeEffectiveAddress
(
endAddr
endReg
)
;
Label
copyDone
;
Label
copyStart
;
masm
.
bind
(
&
copyStart
)
;
masm
.
branchPtr
(
Assembler
:
:
Equal
endReg
startReg
&
copyDone
)
;
masm
.
subPtr
(
Imm32
(
sizeof
(
Value
)
)
endReg
)
;
masm
.
pushValue
(
Address
(
endReg
0
)
)
;
masm
.
jump
(
&
copyStart
)
;
masm
.
bind
(
&
copyDone
)
;
masm
.
pushValue
(
Address
(
BaselineFrameReg
STUB_FRAME_SIZE
+
(
1
+
isConstructing
)
*
sizeof
(
Value
)
)
)
;
if
(
!
isJitCall
)
{
masm
.
pushValue
(
Address
(
BaselineFrameReg
STUB_FRAME_SIZE
+
(
2
+
isConstructing
)
*
sizeof
(
Value
)
)
)
;
}
}
void
BaselineCacheIRCompiler
:
:
pushFunCallArguments
(
Register
argcReg
Register
calleeReg
Register
scratch
Register
scratch2
bool
isJitCall
)
{
Label
zeroArgs
done
;
masm
.
branchTest32
(
Assembler
:
:
Zero
argcReg
argcReg
&
zeroArgs
)
;
masm
.
sub32
(
Imm32
(
1
)
argcReg
)
;
pushStandardArguments
(
argcReg
scratch
scratch2
isJitCall
false
)
;
masm
.
jump
(
&
done
)
;
masm
.
bind
(
&
zeroArgs
)
;
if
(
isJitCall
)
{
masm
.
alignJitStackBasedOnNArgs
(
0
)
;
}
masm
.
pushValue
(
UndefinedValue
(
)
)
;
if
(
!
isJitCall
)
{
masm
.
Push
(
TypedOrValueRegister
(
MIRType
:
:
Object
AnyRegister
(
calleeReg
)
)
)
;
}
masm
.
bind
(
&
done
)
;
}
void
BaselineCacheIRCompiler
:
:
pushFunApplyArgs
(
Register
argcReg
Register
calleeReg
Register
scratch
Register
scratch2
bool
isJitCall
)
{
Register
startReg
=
scratch
;
masm
.
loadPtr
(
Address
(
BaselineFrameReg
0
)
startReg
)
;
masm
.
addPtr
(
Imm32
(
BaselineFrame
:
:
offsetOfArg
(
0
)
)
startReg
)
;
if
(
isJitCall
)
{
masm
.
alignJitStackBasedOnNArgs
(
argcReg
false
)
;
}
Register
endReg
=
scratch2
;
BaseValueIndex
endAddr
(
startReg
argcReg
)
;
masm
.
computeEffectiveAddress
(
endAddr
endReg
)
;
Label
copyDone
;
Label
copyStart
;
masm
.
bind
(
&
copyStart
)
;
masm
.
branchPtr
(
Assembler
:
:
Equal
endReg
startReg
&
copyDone
)
;
masm
.
subPtr
(
Imm32
(
sizeof
(
Value
)
)
endReg
)
;
masm
.
pushValue
(
Address
(
endReg
0
)
)
;
masm
.
jump
(
&
copyStart
)
;
masm
.
bind
(
&
copyDone
)
;
masm
.
pushValue
(
Address
(
BaselineFrameReg
STUB_FRAME_SIZE
+
sizeof
(
Value
)
)
)
;
if
(
!
isJitCall
)
{
masm
.
Push
(
TypedOrValueRegister
(
MIRType
:
:
Object
AnyRegister
(
calleeReg
)
)
)
;
}
}
bool
BaselineCacheIRCompiler
:
:
emitCallNativeShared
(
NativeCallType
callType
)
{
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
Register
calleeReg
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
Register
argcReg
=
allocator
.
useRegister
(
masm
reader
.
int32OperandId
(
)
)
;
CallFlags
flags
=
reader
.
callFlags
(
)
;
bool
isConstructing
=
flags
.
isConstructing
(
)
;
bool
isSameRealm
=
flags
.
isSameRealm
(
)
;
if
(
!
updateArgc
(
flags
argcReg
scratch
)
)
{
return
false
;
}
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
if
(
!
isSameRealm
)
{
masm
.
switchToObjectRealm
(
calleeReg
scratch
)
;
}
pushArguments
(
argcReg
calleeReg
scratch
scratch2
flags
false
)
;
masm
.
moveStackPtrTo
(
scratch2
.
get
(
)
)
;
masm
.
push
(
argcReg
)
;
EmitBaselineCreateStubFrameDescriptor
(
masm
scratch
ExitFrameLayout
:
:
Size
(
)
)
;
masm
.
push
(
scratch
)
;
masm
.
push
(
ICTailCallReg
)
;
masm
.
loadJSContext
(
scratch
)
;
masm
.
enterFakeExitFrameForNative
(
scratch
scratch
isConstructing
)
;
masm
.
setupUnalignedABICall
(
scratch
)
;
masm
.
loadJSContext
(
scratch
)
;
masm
.
passABIArg
(
scratch
)
;
masm
.
passABIArg
(
argcReg
)
;
masm
.
passABIArg
(
scratch2
)
;
switch
(
callType
)
{
case
NativeCallType
:
:
Native
:
{
#
ifdef
JS_SIMULATOR
Address
redirectedAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
masm
.
callWithABI
(
redirectedAddr
)
;
#
else
bool
ignoresReturnValue
=
reader
.
readBool
(
)
;
if
(
ignoresReturnValue
)
{
masm
.
loadPtr
(
Address
(
calleeReg
JSFunction
:
:
offsetOfJitInfo
(
)
)
calleeReg
)
;
masm
.
callWithABI
(
Address
(
calleeReg
JSJitInfo
:
:
offsetOfIgnoresReturnValueNative
(
)
)
)
;
}
else
{
masm
.
callWithABI
(
Address
(
calleeReg
JSFunction
:
:
offsetOfNative
(
)
)
)
;
}
#
endif
}
break
;
case
NativeCallType
:
:
ClassHook
:
{
Address
nativeAddr
(
stubAddress
(
reader
.
stubOffset
(
)
)
)
;
masm
.
callWithABI
(
nativeAddr
)
;
}
break
;
}
masm
.
branchIfFalseBool
(
ReturnReg
masm
.
exceptionLabel
(
)
)
;
masm
.
loadValue
(
Address
(
masm
.
getStackPointer
(
)
NativeExitFrameLayout
:
:
offsetOfResult
(
)
)
output
.
valueReg
(
)
)
;
stubFrame
.
leave
(
masm
)
;
if
(
!
isSameRealm
)
{
masm
.
switchToBaselineFrameRealm
(
scratch2
)
;
}
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallNativeFunction
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitCallNativeShared
(
NativeCallType
:
:
Native
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallClassHook
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
return
emitCallNativeShared
(
NativeCallType
:
:
ClassHook
)
;
}
void
BaselineCacheIRCompiler
:
:
loadStackObject
(
ArgumentKind
kind
CallFlags
flags
size_t
stackPushed
Register
argcReg
Register
dest
)
{
bool
addArgc
=
false
;
int32_t
slotIndex
=
GetIndexOfArgument
(
kind
flags
&
addArgc
)
;
if
(
addArgc
)
{
int32_t
slotOffset
=
slotIndex
*
sizeof
(
JS
:
:
Value
)
+
stackPushed
;
BaseValueIndex
slotAddr
(
masm
.
getStackPointer
(
)
argcReg
slotOffset
)
;
masm
.
unboxObject
(
slotAddr
dest
)
;
}
else
{
int32_t
slotOffset
=
slotIndex
*
sizeof
(
JS
:
:
Value
)
+
stackPushed
;
Address
slotAddr
(
masm
.
getStackPointer
(
)
slotOffset
)
;
masm
.
unboxObject
(
slotAddr
dest
)
;
}
}
void
BaselineCacheIRCompiler
:
:
createThis
(
Register
argcReg
Register
calleeReg
Register
scratch
CallFlags
flags
)
{
MOZ_ASSERT
(
flags
.
isConstructing
(
)
)
;
size_t
depth
=
STUB_FRAME_SIZE
;
masm
.
push
(
argcReg
)
;
depth
+
=
sizeof
(
size_t
)
;
loadStackObject
(
ArgumentKind
:
:
NewTarget
flags
depth
argcReg
scratch
)
;
masm
.
push
(
scratch
)
;
depth
+
=
sizeof
(
JSObject
*
)
;
loadStackObject
(
ArgumentKind
:
:
Callee
flags
depth
argcReg
scratch
)
;
masm
.
push
(
scratch
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleObject
HandleObject
MutableHandleValue
)
;
callVM
<
Fn
CreateThis
>
(
masm
)
;
#
ifdef
DEBUG
Label
createdThisOK
;
masm
.
branchTestObject
(
Assembler
:
:
Equal
JSReturnOperand
&
createdThisOK
)
;
masm
.
branchTestMagic
(
Assembler
:
:
Equal
JSReturnOperand
&
createdThisOK
)
;
masm
.
assumeUnreachable
(
"
The
return
of
CreateThis
must
be
an
object
or
uninitialized
.
"
)
;
masm
.
bind
(
&
createdThisOK
)
;
#
endif
masm
.
pop
(
argcReg
)
;
switch
(
flags
.
getArgFormat
(
)
)
{
case
CallFlags
:
:
Standard
:
{
BaseValueIndex
thisAddress
(
masm
.
getStackPointer
(
)
argcReg
1
*
sizeof
(
Value
)
+
STUB_FRAME_SIZE
)
;
masm
.
storeValue
(
JSReturnOperand
thisAddress
)
;
}
break
;
case
CallFlags
:
:
Spread
:
{
Address
thisAddress
(
masm
.
getStackPointer
(
)
2
*
sizeof
(
Value
)
+
STUB_FRAME_SIZE
)
;
masm
.
storeValue
(
JSReturnOperand
thisAddress
)
;
}
break
;
default
:
MOZ_CRASH
(
"
Invalid
arg
format
for
scripted
constructor
"
)
;
}
Address
stubRegAddress
(
masm
.
getStackPointer
(
)
STUB_FRAME_SAVED_STUB_OFFSET
)
;
masm
.
loadPtr
(
stubRegAddress
ICStubReg
)
;
depth
=
STUB_FRAME_SIZE
;
loadStackObject
(
ArgumentKind
:
:
Callee
flags
depth
argcReg
calleeReg
)
;
}
void
BaselineCacheIRCompiler
:
:
updateReturnValue
(
)
{
Label
skipThisReplace
;
masm
.
branchTestObject
(
Assembler
:
:
Equal
JSReturnOperand
&
skipThisReplace
)
;
Address
thisAddress
(
masm
.
getStackPointer
(
)
3
*
sizeof
(
size_t
)
)
;
masm
.
loadValue
(
thisAddress
JSReturnOperand
)
;
#
ifdef
DEBUG
masm
.
branchTestObject
(
Assembler
:
:
Equal
JSReturnOperand
&
skipThisReplace
)
;
masm
.
assumeUnreachable
(
"
Return
of
constructing
call
should
be
an
object
.
"
)
;
#
endif
masm
.
bind
(
&
skipThisReplace
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallScriptedFunction
(
)
{
JitSpew
(
JitSpew_Codegen
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
Register
calleeReg
=
allocator
.
useRegister
(
masm
reader
.
objOperandId
(
)
)
;
Register
argcReg
=
allocator
.
useRegister
(
masm
reader
.
int32OperandId
(
)
)
;
CallFlags
flags
=
reader
.
callFlags
(
)
;
bool
isConstructing
=
flags
.
isConstructing
(
)
;
bool
isSameRealm
=
flags
.
isSameRealm
(
)
;
if
(
!
updateArgc
(
flags
argcReg
scratch
)
)
{
return
false
;
}
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
if
(
!
isSameRealm
)
{
masm
.
switchToObjectRealm
(
calleeReg
scratch
)
;
}
if
(
isConstructing
)
{
createThis
(
argcReg
calleeReg
scratch
flags
)
;
}
pushArguments
(
argcReg
calleeReg
scratch
scratch2
flags
true
)
;
Register
code
=
scratch2
;
masm
.
loadJitCodeRaw
(
calleeReg
code
)
;
EmitBaselineCreateStubFrameDescriptor
(
masm
scratch
JitFrameLayout
:
:
Size
(
)
)
;
masm
.
Push
(
argcReg
)
;
masm
.
PushCalleeToken
(
calleeReg
isConstructing
)
;
masm
.
Push
(
scratch
)
;
Label
noUnderflow
;
masm
.
load16ZeroExtend
(
Address
(
calleeReg
JSFunction
:
:
offsetOfNargs
(
)
)
calleeReg
)
;
masm
.
branch32
(
Assembler
:
:
AboveOrEqual
argcReg
calleeReg
&
noUnderflow
)
;
{
TrampolinePtr
argumentsRectifier
=
cx_
-
>
runtime
(
)
-
>
jitRuntime
(
)
-
>
getArgumentsRectifier
(
)
;
masm
.
movePtr
(
argumentsRectifier
code
)
;
}
masm
.
bind
(
&
noUnderflow
)
;
masm
.
callJit
(
code
)
;
if
(
isConstructing
)
{
updateReturnValue
(
)
;
}
stubFrame
.
leave
(
masm
true
)
;
if
(
!
isSameRealm
)
{
masm
.
switchToBaselineFrameRealm
(
scratch2
)
;
}
return
true
;
}
