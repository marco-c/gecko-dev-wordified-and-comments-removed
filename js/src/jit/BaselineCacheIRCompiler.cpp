#
include
"
jit
/
BaselineCacheIRCompiler
.
h
"
#
include
"
jit
/
CacheIR
.
h
"
#
include
"
jit
/
Linker
.
h
"
#
include
"
jit
/
SharedICHelpers
.
h
"
#
include
"
jit
/
VMFunctions
.
h
"
#
include
"
js
/
experimental
/
JitInfo
.
h
"
#
include
"
js
/
friend
/
DOMProxy
.
h
"
#
include
"
proxy
/
DeadObjectProxy
.
h
"
#
include
"
proxy
/
Proxy
.
h
"
#
include
"
util
/
Unicode
.
h
"
#
include
"
jit
/
MacroAssembler
-
inl
.
h
"
#
include
"
jit
/
SharedICHelpers
-
inl
.
h
"
#
include
"
jit
/
VMFunctionList
-
inl
.
h
"
#
include
"
vm
/
JSContext
-
inl
.
h
"
#
include
"
vm
/
Realm
-
inl
.
h
"
using
namespace
js
;
using
namespace
js
:
:
jit
;
using
mozilla
:
:
Maybe
;
using
JS
:
:
ExpandoAndGeneration
;
namespace
js
{
namespace
jit
{
class
AutoStubFrame
;
Address
CacheRegisterAllocator
:
:
addressOf
(
MacroAssembler
&
masm
BaselineFrameSlot
slot
)
const
{
uint32_t
offset
=
stackPushed_
+
ICStackValueOffset
+
slot
.
slot
(
)
*
sizeof
(
JS
:
:
Value
)
;
return
Address
(
masm
.
getStackPointer
(
)
offset
)
;
}
BaseValueIndex
CacheRegisterAllocator
:
:
addressOf
(
MacroAssembler
&
masm
Register
argcReg
BaselineFrameSlot
slot
)
const
{
uint32_t
offset
=
stackPushed_
+
ICStackValueOffset
+
slot
.
slot
(
)
*
sizeof
(
JS
:
:
Value
)
;
return
BaseValueIndex
(
masm
.
getStackPointer
(
)
argcReg
offset
)
;
}
BaselineCacheIRCompiler
:
:
BaselineCacheIRCompiler
(
JSContext
*
cx
const
CacheIRWriter
&
writer
uint32_t
stubDataOffset
BaselineCacheIRStubKind
stubKind
)
:
CacheIRCompiler
(
cx
writer
stubDataOffset
Mode
:
:
Baseline
StubFieldPolicy
:
:
Address
)
makesGCCalls_
(
false
)
kind_
(
stubKind
)
{
}
AutoStubFrame
:
:
AutoStubFrame
(
BaselineCacheIRCompiler
&
compiler
)
:
compiler
(
compiler
)
#
ifdef
DEBUG
framePushedAtEnterStubFrame_
(
0
)
#
endif
{
}
void
AutoStubFrame
:
:
enter
(
MacroAssembler
&
masm
Register
scratch
CallCanGC
canGC
)
{
MOZ_ASSERT
(
compiler
.
allocator
.
stackPushed
(
)
=
=
0
)
;
EmitBaselineEnterStubFrame
(
masm
scratch
)
;
#
ifdef
DEBUG
framePushedAtEnterStubFrame_
=
masm
.
framePushed
(
)
;
#
endif
MOZ_ASSERT
(
!
compiler
.
preparedForVMCall_
)
;
compiler
.
preparedForVMCall_
=
true
;
if
(
canGC
=
=
CallCanGC
:
:
CanGC
)
{
compiler
.
makesGCCalls_
=
true
;
}
}
void
AutoStubFrame
:
:
leave
(
MacroAssembler
&
masm
bool
calledIntoIon
)
{
MOZ_ASSERT
(
compiler
.
preparedForVMCall_
)
;
compiler
.
preparedForVMCall_
=
false
;
#
ifdef
DEBUG
masm
.
setFramePushed
(
framePushedAtEnterStubFrame_
)
;
if
(
calledIntoIon
)
{
masm
.
adjustFrame
(
sizeof
(
intptr_t
)
)
;
}
#
endif
EmitBaselineLeaveStubFrame
(
masm
calledIntoIon
)
;
}
#
ifdef
DEBUG
AutoStubFrame
:
:
~
AutoStubFrame
(
)
{
MOZ_ASSERT
(
!
compiler
.
preparedForVMCall_
)
;
}
#
endif
}
}
bool
BaselineCacheIRCompiler
:
:
makesGCCalls
(
)
const
{
return
makesGCCalls_
;
}
Address
BaselineCacheIRCompiler
:
:
stubAddress
(
uint32_t
offset
)
const
{
return
Address
(
ICStubReg
stubDataOffset_
+
offset
)
;
}
template
<
typename
Fn
Fn
fn
>
void
BaselineCacheIRCompiler
:
:
callVM
(
MacroAssembler
&
masm
)
{
VMFunctionId
id
=
VMFunctionToId
<
Fn
fn
>
:
:
id
;
callVMInternal
(
masm
id
)
;
}
template
<
typename
Fn
Fn
fn
>
void
BaselineCacheIRCompiler
:
:
tailCallVM
(
MacroAssembler
&
masm
)
{
TailCallVMFunctionId
id
=
TailCallVMFunctionToId
<
Fn
fn
>
:
:
id
;
tailCallVMInternal
(
masm
id
)
;
}
void
BaselineCacheIRCompiler
:
:
tailCallVMInternal
(
MacroAssembler
&
masm
TailCallVMFunctionId
id
)
{
MOZ_ASSERT
(
!
preparedForVMCall_
)
;
TrampolinePtr
code
=
cx_
-
>
runtime
(
)
-
>
jitRuntime
(
)
-
>
getVMWrapper
(
id
)
;
const
VMFunctionData
&
fun
=
GetVMFunction
(
id
)
;
MOZ_ASSERT
(
fun
.
expectTailCall
=
=
TailCall
)
;
size_t
argSize
=
fun
.
explicitStackSlots
(
)
*
sizeof
(
void
*
)
;
EmitBaselineTailCallVM
(
code
masm
argSize
)
;
}
static
size_t
GetEnteredOffset
(
BaselineCacheIRStubKind
kind
)
{
switch
(
kind
)
{
case
BaselineCacheIRStubKind
:
:
Regular
:
return
ICCacheIR_Regular
:
:
offsetOfEnteredCount
(
)
;
case
BaselineCacheIRStubKind
:
:
Updated
:
return
ICCacheIR_Updated
:
:
offsetOfEnteredCount
(
)
;
case
BaselineCacheIRStubKind
:
:
Monitored
:
return
ICCacheIR_Monitored
:
:
offsetOfEnteredCount
(
)
;
}
MOZ_CRASH
(
"
unhandled
BaselineCacheIRStubKind
"
)
;
}
JitCode
*
BaselineCacheIRCompiler
:
:
compile
(
)
{
#
ifndef
JS_USE_LINK_REGISTER
masm
.
adjustFrame
(
sizeof
(
intptr_t
)
)
;
#
endif
#
ifdef
JS_CODEGEN_ARM
masm
.
setSecondScratchReg
(
BaselineSecondScratchReg
)
;
#
endif
Address
enteredCount
(
ICStubReg
GetEnteredOffset
(
kind_
)
)
;
masm
.
add32
(
Imm32
(
1
)
enteredCount
)
;
CacheIRReader
reader
(
writer_
)
;
do
{
switch
(
reader
.
readOp
(
)
)
{
#
define
DEFINE_OP
(
op
.
.
.
)
\
case
CacheOp
:
:
op
:
\
if
(
!
emit
#
#
op
(
reader
)
)
return
nullptr
;
\
break
;
CACHE_IR_OPS
(
DEFINE_OP
)
#
undef
DEFINE_OP
default
:
MOZ_CRASH
(
"
Invalid
op
"
)
;
}
allocator
.
nextOp
(
)
;
}
while
(
reader
.
more
(
)
)
;
MOZ_ASSERT
(
!
preparedForVMCall_
)
;
masm
.
assumeUnreachable
(
"
Should
have
returned
from
IC
"
)
;
for
(
size_t
i
=
0
;
i
<
failurePaths
.
length
(
)
;
i
+
+
)
{
if
(
!
emitFailurePath
(
i
)
)
{
return
nullptr
;
}
EmitStubGuardFailure
(
masm
)
;
}
Linker
linker
(
masm
)
;
Rooted
<
JitCode
*
>
newStubCode
(
cx_
linker
.
newCode
(
cx_
CodeKind
:
:
Baseline
)
)
;
if
(
!
newStubCode
)
{
cx_
-
>
recoverFromOutOfMemory
(
)
;
return
nullptr
;
}
return
newStubCode
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardShape
(
ObjOperandId
objId
uint32_t
shapeOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch1
(
allocator
masm
)
;
bool
needSpectreMitigations
=
objectGuardNeedsSpectreMitigations
(
objId
)
;
Maybe
<
AutoScratchRegister
>
maybeScratch2
;
if
(
needSpectreMitigations
)
{
maybeScratch2
.
emplace
(
allocator
masm
)
;
}
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
addr
(
stubAddress
(
shapeOffset
)
)
;
masm
.
loadPtr
(
addr
scratch1
)
;
if
(
needSpectreMitigations
)
{
masm
.
branchTestObjShape
(
Assembler
:
:
NotEqual
obj
scratch1
*
maybeScratch2
obj
failure
-
>
label
(
)
)
;
}
else
{
masm
.
branchTestObjShapeNoSpectreMitigations
(
Assembler
:
:
NotEqual
obj
scratch1
failure
-
>
label
(
)
)
;
}
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardGroup
(
ObjOperandId
objId
uint32_t
groupOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch1
(
allocator
masm
)
;
bool
needSpectreMitigations
=
objectGuardNeedsSpectreMitigations
(
objId
)
;
Maybe
<
AutoScratchRegister
>
maybeScratch2
;
if
(
needSpectreMitigations
)
{
maybeScratch2
.
emplace
(
allocator
masm
)
;
}
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
addr
(
stubAddress
(
groupOffset
)
)
;
masm
.
loadPtr
(
addr
scratch1
)
;
if
(
needSpectreMitigations
)
{
masm
.
branchTestObjGroup
(
Assembler
:
:
NotEqual
obj
scratch1
*
maybeScratch2
obj
failure
-
>
label
(
)
)
;
}
else
{
masm
.
branchTestObjGroupNoSpectreMitigations
(
Assembler
:
:
NotEqual
obj
scratch1
failure
-
>
label
(
)
)
;
}
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardProto
(
ObjOperandId
objId
uint32_t
protoOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
addr
(
stubAddress
(
protoOffset
)
)
;
masm
.
loadObjProto
(
obj
scratch
)
;
masm
.
branchPtr
(
Assembler
:
:
NotEqual
addr
scratch
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardCompartment
(
ObjOperandId
objId
uint32_t
globalOffset
uint32_t
compartmentOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
globalWrapper
(
stubAddress
(
globalOffset
)
)
;
masm
.
loadPtr
(
globalWrapper
scratch
)
;
Address
handlerAddr
(
scratch
ProxyObject
:
:
offsetOfHandler
(
)
)
;
masm
.
branchPtr
(
Assembler
:
:
Equal
handlerAddr
ImmPtr
(
&
DeadObjectProxy
:
:
singleton
)
failure
-
>
label
(
)
)
;
Address
addr
(
stubAddress
(
compartmentOffset
)
)
;
masm
.
branchTestObjCompartment
(
Assembler
:
:
NotEqual
obj
addr
scratch
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardAnyClass
(
ObjOperandId
objId
uint32_t
claspOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
testAddr
(
stubAddress
(
claspOffset
)
)
;
if
(
objectGuardNeedsSpectreMitigations
(
objId
)
)
{
masm
.
branchTestObjClass
(
Assembler
:
:
NotEqual
obj
testAddr
scratch
obj
failure
-
>
label
(
)
)
;
}
else
{
masm
.
branchTestObjClassNoSpectreMitigations
(
Assembler
:
:
NotEqual
obj
testAddr
scratch
failure
-
>
label
(
)
)
;
}
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardHasProxyHandler
(
ObjOperandId
objId
uint32_t
handlerOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
testAddr
(
stubAddress
(
handlerOffset
)
)
;
masm
.
loadPtr
(
testAddr
scratch
)
;
Address
handlerAddr
(
obj
ProxyObject
:
:
offsetOfHandler
(
)
)
;
masm
.
branchPtr
(
Assembler
:
:
NotEqual
handlerAddr
scratch
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardSpecificObject
(
ObjOperandId
objId
uint32_t
expectedOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
addr
(
stubAddress
(
expectedOffset
)
)
;
masm
.
branchPtr
(
Assembler
:
:
NotEqual
addr
obj
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardSpecificFunction
(
ObjOperandId
objId
uint32_t
expectedOffset
uint32_t
nargsAndFlagsOffset
)
{
return
emitGuardSpecificObject
(
objId
expectedOffset
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardFunctionScript
(
ObjOperandId
funId
uint32_t
expectedOffset
uint32_t
nargsAndFlagsOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
fun
=
allocator
.
useRegister
(
masm
funId
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
addr
(
stubAddress
(
expectedOffset
)
)
;
masm
.
loadPtr
(
Address
(
fun
JSFunction
:
:
offsetOfBaseScript
(
)
)
scratch
)
;
masm
.
branchPtr
(
Assembler
:
:
NotEqual
addr
scratch
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardSpecificAtom
(
StringOperandId
strId
uint32_t
expectedOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
str
=
allocator
.
useRegister
(
masm
strId
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
atomAddr
(
stubAddress
(
expectedOffset
)
)
;
Label
done
;
masm
.
branchPtr
(
Assembler
:
:
Equal
atomAddr
str
&
done
)
;
masm
.
branchTest32
(
Assembler
:
:
NonZero
Address
(
str
JSString
:
:
offsetOfFlags
(
)
)
Imm32
(
JSString
:
:
ATOM_BIT
)
failure
-
>
label
(
)
)
;
masm
.
loadPtr
(
atomAddr
scratch
)
;
masm
.
loadStringLength
(
scratch
scratch
)
;
masm
.
branch32
(
Assembler
:
:
NotEqual
Address
(
str
JSString
:
:
offsetOfLength
(
)
)
scratch
failure
-
>
label
(
)
)
;
LiveRegisterSet
volatileRegs
(
GeneralRegisterSet
:
:
Volatile
(
)
liveVolatileFloatRegs
(
)
)
;
masm
.
PushRegsInMask
(
volatileRegs
)
;
masm
.
setupUnalignedABICall
(
scratch
)
;
masm
.
loadPtr
(
atomAddr
scratch
)
;
masm
.
passABIArg
(
scratch
)
;
masm
.
passABIArg
(
str
)
;
masm
.
callWithABI
(
JS_FUNC_TO_DATA_PTR
(
void
*
EqualStringsHelperPure
)
)
;
masm
.
mov
(
ReturnReg
scratch
)
;
LiveRegisterSet
ignore
;
ignore
.
add
(
scratch
)
;
masm
.
PopRegsInMaskIgnore
(
volatileRegs
ignore
)
;
masm
.
branchIfFalseBool
(
scratch
failure
-
>
label
(
)
)
;
masm
.
bind
(
&
done
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardSpecificSymbol
(
SymbolOperandId
symId
uint32_t
expectedOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
sym
=
allocator
.
useRegister
(
masm
symId
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Address
addr
(
stubAddress
(
expectedOffset
)
)
;
masm
.
branchPtr
(
Assembler
:
:
NotEqual
addr
sym
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadValueResult
(
uint32_t
valOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
masm
.
loadValue
(
stubAddress
(
valOffset
)
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadFixedSlotResult
(
ObjOperandId
objId
uint32_t
offsetOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
masm
.
load32
(
stubAddress
(
offsetOffset
)
scratch
)
;
masm
.
loadValue
(
BaseIndex
(
obj
scratch
TimesOne
)
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadFixedSlotTypedResult
(
ObjOperandId
objId
uint32_t
offsetOffset
ValueType
)
{
return
emitLoadFixedSlotResult
(
objId
offsetOffset
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadDynamicSlotResult
(
ObjOperandId
objId
uint32_t
offsetOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
masm
.
load32
(
stubAddress
(
offsetOffset
)
scratch
)
;
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfSlots
(
)
)
scratch2
)
;
masm
.
loadValue
(
BaseIndex
(
scratch2
scratch
TimesOne
)
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardHasGetterSetter
(
ObjOperandId
objId
uint32_t
shapeOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
Address
shapeAddr
=
stubAddress
(
shapeOffset
)
;
AutoScratchRegister
scratch1
(
allocator
masm
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
LiveRegisterSet
volatileRegs
(
GeneralRegisterSet
:
:
Volatile
(
)
liveVolatileFloatRegs
(
)
)
;
volatileRegs
.
takeUnchecked
(
scratch1
)
;
volatileRegs
.
takeUnchecked
(
scratch2
)
;
masm
.
PushRegsInMask
(
volatileRegs
)
;
masm
.
setupUnalignedABICall
(
scratch1
)
;
masm
.
loadJSContext
(
scratch1
)
;
masm
.
passABIArg
(
scratch1
)
;
masm
.
passABIArg
(
obj
)
;
masm
.
loadPtr
(
shapeAddr
scratch2
)
;
masm
.
passABIArg
(
scratch2
)
;
masm
.
callWithABI
(
JS_FUNC_TO_DATA_PTR
(
void
*
ObjectHasGetterSetterPure
)
)
;
masm
.
mov
(
ReturnReg
scratch1
)
;
masm
.
PopRegsInMask
(
volatileRegs
)
;
masm
.
branchIfFalseBool
(
scratch1
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallScriptedGetterResult
(
ValOperandId
receiverId
uint32_t
getterOffset
bool
sameRealm
uint32_t
nargsAndFlagsOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
ValueOperand
receiver
=
allocator
.
useValueRegister
(
masm
receiverId
)
;
Address
getterAddr
(
stubAddress
(
getterOffset
)
)
;
AutoScratchRegister
code
(
allocator
masm
)
;
AutoScratchRegister
callee
(
allocator
masm
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
masm
.
loadPtr
(
getterAddr
callee
)
;
masm
.
loadJitCodeRaw
(
callee
code
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
if
(
!
sameRealm
)
{
masm
.
switchToObjectRealm
(
callee
scratch
)
;
}
masm
.
alignJitStackBasedOnNArgs
(
0
)
;
masm
.
Push
(
receiver
)
;
EmitBaselineCreateStubFrameDescriptor
(
masm
scratch
JitFrameLayout
:
:
Size
(
)
)
;
masm
.
Push
(
Imm32
(
0
)
)
;
masm
.
Push
(
callee
)
;
masm
.
Push
(
scratch
)
;
Label
noUnderflow
;
masm
.
load16ZeroExtend
(
Address
(
callee
JSFunction
:
:
offsetOfNargs
(
)
)
callee
)
;
masm
.
branch32
(
Assembler
:
:
Equal
callee
Imm32
(
0
)
&
noUnderflow
)
;
{
TrampolinePtr
argumentsRectifier
=
cx_
-
>
runtime
(
)
-
>
jitRuntime
(
)
-
>
getArgumentsRectifier
(
)
;
masm
.
movePtr
(
argumentsRectifier
code
)
;
}
masm
.
bind
(
&
noUnderflow
)
;
masm
.
callJit
(
code
)
;
stubFrame
.
leave
(
masm
true
)
;
if
(
!
sameRealm
)
{
masm
.
switchToBaselineFrameRealm
(
R1
.
scratchReg
(
)
)
;
}
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallNativeGetterResult
(
ValOperandId
receiverId
uint32_t
getterOffset
bool
sameRealm
uint32_t
nargsAndFlagsOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
ValueOperand
receiver
=
allocator
.
useValueRegister
(
masm
receiverId
)
;
Address
getterAddr
(
stubAddress
(
getterOffset
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
loadPtr
(
getterAddr
scratch
)
;
masm
.
Push
(
receiver
)
;
masm
.
Push
(
scratch
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleFunction
HandleValue
MutableHandleValue
)
;
callVM
<
Fn
CallNativeGetter
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallDOMGetterResult
(
ObjOperandId
objId
uint32_t
jitInfoOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
Address
jitInfoAddr
(
stubAddress
(
jitInfoOffset
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
loadPtr
(
jitInfoAddr
scratch
)
;
masm
.
Push
(
obj
)
;
masm
.
Push
(
scratch
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
const
JSJitInfo
*
HandleObject
MutableHandleValue
)
;
callVM
<
Fn
CallDOMGetter
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitProxyGetResult
(
ObjOperandId
objId
uint32_t
idOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
Address
idAddr
(
stubAddress
(
idOffset
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
loadPtr
(
idAddr
scratch
)
;
masm
.
Push
(
scratch
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleObject
HandleId
MutableHandleValue
)
;
callVM
<
Fn
ProxyGetProperty
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardFrameHasNoArgumentsObject
(
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
branchTest32
(
Assembler
:
:
NonZero
Address
(
BaselineFrameReg
BaselineFrame
:
:
reverseOffsetOfFlags
(
)
)
Imm32
(
BaselineFrame
:
:
HAS_ARGS_OBJ
)
failure
-
>
label
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadFrameCalleeResult
(
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
Address
callee
(
BaselineFrameReg
BaselineFrame
:
:
offsetOfCalleeToken
(
)
)
;
masm
.
loadFunctionFromCalleeToken
(
callee
scratch
)
;
masm
.
tagValue
(
JSVAL_TYPE_OBJECT
scratch
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadFrameNumActualArgsResult
(
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
Address
actualArgs
(
BaselineFrameReg
BaselineFrame
:
:
offsetOfNumActualArgs
(
)
)
;
masm
.
loadPtr
(
actualArgs
scratch
)
;
masm
.
tagValue
(
JSVAL_TYPE_INT32
scratch
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadFrameArgumentResult
(
Int32OperandId
indexId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
index
=
allocator
.
useRegister
(
masm
indexId
)
;
AutoScratchRegister
scratch1
(
allocator
masm
)
;
AutoScratchRegisterMaybeOutput
scratch2
(
allocator
masm
output
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
BaselineFrameReg
BaselineFrame
:
:
offsetOfNumActualArgs
(
)
)
scratch1
)
;
masm
.
spectreBoundsCheck32
(
index
scratch1
scratch2
failure
-
>
label
(
)
)
;
masm
.
loadValue
(
BaseValueIndex
(
BaselineFrameReg
index
BaselineFrame
:
:
offsetOfArg
(
0
)
)
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitFrameIsConstructingResult
(
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
outputScratch
=
output
.
valueReg
(
)
.
scratchReg
(
)
;
Address
tokenAddr
(
BaselineFrameReg
BaselineFrame
:
:
offsetOfCalleeToken
(
)
)
;
masm
.
loadPtr
(
tokenAddr
outputScratch
)
;
static_assert
(
CalleeToken_Function
=
=
0x0
)
;
static_assert
(
CalleeToken_FunctionConstructing
=
=
0x1
)
;
masm
.
andPtr
(
Imm32
(
0x1
)
outputScratch
)
;
masm
.
tagValue
(
JSVAL_TYPE_BOOLEAN
outputScratch
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadEnvironmentFixedSlotResult
(
ObjOperandId
objId
uint32_t
offsetOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
load32
(
stubAddress
(
offsetOffset
)
scratch
)
;
BaseIndex
slot
(
obj
scratch
TimesOne
)
;
masm
.
branchTestMagic
(
Assembler
:
:
Equal
slot
failure
-
>
label
(
)
)
;
masm
.
loadValue
(
slot
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadEnvironmentDynamicSlotResult
(
ObjOperandId
objId
uint32_t
offsetOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
AutoScratchRegisterMaybeOutput
scratch2
(
allocator
masm
output
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
load32
(
stubAddress
(
offsetOffset
)
scratch
)
;
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfSlots
(
)
)
scratch2
)
;
BaseIndex
slot
(
scratch2
scratch
TimesOne
)
;
masm
.
branchTestMagic
(
Assembler
:
:
Equal
slot
failure
-
>
label
(
)
)
;
masm
.
loadValue
(
slot
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadConstantStringResult
(
uint32_t
strOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
masm
.
loadPtr
(
stubAddress
(
strOffset
)
scratch
)
;
masm
.
tagValue
(
JSVAL_TYPE_STRING
scratch
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCompareStringResult
(
JSOp
op
StringOperandId
lhsId
StringOperandId
rhsId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
left
=
allocator
.
useRegister
(
masm
lhsId
)
;
Register
right
=
allocator
.
useRegister
(
masm
rhsId
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
allocator
.
discardStack
(
masm
)
;
Label
slow
done
;
masm
.
compareStrings
(
op
left
right
scratch
&
slow
)
;
masm
.
jump
(
&
done
)
;
masm
.
bind
(
&
slow
)
;
{
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
if
(
op
=
=
JSOp
:
:
Le
|
|
op
=
=
JSOp
:
:
Gt
)
{
masm
.
Push
(
left
)
;
masm
.
Push
(
right
)
;
}
else
{
masm
.
Push
(
right
)
;
masm
.
Push
(
left
)
;
}
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleString
HandleString
bool
*
)
;
if
(
op
=
=
JSOp
:
:
Eq
|
|
op
=
=
JSOp
:
:
StrictEq
)
{
callVM
<
Fn
jit
:
:
StringsEqual
<
EqualityKind
:
:
Equal
>
>
(
masm
)
;
}
else
if
(
op
=
=
JSOp
:
:
Ne
|
|
op
=
=
JSOp
:
:
StrictNe
)
{
callVM
<
Fn
jit
:
:
StringsEqual
<
EqualityKind
:
:
NotEqual
>
>
(
masm
)
;
}
else
if
(
op
=
=
JSOp
:
:
Lt
|
|
op
=
=
JSOp
:
:
Gt
)
{
callVM
<
Fn
jit
:
:
StringsCompare
<
ComparisonKind
:
:
LessThan
>
>
(
masm
)
;
}
else
{
MOZ_ASSERT
(
op
=
=
JSOp
:
:
Le
|
|
op
=
=
JSOp
:
:
Ge
)
;
callVM
<
Fn
jit
:
:
StringsCompare
<
ComparisonKind
:
:
GreaterThanOrEqual
>
>
(
masm
)
;
}
stubFrame
.
leave
(
masm
)
;
masm
.
mov
(
ReturnReg
scratch
)
;
}
masm
.
bind
(
&
done
)
;
masm
.
tagValue
(
JSVAL_TYPE_BOOLEAN
scratch
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
callTypeUpdateIC
(
Register
obj
ValueOperand
val
Register
scratch
LiveGeneralRegisterSet
saveRegs
)
{
allocator
.
discardStack
(
masm
)
;
if
(
!
IsTypeInferenceEnabled
(
)
)
{
return
true
;
}
MOZ_ASSERT
(
val
=
=
R0
)
;
MOZ_ASSERT
(
scratch
=
=
R1
.
scratchReg
(
)
)
;
#
if
defined
(
JS_CODEGEN_X86
)
|
|
defined
(
JS_CODEGEN_X64
)
static
const
bool
CallClobbersTailReg
=
false
;
#
else
static
const
bool
CallClobbersTailReg
=
true
;
#
endif
if
(
CallClobbersTailReg
)
{
masm
.
push
(
ICTailCallReg
)
;
}
masm
.
push
(
ICStubReg
)
;
masm
.
loadPtr
(
Address
(
ICStubReg
ICCacheIR_Updated
:
:
offsetOfFirstUpdateStub
(
)
)
ICStubReg
)
;
masm
.
call
(
Address
(
ICStubReg
ICStub
:
:
offsetOfStubCode
(
)
)
)
;
masm
.
pop
(
ICStubReg
)
;
if
(
CallClobbersTailReg
)
{
masm
.
pop
(
ICTailCallReg
)
;
}
Label
done
;
masm
.
branch32
(
Assembler
:
:
Equal
scratch
Imm32
(
1
)
&
done
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
CallCanGC
:
:
CanNotGC
)
;
masm
.
PushRegsInMask
(
saveRegs
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
TypedOrValueRegister
(
MIRType
:
:
Object
AnyRegister
(
obj
)
)
)
;
masm
.
Push
(
ICStubReg
)
;
masm
.
loadPtr
(
Address
(
BaselineFrameReg
0
)
scratch
)
;
masm
.
pushBaselineFramePtr
(
scratch
scratch
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
BaselineFrame
*
ICCacheIR_Updated
*
HandleValue
HandleValue
)
;
callVM
<
Fn
DoTypeUpdateFallback
>
(
masm
)
;
masm
.
PopRegsInMask
(
saveRegs
)
;
stubFrame
.
leave
(
masm
)
;
masm
.
bind
(
&
done
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitStoreSlotShared
(
bool
isFixed
ObjOperandId
objId
uint32_t
offsetOffset
ValOperandId
rhsId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Address
offsetAddr
=
stubAddress
(
offsetOffset
)
;
AutoScratchRegister
scratch1
(
allocator
masm
R1
.
scratchReg
(
)
)
;
ValueOperand
val
=
allocator
.
useFixedValueRegister
(
masm
rhsId
R0
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
Maybe
<
AutoScratchRegister
>
scratch2
;
if
(
!
isFixed
)
{
scratch2
.
emplace
(
allocator
masm
)
;
}
LiveGeneralRegisterSet
saveRegs
;
saveRegs
.
add
(
obj
)
;
saveRegs
.
add
(
val
)
;
if
(
!
callTypeUpdateIC
(
obj
val
scratch1
saveRegs
)
)
{
return
false
;
}
masm
.
load32
(
offsetAddr
scratch1
)
;
if
(
isFixed
)
{
BaseIndex
slot
(
obj
scratch1
TimesOne
)
;
EmitPreBarrier
(
masm
slot
MIRType
:
:
Value
)
;
masm
.
storeValue
(
val
slot
)
;
}
else
{
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfSlots
(
)
)
scratch2
.
ref
(
)
)
;
BaseIndex
slot
(
scratch2
.
ref
(
)
scratch1
TimesOne
)
;
EmitPreBarrier
(
masm
slot
MIRType
:
:
Value
)
;
masm
.
storeValue
(
val
slot
)
;
}
emitPostBarrierSlot
(
obj
val
scratch1
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitStoreFixedSlot
(
ObjOperandId
objId
uint32_t
offsetOffset
ValOperandId
rhsId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
return
emitStoreSlotShared
(
true
objId
offsetOffset
rhsId
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitStoreDynamicSlot
(
ObjOperandId
objId
uint32_t
offsetOffset
ValOperandId
rhsId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
return
emitStoreSlotShared
(
false
objId
offsetOffset
rhsId
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitAddAndStoreSlotShared
(
CacheOp
op
ObjOperandId
objId
uint32_t
offsetOffset
ValOperandId
rhsId
bool
changeGroup
uint32_t
newGroupOffset
uint32_t
newShapeOffset
Maybe
<
uint32_t
>
numNewSlotsOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Address
offsetAddr
=
stubAddress
(
offsetOffset
)
;
AutoScratchRegister
scratch1
(
allocator
masm
R1
.
scratchReg
(
)
)
;
ValueOperand
val
=
allocator
.
useFixedValueRegister
(
masm
rhsId
R0
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
Address
newGroupAddr
=
stubAddress
(
newGroupOffset
)
;
Address
newShapeAddr
=
stubAddress
(
newShapeOffset
)
;
if
(
op
=
=
CacheOp
:
:
AllocateAndStoreDynamicSlot
)
{
Address
numNewSlotsAddr
=
stubAddress
(
*
numNewSlotsOffset
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
LiveRegisterSet
save
(
GeneralRegisterSet
:
:
Volatile
(
)
liveVolatileFloatRegs
(
)
)
;
masm
.
PushRegsInMask
(
save
)
;
masm
.
setupUnalignedABICall
(
scratch1
)
;
masm
.
loadJSContext
(
scratch1
)
;
masm
.
passABIArg
(
scratch1
)
;
masm
.
passABIArg
(
obj
)
;
masm
.
load32
(
numNewSlotsAddr
scratch2
)
;
masm
.
passABIArg
(
scratch2
)
;
masm
.
callWithABI
(
JS_FUNC_TO_DATA_PTR
(
void
*
NativeObject
:
:
growSlotsPure
)
)
;
masm
.
mov
(
ReturnReg
scratch1
)
;
LiveRegisterSet
ignore
;
ignore
.
add
(
scratch1
)
;
masm
.
PopRegsInMaskIgnore
(
save
ignore
)
;
masm
.
branchIfFalseBool
(
scratch1
failure
-
>
label
(
)
)
;
}
LiveGeneralRegisterSet
saveRegs
;
saveRegs
.
add
(
obj
)
;
saveRegs
.
add
(
val
)
;
if
(
!
callTypeUpdateIC
(
obj
val
scratch1
saveRegs
)
)
{
return
false
;
}
if
(
changeGroup
)
{
Label
noGroupChange
;
masm
.
branchIfObjGroupHasNoAddendum
(
obj
scratch1
&
noGroupChange
)
;
masm
.
loadPtr
(
newGroupAddr
scratch1
)
;
masm
.
storeObjGroup
(
scratch1
obj
[
]
(
MacroAssembler
&
masm
const
Address
&
addr
)
{
EmitPreBarrier
(
masm
addr
MIRType
:
:
ObjectGroup
)
;
}
)
;
masm
.
bind
(
&
noGroupChange
)
;
}
masm
.
loadPtr
(
newShapeAddr
scratch1
)
;
masm
.
storeObjShape
(
scratch1
obj
[
]
(
MacroAssembler
&
masm
const
Address
&
addr
)
{
EmitPreBarrier
(
masm
addr
MIRType
:
:
Shape
)
;
}
)
;
masm
.
load32
(
offsetAddr
scratch1
)
;
if
(
op
=
=
CacheOp
:
:
AddAndStoreFixedSlot
)
{
BaseIndex
slot
(
obj
scratch1
TimesOne
)
;
masm
.
storeValue
(
val
slot
)
;
}
else
{
MOZ_ASSERT
(
op
=
=
CacheOp
:
:
AddAndStoreDynamicSlot
|
|
op
=
=
CacheOp
:
:
AllocateAndStoreDynamicSlot
)
;
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfSlots
(
)
)
scratch2
)
;
BaseIndex
slot
(
scratch2
scratch1
TimesOne
)
;
masm
.
storeValue
(
val
slot
)
;
}
emitPostBarrierSlot
(
obj
val
scratch1
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitAddAndStoreFixedSlot
(
ObjOperandId
objId
uint32_t
offsetOffset
ValOperandId
rhsId
bool
changeGroup
uint32_t
newGroupOffset
uint32_t
newShapeOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Maybe
<
uint32_t
>
numNewSlotsOffset
=
mozilla
:
:
Nothing
(
)
;
return
emitAddAndStoreSlotShared
(
CacheOp
:
:
AddAndStoreFixedSlot
objId
offsetOffset
rhsId
changeGroup
newGroupOffset
newShapeOffset
numNewSlotsOffset
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitAddAndStoreDynamicSlot
(
ObjOperandId
objId
uint32_t
offsetOffset
ValOperandId
rhsId
bool
changeGroup
uint32_t
newGroupOffset
uint32_t
newShapeOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Maybe
<
uint32_t
>
numNewSlotsOffset
=
mozilla
:
:
Nothing
(
)
;
return
emitAddAndStoreSlotShared
(
CacheOp
:
:
AddAndStoreDynamicSlot
objId
offsetOffset
rhsId
changeGroup
newGroupOffset
newShapeOffset
numNewSlotsOffset
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitAllocateAndStoreDynamicSlot
(
ObjOperandId
objId
uint32_t
offsetOffset
ValOperandId
rhsId
bool
changeGroup
uint32_t
newGroupOffset
uint32_t
newShapeOffset
uint32_t
numNewSlotsOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
return
emitAddAndStoreSlotShared
(
CacheOp
:
:
AllocateAndStoreDynamicSlot
objId
offsetOffset
rhsId
changeGroup
newGroupOffset
newShapeOffset
mozilla
:
:
Some
(
numNewSlotsOffset
)
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitStoreTypedObjectReferenceProperty
(
ObjOperandId
objId
uint32_t
offsetOffset
TypedThingLayout
layout
ReferenceType
type
ValOperandId
rhsId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Address
offsetAddr
=
stubAddress
(
offsetOffset
)
;
AutoScratchRegister
scratch1
(
allocator
masm
R1
.
scratchReg
(
)
)
;
ValueOperand
val
=
allocator
.
useFixedValueRegister
(
masm
rhsId
R0
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
if
(
type
!
=
ReferenceType
:
:
TYPE_STRING
)
{
LiveGeneralRegisterSet
saveRegs
;
saveRegs
.
add
(
obj
)
;
saveRegs
.
add
(
val
)
;
if
(
!
callTypeUpdateIC
(
obj
val
scratch1
saveRegs
)
)
{
return
false
;
}
}
LoadTypedThingData
(
masm
layout
obj
scratch1
)
;
masm
.
addPtr
(
offsetAddr
scratch1
)
;
Address
dest
(
scratch1
0
)
;
emitStoreTypedObjectReferenceProp
(
val
type
dest
scratch2
)
;
emitPostBarrierSlot
(
obj
val
scratch1
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitStoreDenseElement
(
ObjOperandId
objId
Int32OperandId
indexId
ValOperandId
rhsId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoScratchRegister
scratch
(
allocator
masm
R1
.
scratchReg
(
)
)
;
ValueOperand
val
=
allocator
.
useFixedValueRegister
(
masm
rhsId
R0
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
Register
index
=
allocator
.
useRegister
(
masm
indexId
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
Register
spectreTemp
=
InvalidReg
;
Address
initLength
(
scratch
ObjectElements
:
:
offsetOfInitializedLength
(
)
)
;
masm
.
spectreBoundsCheck32
(
index
initLength
spectreTemp
failure
-
>
label
(
)
)
;
BaseObjectElementIndex
element
(
scratch
index
)
;
masm
.
branchTestMagic
(
Assembler
:
:
Equal
element
failure
-
>
label
(
)
)
;
if
(
IsTypeInferenceEnabled
(
)
)
{
Label
noSpecialHandling
;
Address
elementsFlags
(
scratch
ObjectElements
:
:
offsetOfFlags
(
)
)
;
masm
.
branchTest32
(
Assembler
:
:
Zero
elementsFlags
Imm32
(
ObjectElements
:
:
CONVERT_DOUBLE_ELEMENTS
|
ObjectElements
:
:
COPY_ON_WRITE
)
&
noSpecialHandling
)
;
masm
.
branchTest32
(
Assembler
:
:
NonZero
elementsFlags
Imm32
(
ObjectElements
:
:
COPY_ON_WRITE
)
failure
-
>
label
(
)
)
;
masm
.
convertInt32ValueToDouble
(
val
)
;
masm
.
bind
(
&
noSpecialHandling
)
;
}
LiveGeneralRegisterSet
saveRegs
;
saveRegs
.
add
(
obj
)
;
saveRegs
.
add
(
index
)
;
saveRegs
.
add
(
val
)
;
if
(
!
callTypeUpdateIC
(
obj
val
scratch
saveRegs
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
EmitPreBarrier
(
masm
element
MIRType
:
:
Value
)
;
masm
.
storeValue
(
val
element
)
;
emitPostBarrierElement
(
obj
val
scratch
index
)
;
return
true
;
}
static
void
EmitAssertExtensibleElements
(
MacroAssembler
&
masm
Register
elementsReg
)
{
#
ifdef
DEBUG
Address
elementsFlags
(
elementsReg
ObjectElements
:
:
offsetOfFlags
(
)
)
;
Label
ok
;
masm
.
branchTest32
(
Assembler
:
:
Zero
elementsFlags
Imm32
(
ObjectElements
:
:
Flags
:
:
NOT_EXTENSIBLE
)
&
ok
)
;
masm
.
assumeUnreachable
(
"
Unexpected
non
-
extensible
elements
"
)
;
masm
.
bind
(
&
ok
)
;
#
endif
}
static
void
EmitAssertWritableArrayLengthElements
(
MacroAssembler
&
masm
Register
elementsReg
)
{
#
ifdef
DEBUG
Address
elementsFlags
(
elementsReg
ObjectElements
:
:
offsetOfFlags
(
)
)
;
Label
ok
;
masm
.
branchTest32
(
Assembler
:
:
Zero
elementsFlags
Imm32
(
ObjectElements
:
:
Flags
:
:
NONWRITABLE_ARRAY_LENGTH
)
&
ok
)
;
masm
.
assumeUnreachable
(
"
Unexpected
non
-
writable
array
length
elements
"
)
;
masm
.
bind
(
&
ok
)
;
#
endif
}
bool
BaselineCacheIRCompiler
:
:
emitStoreDenseElementHole
(
ObjOperandId
objId
Int32OperandId
indexId
ValOperandId
rhsId
bool
handleAdd
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoScratchRegister
scratch
(
allocator
masm
R1
.
scratchReg
(
)
)
;
ValueOperand
val
=
allocator
.
useFixedValueRegister
(
masm
rhsId
R0
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
Register
index
=
allocator
.
useRegister
(
masm
indexId
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
EmitAssertExtensibleElements
(
masm
scratch
)
;
if
(
handleAdd
)
{
EmitAssertWritableArrayLengthElements
(
masm
scratch
)
;
}
BaseObjectElementIndex
element
(
scratch
index
)
;
Address
initLength
(
scratch
ObjectElements
:
:
offsetOfInitializedLength
(
)
)
;
Address
elementsFlags
(
scratch
ObjectElements
:
:
offsetOfFlags
(
)
)
;
if
(
IsTypeInferenceEnabled
(
)
)
{
masm
.
branchTest32
(
Assembler
:
:
NonZero
elementsFlags
Imm32
(
ObjectElements
:
:
COPY_ON_WRITE
)
failure
-
>
label
(
)
)
;
}
Register
spectreTemp
=
InvalidReg
;
if
(
handleAdd
)
{
Label
capacityOk
outOfBounds
;
masm
.
spectreBoundsCheck32
(
index
initLength
spectreTemp
&
outOfBounds
)
;
masm
.
jump
(
&
capacityOk
)
;
masm
.
bind
(
&
outOfBounds
)
;
masm
.
branch32
(
Assembler
:
:
NotEqual
initLength
index
failure
-
>
label
(
)
)
;
Label
allocElement
;
Address
capacity
(
scratch
ObjectElements
:
:
offsetOfCapacity
(
)
)
;
masm
.
spectreBoundsCheck32
(
index
capacity
spectreTemp
&
allocElement
)
;
masm
.
jump
(
&
capacityOk
)
;
masm
.
bind
(
&
allocElement
)
;
LiveRegisterSet
save
(
GeneralRegisterSet
:
:
Volatile
(
)
liveVolatileFloatRegs
(
)
)
;
save
.
takeUnchecked
(
scratch
)
;
masm
.
PushRegsInMask
(
save
)
;
masm
.
setupUnalignedABICall
(
scratch
)
;
masm
.
loadJSContext
(
scratch
)
;
masm
.
passABIArg
(
scratch
)
;
masm
.
passABIArg
(
obj
)
;
masm
.
callWithABI
(
JS_FUNC_TO_DATA_PTR
(
void
*
NativeObject
:
:
addDenseElementPure
)
)
;
masm
.
mov
(
ReturnReg
scratch
)
;
masm
.
PopRegsInMask
(
save
)
;
masm
.
branchIfFalseBool
(
scratch
failure
-
>
label
(
)
)
;
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
masm
.
bind
(
&
capacityOk
)
;
}
else
{
masm
.
spectreBoundsCheck32
(
index
initLength
spectreTemp
failure
-
>
label
(
)
)
;
}
if
(
IsTypeInferenceEnabled
(
)
)
{
Label
noConversion
;
masm
.
branchTest32
(
Assembler
:
:
Zero
elementsFlags
Imm32
(
ObjectElements
:
:
CONVERT_DOUBLE_ELEMENTS
)
&
noConversion
)
;
masm
.
convertInt32ValueToDouble
(
val
)
;
masm
.
bind
(
&
noConversion
)
;
}
LiveGeneralRegisterSet
saveRegs
;
saveRegs
.
add
(
obj
)
;
saveRegs
.
add
(
index
)
;
saveRegs
.
add
(
val
)
;
if
(
!
callTypeUpdateIC
(
obj
val
scratch
saveRegs
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
Label
doStore
;
if
(
handleAdd
)
{
Label
inBounds
;
masm
.
branch32
(
Assembler
:
:
NotEqual
initLength
index
&
inBounds
)
;
masm
.
add32
(
Imm32
(
1
)
initLength
)
;
Label
skipIncrementLength
;
Address
length
(
scratch
ObjectElements
:
:
offsetOfLength
(
)
)
;
masm
.
branch32
(
Assembler
:
:
Above
length
index
&
skipIncrementLength
)
;
masm
.
add32
(
Imm32
(
1
)
length
)
;
masm
.
bind
(
&
skipIncrementLength
)
;
masm
.
jump
(
&
doStore
)
;
masm
.
bind
(
&
inBounds
)
;
}
EmitPreBarrier
(
masm
element
MIRType
:
:
Value
)
;
masm
.
bind
(
&
doStore
)
;
masm
.
storeValue
(
val
element
)
;
emitPostBarrierElement
(
obj
val
scratch
index
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitArrayPush
(
ObjOperandId
objId
ValOperandId
rhsId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoScratchRegister
scratch
(
allocator
masm
R1
.
scratchReg
(
)
)
;
ValueOperand
val
=
allocator
.
useFixedValueRegister
(
masm
rhsId
R0
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratchLength
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
EmitAssertExtensibleElements
(
masm
scratch
)
;
EmitAssertWritableArrayLengthElements
(
masm
scratch
)
;
Address
elementsInitLength
(
scratch
ObjectElements
:
:
offsetOfInitializedLength
(
)
)
;
Address
elementsLength
(
scratch
ObjectElements
:
:
offsetOfLength
(
)
)
;
Address
elementsFlags
(
scratch
ObjectElements
:
:
offsetOfFlags
(
)
)
;
if
(
IsTypeInferenceEnabled
(
)
)
{
masm
.
branchTest32
(
Assembler
:
:
NonZero
elementsFlags
Imm32
(
ObjectElements
:
:
COPY_ON_WRITE
)
failure
-
>
label
(
)
)
;
}
masm
.
load32
(
elementsInitLength
scratchLength
)
;
masm
.
branch32
(
Assembler
:
:
NotEqual
elementsLength
scratchLength
failure
-
>
label
(
)
)
;
Label
capacityOk
allocElement
;
Address
capacity
(
scratch
ObjectElements
:
:
offsetOfCapacity
(
)
)
;
masm
.
spectreBoundsCheck32
(
scratchLength
capacity
InvalidReg
&
allocElement
)
;
masm
.
jump
(
&
capacityOk
)
;
masm
.
bind
(
&
allocElement
)
;
LiveRegisterSet
save
(
GeneralRegisterSet
:
:
Volatile
(
)
liveVolatileFloatRegs
(
)
)
;
save
.
takeUnchecked
(
scratch
)
;
masm
.
PushRegsInMask
(
save
)
;
masm
.
setupUnalignedABICall
(
scratch
)
;
masm
.
loadJSContext
(
scratch
)
;
masm
.
passABIArg
(
scratch
)
;
masm
.
passABIArg
(
obj
)
;
masm
.
callWithABI
(
JS_FUNC_TO_DATA_PTR
(
void
*
NativeObject
:
:
addDenseElementPure
)
)
;
masm
.
mov
(
ReturnReg
scratch
)
;
masm
.
PopRegsInMask
(
save
)
;
masm
.
branchIfFalseBool
(
scratch
failure
-
>
label
(
)
)
;
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
masm
.
bind
(
&
capacityOk
)
;
if
(
IsTypeInferenceEnabled
(
)
)
{
Label
noConversion
;
masm
.
branchTest32
(
Assembler
:
:
Zero
elementsFlags
Imm32
(
ObjectElements
:
:
CONVERT_DOUBLE_ELEMENTS
)
&
noConversion
)
;
masm
.
convertInt32ValueToDouble
(
val
)
;
masm
.
bind
(
&
noConversion
)
;
}
LiveGeneralRegisterSet
saveRegs
;
saveRegs
.
add
(
obj
)
;
saveRegs
.
add
(
val
)
;
if
(
!
callTypeUpdateIC
(
obj
val
scratch
saveRegs
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
masm
.
add32
(
Imm32
(
1
)
elementsInitLength
)
;
masm
.
load32
(
elementsLength
scratchLength
)
;
masm
.
add32
(
Imm32
(
1
)
elementsLength
)
;
BaseObjectElementIndex
element
(
scratch
scratchLength
)
;
masm
.
storeValue
(
val
element
)
;
emitPostBarrierElement
(
obj
val
scratch
scratchLength
)
;
masm
.
add32
(
Imm32
(
1
)
scratchLength
)
;
masm
.
tagValue
(
JSVAL_TYPE_INT32
scratchLength
val
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitArrayJoinResult
(
ObjOperandId
objId
StringOperandId
sepId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
Register
sep
=
allocator
.
useRegister
(
masm
sepId
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
allocator
.
discardStack
(
masm
)
;
masm
.
loadPtr
(
Address
(
obj
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
Address
lengthAddr
(
scratch
ObjectElements
:
:
offsetOfLength
(
)
)
;
Label
finished
;
{
Label
arrayNotEmpty
;
masm
.
branch32
(
Assembler
:
:
NotEqual
lengthAddr
Imm32
(
0
)
&
arrayNotEmpty
)
;
masm
.
movePtr
(
ImmGCPtr
(
cx_
-
>
names
(
)
.
empty
)
scratch
)
;
masm
.
tagValue
(
JSVAL_TYPE_STRING
scratch
output
.
valueReg
(
)
)
;
masm
.
jump
(
&
finished
)
;
masm
.
bind
(
&
arrayNotEmpty
)
;
}
Label
vmCall
;
masm
.
branch32
(
Assembler
:
:
NotEqual
lengthAddr
Imm32
(
1
)
&
vmCall
)
;
Address
initLength
(
scratch
ObjectElements
:
:
offsetOfInitializedLength
(
)
)
;
masm
.
branch32
(
Assembler
:
:
NotEqual
initLength
Imm32
(
1
)
&
vmCall
)
;
Address
elementAddr
(
scratch
0
)
;
masm
.
branchTestString
(
Assembler
:
:
NotEqual
elementAddr
&
vmCall
)
;
masm
.
loadValue
(
elementAddr
output
.
valueReg
(
)
)
;
masm
.
jump
(
&
finished
)
;
{
masm
.
bind
(
&
vmCall
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
Push
(
sep
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
JSString
*
(
*
)
(
JSContext
*
HandleObject
HandleString
)
;
callVM
<
Fn
jit
:
:
ArrayJoin
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
masm
.
tagValue
(
JSVAL_TYPE_STRING
ReturnReg
output
.
valueReg
(
)
)
;
}
masm
.
bind
(
&
finished
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitPackedArraySliceResult
(
uint32_t
templateObjectOffset
ObjOperandId
arrayId
Int32OperandId
beginId
Int32OperandId
endId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch1
(
allocator
masm
output
)
;
AutoScratchRegisterMaybeOutputType
scratch2
(
allocator
masm
output
)
;
Register
array
=
allocator
.
useRegister
(
masm
arrayId
)
;
Register
begin
=
allocator
.
useRegister
(
masm
beginId
)
;
Register
end
=
allocator
.
useRegister
(
masm
endId
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
branchArrayIsNotPacked
(
array
scratch1
scratch2
failure
-
>
label
(
)
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch1
)
;
ImmPtr
result
(
nullptr
)
;
masm
.
Push
(
result
)
;
masm
.
Push
(
end
)
;
masm
.
Push
(
begin
)
;
masm
.
Push
(
array
)
;
using
Fn
=
JSObject
*
(
*
)
(
JSContext
*
HandleObject
int32_t
int32_t
HandleObject
)
;
callVM
<
Fn
ArraySliceDense
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
masm
.
tagValue
(
JSVAL_TYPE_OBJECT
ReturnReg
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitIsArrayResult
(
ValOperandId
inputId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegister
scratch1
(
allocator
masm
)
;
AutoScratchRegisterMaybeOutput
scratch2
(
allocator
masm
output
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
inputId
)
;
allocator
.
discardStack
(
masm
)
;
Label
isNotArray
;
masm
.
fallibleUnboxObject
(
val
scratch1
&
isNotArray
)
;
Label
isArray
;
masm
.
branchTestObjClass
(
Assembler
:
:
Equal
scratch1
&
ArrayObject
:
:
class_
scratch2
scratch1
&
isArray
)
;
masm
.
branchTestObjectIsProxy
(
false
scratch1
scratch2
&
isNotArray
)
;
Label
done
;
{
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch2
)
;
masm
.
Push
(
scratch1
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleObject
bool
*
)
;
callVM
<
Fn
js
:
:
IsArrayFromJit
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
masm
.
tagValue
(
JSVAL_TYPE_BOOLEAN
ReturnReg
output
.
valueReg
(
)
)
;
masm
.
jump
(
&
done
)
;
}
masm
.
bind
(
&
isNotArray
)
;
masm
.
moveValue
(
BooleanValue
(
false
)
output
.
valueReg
(
)
)
;
masm
.
jump
(
&
done
)
;
masm
.
bind
(
&
isArray
)
;
masm
.
moveValue
(
BooleanValue
(
true
)
output
.
valueReg
(
)
)
;
masm
.
bind
(
&
done
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitIsTypedArrayResult
(
ObjOperandId
objId
bool
isPossiblyWrapped
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
allocator
.
discardStack
(
masm
)
;
Label
notTypedArray
isProxy
done
;
masm
.
loadObjClassUnsafe
(
obj
scratch
)
;
masm
.
branchIfClassIsNotTypedArray
(
scratch
&
notTypedArray
)
;
masm
.
moveValue
(
BooleanValue
(
true
)
output
.
valueReg
(
)
)
;
masm
.
jump
(
&
done
)
;
masm
.
bind
(
&
notTypedArray
)
;
if
(
isPossiblyWrapped
)
{
masm
.
branchTestClassIsProxy
(
true
scratch
&
isProxy
)
;
}
masm
.
moveValue
(
BooleanValue
(
false
)
output
.
valueReg
(
)
)
;
if
(
isPossiblyWrapped
)
{
masm
.
jump
(
&
done
)
;
masm
.
bind
(
&
isProxy
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
JSObject
*
bool
*
)
;
callVM
<
Fn
jit
:
:
IsPossiblyWrappedTypedArray
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
masm
.
tagValue
(
JSVAL_TYPE_BOOLEAN
ReturnReg
output
.
valueReg
(
)
)
;
}
masm
.
bind
(
&
done
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadStringCharResult
(
StringOperandId
strId
Int32OperandId
indexId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
str
=
allocator
.
useRegister
(
masm
strId
)
;
Register
index
=
allocator
.
useRegister
(
masm
indexId
)
;
AutoScratchRegisterMaybeOutput
scratch1
(
allocator
masm
output
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
spectreBoundsCheck32
(
index
Address
(
str
JSString
:
:
offsetOfLength
(
)
)
scratch1
failure
-
>
label
(
)
)
;
masm
.
loadStringChar
(
str
index
scratch1
scratch2
failure
-
>
label
(
)
)
;
allocator
.
discardStack
(
masm
)
;
Label
vmCall
;
masm
.
boundsCheck32PowerOfTwo
(
scratch1
StaticStrings
:
:
UNIT_STATIC_LIMIT
&
vmCall
)
;
masm
.
movePtr
(
ImmPtr
(
&
cx_
-
>
staticStrings
(
)
.
unitStaticTable
)
scratch2
)
;
masm
.
loadPtr
(
BaseIndex
(
scratch2
scratch1
ScalePointer
)
scratch2
)
;
Label
done
;
masm
.
jump
(
&
done
)
;
{
masm
.
bind
(
&
vmCall
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch2
)
;
masm
.
Push
(
scratch1
)
;
using
Fn
=
JSLinearString
*
(
*
)
(
JSContext
*
int32_t
)
;
callVM
<
Fn
jit
:
:
StringFromCharCode
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
masm
.
storeCallPointerResult
(
scratch2
)
;
}
masm
.
bind
(
&
done
)
;
masm
.
tagValue
(
JSVAL_TYPE_STRING
scratch2
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitStringFromCodeResult
(
Int32OperandId
codeId
StringCode
stringCode
)
{
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
Register
code
=
allocator
.
useRegister
(
masm
codeId
)
;
FailurePath
*
failure
=
nullptr
;
if
(
stringCode
=
=
StringCode
:
:
CodePoint
)
{
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
}
if
(
stringCode
=
=
StringCode
:
:
CodePoint
)
{
masm
.
branch32
(
Assembler
:
:
Above
code
Imm32
(
unicode
:
:
NonBMPMax
)
failure
-
>
label
(
)
)
;
}
allocator
.
discardStack
(
masm
)
;
Label
vmCall
;
masm
.
boundsCheck32PowerOfTwo
(
code
StaticStrings
:
:
UNIT_STATIC_LIMIT
&
vmCall
)
;
masm
.
movePtr
(
ImmPtr
(
cx_
-
>
runtime
(
)
-
>
staticStrings
-
>
unitStaticTable
)
scratch
)
;
masm
.
loadPtr
(
BaseIndex
(
scratch
code
ScalePointer
)
scratch
)
;
Label
done
;
masm
.
jump
(
&
done
)
;
{
masm
.
bind
(
&
vmCall
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
Push
(
code
)
;
if
(
stringCode
=
=
StringCode
:
:
CodeUnit
)
{
using
Fn
=
JSLinearString
*
(
*
)
(
JSContext
*
int32_t
)
;
callVM
<
Fn
jit
:
:
StringFromCharCode
>
(
masm
)
;
}
else
{
using
Fn
=
JSString
*
(
*
)
(
JSContext
*
int32_t
)
;
callVM
<
Fn
jit
:
:
StringFromCodePoint
>
(
masm
)
;
}
stubFrame
.
leave
(
masm
)
;
masm
.
mov
(
ReturnReg
scratch
)
;
}
masm
.
bind
(
&
done
)
;
masm
.
tagValue
(
JSVAL_TYPE_STRING
scratch
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitStringFromCharCodeResult
(
Int32OperandId
codeId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
return
emitStringFromCodeResult
(
codeId
StringCode
:
:
CodeUnit
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitStringFromCodePointResult
(
Int32OperandId
codeId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
return
emitStringFromCodeResult
(
codeId
StringCode
:
:
CodePoint
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitMathRandomResult
(
uint32_t
rngOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegister
scratch1
(
allocator
masm
)
;
AutoScratchRegister64
scratch2
(
allocator
masm
)
;
AutoAvailableFloatRegister
scratchFloat
(
*
this
FloatReg0
)
;
Address
rngAddr
(
stubAddress
(
rngOffset
)
)
;
masm
.
loadPtr
(
rngAddr
scratch1
)
;
masm
.
randomDouble
(
scratch1
scratchFloat
scratch2
output
.
valueReg
(
)
.
toRegister64
(
)
)
;
masm
.
boxDouble
(
scratchFloat
output
.
valueReg
(
)
scratchFloat
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitReflectGetPrototypeOfResult
(
ObjOperandId
objId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
allocator
.
discardStack
(
masm
)
;
MOZ_ASSERT
(
uintptr_t
(
TaggedProto
:
:
LazyProto
)
=
=
1
)
;
masm
.
loadObjProto
(
obj
scratch
)
;
Label
hasProto
;
masm
.
branchPtr
(
Assembler
:
:
Above
scratch
ImmWord
(
1
)
&
hasProto
)
;
Label
slow
done
;
masm
.
branchPtr
(
Assembler
:
:
Equal
scratch
ImmWord
(
1
)
&
slow
)
;
masm
.
moveValue
(
NullValue
(
)
output
.
valueReg
(
)
)
;
masm
.
jump
(
&
done
)
;
masm
.
bind
(
&
hasProto
)
;
masm
.
tagValue
(
JSVAL_TYPE_OBJECT
scratch
output
.
valueReg
(
)
)
;
masm
.
jump
(
&
done
)
;
{
masm
.
bind
(
&
slow
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleObject
MutableHandleValue
)
;
callVM
<
Fn
jit
:
:
GetPrototypeOf
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
}
masm
.
bind
(
&
done
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitHasClassResult
(
ObjOperandId
objId
uint32_t
claspOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
Address
claspAddr
(
stubAddress
(
claspOffset
)
)
;
masm
.
loadObjClassUnsafe
(
obj
scratch
)
;
masm
.
cmpPtrSet
(
Assembler
:
:
Equal
claspAddr
scratch
.
get
(
)
scratch
)
;
masm
.
tagValue
(
JSVAL_TYPE_BOOLEAN
scratch
output
.
valueReg
(
)
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallNativeSetter
(
ObjOperandId
receiverId
uint32_t
setterOffset
ValOperandId
rhsId
bool
sameRealm
uint32_t
nargsAndFlagsOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
receiver
=
allocator
.
useRegister
(
masm
receiverId
)
;
Address
setterAddr
(
stubAddress
(
setterOffset
)
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
rhsId
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
loadPtr
(
setterAddr
scratch
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
receiver
)
;
masm
.
Push
(
scratch
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleFunction
HandleObject
HandleValue
)
;
callVM
<
Fn
CallNativeSetter
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallScriptedSetter
(
ObjOperandId
receiverId
uint32_t
setterOffset
ValOperandId
rhsId
bool
sameRealm
uint32_t
nargsAndFlagsOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoScratchRegister
scratch1
(
allocator
masm
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
Register
receiver
=
allocator
.
useRegister
(
masm
receiverId
)
;
Address
setterAddr
(
stubAddress
(
setterOffset
)
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
rhsId
)
;
masm
.
loadPtr
(
setterAddr
scratch1
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch2
)
;
if
(
!
sameRealm
)
{
masm
.
switchToObjectRealm
(
scratch1
scratch2
)
;
}
masm
.
alignJitStackBasedOnNArgs
(
1
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
TypedOrValueRegister
(
MIRType
:
:
Object
AnyRegister
(
receiver
)
)
)
;
EmitBaselineCreateStubFrameDescriptor
(
masm
scratch2
JitFrameLayout
:
:
Size
(
)
)
;
masm
.
Push
(
Imm32
(
1
)
)
;
masm
.
Push
(
scratch1
)
;
masm
.
Push
(
scratch2
)
;
Label
noUnderflow
;
masm
.
load16ZeroExtend
(
Address
(
scratch1
JSFunction
:
:
offsetOfNargs
(
)
)
scratch2
)
;
masm
.
loadJitCodeRaw
(
scratch1
scratch1
)
;
masm
.
branch32
(
Assembler
:
:
BelowOrEqual
scratch2
Imm32
(
1
)
&
noUnderflow
)
;
{
TrampolinePtr
argumentsRectifier
=
cx_
-
>
runtime
(
)
-
>
jitRuntime
(
)
-
>
getArgumentsRectifier
(
)
;
masm
.
movePtr
(
argumentsRectifier
scratch1
)
;
}
masm
.
bind
(
&
noUnderflow
)
;
masm
.
callJit
(
scratch1
)
;
stubFrame
.
leave
(
masm
true
)
;
if
(
!
sameRealm
)
{
masm
.
switchToBaselineFrameRealm
(
R1
.
scratchReg
(
)
)
;
}
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallSetArrayLength
(
ObjOperandId
objId
bool
strict
ValOperandId
rhsId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
rhsId
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
Push
(
Imm32
(
strict
)
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleObject
HandleValue
bool
)
;
callVM
<
Fn
jit
:
:
SetArrayLength
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitProxySet
(
ObjOperandId
objId
uint32_t
idOffset
ValOperandId
rhsId
bool
strict
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
rhsId
)
;
Address
idAddr
(
stubAddress
(
idOffset
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
loadPtr
(
idAddr
scratch
)
;
masm
.
Push
(
Imm32
(
strict
)
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
scratch
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleObject
HandleId
HandleValue
bool
)
;
callVM
<
Fn
ProxySetProperty
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitProxySetByValue
(
ObjOperandId
objId
ValOperandId
idId
ValOperandId
rhsId
bool
strict
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
ValueOperand
idVal
=
allocator
.
useValueRegister
(
masm
idId
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
rhsId
)
;
allocator
.
discardStack
(
masm
)
;
int
scratchOffset
=
BaselineFrame
:
:
reverseOffsetOfScratchValue
(
)
;
masm
.
storePtr
(
obj
Address
(
BaselineFrameReg
scratchOffset
)
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
obj
)
;
masm
.
loadPtr
(
Address
(
BaselineFrameReg
0
)
obj
)
;
masm
.
loadPtr
(
Address
(
obj
scratchOffset
)
obj
)
;
masm
.
Push
(
Imm32
(
strict
)
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
idVal
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleObject
HandleValue
HandleValue
bool
)
;
callVM
<
Fn
ProxySetPropertyByValue
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallAddOrUpdateSparseElementHelper
(
ObjOperandId
objId
Int32OperandId
idId
ValOperandId
rhsId
bool
strict
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
Register
id
=
allocator
.
useRegister
(
masm
idId
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
rhsId
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
masm
.
Push
(
Imm32
(
strict
)
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
id
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
cx
HandleArrayObject
obj
int32_t
int_id
HandleValue
v
bool
strict
)
;
callVM
<
Fn
AddOrUpdateSparseElementHelper
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitMegamorphicSetElement
(
ObjOperandId
objId
ValOperandId
idId
ValOperandId
rhsId
bool
strict
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
ValueOperand
idVal
=
allocator
.
useValueRegister
(
masm
idId
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
rhsId
)
;
allocator
.
discardStack
(
masm
)
;
int
scratchOffset
=
BaselineFrame
:
:
reverseOffsetOfScratchValue
(
)
;
masm
.
storePtr
(
obj
Address
(
BaselineFrameReg
scratchOffset
)
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
obj
)
;
masm
.
loadPtr
(
Address
(
BaselineFrameReg
0
)
obj
)
;
masm
.
loadPtr
(
Address
(
obj
scratchOffset
)
obj
)
;
masm
.
Push
(
Imm32
(
strict
)
)
;
masm
.
Push
(
TypedOrValueRegister
(
MIRType
:
:
Object
AnyRegister
(
obj
)
)
)
;
masm
.
Push
(
val
)
;
masm
.
Push
(
idVal
)
;
masm
.
Push
(
obj
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleObject
HandleValue
HandleValue
HandleValue
bool
)
;
callVM
<
Fn
SetObjectElementWithReceiver
>
(
masm
)
;
stubFrame
.
leave
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitTypeMonitorResult
(
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
allocator
.
discardStack
(
masm
)
;
if
(
IsTypeInferenceEnabled
(
)
)
{
EmitEnterTypeMonitorIC
(
masm
)
;
}
else
{
EmitReturnFromIC
(
masm
)
;
}
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitReturnFromIC
(
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
allocator
.
discardStack
(
masm
)
;
EmitReturnFromIC
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadArgumentFixedSlot
(
ValOperandId
resultId
uint8_t
slotIndex
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
ValueOperand
resultReg
=
allocator
.
defineValueRegister
(
masm
resultId
)
;
Address
addr
=
allocator
.
addressOf
(
masm
BaselineFrameSlot
(
slotIndex
)
)
;
masm
.
loadValue
(
addr
resultReg
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadArgumentDynamicSlot
(
ValOperandId
resultId
Int32OperandId
argcId
uint8_t
slotIndex
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
ValueOperand
resultReg
=
allocator
.
defineValueRegister
(
masm
resultId
)
;
Register
argcReg
=
allocator
.
useRegister
(
masm
argcId
)
;
BaseValueIndex
addr
=
allocator
.
addressOf
(
masm
argcReg
BaselineFrameSlot
(
slotIndex
)
)
;
masm
.
loadValue
(
addr
resultReg
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardAndGetIterator
(
ObjOperandId
objId
uint32_t
iterOffset
uint32_t
enumeratorsAddrOffset
ObjOperandId
resultId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
AutoScratchRegister
scratch1
(
allocator
masm
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
AutoScratchRegister
niScratch
(
allocator
masm
)
;
Address
iterAddr
(
stubAddress
(
iterOffset
)
)
;
Address
enumeratorsAddr
(
stubAddress
(
enumeratorsAddrOffset
)
)
;
Register
output
=
allocator
.
defineRegister
(
masm
resultId
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
loadPtr
(
iterAddr
output
)
;
masm
.
loadObjPrivate
(
output
PropertyIteratorObject
:
:
NUM_FIXED_SLOTS
niScratch
)
;
masm
.
branchIfNativeIteratorNotReusable
(
niScratch
failure
-
>
label
(
)
)
;
Address
iterObjAddr
(
niScratch
NativeIterator
:
:
offsetOfObjectBeingIterated
(
)
)
;
EmitPreBarrier
(
masm
iterObjAddr
MIRType
:
:
Object
)
;
Address
iterFlagsAddr
(
niScratch
NativeIterator
:
:
offsetOfFlagsAndCount
(
)
)
;
masm
.
storePtr
(
obj
iterObjAddr
)
;
masm
.
or32
(
Imm32
(
NativeIterator
:
:
Flags
:
:
Active
)
iterFlagsAddr
)
;
emitPostBarrierSlot
(
output
TypedOrValueRegister
(
MIRType
:
:
Object
AnyRegister
(
obj
)
)
scratch1
)
;
masm
.
loadPtr
(
enumeratorsAddr
scratch1
)
;
masm
.
loadPtr
(
Address
(
scratch1
0
)
scratch1
)
;
emitRegisterEnumerator
(
scratch1
niScratch
scratch2
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitGuardDOMExpandoMissingOrGuardShape
(
ValOperandId
expandoId
uint32_t
shapeOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
ValueOperand
val
=
allocator
.
useValueRegister
(
masm
expandoId
)
;
AutoScratchRegister
shapeScratch
(
allocator
masm
)
;
AutoScratchRegister
objScratch
(
allocator
masm
)
;
Address
shapeAddr
(
stubAddress
(
shapeOffset
)
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
Label
done
;
masm
.
branchTestUndefined
(
Assembler
:
:
Equal
val
&
done
)
;
masm
.
debugAssertIsObject
(
val
)
;
masm
.
loadPtr
(
shapeAddr
shapeScratch
)
;
masm
.
unboxObject
(
val
objScratch
)
;
masm
.
branchTestObjShapeNoSpectreMitigations
(
Assembler
:
:
NotEqual
objScratch
shapeScratch
failure
-
>
label
(
)
)
;
masm
.
bind
(
&
done
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitLoadDOMExpandoValueGuardGeneration
(
ObjOperandId
objId
uint32_t
expandoAndGenerationOffset
uint32_t
generationOffset
ValOperandId
resultId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Register
obj
=
allocator
.
useRegister
(
masm
objId
)
;
Address
expandoAndGenerationAddr
(
stubAddress
(
expandoAndGenerationOffset
)
)
;
Address
generationAddr
(
stubAddress
(
generationOffset
)
)
;
AutoScratchRegister
scratch
(
allocator
masm
)
;
ValueOperand
output
=
allocator
.
defineValueRegister
(
masm
resultId
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
loadPtr
(
Address
(
obj
ProxyObject
:
:
offsetOfReservedSlots
(
)
)
scratch
)
;
Address
expandoAddr
(
scratch
js
:
:
detail
:
:
ProxyReservedSlots
:
:
offsetOfPrivateSlot
(
)
)
;
masm
.
loadPtr
(
expandoAndGenerationAddr
output
.
scratchReg
(
)
)
;
masm
.
branchPrivatePtr
(
Assembler
:
:
NotEqual
expandoAddr
output
.
scratchReg
(
)
failure
-
>
label
(
)
)
;
masm
.
branch64
(
Assembler
:
:
NotEqual
Address
(
output
.
scratchReg
(
)
ExpandoAndGeneration
:
:
offsetOfGeneration
(
)
)
generationAddr
scratch
failure
-
>
label
(
)
)
;
masm
.
loadValue
(
Address
(
output
.
scratchReg
(
)
ExpandoAndGeneration
:
:
offsetOfExpando
(
)
)
output
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
init
(
CacheKind
kind
)
{
if
(
!
allocator
.
init
(
)
)
{
return
false
;
}
allowDoubleResult_
.
emplace
(
true
)
;
size_t
numInputs
=
writer_
.
numInputOperands
(
)
;
MOZ_ASSERT
(
numInputs
=
=
NumInputsForCacheKind
(
kind
)
)
;
size_t
numInputsInRegs
=
std
:
:
min
(
numInputs
size_t
(
2
)
)
;
AllocatableGeneralRegisterSet
available
(
ICStubCompiler
:
:
availableGeneralRegs
(
numInputsInRegs
)
)
;
switch
(
kind
)
{
case
CacheKind
:
:
NewObject
:
case
CacheKind
:
:
GetIntrinsic
:
MOZ_ASSERT
(
numInputs
=
=
0
)
;
break
;
case
CacheKind
:
:
GetProp
:
case
CacheKind
:
:
TypeOf
:
case
CacheKind
:
:
ToPropertyKey
:
case
CacheKind
:
:
GetIterator
:
case
CacheKind
:
:
OptimizeSpreadCall
:
case
CacheKind
:
:
ToBool
:
case
CacheKind
:
:
UnaryArith
:
MOZ_ASSERT
(
numInputs
=
=
1
)
;
allocator
.
initInputLocation
(
0
R0
)
;
break
;
case
CacheKind
:
:
Compare
:
case
CacheKind
:
:
GetElem
:
case
CacheKind
:
:
GetPropSuper
:
case
CacheKind
:
:
SetProp
:
case
CacheKind
:
:
In
:
case
CacheKind
:
:
HasOwn
:
case
CacheKind
:
:
CheckPrivateField
:
case
CacheKind
:
:
InstanceOf
:
case
CacheKind
:
:
BinaryArith
:
MOZ_ASSERT
(
numInputs
=
=
2
)
;
allocator
.
initInputLocation
(
0
R0
)
;
allocator
.
initInputLocation
(
1
R1
)
;
break
;
case
CacheKind
:
:
GetElemSuper
:
MOZ_ASSERT
(
numInputs
=
=
3
)
;
allocator
.
initInputLocation
(
0
BaselineFrameSlot
(
0
)
)
;
allocator
.
initInputLocation
(
1
R0
)
;
allocator
.
initInputLocation
(
2
R1
)
;
break
;
case
CacheKind
:
:
SetElem
:
MOZ_ASSERT
(
numInputs
=
=
3
)
;
allocator
.
initInputLocation
(
0
R0
)
;
allocator
.
initInputLocation
(
1
R1
)
;
allocator
.
initInputLocation
(
2
BaselineFrameSlot
(
0
)
)
;
break
;
case
CacheKind
:
:
GetName
:
case
CacheKind
:
:
BindName
:
MOZ_ASSERT
(
numInputs
=
=
1
)
;
allocator
.
initInputLocation
(
0
R0
.
scratchReg
(
)
JSVAL_TYPE_OBJECT
)
;
#
if
defined
(
JS_NUNBOX32
)
available
.
add
(
R0
.
typeReg
(
)
)
;
#
endif
break
;
case
CacheKind
:
:
Call
:
MOZ_ASSERT
(
numInputs
=
=
1
)
;
allocator
.
initInputLocation
(
0
R0
.
scratchReg
(
)
JSVAL_TYPE_INT32
)
;
#
if
defined
(
JS_NUNBOX32
)
available
.
add
(
R0
.
typeReg
(
)
)
;
#
endif
break
;
}
liveFloatRegs_
=
LiveFloatRegisterSet
(
FloatRegisterSet
(
)
)
;
allocator
.
initAvailableRegs
(
available
)
;
outputUnchecked_
.
emplace
(
R0
)
;
return
true
;
}
static
void
ResetEnteredCounts
(
ICFallbackStub
*
stub
)
{
for
(
ICStubIterator
iter
=
stub
-
>
beginChain
(
)
;
!
iter
.
atEnd
(
)
;
iter
+
+
)
{
switch
(
iter
-
>
kind
(
)
)
{
case
ICStub
:
:
CacheIR_Regular
:
iter
-
>
toCacheIR_Regular
(
)
-
>
resetEnteredCount
(
)
;
break
;
case
ICStub
:
:
CacheIR_Updated
:
iter
-
>
toCacheIR_Updated
(
)
-
>
resetEnteredCount
(
)
;
break
;
case
ICStub
:
:
CacheIR_Monitored
:
iter
-
>
toCacheIR_Monitored
(
)
-
>
resetEnteredCount
(
)
;
break
;
default
:
break
;
}
}
stub
-
>
resetEnteredCount
(
)
;
}
ICStub
*
js
:
:
jit
:
:
AttachBaselineCacheIRStub
(
JSContext
*
cx
const
CacheIRWriter
&
writer
CacheKind
kind
BaselineCacheIRStubKind
stubKind
JSScript
*
outerScript
ICScript
*
icScript
ICFallbackStub
*
stub
bool
*
attached
)
{
AutoAssertNoPendingException
aanpe
(
cx
)
;
JS
:
:
AutoCheckCannotGC
nogc
;
MOZ_ASSERT
(
!
*
attached
)
;
if
(
writer
.
failed
(
)
)
{
return
nullptr
;
}
#
ifdef
DEBUG
static
const
size_t
MaxOptimizedCacheIRStubs
=
16
;
MOZ_ASSERT
(
stub
-
>
numOptimizedStubs
(
)
<
MaxOptimizedCacheIRStubs
)
;
#
endif
uint32_t
stubDataOffset
=
0
;
switch
(
stubKind
)
{
case
BaselineCacheIRStubKind
:
:
Monitored
:
stubDataOffset
=
sizeof
(
ICCacheIR_Monitored
)
;
break
;
case
BaselineCacheIRStubKind
:
:
Regular
:
stubDataOffset
=
sizeof
(
ICCacheIR_Regular
)
;
break
;
case
BaselineCacheIRStubKind
:
:
Updated
:
stubDataOffset
=
sizeof
(
ICCacheIR_Updated
)
;
break
;
}
JitZone
*
jitZone
=
cx
-
>
zone
(
)
-
>
jitZone
(
)
;
JSScript
*
invalidationScript
=
icScript
-
>
isInlined
(
)
?
icScript
-
>
inliningRoot
(
)
-
>
owningScript
(
)
:
outerScript
;
CacheIRStubInfo
*
stubInfo
;
CacheIRStubKey
:
:
Lookup
lookup
(
kind
ICStubEngine
:
:
Baseline
writer
.
codeStart
(
)
writer
.
codeLength
(
)
)
;
JitCode
*
code
=
jitZone
-
>
getBaselineCacheIRStubCode
(
lookup
&
stubInfo
)
;
if
(
!
code
)
{
JitContext
jctx
(
cx
nullptr
)
;
BaselineCacheIRCompiler
comp
(
cx
writer
stubDataOffset
stubKind
)
;
if
(
!
comp
.
init
(
kind
)
)
{
return
nullptr
;
}
code
=
comp
.
compile
(
)
;
if
(
!
code
)
{
return
nullptr
;
}
MOZ_ASSERT
(
!
stubInfo
)
;
stubInfo
=
CacheIRStubInfo
:
:
New
(
kind
ICStubEngine
:
:
Baseline
comp
.
makesGCCalls
(
)
stubDataOffset
writer
)
;
if
(
!
stubInfo
)
{
return
nullptr
;
}
CacheIRStubKey
key
(
stubInfo
)
;
if
(
!
jitZone
-
>
putBaselineCacheIRStubCode
(
lookup
key
code
)
)
{
return
nullptr
;
}
}
MOZ_ASSERT
(
code
)
;
MOZ_ASSERT
(
stubInfo
)
;
MOZ_ASSERT
(
stubInfo
-
>
stubDataSize
(
)
=
=
writer
.
stubDataSize
(
)
)
;
for
(
ICStubConstIterator
iter
=
stub
-
>
beginChainConst
(
)
;
!
iter
.
atEnd
(
)
;
iter
+
+
)
{
bool
updated
=
false
;
switch
(
stubKind
)
{
case
BaselineCacheIRStubKind
:
:
Regular
:
{
if
(
!
iter
-
>
isCacheIR_Regular
(
)
)
{
continue
;
}
auto
otherStub
=
iter
-
>
toCacheIR_Regular
(
)
;
if
(
otherStub
-
>
stubInfo
(
)
!
=
stubInfo
)
{
continue
;
}
if
(
!
writer
.
stubDataEqualsMaybeUpdate
(
otherStub
-
>
stubDataStart
(
)
&
updated
)
)
{
continue
;
}
break
;
}
case
BaselineCacheIRStubKind
:
:
Monitored
:
{
if
(
!
iter
-
>
isCacheIR_Monitored
(
)
)
{
continue
;
}
auto
otherStub
=
iter
-
>
toCacheIR_Monitored
(
)
;
if
(
otherStub
-
>
stubInfo
(
)
!
=
stubInfo
)
{
continue
;
}
if
(
!
writer
.
stubDataEqualsMaybeUpdate
(
otherStub
-
>
stubDataStart
(
)
&
updated
)
)
{
continue
;
}
break
;
}
case
BaselineCacheIRStubKind
:
:
Updated
:
{
if
(
!
iter
-
>
isCacheIR_Updated
(
)
)
{
continue
;
}
auto
otherStub
=
iter
-
>
toCacheIR_Updated
(
)
;
if
(
otherStub
-
>
stubInfo
(
)
!
=
stubInfo
)
{
continue
;
}
if
(
!
writer
.
stubDataEqualsMaybeUpdate
(
otherStub
-
>
stubDataStart
(
)
&
updated
)
)
{
continue
;
}
break
;
}
}
if
(
updated
)
{
stub
-
>
maybeInvalidateWarp
(
cx
invalidationScript
)
;
*
attached
=
true
;
}
else
{
JitSpew
(
JitSpew_BaselineICFallback
"
Tried
attaching
identical
stub
for
(
%
s
:
%
u
:
%
u
)
"
outerScript
-
>
filename
(
)
outerScript
-
>
lineno
(
)
outerScript
-
>
column
(
)
)
;
}
return
nullptr
;
}
size_t
bytesNeeded
=
stubInfo
-
>
stubDataOffset
(
)
+
stubInfo
-
>
stubDataSize
(
)
;
ICStubSpace
*
stubSpace
=
ICStubCompiler
:
:
StubSpaceForStub
(
stubInfo
-
>
makesGCCalls
(
)
outerScript
icScript
)
;
void
*
newStubMem
=
stubSpace
-
>
alloc
(
bytesNeeded
)
;
if
(
!
newStubMem
)
{
return
nullptr
;
}
ResetEnteredCounts
(
stub
)
;
stub
-
>
maybeInvalidateWarp
(
cx
invalidationScript
)
;
switch
(
stubKind
)
{
case
BaselineCacheIRStubKind
:
:
Regular
:
{
auto
newStub
=
new
(
newStubMem
)
ICCacheIR_Regular
(
code
stubInfo
)
;
writer
.
copyStubData
(
newStub
-
>
stubDataStart
(
)
)
;
stub
-
>
addNewStub
(
newStub
)
;
*
attached
=
true
;
return
newStub
;
}
case
BaselineCacheIRStubKind
:
:
Monitored
:
{
ICStub
*
monitorStub
=
nullptr
;
if
(
IsTypeInferenceEnabled
(
)
)
{
ICTypeMonitor_Fallback
*
typeMonitorFallback
=
stub
-
>
toMonitoredFallbackStub
(
)
-
>
getFallbackMonitorStub
(
cx
outerScript
)
;
if
(
!
typeMonitorFallback
)
{
cx
-
>
recoverFromOutOfMemory
(
)
;
return
nullptr
;
}
monitorStub
=
typeMonitorFallback
-
>
firstMonitorStub
(
)
;
}
auto
newStub
=
new
(
newStubMem
)
ICCacheIR_Monitored
(
code
monitorStub
stubInfo
)
;
writer
.
copyStubData
(
newStub
-
>
stubDataStart
(
)
)
;
stub
-
>
addNewStub
(
newStub
)
;
*
attached
=
true
;
return
newStub
;
}
case
BaselineCacheIRStubKind
:
:
Updated
:
{
auto
newStub
=
new
(
newStubMem
)
ICCacheIR_Updated
(
code
stubInfo
)
;
if
(
!
newStub
-
>
initUpdatingChain
(
cx
stubSpace
)
)
{
cx
-
>
recoverFromOutOfMemory
(
)
;
return
nullptr
;
}
writer
.
copyStubData
(
newStub
-
>
stubDataStart
(
)
)
;
stub
-
>
addNewStub
(
newStub
)
;
*
attached
=
true
;
return
newStub
;
}
}
MOZ_CRASH
(
"
Invalid
kind
"
)
;
}
template
<
typename
Base
>
uint8_t
*
ICCacheIR_Trait
<
Base
>
:
:
stubDataStart
(
)
{
return
reinterpret_cast
<
uint8_t
*
>
(
this
)
+
stubInfo_
-
>
stubDataOffset
(
)
;
}
template
uint8_t
*
ICCacheIR_Trait
<
ICStub
>
:
:
stubDataStart
(
)
;
template
uint8_t
*
ICCacheIR_Trait
<
ICMonitoredStub
>
:
:
stubDataStart
(
)
;
bool
BaselineCacheIRCompiler
:
:
emitCallStringObjectConcatResult
(
ValOperandId
lhsId
ValOperandId
rhsId
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
ValueOperand
lhs
=
allocator
.
useValueRegister
(
masm
lhsId
)
;
ValueOperand
rhs
=
allocator
.
useValueRegister
(
masm
rhsId
)
;
allocator
.
discardStack
(
masm
)
;
EmitRestoreTailCallReg
(
masm
)
;
masm
.
pushValue
(
lhs
)
;
masm
.
pushValue
(
rhs
)
;
masm
.
pushValue
(
rhs
)
;
masm
.
pushValue
(
lhs
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleValue
HandleValue
MutableHandleValue
)
;
tailCallVM
<
Fn
DoConcatStringObject
>
(
masm
)
;
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
updateArgc
(
CallFlags
flags
Register
argcReg
Register
scratch
)
{
CallFlags
:
:
ArgFormat
format
=
flags
.
getArgFormat
(
)
;
switch
(
format
)
{
case
CallFlags
:
:
Standard
:
return
true
;
case
CallFlags
:
:
FunCall
:
return
true
;
default
:
break
;
}
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
switch
(
flags
.
getArgFormat
(
)
)
{
case
CallFlags
:
:
Spread
:
case
CallFlags
:
:
FunApplyArray
:
{
BaselineFrameSlot
slot
(
flags
.
isConstructing
(
)
)
;
masm
.
unboxObject
(
allocator
.
addressOf
(
masm
slot
)
scratch
)
;
masm
.
loadPtr
(
Address
(
scratch
NativeObject
:
:
offsetOfElements
(
)
)
scratch
)
;
masm
.
load32
(
Address
(
scratch
ObjectElements
:
:
offsetOfLength
(
)
)
scratch
)
;
}
break
;
case
CallFlags
:
:
FunApplyArgs
:
{
Address
numActualArgsAddr
(
BaselineFrameReg
BaselineFrame
:
:
offsetOfNumActualArgs
(
)
)
;
masm
.
load32
(
numActualArgsAddr
scratch
)
;
}
break
;
default
:
MOZ_CRASH
(
"
Unknown
arg
format
"
)
;
}
masm
.
branch32
(
Assembler
:
:
Above
scratch
Imm32
(
JIT_ARGS_LENGTH_MAX
)
failure
-
>
label
(
)
)
;
masm
.
move32
(
scratch
argcReg
)
;
return
true
;
}
void
BaselineCacheIRCompiler
:
:
pushArguments
(
Register
argcReg
Register
calleeReg
Register
scratch
Register
scratch2
CallFlags
flags
bool
isJitCall
)
{
switch
(
flags
.
getArgFormat
(
)
)
{
case
CallFlags
:
:
Standard
:
pushStandardArguments
(
argcReg
scratch
scratch2
isJitCall
flags
.
isConstructing
(
)
)
;
break
;
case
CallFlags
:
:
Spread
:
pushArrayArguments
(
argcReg
scratch
scratch2
isJitCall
flags
.
isConstructing
(
)
)
;
break
;
case
CallFlags
:
:
FunCall
:
pushFunCallArguments
(
argcReg
calleeReg
scratch
scratch2
isJitCall
)
;
break
;
case
CallFlags
:
:
FunApplyArgs
:
pushFunApplyArgs
(
argcReg
calleeReg
scratch
scratch2
isJitCall
)
;
break
;
case
CallFlags
:
:
FunApplyArray
:
pushArrayArguments
(
argcReg
scratch
scratch2
isJitCall
false
)
;
break
;
default
:
MOZ_CRASH
(
"
Invalid
arg
format
"
)
;
}
}
void
BaselineCacheIRCompiler
:
:
pushStandardArguments
(
Register
argcReg
Register
scratch
Register
scratch2
bool
isJitCall
bool
isConstructing
)
{
Register
countReg
=
scratch
;
masm
.
move32
(
argcReg
countReg
)
;
masm
.
add32
(
Imm32
(
1
+
!
isJitCall
+
isConstructing
)
countReg
)
;
Register
argPtr
=
scratch2
;
Address
argAddress
(
masm
.
getStackPointer
(
)
STUB_FRAME_SIZE
)
;
masm
.
computeEffectiveAddress
(
argAddress
argPtr
)
;
if
(
isJitCall
)
{
masm
.
alignJitStackBasedOnNArgs
(
countReg
true
)
;
}
Label
loop
done
;
masm
.
branchTest32
(
Assembler
:
:
Zero
countReg
countReg
&
done
)
;
masm
.
bind
(
&
loop
)
;
{
masm
.
pushValue
(
Address
(
argPtr
0
)
)
;
masm
.
addPtr
(
Imm32
(
sizeof
(
Value
)
)
argPtr
)
;
masm
.
branchSub32
(
Assembler
:
:
NonZero
Imm32
(
1
)
countReg
&
loop
)
;
}
masm
.
bind
(
&
done
)
;
}
void
BaselineCacheIRCompiler
:
:
pushArrayArguments
(
Register
argcReg
Register
scratch
Register
scratch2
bool
isJitCall
bool
isConstructing
)
{
Register
startReg
=
scratch
;
masm
.
unboxObject
(
Address
(
masm
.
getStackPointer
(
)
(
isConstructing
*
sizeof
(
Value
)
)
+
STUB_FRAME_SIZE
)
startReg
)
;
masm
.
loadPtr
(
Address
(
startReg
NativeObject
:
:
offsetOfElements
(
)
)
startReg
)
;
if
(
isJitCall
)
{
Register
alignReg
=
argcReg
;
if
(
isConstructing
)
{
alignReg
=
scratch2
;
masm
.
computeEffectiveAddress
(
Address
(
argcReg
1
)
alignReg
)
;
}
masm
.
alignJitStackBasedOnNArgs
(
alignReg
false
)
;
}
if
(
isConstructing
)
{
masm
.
pushValue
(
Address
(
BaselineFrameReg
STUB_FRAME_SIZE
)
)
;
}
Register
endReg
=
scratch2
;
BaseValueIndex
endAddr
(
startReg
argcReg
)
;
masm
.
computeEffectiveAddress
(
endAddr
endReg
)
;
Label
copyDone
;
Label
copyStart
;
masm
.
bind
(
&
copyStart
)
;
masm
.
branchPtr
(
Assembler
:
:
Equal
endReg
startReg
&
copyDone
)
;
masm
.
subPtr
(
Imm32
(
sizeof
(
Value
)
)
endReg
)
;
masm
.
pushValue
(
Address
(
endReg
0
)
)
;
masm
.
jump
(
&
copyStart
)
;
masm
.
bind
(
&
copyDone
)
;
masm
.
pushValue
(
Address
(
BaselineFrameReg
STUB_FRAME_SIZE
+
(
1
+
isConstructing
)
*
sizeof
(
Value
)
)
)
;
if
(
!
isJitCall
)
{
masm
.
pushValue
(
Address
(
BaselineFrameReg
STUB_FRAME_SIZE
+
(
2
+
isConstructing
)
*
sizeof
(
Value
)
)
)
;
}
}
void
BaselineCacheIRCompiler
:
:
pushFunCallArguments
(
Register
argcReg
Register
calleeReg
Register
scratch
Register
scratch2
bool
isJitCall
)
{
Label
zeroArgs
done
;
masm
.
branchTest32
(
Assembler
:
:
Zero
argcReg
argcReg
&
zeroArgs
)
;
masm
.
sub32
(
Imm32
(
1
)
argcReg
)
;
pushStandardArguments
(
argcReg
scratch
scratch2
isJitCall
false
)
;
masm
.
jump
(
&
done
)
;
masm
.
bind
(
&
zeroArgs
)
;
if
(
isJitCall
)
{
masm
.
alignJitStackBasedOnNArgs
(
0
)
;
}
masm
.
pushValue
(
UndefinedValue
(
)
)
;
if
(
!
isJitCall
)
{
masm
.
Push
(
TypedOrValueRegister
(
MIRType
:
:
Object
AnyRegister
(
calleeReg
)
)
)
;
}
masm
.
bind
(
&
done
)
;
}
void
BaselineCacheIRCompiler
:
:
pushFunApplyArgs
(
Register
argcReg
Register
calleeReg
Register
scratch
Register
scratch2
bool
isJitCall
)
{
Register
startReg
=
scratch
;
masm
.
loadPtr
(
Address
(
BaselineFrameReg
0
)
startReg
)
;
masm
.
addPtr
(
Imm32
(
BaselineFrame
:
:
offsetOfArg
(
0
)
)
startReg
)
;
if
(
isJitCall
)
{
masm
.
alignJitStackBasedOnNArgs
(
argcReg
false
)
;
}
Register
endReg
=
scratch2
;
BaseValueIndex
endAddr
(
startReg
argcReg
)
;
masm
.
computeEffectiveAddress
(
endAddr
endReg
)
;
Label
copyDone
;
Label
copyStart
;
masm
.
bind
(
&
copyStart
)
;
masm
.
branchPtr
(
Assembler
:
:
Equal
endReg
startReg
&
copyDone
)
;
masm
.
subPtr
(
Imm32
(
sizeof
(
Value
)
)
endReg
)
;
masm
.
pushValue
(
Address
(
endReg
0
)
)
;
masm
.
jump
(
&
copyStart
)
;
masm
.
bind
(
&
copyDone
)
;
masm
.
pushValue
(
Address
(
BaselineFrameReg
STUB_FRAME_SIZE
+
sizeof
(
Value
)
)
)
;
if
(
!
isJitCall
)
{
masm
.
Push
(
TypedOrValueRegister
(
MIRType
:
:
Object
AnyRegister
(
calleeReg
)
)
)
;
}
}
bool
BaselineCacheIRCompiler
:
:
emitCallNativeShared
(
NativeCallType
callType
ObjOperandId
calleeId
Int32OperandId
argcId
CallFlags
flags
Maybe
<
bool
>
ignoresReturnValue
Maybe
<
uint32_t
>
targetOffset
)
{
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
Register
calleeReg
=
allocator
.
useRegister
(
masm
calleeId
)
;
Register
argcReg
=
allocator
.
useRegister
(
masm
argcId
)
;
bool
isConstructing
=
flags
.
isConstructing
(
)
;
bool
isSameRealm
=
flags
.
isSameRealm
(
)
;
if
(
!
updateArgc
(
flags
argcReg
scratch
)
)
{
return
false
;
}
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
if
(
!
isSameRealm
)
{
masm
.
switchToObjectRealm
(
calleeReg
scratch
)
;
}
pushArguments
(
argcReg
calleeReg
scratch
scratch2
flags
false
)
;
masm
.
moveStackPtrTo
(
scratch2
.
get
(
)
)
;
masm
.
push
(
argcReg
)
;
EmitBaselineCreateStubFrameDescriptor
(
masm
scratch
ExitFrameLayout
:
:
Size
(
)
)
;
masm
.
push
(
scratch
)
;
masm
.
push
(
ICTailCallReg
)
;
masm
.
loadJSContext
(
scratch
)
;
masm
.
enterFakeExitFrameForNative
(
scratch
scratch
isConstructing
)
;
masm
.
setupUnalignedABICall
(
scratch
)
;
masm
.
loadJSContext
(
scratch
)
;
masm
.
passABIArg
(
scratch
)
;
masm
.
passABIArg
(
argcReg
)
;
masm
.
passABIArg
(
scratch2
)
;
switch
(
callType
)
{
case
NativeCallType
:
:
Native
:
{
#
ifdef
JS_SIMULATOR
Address
redirectedAddr
(
stubAddress
(
*
targetOffset
)
)
;
masm
.
callWithABI
(
redirectedAddr
)
;
#
else
if
(
*
ignoresReturnValue
)
{
masm
.
loadPtr
(
Address
(
calleeReg
JSFunction
:
:
offsetOfJitInfo
(
)
)
calleeReg
)
;
masm
.
callWithABI
(
Address
(
calleeReg
JSJitInfo
:
:
offsetOfIgnoresReturnValueNative
(
)
)
)
;
}
else
{
masm
.
callWithABI
(
Address
(
calleeReg
JSFunction
:
:
offsetOfNative
(
)
)
)
;
}
#
endif
}
break
;
case
NativeCallType
:
:
ClassHook
:
{
Address
nativeAddr
(
stubAddress
(
*
targetOffset
)
)
;
masm
.
callWithABI
(
nativeAddr
)
;
}
break
;
}
masm
.
branchIfFalseBool
(
ReturnReg
masm
.
exceptionLabel
(
)
)
;
masm
.
loadValue
(
Address
(
masm
.
getStackPointer
(
)
NativeExitFrameLayout
:
:
offsetOfResult
(
)
)
output
.
valueReg
(
)
)
;
stubFrame
.
leave
(
masm
)
;
if
(
!
isSameRealm
)
{
masm
.
switchToBaselineFrameRealm
(
scratch2
)
;
}
return
true
;
}
#
ifdef
JS_SIMULATOR
bool
BaselineCacheIRCompiler
:
:
emitCallNativeFunction
(
ObjOperandId
calleeId
Int32OperandId
argcId
CallFlags
flags
uint32_t
targetOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Maybe
<
bool
>
ignoresReturnValue
;
Maybe
<
uint32_t
>
targetOffset_
=
mozilla
:
:
Some
(
targetOffset
)
;
return
emitCallNativeShared
(
NativeCallType
:
:
Native
calleeId
argcId
flags
ignoresReturnValue
targetOffset_
)
;
}
#
else
bool
BaselineCacheIRCompiler
:
:
emitCallNativeFunction
(
ObjOperandId
calleeId
Int32OperandId
argcId
CallFlags
flags
bool
ignoresReturnValue
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Maybe
<
bool
>
ignoresReturnValue_
=
mozilla
:
:
Some
(
ignoresReturnValue
)
;
Maybe
<
uint32_t
>
targetOffset
;
return
emitCallNativeShared
(
NativeCallType
:
:
Native
calleeId
argcId
flags
ignoresReturnValue_
targetOffset
)
;
}
#
endif
bool
BaselineCacheIRCompiler
:
:
emitCallClassHook
(
ObjOperandId
calleeId
Int32OperandId
argcId
CallFlags
flags
uint32_t
targetOffset
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
Maybe
<
bool
>
ignoresReturnValue
;
Maybe
<
uint32_t
>
targetOffset_
=
mozilla
:
:
Some
(
targetOffset
)
;
return
emitCallNativeShared
(
NativeCallType
:
:
ClassHook
calleeId
argcId
flags
ignoresReturnValue
targetOffset_
)
;
}
void
BaselineCacheIRCompiler
:
:
loadStackObject
(
ArgumentKind
kind
CallFlags
flags
size_t
stackPushed
Register
argcReg
Register
dest
)
{
bool
addArgc
=
false
;
int32_t
slotIndex
=
GetIndexOfArgument
(
kind
flags
&
addArgc
)
;
if
(
addArgc
)
{
int32_t
slotOffset
=
slotIndex
*
sizeof
(
JS
:
:
Value
)
+
stackPushed
;
BaseValueIndex
slotAddr
(
masm
.
getStackPointer
(
)
argcReg
slotOffset
)
;
masm
.
unboxObject
(
slotAddr
dest
)
;
}
else
{
int32_t
slotOffset
=
slotIndex
*
sizeof
(
JS
:
:
Value
)
+
stackPushed
;
Address
slotAddr
(
masm
.
getStackPointer
(
)
slotOffset
)
;
masm
.
unboxObject
(
slotAddr
dest
)
;
}
}
template
<
typename
T
>
void
BaselineCacheIRCompiler
:
:
storeThis
(
const
T
&
newThis
Register
argcReg
CallFlags
flags
)
{
switch
(
flags
.
getArgFormat
(
)
)
{
case
CallFlags
:
:
Standard
:
{
BaseValueIndex
thisAddress
(
masm
.
getStackPointer
(
)
argcReg
1
*
sizeof
(
Value
)
+
STUB_FRAME_SIZE
)
;
masm
.
storeValue
(
newThis
thisAddress
)
;
}
break
;
case
CallFlags
:
:
Spread
:
{
Address
thisAddress
(
masm
.
getStackPointer
(
)
2
*
sizeof
(
Value
)
+
STUB_FRAME_SIZE
)
;
masm
.
storeValue
(
newThis
thisAddress
)
;
}
break
;
default
:
MOZ_CRASH
(
"
Invalid
arg
format
for
scripted
constructor
"
)
;
}
}
void
BaselineCacheIRCompiler
:
:
createThis
(
Register
argcReg
Register
calleeReg
Register
scratch
CallFlags
flags
)
{
MOZ_ASSERT
(
flags
.
isConstructing
(
)
)
;
if
(
flags
.
needsUninitializedThis
(
)
)
{
storeThis
(
MagicValue
(
JS_UNINITIALIZED_LEXICAL
)
argcReg
flags
)
;
return
;
}
size_t
depth
=
STUB_FRAME_SIZE
;
masm
.
push
(
argcReg
)
;
depth
+
=
sizeof
(
size_t
)
;
loadStackObject
(
ArgumentKind
:
:
NewTarget
flags
depth
argcReg
scratch
)
;
masm
.
push
(
scratch
)
;
depth
+
=
sizeof
(
JSObject
*
)
;
loadStackObject
(
ArgumentKind
:
:
Callee
flags
depth
argcReg
scratch
)
;
masm
.
push
(
scratch
)
;
using
Fn
=
bool
(
*
)
(
JSContext
*
HandleObject
HandleObject
MutableHandleValue
)
;
callVM
<
Fn
CreateThisFromIC
>
(
masm
)
;
#
ifdef
DEBUG
Label
createdThisOK
;
masm
.
branchTestObject
(
Assembler
:
:
Equal
JSReturnOperand
&
createdThisOK
)
;
masm
.
branchTestMagic
(
Assembler
:
:
Equal
JSReturnOperand
&
createdThisOK
)
;
masm
.
assumeUnreachable
(
"
The
return
of
CreateThis
must
be
an
object
or
uninitialized
.
"
)
;
masm
.
bind
(
&
createdThisOK
)
;
#
endif
masm
.
pop
(
argcReg
)
;
storeThis
(
JSReturnOperand
argcReg
flags
)
;
Address
stubRegAddress
(
masm
.
getStackPointer
(
)
STUB_FRAME_SAVED_STUB_OFFSET
)
;
masm
.
loadPtr
(
stubRegAddress
ICStubReg
)
;
depth
=
STUB_FRAME_SIZE
;
loadStackObject
(
ArgumentKind
:
:
Callee
flags
depth
argcReg
calleeReg
)
;
}
void
BaselineCacheIRCompiler
:
:
updateReturnValue
(
)
{
Label
skipThisReplace
;
masm
.
branchTestObject
(
Assembler
:
:
Equal
JSReturnOperand
&
skipThisReplace
)
;
Address
thisAddress
(
masm
.
getStackPointer
(
)
3
*
sizeof
(
size_t
)
)
;
masm
.
loadValue
(
thisAddress
JSReturnOperand
)
;
#
ifdef
DEBUG
masm
.
branchTestObject
(
Assembler
:
:
Equal
JSReturnOperand
&
skipThisReplace
)
;
masm
.
assumeUnreachable
(
"
Return
of
constructing
call
should
be
an
object
.
"
)
;
#
endif
masm
.
bind
(
&
skipThisReplace
)
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallScriptedFunction
(
ObjOperandId
calleeId
Int32OperandId
argcId
CallFlags
flags
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
Register
calleeReg
=
allocator
.
useRegister
(
masm
calleeId
)
;
Register
argcReg
=
allocator
.
useRegister
(
masm
argcId
)
;
bool
isConstructing
=
flags
.
isConstructing
(
)
;
bool
isSameRealm
=
flags
.
isSameRealm
(
)
;
if
(
!
updateArgc
(
flags
argcReg
scratch
)
)
{
return
false
;
}
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
if
(
!
isSameRealm
)
{
masm
.
switchToObjectRealm
(
calleeReg
scratch
)
;
}
if
(
isConstructing
)
{
createThis
(
argcReg
calleeReg
scratch
flags
)
;
}
pushArguments
(
argcReg
calleeReg
scratch
scratch2
flags
true
)
;
Register
code
=
scratch2
;
masm
.
loadJitCodeRaw
(
calleeReg
code
)
;
EmitBaselineCreateStubFrameDescriptor
(
masm
scratch
JitFrameLayout
:
:
Size
(
)
)
;
masm
.
Push
(
argcReg
)
;
masm
.
PushCalleeToken
(
calleeReg
isConstructing
)
;
masm
.
Push
(
scratch
)
;
Label
noUnderflow
;
masm
.
load16ZeroExtend
(
Address
(
calleeReg
JSFunction
:
:
offsetOfNargs
(
)
)
calleeReg
)
;
masm
.
branch32
(
Assembler
:
:
AboveOrEqual
argcReg
calleeReg
&
noUnderflow
)
;
{
TrampolinePtr
argumentsRectifier
=
cx_
-
>
runtime
(
)
-
>
jitRuntime
(
)
-
>
getArgumentsRectifier
(
)
;
masm
.
movePtr
(
argumentsRectifier
code
)
;
}
masm
.
bind
(
&
noUnderflow
)
;
masm
.
callJit
(
code
)
;
if
(
isConstructing
)
{
updateReturnValue
(
)
;
}
stubFrame
.
leave
(
masm
true
)
;
if
(
!
isSameRealm
)
{
masm
.
switchToBaselineFrameRealm
(
scratch2
)
;
}
return
true
;
}
bool
BaselineCacheIRCompiler
:
:
emitCallInlinedFunction
(
ObjOperandId
calleeId
Int32OperandId
argcId
uint32_t
icScriptOffset
CallFlags
flags
)
{
JitSpew
(
JitSpew_Codegen
"
%
s
"
__FUNCTION__
)
;
AutoOutputRegister
output
(
*
this
)
;
AutoScratchRegisterMaybeOutput
scratch
(
allocator
masm
output
)
;
AutoScratchRegisterMaybeOutputType
codeReg
(
allocator
masm
output
)
;
AutoScratchRegister
scratch2
(
allocator
masm
)
;
Register
calleeReg
=
allocator
.
useRegister
(
masm
calleeId
)
;
Register
argcReg
=
allocator
.
useRegister
(
masm
argcId
)
;
bool
isConstructing
=
flags
.
isConstructing
(
)
;
bool
isSameRealm
=
flags
.
isSameRealm
(
)
;
FailurePath
*
failure
;
if
(
!
addFailurePath
(
&
failure
)
)
{
return
false
;
}
masm
.
loadBaselineJitCodeRaw
(
calleeReg
codeReg
failure
-
>
label
(
)
)
;
if
(
!
updateArgc
(
flags
argcReg
scratch
)
)
{
return
false
;
}
allocator
.
discardStack
(
masm
)
;
AutoStubFrame
stubFrame
(
*
this
)
;
stubFrame
.
enter
(
masm
scratch
)
;
if
(
!
isSameRealm
)
{
masm
.
switchToObjectRealm
(
calleeReg
scratch
)
;
}
if
(
isConstructing
)
{
createThis
(
argcReg
calleeReg
scratch
flags
)
;
}
pushArguments
(
argcReg
calleeReg
scratch
scratch2
flags
true
)
;
Address
icScriptAddr
(
stubAddress
(
icScriptOffset
)
)
;
masm
.
loadPtr
(
icScriptAddr
scratch
)
;
masm
.
loadJSContext
(
scratch2
)
;
masm
.
storePtr
(
scratch
Address
(
scratch2
JSContext
:
:
offsetOfInlinedICScript
(
)
)
)
;
EmitBaselineCreateStubFrameDescriptor
(
masm
scratch
JitFrameLayout
:
:
Size
(
)
)
;
masm
.
Push
(
argcReg
)
;
masm
.
PushCalleeToken
(
calleeReg
isConstructing
)
;
masm
.
Push
(
scratch
)
;
Label
noUnderflow
;
masm
.
load16ZeroExtend
(
Address
(
calleeReg
JSFunction
:
:
offsetOfNargs
(
)
)
calleeReg
)
;
masm
.
branch32
(
Assembler
:
:
AboveOrEqual
argcReg
calleeReg
&
noUnderflow
)
;
ArgumentsRectifierKind
kind
=
ArgumentsRectifierKind
:
:
TrialInlining
;
TrampolinePtr
argumentsRectifier
=
cx_
-
>
runtime
(
)
-
>
jitRuntime
(
)
-
>
getArgumentsRectifier
(
kind
)
;
masm
.
movePtr
(
argumentsRectifier
codeReg
)
;
masm
.
bind
(
&
noUnderflow
)
;
masm
.
callJit
(
codeReg
)
;
if
(
isConstructing
)
{
updateReturnValue
(
)
;
}
stubFrame
.
leave
(
masm
true
)
;
if
(
!
isSameRealm
)
{
masm
.
switchToBaselineFrameRealm
(
scratch2
)
;
}
return
true
;
}
