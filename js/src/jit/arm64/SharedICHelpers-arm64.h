#
ifndef
jit_arm64_SharedICHelpers_arm64_h
#
define
jit_arm64_SharedICHelpers_arm64_h
#
include
"
jit
/
BaselineIC
.
h
"
#
include
"
jit
/
JitFrames
.
h
"
#
include
"
jit
/
MacroAssembler
.
h
"
#
include
"
jit
/
SharedICRegisters
.
h
"
namespace
js
{
namespace
jit
{
static
const
size_t
ICStackValueOffset
=
0
;
inline
void
EmitRestoreTailCallReg
(
MacroAssembler
&
masm
)
{
}
inline
void
EmitRepushTailCallReg
(
MacroAssembler
&
masm
)
{
}
inline
void
EmitCallIC
(
MacroAssembler
&
masm
CodeOffset
*
callOffset
)
{
static_assert
(
R2
=
=
ValueOperand
(
r0
)
)
;
masm
.
loadPtr
(
Address
(
ICStubReg
ICStub
:
:
offsetOfStubCode
(
)
)
r0
)
;
masm
.
Blr
(
x0
)
;
*
callOffset
=
CodeOffset
(
masm
.
currentOffset
(
)
)
;
}
inline
void
EmitReturnFromIC
(
MacroAssembler
&
masm
)
{
masm
.
abiret
(
)
;
}
inline
void
EmitBaselineLeaveStubFrame
(
MacroAssembler
&
masm
bool
calledIntoIon
=
false
)
{
vixl
:
:
UseScratchRegisterScope
temps
(
&
masm
.
asVIXL
(
)
)
;
const
ARMRegister
scratch64
=
temps
.
AcquireX
(
)
;
if
(
calledIntoIon
)
{
masm
.
pop
(
scratch64
.
asUnsized
(
)
)
;
masm
.
Lsr
(
scratch64
scratch64
FRAMESIZE_SHIFT
)
;
masm
.
Add
(
masm
.
GetStackPointer64
(
)
masm
.
GetStackPointer64
(
)
scratch64
)
;
}
else
{
masm
.
Mov
(
masm
.
GetStackPointer64
(
)
BaselineFrameReg64
)
;
}
masm
.
pop
(
BaselineFrameReg
ICStubReg
ICTailCallReg
scratch64
.
asUnsized
(
)
)
;
masm
.
checkStackAlignment
(
)
;
}
template
<
typename
AddrType
>
inline
void
EmitPreBarrier
(
MacroAssembler
&
masm
const
AddrType
&
addr
MIRType
type
)
{
masm
.
push
(
lr
)
;
masm
.
guardedCallPreBarrier
(
addr
type
)
;
masm
.
pop
(
lr
)
;
}
inline
void
EmitStubGuardFailure
(
MacroAssembler
&
masm
)
{
masm
.
loadPtr
(
Address
(
ICStubReg
ICCacheIRStub
:
:
offsetOfNext
(
)
)
ICStubReg
)
;
masm
.
jump
(
Address
(
ICStubReg
ICStub
:
:
offsetOfStubCode
(
)
)
)
;
}
}
}
#
endif
