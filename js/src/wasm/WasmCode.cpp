#
include
"
wasm
/
WasmCode
.
h
"
#
include
"
mozilla
/
Atomics
.
h
"
#
include
"
mozilla
/
BinarySearch
.
h
"
#
include
"
mozilla
/
EnumeratedRange
.
h
"
#
include
"
mozilla
/
Sprintf
.
h
"
#
include
<
algorithm
>
#
include
"
jsnum
.
h
"
#
include
"
jit
/
Disassemble
.
h
"
#
include
"
jit
/
ExecutableAllocator
.
h
"
#
include
"
jit
/
FlushICache
.
h
"
#
include
"
jit
/
MacroAssembler
.
h
"
#
include
"
jit
/
PerfSpewer
.
h
"
#
include
"
util
/
Poison
.
h
"
#
ifdef
MOZ_VTUNE
#
include
"
vtune
/
VTuneWrapper
.
h
"
#
endif
#
include
"
wasm
/
WasmModule
.
h
"
#
include
"
wasm
/
WasmProcess
.
h
"
#
include
"
wasm
/
WasmSerialize
.
h
"
#
include
"
wasm
/
WasmStubs
.
h
"
#
include
"
wasm
/
WasmUtility
.
h
"
using
namespace
js
;
using
namespace
js
:
:
jit
;
using
namespace
js
:
:
wasm
;
using
mozilla
:
:
BinarySearch
;
using
mozilla
:
:
BinarySearchIf
;
using
mozilla
:
:
MakeEnumeratedRange
;
using
mozilla
:
:
PodAssign
;
size_t
LinkData
:
:
SymbolicLinkArray
:
:
sizeOfExcludingThis
(
MallocSizeOf
mallocSizeOf
)
const
{
size_t
size
=
0
;
for
(
const
Uint32Vector
&
offsets
:
*
this
)
{
size
+
=
offsets
.
sizeOfExcludingThis
(
mallocSizeOf
)
;
}
return
size
;
}
static
uint32_t
RoundupCodeLength
(
uint32_t
codeLength
)
{
return
RoundUp
(
codeLength
ExecutableCodePageSize
)
;
}
UniqueCodeBytes
wasm
:
:
AllocateCodeBytes
(
Maybe
<
AutoMarkJitCodeWritableForThread
>
&
writable
uint32_t
codeLength
)
{
if
(
codeLength
>
MaxCodeBytesPerProcess
)
{
return
nullptr
;
}
static_assert
(
MaxCodeBytesPerProcess
<
=
INT32_MAX
"
rounding
won
'
t
overflow
"
)
;
uint32_t
roundedCodeLength
=
RoundupCodeLength
(
codeLength
)
;
void
*
p
=
AllocateExecutableMemory
(
roundedCodeLength
ProtectionSetting
:
:
Writable
MemCheckKind
:
:
MakeUndefined
)
;
if
(
!
p
)
{
if
(
OnLargeAllocationFailure
)
{
OnLargeAllocationFailure
(
)
;
p
=
AllocateExecutableMemory
(
roundedCodeLength
ProtectionSetting
:
:
Writable
MemCheckKind
:
:
MakeUndefined
)
;
}
}
if
(
!
p
)
{
return
nullptr
;
}
writable
.
emplace
(
)
;
memset
(
(
(
uint8_t
*
)
p
)
+
codeLength
0
roundedCodeLength
-
codeLength
)
;
return
UniqueCodeBytes
(
(
uint8_t
*
)
p
FreeCode
(
roundedCodeLength
)
)
;
}
void
FreeCode
:
:
operator
(
)
(
uint8_t
*
bytes
)
{
MOZ_ASSERT
(
codeLength
)
;
MOZ_ASSERT
(
codeLength
=
=
RoundupCodeLength
(
codeLength
)
)
;
#
ifdef
MOZ_VTUNE
vtune
:
:
UnmarkBytes
(
bytes
codeLength
)
;
#
endif
DeallocateExecutableMemory
(
bytes
codeLength
)
;
}
bool
wasm
:
:
StaticallyLink
(
jit
:
:
AutoMarkJitCodeWritableForThread
&
writable
uint8_t
*
base
const
LinkData
&
linkData
const
CodeBlock
*
maybeSharedStubs
)
{
if
(
!
EnsureBuiltinThunksInitialized
(
writable
)
)
{
return
false
;
}
for
(
LinkData
:
:
InternalLink
link
:
linkData
.
internalLinks
)
{
CodeLabel
label
;
label
.
patchAt
(
)
-
>
bind
(
link
.
patchAtOffset
)
;
label
.
target
(
)
-
>
bind
(
link
.
targetOffset
)
;
#
ifdef
JS_CODELABEL_LINKMODE
label
.
setLinkMode
(
static_cast
<
CodeLabel
:
:
LinkMode
>
(
link
.
mode
)
)
;
#
endif
Assembler
:
:
Bind
(
base
label
)
;
}
for
(
auto
imm
:
MakeEnumeratedRange
(
SymbolicAddress
:
:
Limit
)
)
{
const
Uint32Vector
&
offsets
=
linkData
.
symbolicLinks
[
imm
]
;
if
(
offsets
.
empty
(
)
)
{
continue
;
}
void
*
target
=
SymbolicAddressTarget
(
imm
)
;
for
(
uint32_t
offset
:
offsets
)
{
uint8_t
*
patchAt
=
base
+
offset
;
Assembler
:
:
PatchDataWithValueCheck
(
CodeLocationLabel
(
patchAt
)
PatchedImmPtr
(
target
)
PatchedImmPtr
(
(
void
*
)
-
1
)
)
;
}
}
return
true
;
}
void
wasm
:
:
StaticallyUnlink
(
uint8_t
*
base
const
LinkData
&
linkData
)
{
for
(
LinkData
:
:
InternalLink
link
:
linkData
.
internalLinks
)
{
CodeLabel
label
;
label
.
patchAt
(
)
-
>
bind
(
link
.
patchAtOffset
)
;
label
.
target
(
)
-
>
bind
(
-
size_t
(
base
)
)
;
#
ifdef
JS_CODELABEL_LINKMODE
label
.
setLinkMode
(
static_cast
<
CodeLabel
:
:
LinkMode
>
(
link
.
mode
)
)
;
#
endif
Assembler
:
:
Bind
(
base
label
)
;
}
for
(
auto
imm
:
MakeEnumeratedRange
(
SymbolicAddress
:
:
Limit
)
)
{
const
Uint32Vector
&
offsets
=
linkData
.
symbolicLinks
[
imm
]
;
if
(
offsets
.
empty
(
)
)
{
continue
;
}
void
*
target
=
SymbolicAddressTarget
(
imm
)
;
for
(
uint32_t
offset
:
offsets
)
{
uint8_t
*
patchAt
=
base
+
offset
;
Assembler
:
:
PatchDataWithValueCheck
(
CodeLocationLabel
(
patchAt
)
PatchedImmPtr
(
(
void
*
)
-
1
)
PatchedImmPtr
(
target
)
)
;
}
}
}
static
bool
AppendToString
(
const
char
*
str
UTF8Bytes
*
bytes
)
{
return
bytes
-
>
append
(
str
strlen
(
str
)
)
&
&
bytes
-
>
append
(
'
\
0
'
)
;
}
static
void
SendCodeRangesToProfiler
(
const
uint8_t
*
segmentBase
const
CodeMetadata
&
codeMeta
const
CodeMetadataForAsmJS
*
codeMetaForAsmJS
const
CodeRangeVector
&
codeRanges
)
{
bool
enabled
=
false
;
enabled
|
=
PerfEnabled
(
)
;
#
ifdef
MOZ_VTUNE
enabled
|
=
vtune
:
:
IsProfilingActive
(
)
;
#
endif
if
(
!
enabled
)
{
return
;
}
for
(
const
CodeRange
&
codeRange
:
codeRanges
)
{
if
(
!
codeRange
.
hasFuncIndex
(
)
)
{
continue
;
}
uintptr_t
start
=
uintptr_t
(
segmentBase
+
codeRange
.
begin
(
)
)
;
uintptr_t
size
=
codeRange
.
end
(
)
-
codeRange
.
begin
(
)
;
UTF8Bytes
name
;
bool
ok
;
if
(
codeMetaForAsmJS
)
{
ok
=
codeMetaForAsmJS
-
>
getFuncNameForAsmJS
(
codeRange
.
funcIndex
(
)
&
name
)
;
}
else
{
ok
=
codeMeta
.
getFuncNameForWasm
(
NameContext
:
:
Standalone
codeRange
.
funcIndex
(
)
&
name
)
;
}
if
(
!
ok
)
{
return
;
}
(
void
)
start
;
(
void
)
size
;
if
(
PerfEnabled
(
)
)
{
const
char
*
file
=
codeMeta
.
filename
.
get
(
)
;
if
(
codeRange
.
isFunction
(
)
)
{
if
(
!
name
.
append
(
'
\
0
'
)
)
{
return
;
}
unsigned
line
=
codeRange
.
funcLineOrBytecode
(
)
;
CollectPerfSpewerWasmFunctionMap
(
start
size
file
line
name
.
begin
(
)
)
;
}
else
if
(
codeRange
.
isInterpEntry
(
)
)
{
if
(
!
AppendToString
(
"
slow
entry
"
&
name
)
)
{
return
;
}
CollectPerfSpewerWasmMap
(
start
size
file
name
.
begin
(
)
)
;
}
else
if
(
codeRange
.
isJitEntry
(
)
)
{
if
(
!
AppendToString
(
"
fast
entry
"
&
name
)
)
{
return
;
}
CollectPerfSpewerWasmMap
(
start
size
file
name
.
begin
(
)
)
;
}
else
if
(
codeRange
.
isImportInterpExit
(
)
)
{
if
(
!
AppendToString
(
"
slow
exit
"
&
name
)
)
{
return
;
}
CollectPerfSpewerWasmMap
(
start
size
file
name
.
begin
(
)
)
;
}
else
if
(
codeRange
.
isImportJitExit
(
)
)
{
if
(
!
AppendToString
(
"
fast
exit
"
&
name
)
)
{
return
;
}
CollectPerfSpewerWasmMap
(
start
size
file
name
.
begin
(
)
)
;
}
else
{
MOZ_CRASH
(
"
unhandled
perf
hasFuncIndex
type
"
)
;
}
}
#
ifdef
MOZ_VTUNE
if
(
!
vtune
:
:
IsProfilingActive
(
)
)
{
continue
;
}
if
(
!
codeRange
.
isFunction
(
)
)
{
continue
;
}
if
(
!
name
.
append
(
'
\
0
'
)
)
{
return
;
}
vtune
:
:
MarkWasm
(
vtune
:
:
GenerateUniqueMethodID
(
)
name
.
begin
(
)
(
void
*
)
start
size
)
;
#
endif
}
}
bool
CodeSegment
:
:
linkAndMakeExecutable
(
jit
:
:
AutoMarkJitCodeWritableForThread
&
writable
const
LinkData
&
linkData
const
CodeBlock
*
maybeSharedStubs
)
{
if
(
!
StaticallyLink
(
writable
bytes_
.
get
(
)
linkData
maybeSharedStubs
)
)
{
return
false
;
}
return
ExecutableAllocator
:
:
makeExecutableAndFlushICache
(
base
(
)
RoundupCodeLength
(
lengthBytes
(
)
)
)
;
}
SharedCodeSegment
CodeSegment
:
:
createEmpty
(
size_t
capacityBytes
)
{
uint32_t
codeLength
=
0
;
uint32_t
codeCapacity
=
RoundupCodeLength
(
capacityBytes
)
;
Maybe
<
AutoMarkJitCodeWritableForThread
>
writable
;
UniqueCodeBytes
codeBytes
=
AllocateCodeBytes
(
writable
codeCapacity
)
;
if
(
!
codeBytes
)
{
return
nullptr
;
}
return
js_new
<
CodeSegment
>
(
std
:
:
move
(
codeBytes
)
codeLength
codeCapacity
)
;
}
SharedCodeSegment
CodeSegment
:
:
createFromMasm
(
MacroAssembler
&
masm
const
LinkData
&
linkData
const
CodeBlock
*
maybeSharedStubs
)
{
uint32_t
codeLength
=
masm
.
bytesNeeded
(
)
;
if
(
codeLength
=
=
0
)
{
return
js_new
<
CodeSegment
>
(
nullptr
0
0
)
;
}
uint32_t
codeCapacity
=
RoundupCodeLength
(
codeLength
)
;
Maybe
<
AutoMarkJitCodeWritableForThread
>
writable
;
UniqueCodeBytes
codeBytes
=
AllocateCodeBytes
(
writable
codeCapacity
)
;
if
(
!
codeBytes
)
{
return
nullptr
;
}
masm
.
executableCopy
(
codeBytes
.
get
(
)
)
;
SharedCodeSegment
segment
=
js_new
<
CodeSegment
>
(
std
:
:
move
(
codeBytes
)
codeLength
codeCapacity
)
;
if
(
!
segment
|
|
!
segment
-
>
linkAndMakeExecutable
(
*
writable
linkData
maybeSharedStubs
)
)
{
return
nullptr
;
}
return
segment
;
}
SharedCodeSegment
CodeSegment
:
:
createFromBytes
(
const
uint8_t
*
unlinkedBytes
size_t
unlinkedBytesLength
const
LinkData
&
linkData
const
CodeBlock
*
maybeSharedStubs
)
{
uint32_t
codeLength
=
unlinkedBytesLength
;
if
(
codeLength
=
=
0
)
{
return
js_new
<
CodeSegment
>
(
nullptr
0
0
)
;
}
uint32_t
codeCapacity
=
RoundupCodeLength
(
codeLength
)
;
Maybe
<
AutoMarkJitCodeWritableForThread
>
writable
;
UniqueCodeBytes
codeBytes
=
AllocateCodeBytes
(
writable
codeLength
)
;
if
(
!
codeBytes
)
{
return
nullptr
;
}
memcpy
(
codeBytes
.
get
(
)
unlinkedBytes
unlinkedBytesLength
)
;
SharedCodeSegment
segment
=
js_new
<
CodeSegment
>
(
std
:
:
move
(
codeBytes
)
codeLength
codeCapacity
)
;
if
(
!
segment
|
|
!
segment
-
>
linkAndMakeExecutable
(
*
writable
linkData
maybeSharedStubs
)
)
{
return
nullptr
;
}
return
segment
;
}
void
CodeSegment
:
:
addSizeOfMisc
(
MallocSizeOf
mallocSizeOf
size_t
*
code
size_t
*
data
)
const
{
*
code
+
=
capacityBytes
(
)
;
*
data
+
=
mallocSizeOf
(
this
)
;
}
size_t
CacheableChars
:
:
sizeOfExcludingThis
(
MallocSizeOf
mallocSizeOf
)
const
{
return
mallocSizeOf
(
get
(
)
)
;
}
static
void
PadCodeForSingleStub
(
MacroAssembler
&
masm
)
{
static
uint8_t
zeroes
[
64
]
;
static
mozilla
:
:
Atomic
<
uint32_t
mozilla
:
:
MemoryOrdering
:
:
ReleaseAcquire
>
counter
(
0
)
;
uint32_t
maxPadLines
=
(
(
gc
:
:
SystemPageSize
(
)
*
3
)
/
4
)
/
sizeof
(
zeroes
)
;
uint32_t
padLines
=
counter
+
+
%
maxPadLines
;
for
(
uint32_t
i
=
0
;
i
<
padLines
;
i
+
+
)
{
masm
.
appendRawCode
(
zeroes
sizeof
(
zeroes
)
)
;
}
}
static
constexpr
unsigned
LAZY_STUB_LIFO_DEFAULT_CHUNK_SIZE
=
8
*
1024
;
bool
Code
:
:
createManyLazyEntryStubs
(
const
WriteGuard
&
guard
const
Uint32Vector
&
funcExportIndices
const
CodeBlock
&
tierCodeBlock
size_t
*
stubBlockIndex
)
const
{
MOZ_ASSERT
(
funcExportIndices
.
length
(
)
)
;
LifoAlloc
lifo
(
LAZY_STUB_LIFO_DEFAULT_CHUNK_SIZE
)
;
TempAllocator
alloc
(
&
lifo
)
;
JitContext
jitContext
;
WasmMacroAssembler
masm
(
alloc
)
;
if
(
funcExportIndices
.
length
(
)
=
=
1
)
{
PadCodeForSingleStub
(
masm
)
;
}
const
FuncExportVector
&
funcExports
=
tierCodeBlock
.
funcExports
;
uint8_t
*
segmentBase
=
tierCodeBlock
.
segment
-
>
base
(
)
;
CodeRangeVector
codeRanges
;
DebugOnly
<
uint32_t
>
numExpectedRanges
=
0
;
for
(
uint32_t
funcExportIndex
:
funcExportIndices
)
{
const
FuncExport
&
fe
=
funcExports
[
funcExportIndex
]
;
const
FuncType
&
funcType
=
getFuncExportType
(
fe
)
;
numExpectedRanges
+
=
(
funcType
.
canHaveJitEntry
(
)
?
2
:
1
)
;
void
*
calleePtr
=
segmentBase
+
tierCodeBlock
.
codeRange
(
fe
)
.
funcUncheckedCallEntry
(
)
;
Maybe
<
ImmPtr
>
callee
;
callee
.
emplace
(
calleePtr
ImmPtr
:
:
NoCheckToken
(
)
)
;
if
(
!
GenerateEntryStubs
(
masm
funcExportIndex
fe
funcType
callee
false
&
codeRanges
)
)
{
return
false
;
}
}
MOZ_ASSERT
(
codeRanges
.
length
(
)
=
=
numExpectedRanges
"
incorrect
number
of
entries
per
function
"
)
;
masm
.
finish
(
)
;
MOZ_ASSERT
(
masm
.
callSites
(
)
.
empty
(
)
)
;
MOZ_ASSERT
(
masm
.
callSiteTargets
(
)
.
empty
(
)
)
;
MOZ_ASSERT
(
masm
.
trapSites
(
)
.
empty
(
)
)
;
MOZ_ASSERT
(
masm
.
tryNotes
(
)
.
empty
(
)
)
;
MOZ_ASSERT
(
masm
.
codeRangeUnwindInfos
(
)
.
empty
(
)
)
;
if
(
masm
.
oom
(
)
)
{
return
false
;
}
size_t
codeLength
=
CodeSegment
:
:
AlignBytesNeeded
(
masm
.
bytesNeeded
(
)
)
;
if
(
guard
-
>
lazySegments
.
length
(
)
=
=
0
|
|
!
guard
-
>
lazySegments
[
guard
-
>
lazySegments
.
length
(
)
-
1
]
-
>
hasSpace
(
codeLength
)
)
{
SharedCodeSegment
newSegment
=
CodeSegment
:
:
createEmpty
(
codeLength
)
;
if
(
!
newSegment
)
{
return
false
;
}
if
(
!
guard
-
>
lazySegments
.
emplaceBack
(
std
:
:
move
(
newSegment
)
)
)
{
return
false
;
}
}
MOZ_ASSERT
(
guard
-
>
lazySegments
.
length
(
)
>
0
)
;
CodeSegment
*
segment
=
guard
-
>
lazySegments
[
guard
-
>
lazySegments
.
length
(
)
-
1
]
.
get
(
)
;
uint8_t
*
codePtr
=
nullptr
;
segment
-
>
claimSpace
(
codeLength
&
codePtr
)
;
size_t
offsetInSegment
=
codePtr
-
segment
-
>
base
(
)
;
UniqueCodeBlock
stubCodeBlock
=
MakeUnique
<
CodeBlock
>
(
CodeBlockKind
:
:
LazyStubs
)
;
if
(
!
stubCodeBlock
)
{
return
false
;
}
stubCodeBlock
-
>
segment
=
segment
;
stubCodeBlock
-
>
codeBase
=
codePtr
;
stubCodeBlock
-
>
codeLength
=
codeLength
;
stubCodeBlock
-
>
codeRanges
=
std
:
:
move
(
codeRanges
)
;
{
AutoMarkJitCodeWritableForThread
writable
;
masm
.
executableCopy
(
codePtr
)
;
PatchDebugSymbolicAccesses
(
codePtr
masm
)
;
memset
(
codePtr
+
masm
.
bytesNeeded
(
)
0
codeLength
-
masm
.
bytesNeeded
(
)
)
;
for
(
const
CodeLabel
&
label
:
masm
.
codeLabels
(
)
)
{
Assembler
:
:
Bind
(
codePtr
label
)
;
}
}
if
(
!
ExecutableAllocator
:
:
makeExecutableAndFlushICache
(
codePtr
codeLength
)
)
{
return
false
;
}
*
stubBlockIndex
=
guard
-
>
blocks
.
length
(
)
;
uint32_t
codeRangeIndex
=
0
;
for
(
uint32_t
funcExportIndex
:
funcExportIndices
)
{
const
FuncExport
&
fe
=
funcExports
[
funcExportIndex
]
;
const
FuncType
&
funcType
=
getFuncExportType
(
fe
)
;
LazyFuncExport
lazyExport
(
fe
.
funcIndex
(
)
*
stubBlockIndex
codeRangeIndex
tierCodeBlock
.
kind
)
;
CodeRange
&
interpRange
=
stubCodeBlock
-
>
codeRanges
[
codeRangeIndex
]
;
MOZ_ASSERT
(
interpRange
.
isInterpEntry
(
)
)
;
MOZ_ASSERT
(
interpRange
.
funcIndex
(
)
=
=
fe
.
funcIndex
(
)
)
;
interpRange
.
offsetBy
(
offsetInSegment
)
;
codeRangeIndex
+
=
1
;
if
(
funcType
.
canHaveJitEntry
(
)
)
{
CodeRange
&
jitRange
=
stubCodeBlock
-
>
codeRanges
[
codeRangeIndex
]
;
MOZ_ASSERT
(
jitRange
.
isJitEntry
(
)
)
;
MOZ_ASSERT
(
jitRange
.
funcIndex
(
)
=
=
fe
.
funcIndex
(
)
)
;
codeRangeIndex
+
=
1
;
jitRange
.
offsetBy
(
offsetInSegment
)
;
}
size_t
exportIndex
;
const
uint32_t
targetFunctionIndex
=
fe
.
funcIndex
(
)
;
if
(
BinarySearchIf
(
guard
-
>
lazyExports
0
guard
-
>
lazyExports
.
length
(
)
[
targetFunctionIndex
]
(
const
LazyFuncExport
&
funcExport
)
{
return
targetFunctionIndex
-
funcExport
.
funcIndex
;
}
&
exportIndex
)
)
{
DebugOnly
<
CodeBlockKind
>
oldKind
=
guard
-
>
lazyExports
[
exportIndex
]
.
funcKind
;
MOZ_ASSERT
(
oldKind
=
=
CodeBlockKind
:
:
SharedStubs
|
|
oldKind
=
=
CodeBlockKind
:
:
BaselineTier
)
;
guard
-
>
lazyExports
[
exportIndex
]
=
std
:
:
move
(
lazyExport
)
;
}
else
if
(
!
guard
-
>
lazyExports
.
insert
(
guard
-
>
lazyExports
.
begin
(
)
+
exportIndex
std
:
:
move
(
lazyExport
)
)
)
{
return
false
;
}
}
if
(
!
stubCodeBlock
-
>
initialize
(
*
tierCodeBlock
.
code
)
)
{
return
false
;
}
return
guard
-
>
blocks
.
append
(
std
:
:
move
(
stubCodeBlock
)
)
;
}
bool
Code
:
:
createOneLazyEntryStub
(
const
WriteGuard
&
guard
uint32_t
funcExportIndex
const
CodeBlock
&
tierCodeBlock
void
*
*
interpEntry
)
const
{
Uint32Vector
funcExportIndexes
;
if
(
!
funcExportIndexes
.
append
(
funcExportIndex
)
)
{
return
false
;
}
size_t
stubBlockIndex
;
if
(
!
createManyLazyEntryStubs
(
guard
funcExportIndexes
tierCodeBlock
&
stubBlockIndex
)
)
{
return
false
;
}
const
CodeBlock
&
block
=
*
guard
-
>
blocks
[
stubBlockIndex
]
;
const
CodeSegment
&
segment
=
*
block
.
segment
;
const
CodeRangeVector
&
codeRanges
=
block
.
codeRanges
;
const
FuncExport
&
fe
=
tierCodeBlock
.
funcExports
[
funcExportIndex
]
;
const
FuncType
&
funcType
=
getFuncExportType
(
fe
)
;
uint32_t
funcEntryRanges
=
funcType
.
canHaveJitEntry
(
)
?
2
:
1
;
MOZ_ASSERT
(
codeRanges
.
length
(
)
>
=
funcEntryRanges
)
;
const
CodeRange
&
interpRange
=
codeRanges
[
codeRanges
.
length
(
)
-
funcEntryRanges
]
;
MOZ_ASSERT
(
interpRange
.
isInterpEntry
(
)
)
;
*
interpEntry
=
segment
.
base
(
)
+
interpRange
.
begin
(
)
;
if
(
funcType
.
canHaveJitEntry
(
)
)
{
const
CodeRange
&
jitRange
=
codeRanges
[
codeRanges
.
length
(
)
-
funcEntryRanges
+
1
]
;
MOZ_ASSERT
(
jitRange
.
isJitEntry
(
)
)
;
jumpTables_
.
setJitEntry
(
jitRange
.
funcIndex
(
)
segment
.
base
(
)
+
jitRange
.
begin
(
)
)
;
}
return
true
;
}
bool
Code
:
:
getOrCreateInterpEntry
(
uint32_t
funcIndex
const
FuncExport
*
*
funcExport
void
*
*
interpEntry
)
const
{
size_t
funcExportIndex
;
const
CodeBlock
&
codeBlock
=
funcCodeBlock
(
funcIndex
)
;
*
funcExport
=
&
codeBlock
.
lookupFuncExport
(
funcIndex
&
funcExportIndex
)
;
const
FuncExport
&
fe
=
*
*
funcExport
;
if
(
fe
.
hasEagerStubs
(
)
)
{
*
interpEntry
=
codeBlock
.
segment
-
>
base
(
)
+
fe
.
eagerInterpEntryOffset
(
)
;
return
true
;
}
MOZ_ASSERT
(
!
codeMetaForAsmJS_
"
only
wasm
can
lazily
export
functions
"
)
;
auto
guard
=
data_
.
writeLock
(
)
;
*
interpEntry
=
lookupLazyInterpEntry
(
guard
funcIndex
)
;
if
(
*
interpEntry
)
{
return
true
;
}
return
createOneLazyEntryStub
(
guard
funcExportIndex
codeBlock
interpEntry
)
;
}
bool
Code
:
:
createTier2LazyEntryStubs
(
const
WriteGuard
&
guard
const
CodeBlock
&
tier2Code
Maybe
<
size_t
>
*
outStubBlockIndex
)
const
{
if
(
!
guard
-
>
lazyExports
.
length
(
)
)
{
return
true
;
}
Uint32Vector
funcExportIndices
;
if
(
!
funcExportIndices
.
reserve
(
guard
-
>
lazyExports
.
length
(
)
)
)
{
return
false
;
}
for
(
size_t
i
=
0
;
i
<
guard
-
>
lazyExports
.
length
(
)
;
i
+
+
)
{
const
LazyFuncExport
&
lfe
=
guard
-
>
lazyExports
[
i
]
;
MOZ_ASSERT
(
lfe
.
funcKind
=
=
CodeBlockKind
:
:
SharedStubs
|
|
lfe
.
funcKind
=
=
CodeBlockKind
:
:
BaselineTier
)
;
size_t
funcExportIndex
;
tier2Code
.
lookupFuncExport
(
lfe
.
funcIndex
&
funcExportIndex
)
;
funcExportIndices
.
infallibleAppend
(
funcExportIndex
)
;
}
size_t
stubBlockIndex
;
if
(
!
createManyLazyEntryStubs
(
guard
funcExportIndices
tier2Code
&
stubBlockIndex
)
)
{
return
false
;
}
outStubBlockIndex
-
>
emplace
(
stubBlockIndex
)
;
return
true
;
}
bool
Code
:
:
finishCompleteTier2
(
const
LinkData
&
linkData
UniqueCodeBlock
tier2Code
)
const
{
MOZ_RELEASE_ASSERT
(
bestTier
(
)
=
=
Tier
:
:
Baseline
&
&
tier2Code
-
>
tier
(
)
=
=
Tier
:
:
Optimized
)
;
{
auto
guard
=
data_
.
writeLock
(
)
;
CodeBlock
*
tier2CodePointer
=
tier2Code
.
get
(
)
;
if
(
!
tier2Code
-
>
initialize
(
*
this
)
|
|
!
guard
-
>
blocks
.
append
(
std
:
:
move
(
tier2Code
)
)
|
|
!
blockMap_
.
insert
(
tier2CodePointer
)
)
{
return
false
;
}
Maybe
<
size_t
>
stub2Index
;
if
(
!
createTier2LazyEntryStubs
(
guard
*
tier2CodePointer
&
stub2Index
)
)
{
return
false
;
}
jit
:
:
FlushExecutionContextForAllThreads
(
)
;
tier2_
=
tier2CodePointer
;
hasTier2_
=
true
;
MOZ_ASSERT
(
hasTier2
(
)
)
;
if
(
stub2Index
)
{
const
CodeBlock
&
block
=
*
guard
-
>
blocks
[
*
stub2Index
]
;
const
CodeSegment
&
segment
=
*
block
.
segment
;
for
(
const
CodeRange
&
cr
:
block
.
codeRanges
)
{
if
(
!
cr
.
isJitEntry
(
)
)
{
continue
;
}
jumpTables_
.
setJitEntry
(
cr
.
funcIndex
(
)
segment
.
base
(
)
+
cr
.
begin
(
)
)
;
}
}
}
const
CodeBlock
&
optimizedTierCode
=
completeTierCodeBlock
(
Tier
:
:
Optimized
)
;
uint8_t
*
base
=
optimizedTierCode
.
segment
-
>
base
(
)
;
for
(
const
CodeRange
&
cr
:
optimizedTierCode
.
codeRanges
)
{
if
(
cr
.
isFunction
(
)
)
{
jumpTables_
.
setTieringEntry
(
cr
.
funcIndex
(
)
base
+
cr
.
funcTierEntry
(
)
)
;
}
else
if
(
cr
.
isJitEntry
(
)
)
{
jumpTables_
.
setJitEntry
(
cr
.
funcIndex
(
)
base
+
cr
.
begin
(
)
)
;
}
}
return
true
;
}
void
*
Code
:
:
lookupLazyInterpEntry
(
const
WriteGuard
&
guard
uint32_t
funcIndex
)
const
{
size_t
match
;
if
(
!
BinarySearchIf
(
guard
-
>
lazyExports
0
guard
-
>
lazyExports
.
length
(
)
[
funcIndex
]
(
const
LazyFuncExport
&
funcExport
)
{
return
funcIndex
-
funcExport
.
funcIndex
;
}
&
match
)
)
{
return
nullptr
;
}
const
LazyFuncExport
&
fe
=
guard
-
>
lazyExports
[
match
]
;
const
CodeBlock
&
block
=
*
guard
-
>
blocks
[
fe
.
lazyStubBlockIndex
]
;
const
CodeSegment
&
segment
=
*
block
.
segment
;
return
segment
.
base
(
)
+
block
.
codeRanges
[
fe
.
funcCodeRangeIndex
]
.
begin
(
)
;
}
CodeBlock
:
:
~
CodeBlock
(
)
{
if
(
unregisterOnDestroy_
)
{
UnregisterCodeBlock
(
this
)
;
}
}
bool
CodeBlock
:
:
initialize
(
const
Code
&
code
)
{
MOZ_ASSERT
(
!
initialized
(
)
)
;
this
-
>
code
=
&
code
;
segment
-
>
setCode
(
code
)
;
SendCodeRangesToProfiler
(
segment
-
>
base
(
)
code
.
codeMeta
(
)
code
.
codeMetaForAsmJS
(
)
codeRanges
)
;
if
(
!
RegisterCodeBlock
(
this
)
)
{
return
false
;
}
MOZ_ASSERT
(
!
unregisterOnDestroy_
)
;
unregisterOnDestroy_
=
true
;
MOZ_ASSERT
(
initialized
(
)
)
;
return
true
;
}
void
CodeBlock
:
:
addSizeOfMisc
(
MallocSizeOf
mallocSizeOf
size_t
*
code
size_t
*
data
)
const
{
segment
-
>
addSizeOfMisc
(
mallocSizeOf
code
data
)
;
*
data
+
=
funcToCodeRange
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
codeRanges
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
callSites
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
tryNotes
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
codeRangeUnwindInfos
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
trapSites
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
stackMaps
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
funcExports
.
sizeOfExcludingThis
(
mallocSizeOf
)
;
;
}
const
CodeRange
*
CodeBlock
:
:
lookupRange
(
const
void
*
pc
)
const
{
CodeRange
:
:
OffsetInCode
target
(
(
uint8_t
*
)
pc
-
segment
-
>
base
(
)
)
;
return
LookupInSorted
(
codeRanges
target
)
;
}
struct
CallSiteRetAddrOffset
{
const
CallSiteVector
&
callSites
;
explicit
CallSiteRetAddrOffset
(
const
CallSiteVector
&
callSites
)
:
callSites
(
callSites
)
{
}
uint32_t
operator
[
]
(
size_t
index
)
const
{
return
callSites
[
index
]
.
returnAddressOffset
(
)
;
}
}
;
const
CallSite
*
CodeBlock
:
:
lookupCallSite
(
void
*
pc
)
const
{
uint32_t
target
=
(
(
uint8_t
*
)
pc
)
-
segment
-
>
base
(
)
;
size_t
lowerBound
=
0
;
size_t
upperBound
=
callSites
.
length
(
)
;
size_t
match
;
if
(
BinarySearch
(
CallSiteRetAddrOffset
(
callSites
)
lowerBound
upperBound
target
&
match
)
)
{
return
&
callSites
[
match
]
;
}
return
nullptr
;
}
const
StackMap
*
CodeBlock
:
:
lookupStackMap
(
uint8_t
*
pc
)
const
{
return
stackMaps
.
findMap
(
pc
)
;
}
const
wasm
:
:
TryNote
*
CodeBlock
:
:
lookupTryNote
(
const
void
*
pc
)
const
{
size_t
target
=
(
uint8_t
*
)
pc
-
segment
-
>
base
(
)
;
for
(
const
auto
&
tryNote
:
tryNotes
)
{
if
(
tryNote
.
offsetWithinTryBody
(
target
)
)
{
return
&
tryNote
;
}
}
return
nullptr
;
}
struct
TrapSitePCOffset
{
const
TrapSiteVector
&
trapSites
;
explicit
TrapSitePCOffset
(
const
TrapSiteVector
&
trapSites
)
:
trapSites
(
trapSites
)
{
}
uint32_t
operator
[
]
(
size_t
index
)
const
{
return
trapSites
[
index
]
.
pcOffset
;
}
}
;
bool
CodeBlock
:
:
lookupTrap
(
void
*
pc
Trap
*
trapOut
BytecodeOffset
*
bytecode
)
const
{
uint32_t
target
=
(
(
uint8_t
*
)
pc
)
-
segment
-
>
base
(
)
;
for
(
Trap
trap
:
MakeEnumeratedRange
(
Trap
:
:
Limit
)
)
{
const
TrapSiteVector
&
trapSitesForKind
=
trapSites
[
trap
]
;
size_t
upperBound
=
trapSitesForKind
.
length
(
)
;
size_t
match
;
if
(
BinarySearch
(
TrapSitePCOffset
(
trapSitesForKind
)
0
upperBound
target
&
match
)
)
{
MOZ_ASSERT
(
containsCodePC
(
pc
)
)
;
*
trapOut
=
trap
;
*
bytecode
=
trapSitesForKind
[
match
]
.
bytecode
;
return
true
;
}
}
return
false
;
}
struct
UnwindInfoPCOffset
{
const
CodeRangeUnwindInfoVector
&
info
;
explicit
UnwindInfoPCOffset
(
const
CodeRangeUnwindInfoVector
&
info
)
:
info
(
info
)
{
}
uint32_t
operator
[
]
(
size_t
index
)
const
{
return
info
[
index
]
.
offset
(
)
;
}
}
;
const
CodeRangeUnwindInfo
*
CodeBlock
:
:
lookupUnwindInfo
(
void
*
pc
)
const
{
uint32_t
target
=
(
(
uint8_t
*
)
pc
)
-
segment
-
>
base
(
)
;
size_t
match
;
const
CodeRangeUnwindInfo
*
info
=
nullptr
;
if
(
BinarySearch
(
UnwindInfoPCOffset
(
codeRangeUnwindInfos
)
0
codeRangeUnwindInfos
.
length
(
)
target
&
match
)
)
{
info
=
&
codeRangeUnwindInfos
[
match
]
;
}
else
{
if
(
match
=
=
0
)
return
nullptr
;
if
(
match
=
=
codeRangeUnwindInfos
.
length
(
)
)
{
MOZ_ASSERT
(
codeRangeUnwindInfos
[
codeRangeUnwindInfos
.
length
(
)
-
1
]
.
unwindHow
(
)
=
=
CodeRangeUnwindInfo
:
:
Normal
)
;
return
nullptr
;
}
info
=
&
codeRangeUnwindInfos
[
match
-
1
]
;
}
return
info
-
>
unwindHow
(
)
=
=
CodeRangeUnwindInfo
:
:
Normal
?
nullptr
:
info
;
}
struct
ProjectFuncIndex
{
const
FuncExportVector
&
funcExports
;
explicit
ProjectFuncIndex
(
const
FuncExportVector
&
funcExports
)
:
funcExports
(
funcExports
)
{
}
uint32_t
operator
[
]
(
size_t
index
)
const
{
return
funcExports
[
index
]
.
funcIndex
(
)
;
}
}
;
FuncExport
&
CodeBlock
:
:
lookupFuncExport
(
uint32_t
funcIndex
size_t
*
funcExportIndex
)
{
size_t
match
;
if
(
!
BinarySearch
(
ProjectFuncIndex
(
funcExports
)
0
funcExports
.
length
(
)
funcIndex
&
match
)
)
{
MOZ_CRASH
(
"
missing
function
export
"
)
;
}
if
(
funcExportIndex
)
{
*
funcExportIndex
=
match
;
}
return
funcExports
[
match
]
;
}
const
FuncExport
&
CodeBlock
:
:
lookupFuncExport
(
uint32_t
funcIndex
size_t
*
funcExportIndex
)
const
{
return
const_cast
<
CodeBlock
*
>
(
this
)
-
>
lookupFuncExport
(
funcIndex
funcExportIndex
)
;
}
bool
JumpTables
:
:
initialize
(
CompileMode
mode
const
CodeBlock
&
sharedStubs
const
CodeBlock
&
tier1
)
{
static_assert
(
JSScript
:
:
offsetOfJitCodeRaw
(
)
=
=
0
"
wasm
fast
jit
entry
is
at
(
void
*
)
jit
[
funcIndex
]
"
)
;
mode_
=
mode
;
size_t
numFuncs
=
0
;
for
(
const
CodeRange
&
cr
:
sharedStubs
.
codeRanges
)
{
if
(
cr
.
isFunction
(
)
)
{
numFuncs
+
+
;
}
}
for
(
const
CodeRange
&
cr
:
tier1
.
codeRanges
)
{
if
(
cr
.
isFunction
(
)
)
{
numFuncs
+
+
;
}
}
numFuncs_
=
numFuncs
;
if
(
mode_
=
=
CompileMode
:
:
Tier1
)
{
tiering_
=
TablePointer
(
js_pod_calloc
<
void
*
>
(
numFuncs
)
)
;
if
(
!
tiering_
)
{
return
false
;
}
}
jit_
=
TablePointer
(
js_pod_calloc
<
void
*
>
(
numFuncs
)
)
;
if
(
!
jit_
)
{
return
false
;
}
uint8_t
*
codeBase
=
sharedStubs
.
segment
-
>
base
(
)
;
for
(
const
CodeRange
&
cr
:
sharedStubs
.
codeRanges
)
{
if
(
cr
.
isFunction
(
)
)
{
setTieringEntry
(
cr
.
funcIndex
(
)
codeBase
+
cr
.
funcTierEntry
(
)
)
;
}
else
if
(
cr
.
isJitEntry
(
)
)
{
setJitEntry
(
cr
.
funcIndex
(
)
codeBase
+
cr
.
begin
(
)
)
;
}
}
codeBase
=
tier1
.
segment
-
>
base
(
)
;
for
(
const
CodeRange
&
cr
:
tier1
.
codeRanges
)
{
if
(
cr
.
isFunction
(
)
)
{
setTieringEntry
(
cr
.
funcIndex
(
)
codeBase
+
cr
.
funcTierEntry
(
)
)
;
}
else
if
(
cr
.
isJitEntry
(
)
)
{
setJitEntry
(
cr
.
funcIndex
(
)
codeBase
+
cr
.
begin
(
)
)
;
}
}
return
true
;
}
Code
:
:
Code
(
CompileMode
mode
const
CodeMetadata
&
codeMeta
const
CodeMetadataForAsmJS
*
codeMetaForAsmJS
)
:
mode_
(
mode
)
data_
(
mutexid
:
:
WasmCodeProtected
)
codeMeta_
(
&
codeMeta
)
codeMetaForAsmJS_
(
codeMetaForAsmJS
)
tier1_
(
nullptr
)
tier2_
(
nullptr
)
profilingLabels_
(
mutexid
:
:
WasmCodeProfilingLabels
CacheableCharsVector
(
)
)
trapCode_
(
nullptr
)
{
}
bool
Code
:
:
initialize
(
FuncImportVector
&
&
funcImports
UniqueCodeBlock
sharedStubs
const
LinkData
&
sharedStubsLinkData
UniqueCodeBlock
tierCodeBlock
)
{
MOZ_ASSERT
(
!
initialized
(
)
)
;
funcImports_
=
std
:
:
move
(
funcImports
)
;
auto
guard
=
data_
.
writeLock
(
)
;
CodeBlock
*
sharedStubsCodePointer
=
sharedStubs
.
get
(
)
;
CodeBlock
*
tier1CodePointer
=
tierCodeBlock
.
get
(
)
;
sharedStubs_
=
sharedStubs
.
get
(
)
;
tier1_
=
tierCodeBlock
.
get
(
)
;
trapCode_
=
sharedStubs_
-
>
segment
-
>
base
(
)
+
sharedStubsLinkData
.
trapOffset
;
if
(
!
jumpTables_
.
initialize
(
mode_
*
sharedStubs_
*
tier1_
)
|
|
!
guard
-
>
blocks
.
append
(
std
:
:
move
(
sharedStubs
)
)
|
|
!
guard
-
>
blocks
.
append
(
std
:
:
move
(
tierCodeBlock
)
)
|
|
!
blockMap_
.
insert
(
sharedStubs_
)
|
|
!
blockMap_
.
insert
(
tier1_
)
)
{
tier1_
=
nullptr
;
MOZ_ASSERT
(
!
initialized
(
)
)
;
return
false
;
}
if
(
!
tier1CodePointer
-
>
initialize
(
*
this
)
|
|
!
sharedStubsCodePointer
-
>
initialize
(
*
this
)
)
{
tier1_
=
nullptr
;
MOZ_ASSERT
(
!
initialized
(
)
)
;
return
false
;
}
MOZ_ASSERT
(
initialized
(
)
)
;
return
true
;
}
uint32_t
Code
:
:
getFuncIndex
(
JSFunction
*
fun
)
const
{
MOZ_ASSERT
(
fun
-
>
isWasm
(
)
|
|
fun
-
>
isAsmJSNative
(
)
)
;
if
(
!
fun
-
>
isWasmWithJitEntry
(
)
)
{
return
fun
-
>
wasmFuncIndex
(
)
;
}
return
jumpTables_
.
funcIndexFromJitEntry
(
fun
-
>
wasmJitEntry
(
)
)
;
}
Tiers
Code
:
:
tiers
(
)
const
{
if
(
hasTier2
(
)
)
{
return
Tiers
(
tier1_
-
>
tier
(
)
tier2_
-
>
tier
(
)
)
;
}
return
Tiers
(
tier1_
-
>
tier
(
)
)
;
}
bool
Code
:
:
hasTier
(
Tier
t
)
const
{
if
(
hasTier2
(
)
&
&
tier2_
-
>
tier
(
)
=
=
t
)
{
return
true
;
}
return
tier1_
-
>
tier
(
)
=
=
t
;
}
Tier
Code
:
:
stableTier
(
)
const
{
return
tier1_
-
>
tier
(
)
;
}
Tier
Code
:
:
bestTier
(
)
const
{
if
(
hasTier2
(
)
)
{
return
tier2_
-
>
tier
(
)
;
}
return
tier1_
-
>
tier
(
)
;
}
const
CodeBlock
&
Code
:
:
codeBlock
(
Tier
tier
)
const
{
switch
(
tier
)
{
case
Tier
:
:
Baseline
:
if
(
tier1_
-
>
tier
(
)
=
=
Tier
:
:
Baseline
)
{
MOZ_ASSERT
(
tier1_
-
>
initialized
(
)
)
;
return
*
tier1_
;
}
MOZ_CRASH
(
"
No
code
segment
at
this
tier
"
)
;
case
Tier
:
:
Optimized
:
if
(
tier1_
-
>
tier
(
)
=
=
Tier
:
:
Optimized
)
{
MOZ_ASSERT
(
tier1_
-
>
initialized
(
)
)
;
return
*
tier1_
;
}
MOZ_RELEASE_ASSERT
(
hasTier2
(
)
)
;
MOZ_ASSERT
(
tier2_
-
>
initialized
(
)
)
;
return
*
tier2_
;
}
MOZ_CRASH
(
)
;
}
bool
Code
:
:
lookupFunctionTier
(
const
CodeRange
*
codeRange
Tier
*
tier
)
const
{
MOZ_ASSERT
(
codeRange
-
>
isFunction
(
)
)
;
for
(
Tier
t
:
tiers
(
)
)
{
const
CodeBlock
&
code
=
completeTierCodeBlock
(
t
)
;
if
(
codeRange
>
=
code
.
codeRanges
.
begin
(
)
&
&
codeRange
<
code
.
codeRanges
.
end
(
)
)
{
*
tier
=
t
;
return
true
;
}
}
return
false
;
}
void
Code
:
:
ensureProfilingLabels
(
bool
profilingEnabled
)
const
{
auto
labels
=
profilingLabels_
.
lock
(
)
;
if
(
!
profilingEnabled
)
{
labels
-
>
clear
(
)
;
return
;
}
if
(
!
labels
-
>
empty
(
)
)
{
return
;
}
const
CodeBlock
&
sharedStubsCodeBlock
=
sharedStubs
(
)
;
const
CodeBlock
&
tier1CodeBlock
=
completeTierCodeBlock
(
stableTier
(
)
)
;
(
void
)
appendProfilingLabels
(
labels
sharedStubsCodeBlock
)
;
(
void
)
appendProfilingLabels
(
labels
tier1CodeBlock
)
;
}
bool
Code
:
:
appendProfilingLabels
(
const
ExclusiveData
<
CacheableCharsVector
>
:
:
Guard
&
labels
const
CodeBlock
&
codeBlock
)
const
{
for
(
const
CodeRange
&
codeRange
:
codeBlock
.
codeRanges
)
{
if
(
!
codeRange
.
isFunction
(
)
)
{
continue
;
}
Int32ToCStringBuf
cbuf
;
size_t
bytecodeStrLen
;
const
char
*
bytecodeStr
=
Uint32ToCString
(
&
cbuf
codeRange
.
funcLineOrBytecode
(
)
&
bytecodeStrLen
)
;
MOZ_ASSERT
(
bytecodeStr
)
;
UTF8Bytes
name
;
bool
ok
;
if
(
codeMetaForAsmJS
(
)
)
{
ok
=
codeMetaForAsmJS
(
)
-
>
getFuncNameForAsmJS
(
codeRange
.
funcIndex
(
)
&
name
)
;
}
else
{
ok
=
codeMeta
(
)
.
getFuncNameForWasm
(
NameContext
:
:
Standalone
codeRange
.
funcIndex
(
)
&
name
)
;
}
if
(
!
ok
|
|
!
name
.
append
(
"
(
"
2
)
)
{
return
false
;
}
if
(
const
char
*
filename
=
codeMeta
(
)
.
filename
.
get
(
)
)
{
if
(
!
name
.
append
(
filename
strlen
(
filename
)
)
)
{
return
false
;
}
}
else
{
if
(
!
name
.
append
(
'
?
'
)
)
{
return
false
;
}
}
if
(
!
name
.
append
(
'
:
'
)
|
|
!
name
.
append
(
bytecodeStr
bytecodeStrLen
)
|
|
!
name
.
append
(
"
)
\
0
"
2
)
)
{
return
false
;
}
UniqueChars
label
(
name
.
extractOrCopyRawBuffer
(
)
)
;
if
(
!
label
)
{
return
false
;
}
if
(
codeRange
.
funcIndex
(
)
>
=
labels
-
>
length
(
)
)
{
if
(
!
labels
-
>
resize
(
codeRange
.
funcIndex
(
)
+
1
)
)
{
return
false
;
}
}
(
(
CacheableCharsVector
&
)
labels
)
[
codeRange
.
funcIndex
(
)
]
=
std
:
:
move
(
label
)
;
}
return
true
;
}
const
char
*
Code
:
:
profilingLabel
(
uint32_t
funcIndex
)
const
{
auto
labels
=
profilingLabels_
.
lock
(
)
;
if
(
funcIndex
>
=
labels
-
>
length
(
)
|
|
!
(
(
CacheableCharsVector
&
)
labels
)
[
funcIndex
]
)
{
return
"
?
"
;
}
return
(
(
CacheableCharsVector
&
)
labels
)
[
funcIndex
]
.
get
(
)
;
}
void
Code
:
:
addSizeOfMiscIfNotSeen
(
MallocSizeOf
mallocSizeOf
CodeMetadata
:
:
SeenSet
*
seenCodeMeta
CodeMetadataForAsmJS
:
:
SeenSet
*
seenCodeMetaForAsmJS
Code
:
:
SeenSet
*
seenCode
size_t
*
code
size_t
*
data
)
const
{
auto
p
=
seenCode
-
>
lookupForAdd
(
this
)
;
if
(
p
)
{
return
;
}
bool
ok
=
seenCode
-
>
add
(
p
this
)
;
(
void
)
ok
;
auto
guard
=
data_
.
readLock
(
)
;
*
data
+
=
mallocSizeOf
(
this
)
+
guard
-
>
lazyExports
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
(
codeMetaForAsmJS
(
)
?
codeMetaForAsmJS
(
)
-
>
sizeOfIncludingThisIfNotSeen
(
mallocSizeOf
seenCodeMetaForAsmJS
)
:
0
)
+
funcImports_
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
profilingLabels_
.
lock
(
)
-
>
sizeOfExcludingThis
(
mallocSizeOf
)
+
jumpTables_
.
sizeOfMiscExcludingThis
(
)
;
for
(
const
SharedCodeSegment
&
stub
:
guard
-
>
lazySegments
)
{
stub
-
>
addSizeOfMisc
(
mallocSizeOf
code
data
)
;
}
sharedStubs
(
)
.
addSizeOfMisc
(
mallocSizeOf
code
data
)
;
for
(
auto
t
:
tiers
(
)
)
{
completeTierCodeBlock
(
t
)
.
addSizeOfMisc
(
mallocSizeOf
code
data
)
;
}
}
void
CodeBlock
:
:
disassemble
(
JSContext
*
cx
int
kindSelection
PrintCallback
printString
)
const
{
for
(
const
CodeRange
&
range
:
codeRanges
)
{
if
(
kindSelection
&
(
1
<
<
range
.
kind
(
)
)
)
{
MOZ_ASSERT
(
range
.
begin
(
)
<
segment
-
>
lengthBytes
(
)
)
;
MOZ_ASSERT
(
range
.
end
(
)
<
segment
-
>
lengthBytes
(
)
)
;
const
char
*
kind
;
char
kindbuf
[
128
]
;
switch
(
range
.
kind
(
)
)
{
case
CodeRange
:
:
Function
:
kind
=
"
Function
"
;
break
;
case
CodeRange
:
:
InterpEntry
:
kind
=
"
InterpEntry
"
;
break
;
case
CodeRange
:
:
JitEntry
:
kind
=
"
JitEntry
"
;
break
;
case
CodeRange
:
:
ImportInterpExit
:
kind
=
"
ImportInterpExit
"
;
break
;
case
CodeRange
:
:
ImportJitExit
:
kind
=
"
ImportJitExit
"
;
break
;
default
:
SprintfLiteral
(
kindbuf
"
CodeRange
:
:
Kind
(
%
d
)
"
range
.
kind
(
)
)
;
kind
=
kindbuf
;
break
;
}
const
char
*
separator
=
"
\
n
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
\
n
"
;
char
buf
[
4096
]
;
if
(
range
.
hasFuncIndex
(
)
)
{
const
char
*
funcName
=
"
(
unknown
)
"
;
UTF8Bytes
namebuf
;
bool
ok
;
if
(
code
-
>
codeMetaForAsmJS
(
)
)
{
ok
=
code
-
>
codeMetaForAsmJS
(
)
-
>
getFuncNameForAsmJS
(
range
.
funcIndex
(
)
&
namebuf
)
;
}
else
{
ok
=
code
-
>
codeMeta
(
)
.
getFuncNameForWasm
(
NameContext
:
:
Standalone
range
.
funcIndex
(
)
&
namebuf
)
;
}
if
(
ok
&
&
namebuf
.
append
(
'
\
0
'
)
)
{
funcName
=
namebuf
.
begin
(
)
;
}
SprintfLiteral
(
buf
"
%
sKind
=
%
s
index
=
%
d
name
=
%
s
:
\
n
"
separator
kind
range
.
funcIndex
(
)
funcName
)
;
}
else
{
SprintfLiteral
(
buf
"
%
sKind
=
%
s
\
n
"
separator
kind
)
;
}
printString
(
buf
)
;
uint8_t
*
theCode
=
segment
-
>
base
(
)
+
range
.
begin
(
)
;
jit
:
:
Disassemble
(
theCode
range
.
end
(
)
-
range
.
begin
(
)
printString
)
;
}
}
}
void
Code
:
:
disassemble
(
JSContext
*
cx
Tier
tier
int
kindSelection
PrintCallback
printString
)
const
{
this
-
>
sharedStubs
(
)
.
disassemble
(
cx
kindSelection
printString
)
;
this
-
>
completeTierCodeBlock
(
tier
)
.
disassemble
(
cx
kindSelection
printString
)
;
}
MetadataAnalysisHashMap
Code
:
:
metadataAnalysis
(
JSContext
*
cx
)
const
{
MetadataAnalysisHashMap
hashmap
;
if
(
!
hashmap
.
reserve
(
14
)
)
{
return
hashmap
;
}
for
(
auto
t
:
tiers
(
)
)
{
const
CodeBlock
&
codeBlock
=
completeTierCodeBlock
(
t
)
;
size_t
length
=
codeBlock
.
funcToCodeRange
.
numEntries
(
)
;
length
+
=
codeBlock
.
codeRanges
.
length
(
)
;
length
+
=
codeBlock
.
callSites
.
length
(
)
;
length
+
=
codeBlock
.
trapSites
.
sumOfLengths
(
)
;
length
+
=
codeBlock
.
funcExports
.
length
(
)
;
length
+
=
codeBlock
.
stackMaps
.
length
(
)
;
length
+
=
codeBlock
.
tryNotes
.
length
(
)
;
hashmap
.
putNewInfallible
(
"
metadata
length
"
length
)
;
size_t
code_size
=
0
;
for
(
const
CodeRange
&
codeRange
:
codeBlock
.
codeRanges
)
{
if
(
!
codeRange
.
isFunction
(
)
)
{
continue
;
}
code_size
+
=
codeRange
.
end
(
)
-
codeRange
.
begin
(
)
;
}
hashmap
.
putNewInfallible
(
"
stackmaps
number
"
codeBlock
.
stackMaps
.
length
(
)
)
;
hashmap
.
putNewInfallible
(
"
trapSites
number
"
codeBlock
.
trapSites
.
sumOfLengths
(
)
)
;
hashmap
.
putNewInfallible
(
"
codeRange
size
in
bytes
"
code_size
)
;
hashmap
.
putNewInfallible
(
"
code
segment
capacity
"
codeBlock
.
segment
-
>
capacityBytes
(
)
)
;
auto
mallocSizeOf
=
cx
-
>
runtime
(
)
-
>
debuggerMallocSizeOf
;
hashmap
.
putNewInfallible
(
"
funcToCodeRange
size
"
codeBlock
.
funcToCodeRange
.
sizeOfExcludingThis
(
mallocSizeOf
)
)
;
hashmap
.
putNewInfallible
(
"
codeRanges
size
"
codeBlock
.
codeRanges
.
sizeOfExcludingThis
(
mallocSizeOf
)
)
;
hashmap
.
putNewInfallible
(
"
callSites
size
"
codeBlock
.
callSites
.
sizeOfExcludingThis
(
mallocSizeOf
)
)
;
hashmap
.
putNewInfallible
(
"
tryNotes
size
"
codeBlock
.
tryNotes
.
sizeOfExcludingThis
(
mallocSizeOf
)
)
;
hashmap
.
putNewInfallible
(
"
trapSites
size
"
codeBlock
.
trapSites
.
sizeOfExcludingThis
(
mallocSizeOf
)
)
;
hashmap
.
putNewInfallible
(
"
stackMaps
size
"
codeBlock
.
stackMaps
.
sizeOfExcludingThis
(
mallocSizeOf
)
)
;
hashmap
.
putNewInfallible
(
"
funcExports
size
"
codeBlock
.
funcExports
.
sizeOfExcludingThis
(
mallocSizeOf
)
)
;
}
return
hashmap
;
}
void
wasm
:
:
PatchDebugSymbolicAccesses
(
uint8_t
*
codeBase
MacroAssembler
&
masm
)
{
#
ifdef
WASM_CODEGEN_DEBUG
for
(
auto
&
access
:
masm
.
symbolicAccesses
(
)
)
{
switch
(
access
.
target
)
{
case
SymbolicAddress
:
:
PrintI32
:
case
SymbolicAddress
:
:
PrintPtr
:
case
SymbolicAddress
:
:
PrintF32
:
case
SymbolicAddress
:
:
PrintF64
:
case
SymbolicAddress
:
:
PrintText
:
break
;
default
:
MOZ_CRASH
(
"
unexpected
symbol
in
PatchDebugSymbolicAccesses
"
)
;
}
ABIFunctionType
abiType
;
void
*
target
=
AddressOf
(
access
.
target
&
abiType
)
;
uint8_t
*
patchAt
=
codeBase
+
access
.
patchAt
.
offset
(
)
;
Assembler
:
:
PatchDataWithValueCheck
(
CodeLocationLabel
(
patchAt
)
PatchedImmPtr
(
target
)
PatchedImmPtr
(
(
void
*
)
-
1
)
)
;
}
#
else
MOZ_ASSERT
(
masm
.
symbolicAccesses
(
)
.
empty
(
)
)
;
#
endif
}
