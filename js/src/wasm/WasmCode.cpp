#
include
"
wasm
/
WasmCode
.
h
"
#
include
"
mozilla
/
Atomics
.
h
"
#
include
"
mozilla
/
BinarySearch
.
h
"
#
include
"
mozilla
/
EnumeratedRange
.
h
"
#
include
"
mozilla
/
Sprintf
.
h
"
#
include
<
algorithm
>
#
include
"
jsnum
.
h
"
#
include
"
jit
/
Disassemble
.
h
"
#
include
"
jit
/
ExecutableAllocator
.
h
"
#
include
"
jit
/
FlushICache
.
h
"
#
include
"
jit
/
MacroAssembler
.
h
"
#
include
"
jit
/
PerfSpewer
.
h
"
#
include
"
util
/
Poison
.
h
"
#
include
"
vm
/
HelperThreadState
.
h
"
#
ifdef
MOZ_VTUNE
#
include
"
vtune
/
VTuneWrapper
.
h
"
#
endif
#
include
"
wasm
/
WasmModule
.
h
"
#
include
"
wasm
/
WasmProcess
.
h
"
#
include
"
wasm
/
WasmSerialize
.
h
"
#
include
"
wasm
/
WasmStubs
.
h
"
#
include
"
wasm
/
WasmUtility
.
h
"
using
namespace
js
;
using
namespace
js
:
:
jit
;
using
namespace
js
:
:
wasm
;
using
mozilla
:
:
Atomic
;
using
mozilla
:
:
BinarySearch
;
using
mozilla
:
:
BinarySearchIf
;
using
mozilla
:
:
DebugOnly
;
using
mozilla
:
:
MakeEnumeratedRange
;
using
mozilla
:
:
MallocSizeOf
;
using
mozilla
:
:
Maybe
;
size_t
LinkData
:
:
SymbolicLinkArray
:
:
sizeOfExcludingThis
(
MallocSizeOf
mallocSizeOf
)
const
{
size_t
size
=
0
;
for
(
const
Uint32Vector
&
offsets
:
*
this
)
{
size
+
=
offsets
.
sizeOfExcludingThis
(
mallocSizeOf
)
;
}
return
size
;
}
static
uint32_t
RoundupExecutableCodePageSize
(
uint32_t
codeLength
)
{
return
RoundUp
(
codeLength
ExecutableCodePageSize
)
;
}
UniqueCodeBytes
wasm
:
:
AllocateCodeBytes
(
Maybe
<
AutoMarkJitCodeWritableForThread
>
&
writable
uint32_t
codeLength
bool
allowLastDitchGC
)
{
if
(
codeLength
>
MaxCodeBytesPerProcess
)
{
return
nullptr
;
}
static_assert
(
MaxCodeBytesPerProcess
<
=
INT32_MAX
"
rounding
won
'
t
overflow
"
)
;
uint32_t
roundedCodeLength
=
RoundupExecutableCodePageSize
(
codeLength
)
;
void
*
p
=
AllocateExecutableMemory
(
roundedCodeLength
ProtectionSetting
:
:
Writable
MemCheckKind
:
:
MakeUndefined
)
;
if
(
!
p
&
&
allowLastDitchGC
)
{
if
(
OnLargeAllocationFailure
)
{
OnLargeAllocationFailure
(
)
;
p
=
AllocateExecutableMemory
(
roundedCodeLength
ProtectionSetting
:
:
Writable
MemCheckKind
:
:
MakeUndefined
)
;
}
}
if
(
!
p
)
{
return
nullptr
;
}
writable
.
emplace
(
)
;
memset
(
(
(
uint8_t
*
)
p
)
+
codeLength
0
roundedCodeLength
-
codeLength
)
;
return
UniqueCodeBytes
(
(
uint8_t
*
)
p
FreeCode
(
roundedCodeLength
)
)
;
}
void
FreeCode
:
:
operator
(
)
(
uint8_t
*
bytes
)
{
MOZ_ASSERT
(
codeLength
)
;
MOZ_ASSERT
(
codeLength
=
=
RoundupExecutableCodePageSize
(
codeLength
)
)
;
#
ifdef
MOZ_VTUNE
vtune
:
:
UnmarkBytes
(
bytes
codeLength
)
;
#
endif
DeallocateExecutableMemory
(
bytes
codeLength
)
;
}
bool
wasm
:
:
StaticallyLink
(
jit
:
:
AutoMarkJitCodeWritableForThread
&
writable
uint8_t
*
base
const
LinkData
&
linkData
const
Code
*
maybeCode
)
{
if
(
!
EnsureBuiltinThunksInitialized
(
writable
)
)
{
return
false
;
}
for
(
LinkData
:
:
InternalLink
link
:
linkData
.
internalLinks
)
{
CodeLabel
label
;
label
.
patchAt
(
)
-
>
bind
(
link
.
patchAtOffset
)
;
label
.
target
(
)
-
>
bind
(
link
.
targetOffset
)
;
#
ifdef
JS_CODELABEL_LINKMODE
label
.
setLinkMode
(
static_cast
<
CodeLabel
:
:
LinkMode
>
(
link
.
mode
)
)
;
#
endif
Assembler
:
:
Bind
(
base
label
)
;
}
for
(
CallFarJump
far
:
linkData
.
callFarJumps
)
{
MOZ_ASSERT
(
maybeCode
&
&
maybeCode
-
>
mode
(
)
=
=
CompileMode
:
:
LazyTiering
)
;
const
CodeBlock
&
bestBlock
=
maybeCode
-
>
funcCodeBlock
(
far
.
targetFuncIndex
)
;
uint32_t
stubRangeIndex
=
bestBlock
.
funcToCodeRange
[
far
.
targetFuncIndex
]
;
const
CodeRange
&
stubRange
=
bestBlock
.
codeRanges
[
stubRangeIndex
]
;
uint8_t
*
stubBase
=
bestBlock
.
segment
-
>
base
(
)
;
MacroAssembler
:
:
patchFarJump
(
base
+
far
.
jumpOffset
stubBase
+
stubRange
.
funcUncheckedCallEntry
(
)
)
;
}
for
(
auto
imm
:
MakeEnumeratedRange
(
SymbolicAddress
:
:
Limit
)
)
{
const
Uint32Vector
&
offsets
=
linkData
.
symbolicLinks
[
imm
]
;
if
(
offsets
.
empty
(
)
)
{
continue
;
}
void
*
target
=
SymbolicAddressTarget
(
imm
)
;
for
(
uint32_t
offset
:
offsets
)
{
uint8_t
*
patchAt
=
base
+
offset
;
Assembler
:
:
PatchDataWithValueCheck
(
CodeLocationLabel
(
patchAt
)
PatchedImmPtr
(
target
)
PatchedImmPtr
(
(
void
*
)
-
1
)
)
;
}
}
return
true
;
}
void
wasm
:
:
StaticallyUnlink
(
uint8_t
*
base
const
LinkData
&
linkData
)
{
for
(
LinkData
:
:
InternalLink
link
:
linkData
.
internalLinks
)
{
CodeLabel
label
;
label
.
patchAt
(
)
-
>
bind
(
link
.
patchAtOffset
)
;
label
.
target
(
)
-
>
bind
(
-
size_t
(
base
)
)
;
#
ifdef
JS_CODELABEL_LINKMODE
label
.
setLinkMode
(
static_cast
<
CodeLabel
:
:
LinkMode
>
(
link
.
mode
)
)
;
#
endif
Assembler
:
:
Bind
(
base
label
)
;
}
for
(
auto
imm
:
MakeEnumeratedRange
(
SymbolicAddress
:
:
Limit
)
)
{
const
Uint32Vector
&
offsets
=
linkData
.
symbolicLinks
[
imm
]
;
if
(
offsets
.
empty
(
)
)
{
continue
;
}
void
*
target
=
SymbolicAddressTarget
(
imm
)
;
for
(
uint32_t
offset
:
offsets
)
{
uint8_t
*
patchAt
=
base
+
offset
;
Assembler
:
:
PatchDataWithValueCheck
(
CodeLocationLabel
(
patchAt
)
PatchedImmPtr
(
(
void
*
)
-
1
)
PatchedImmPtr
(
target
)
)
;
}
}
}
static
bool
AppendToString
(
const
char
*
str
UTF8Bytes
*
bytes
)
{
return
bytes
-
>
append
(
str
strlen
(
str
)
)
&
&
bytes
-
>
append
(
'
\
0
'
)
;
}
static
void
SendCodeRangesToProfiler
(
const
uint8_t
*
segmentBase
const
CodeMetadata
&
codeMeta
const
CodeMetadataForAsmJS
*
codeMetaForAsmJS
const
CodeRangeVector
&
codeRanges
)
{
bool
enabled
=
false
;
enabled
|
=
PerfEnabled
(
)
;
#
ifdef
MOZ_VTUNE
enabled
|
=
vtune
:
:
IsProfilingActive
(
)
;
#
endif
if
(
!
enabled
)
{
return
;
}
for
(
const
CodeRange
&
codeRange
:
codeRanges
)
{
if
(
!
codeRange
.
hasFuncIndex
(
)
)
{
continue
;
}
uintptr_t
start
=
uintptr_t
(
segmentBase
+
codeRange
.
begin
(
)
)
;
uintptr_t
size
=
codeRange
.
end
(
)
-
codeRange
.
begin
(
)
;
UTF8Bytes
name
;
bool
ok
;
if
(
codeMetaForAsmJS
)
{
ok
=
codeMetaForAsmJS
-
>
getFuncNameForAsmJS
(
codeRange
.
funcIndex
(
)
&
name
)
;
}
else
{
ok
=
codeMeta
.
getFuncNameForWasm
(
NameContext
:
:
Standalone
codeRange
.
funcIndex
(
)
&
name
)
;
}
if
(
!
ok
)
{
return
;
}
(
void
)
start
;
(
void
)
size
;
if
(
PerfEnabled
(
)
)
{
const
char
*
file
=
codeMeta
.
scriptedCaller
(
)
.
filename
.
get
(
)
;
if
(
codeRange
.
isFunction
(
)
)
{
if
(
!
name
.
append
(
'
\
0
'
)
)
{
return
;
}
CollectPerfSpewerWasmFunctionMap
(
start
size
file
codeMeta
.
funcBytecodeOffset
(
codeRange
.
funcIndex
(
)
)
name
.
begin
(
)
)
;
}
else
if
(
codeRange
.
isInterpEntry
(
)
)
{
if
(
!
AppendToString
(
"
slow
entry
"
&
name
)
)
{
return
;
}
CollectPerfSpewerWasmMap
(
start
size
file
name
.
begin
(
)
)
;
}
else
if
(
codeRange
.
isJitEntry
(
)
)
{
if
(
!
AppendToString
(
"
fast
entry
"
&
name
)
)
{
return
;
}
CollectPerfSpewerWasmMap
(
start
size
file
name
.
begin
(
)
)
;
}
else
if
(
codeRange
.
isImportInterpExit
(
)
)
{
if
(
!
AppendToString
(
"
slow
exit
"
&
name
)
)
{
return
;
}
CollectPerfSpewerWasmMap
(
start
size
file
name
.
begin
(
)
)
;
}
else
if
(
codeRange
.
isImportJitExit
(
)
)
{
if
(
!
AppendToString
(
"
fast
exit
"
&
name
)
)
{
return
;
}
CollectPerfSpewerWasmMap
(
start
size
file
name
.
begin
(
)
)
;
}
else
{
MOZ_CRASH
(
"
unhandled
perf
hasFuncIndex
type
"
)
;
}
}
#
ifdef
MOZ_VTUNE
if
(
!
vtune
:
:
IsProfilingActive
(
)
)
{
continue
;
}
if
(
!
codeRange
.
isFunction
(
)
)
{
continue
;
}
if
(
!
name
.
append
(
'
\
0
'
)
)
{
return
;
}
vtune
:
:
MarkWasm
(
vtune
:
:
GenerateUniqueMethodID
(
)
name
.
begin
(
)
(
void
*
)
start
size
)
;
#
endif
}
}
size_t
CodeSegment
:
:
AllocationAlignment
(
)
{
if
(
JitOptions
.
writeProtectCode
)
{
return
gc
:
:
SystemPageSize
(
)
;
}
return
jit
:
:
CodeAlignment
;
}
size_t
CodeSegment
:
:
AlignAllocationBytes
(
uintptr_t
bytes
)
{
return
AlignBytes
(
bytes
AllocationAlignment
(
)
)
;
}
bool
CodeSegment
:
:
IsAligned
(
uintptr_t
bytes
)
{
return
bytes
=
=
AlignAllocationBytes
(
bytes
)
;
}
bool
CodeSegment
:
:
hasSpace
(
size_t
bytes
)
const
{
MOZ_ASSERT
(
CodeSegment
:
:
IsAligned
(
bytes
)
)
;
return
bytes
<
=
capacityBytes
(
)
&
&
lengthBytes_
<
=
capacityBytes
(
)
-
bytes
;
}
void
CodeSegment
:
:
claimSpace
(
size_t
bytes
uint8_t
*
*
claimedBase
)
{
MOZ_RELEASE_ASSERT
(
hasSpace
(
bytes
)
)
;
*
claimedBase
=
base
(
)
+
lengthBytes_
;
lengthBytes_
+
=
bytes
;
}
bool
CodeSegment
:
:
linkAndMakeExecutableSubRange
(
jit
:
:
AutoMarkJitCodeWritableForThread
&
writable
const
LinkData
&
linkData
const
Code
*
maybeCode
uint8_t
*
allocationStart
uint8_t
*
codeStart
uint32_t
allocationLength
)
{
MOZ_ASSERT
(
CodeSegment
:
:
IsAligned
(
uintptr_t
(
allocationStart
)
)
)
;
MOZ_ASSERT
(
codeStart
>
=
allocationStart
)
;
MOZ_ASSERT_IF
(
JitOptions
.
writeProtectCode
uintptr_t
(
allocationStart
)
%
gc
:
:
SystemPageSize
(
)
=
=
0
&
&
allocationLength
%
gc
:
:
SystemPageSize
(
)
=
=
0
)
;
if
(
!
StaticallyLink
(
writable
codeStart
linkData
maybeCode
)
)
{
return
false
;
}
return
ExecutableAllocator
:
:
makeExecutableAndFlushICache
(
allocationStart
allocationLength
)
;
}
bool
CodeSegment
:
:
linkAndMakeExecutableSubRange
(
jit
:
:
AutoMarkJitCodeWritableForThread
&
writable
jit
:
:
MacroAssembler
&
masm
uint8_t
*
allocationStart
uint8_t
*
codeStart
uint32_t
allocationLength
)
{
MOZ_ASSERT
(
CodeSegment
:
:
IsAligned
(
uintptr_t
(
allocationStart
)
)
)
;
MOZ_ASSERT
(
codeStart
>
=
allocationStart
)
;
MOZ_ASSERT_IF
(
JitOptions
.
writeProtectCode
uintptr_t
(
allocationStart
)
%
gc
:
:
SystemPageSize
(
)
=
=
0
&
&
allocationLength
%
gc
:
:
SystemPageSize
(
)
=
=
0
)
;
PatchDebugSymbolicAccesses
(
codeStart
masm
)
;
for
(
const
CodeLabel
&
label
:
masm
.
codeLabels
(
)
)
{
Assembler
:
:
Bind
(
codeStart
label
)
;
}
return
ExecutableAllocator
:
:
makeExecutableAndFlushICache
(
allocationStart
allocationLength
)
;
}
bool
CodeSegment
:
:
linkAndMakeExecutable
(
jit
:
:
AutoMarkJitCodeWritableForThread
&
writable
const
LinkData
&
linkData
const
Code
*
maybeCode
)
{
MOZ_ASSERT
(
base
(
)
=
=
bytes_
.
get
(
)
)
;
return
linkAndMakeExecutableSubRange
(
writable
linkData
maybeCode
base
(
)
base
(
)
RoundupExecutableCodePageSize
(
lengthBytes
(
)
)
)
;
}
SharedCodeSegment
CodeSegment
:
:
createEmpty
(
size_t
capacityBytes
bool
allowLastDitchGC
)
{
uint32_t
codeLength
=
0
;
uint32_t
codeCapacity
=
RoundupExecutableCodePageSize
(
capacityBytes
)
;
Maybe
<
AutoMarkJitCodeWritableForThread
>
writable
;
UniqueCodeBytes
codeBytes
=
AllocateCodeBytes
(
writable
codeCapacity
allowLastDitchGC
)
;
if
(
!
codeBytes
)
{
return
nullptr
;
}
return
js_new
<
CodeSegment
>
(
std
:
:
move
(
codeBytes
)
codeLength
codeCapacity
)
;
}
SharedCodeSegment
CodeSegment
:
:
createFromMasm
(
MacroAssembler
&
masm
const
LinkData
&
linkData
const
Code
*
maybeCode
bool
allowLastDitchGC
)
{
uint32_t
codeLength
=
masm
.
bytesNeeded
(
)
;
if
(
codeLength
=
=
0
)
{
return
js_new
<
CodeSegment
>
(
nullptr
0
0
)
;
}
uint32_t
codeCapacity
=
RoundupExecutableCodePageSize
(
codeLength
)
;
Maybe
<
AutoMarkJitCodeWritableForThread
>
writable
;
UniqueCodeBytes
codeBytes
=
AllocateCodeBytes
(
writable
codeCapacity
allowLastDitchGC
)
;
if
(
!
codeBytes
)
{
return
nullptr
;
}
masm
.
executableCopy
(
codeBytes
.
get
(
)
)
;
SharedCodeSegment
segment
=
js_new
<
CodeSegment
>
(
std
:
:
move
(
codeBytes
)
codeLength
codeCapacity
)
;
if
(
!
segment
|
|
!
segment
-
>
linkAndMakeExecutable
(
*
writable
linkData
maybeCode
)
)
{
return
nullptr
;
}
return
segment
;
}
SharedCodeSegment
CodeSegment
:
:
createFromBytes
(
const
uint8_t
*
unlinkedBytes
size_t
unlinkedBytesLength
const
LinkData
&
linkData
bool
allowLastDitchGC
)
{
uint32_t
codeLength
=
unlinkedBytesLength
;
if
(
codeLength
=
=
0
)
{
return
js_new
<
CodeSegment
>
(
nullptr
0
0
)
;
}
uint32_t
codeCapacity
=
RoundupExecutableCodePageSize
(
codeLength
)
;
Maybe
<
AutoMarkJitCodeWritableForThread
>
writable
;
UniqueCodeBytes
codeBytes
=
AllocateCodeBytes
(
writable
codeLength
allowLastDitchGC
)
;
if
(
!
codeBytes
)
{
return
nullptr
;
}
memcpy
(
codeBytes
.
get
(
)
unlinkedBytes
unlinkedBytesLength
)
;
SharedCodeSegment
segment
=
js_new
<
CodeSegment
>
(
std
:
:
move
(
codeBytes
)
codeLength
codeCapacity
)
;
if
(
!
segment
|
|
!
segment
-
>
linkAndMakeExecutable
(
*
writable
linkData
nullptr
)
)
{
return
nullptr
;
}
return
segment
;
}
static
uint32_t
RandomPaddingForCodeLength
(
uint32_t
codeLength
)
{
static
mozilla
:
:
Atomic
<
uint32_t
mozilla
:
:
MemoryOrdering
:
:
ReleaseAcquire
>
counter
(
0
)
;
const
size_t
cacheLineSize
=
64
;
const
size_t
systemPageSize
=
gc
:
:
SystemPageSize
(
)
;
if
(
!
JitOptions
.
writeProtectCode
)
{
MOZ_ASSERT
(
CodeSegment
:
:
AllocationAlignment
(
)
!
=
gc
:
:
SystemPageSize
(
)
)
;
return
0
;
}
size_t
maxPadBytes
=
(
(
systemPageSize
*
3
)
/
4
)
;
size_t
maxPadLines
=
maxPadBytes
/
cacheLineSize
;
size_t
remainingBytesInPage
=
AlignBytes
(
codeLength
systemPageSize
)
-
codeLength
;
size_t
remainingLinesInPage
=
remainingBytesInPage
/
cacheLineSize
;
size_t
padLinesAvailable
=
std
:
:
min
(
maxPadLines
remainingLinesInPage
)
;
if
(
padLinesAvailable
=
=
0
)
{
return
0
;
}
uint32_t
random
=
counter
+
+
;
uint32_t
padding
=
(
random
%
padLinesAvailable
)
*
cacheLineSize
;
MOZ_ASSERT
(
AlignBytes
(
codeLength
+
padding
systemPageSize
)
=
=
AlignBytes
(
codeLength
systemPageSize
)
)
;
return
padding
;
}
SharedCodeSegment
CodeSegment
:
:
claimSpaceFromPool
(
uint32_t
codeLength
SharedCodeSegmentVector
*
segmentPool
bool
allowLastDitchGC
uint8_t
*
*
allocationStartOut
uint8_t
*
*
codeStartOut
uint32_t
*
allocationLengthOut
)
{
uint32_t
paddingLength
=
RandomPaddingForCodeLength
(
codeLength
)
;
uint32_t
allocationLength
=
CodeSegment
:
:
AlignAllocationBytes
(
paddingLength
+
codeLength
)
;
if
(
segmentPool
-
>
length
(
)
=
=
0
|
|
!
(
*
segmentPool
)
[
segmentPool
-
>
length
(
)
-
1
]
-
>
hasSpace
(
allocationLength
)
)
{
SharedCodeSegment
newSegment
=
CodeSegment
:
:
createEmpty
(
allocationLength
allowLastDitchGC
)
;
if
(
!
newSegment
)
{
return
nullptr
;
}
if
(
!
segmentPool
-
>
emplaceBack
(
std
:
:
move
(
newSegment
)
)
)
{
return
nullptr
;
}
}
MOZ_ASSERT
(
segmentPool
-
>
length
(
)
>
0
)
;
SharedCodeSegment
segment
=
(
*
segmentPool
)
[
segmentPool
-
>
length
(
)
-
1
]
.
get
(
)
;
uint8_t
*
allocationStart
=
nullptr
;
segment
-
>
claimSpace
(
allocationLength
&
allocationStart
)
;
uint8_t
*
codeStart
=
allocationStart
+
paddingLength
;
MOZ_ASSERT
(
CodeSegment
:
:
IsAligned
(
uintptr_t
(
segment
-
>
base
(
)
)
)
)
;
MOZ_ASSERT
(
CodeSegment
:
:
IsAligned
(
allocationStart
-
segment
-
>
base
(
)
)
)
;
*
allocationStartOut
=
allocationStart
;
*
codeStartOut
=
codeStart
;
*
allocationLengthOut
=
allocationLength
;
return
segment
;
}
void
CodeSegment
:
:
addSizeOfMisc
(
MallocSizeOf
mallocSizeOf
size_t
*
code
size_t
*
data
)
const
{
*
code
+
=
capacityBytes
(
)
;
*
data
+
=
mallocSizeOf
(
this
)
;
}
size_t
CacheableChars
:
:
sizeOfExcludingThis
(
MallocSizeOf
mallocSizeOf
)
const
{
return
mallocSizeOf
(
get
(
)
)
;
}
static
constexpr
unsigned
LAZY_STUB_LIFO_DEFAULT_CHUNK_SIZE
=
8
*
1024
;
bool
Code
:
:
createManyLazyEntryStubs
(
const
WriteGuard
&
guard
const
Uint32Vector
&
funcExportIndices
const
CodeBlock
&
tierCodeBlock
size_t
*
stubBlockIndex
)
const
{
MOZ_ASSERT
(
funcExportIndices
.
length
(
)
)
;
LifoAlloc
lifo
(
LAZY_STUB_LIFO_DEFAULT_CHUNK_SIZE
js
:
:
MallocArena
)
;
TempAllocator
alloc
(
&
lifo
)
;
JitContext
jitContext
;
WasmMacroAssembler
masm
(
alloc
)
;
const
FuncExportVector
&
funcExports
=
tierCodeBlock
.
funcExports
;
uint8_t
*
segmentBase
=
tierCodeBlock
.
segment
-
>
base
(
)
;
CodeRangeVector
codeRanges
;
DebugOnly
<
uint32_t
>
numExpectedRanges
=
0
;
for
(
uint32_t
funcExportIndex
:
funcExportIndices
)
{
const
FuncExport
&
fe
=
funcExports
[
funcExportIndex
]
;
const
FuncType
&
funcType
=
codeMeta_
-
>
getFuncType
(
fe
.
funcIndex
(
)
)
;
numExpectedRanges
+
=
(
funcType
.
canHaveJitEntry
(
)
?
2
:
1
)
;
void
*
calleePtr
=
segmentBase
+
tierCodeBlock
.
codeRange
(
fe
)
.
funcUncheckedCallEntry
(
)
;
Maybe
<
ImmPtr
>
callee
;
callee
.
emplace
(
calleePtr
ImmPtr
:
:
NoCheckToken
(
)
)
;
if
(
!
GenerateEntryStubs
(
masm
funcExportIndex
fe
funcType
callee
false
&
codeRanges
)
)
{
return
false
;
}
}
MOZ_ASSERT
(
codeRanges
.
length
(
)
=
=
numExpectedRanges
"
incorrect
number
of
entries
per
function
"
)
;
masm
.
finish
(
)
;
MOZ_ASSERT
(
masm
.
callSites
(
)
.
empty
(
)
)
;
MOZ_ASSERT
(
masm
.
callSiteTargets
(
)
.
empty
(
)
)
;
MOZ_ASSERT
(
masm
.
trapSites
(
)
.
empty
(
)
)
;
MOZ_ASSERT
(
masm
.
tryNotes
(
)
.
empty
(
)
)
;
MOZ_ASSERT
(
masm
.
codeRangeUnwindInfos
(
)
.
empty
(
)
)
;
if
(
masm
.
oom
(
)
)
{
return
false
;
}
UniqueCodeBlock
stubCodeBlock
=
MakeUnique
<
CodeBlock
>
(
CodeBlockKind
:
:
LazyStubs
)
;
if
(
!
stubCodeBlock
)
{
return
false
;
}
uint32_t
codeLength
=
masm
.
bytesNeeded
(
)
;
uint8_t
*
allocationStart
;
uint8_t
*
codeStart
;
uint32_t
allocationLength
;
stubCodeBlock
-
>
segment
=
CodeSegment
:
:
claimSpaceFromPool
(
codeLength
&
guard
-
>
lazyStubSegments
true
&
allocationStart
&
codeStart
&
allocationLength
)
;
if
(
!
stubCodeBlock
-
>
segment
)
{
return
false
;
}
{
AutoMarkJitCodeWritableForThread
writable
;
masm
.
executableCopy
(
codeStart
)
;
uint8_t
*
allocationEnd
=
allocationStart
+
allocationLength
;
uint8_t
*
codeEnd
=
codeStart
+
codeLength
;
MOZ_ASSERT
(
codeEnd
<
=
allocationEnd
)
;
size_t
paddingAfterCode
=
allocationEnd
-
codeEnd
;
memset
(
codeEnd
0
paddingAfterCode
)
;
if
(
!
stubCodeBlock
-
>
segment
-
>
linkAndMakeExecutableSubRange
(
writable
masm
allocationStart
codeStart
allocationLength
)
)
{
return
false
;
}
}
stubCodeBlock
-
>
codeBase
=
codeStart
;
stubCodeBlock
-
>
codeLength
=
codeLength
;
stubCodeBlock
-
>
codeRanges
=
std
:
:
move
(
codeRanges
)
;
uint32_t
offsetInSegment
=
codeStart
-
stubCodeBlock
-
>
segment
-
>
base
(
)
;
*
stubBlockIndex
=
guard
-
>
blocks
.
length
(
)
;
uint32_t
codeRangeIndex
=
0
;
for
(
uint32_t
funcExportIndex
:
funcExportIndices
)
{
const
FuncExport
&
fe
=
funcExports
[
funcExportIndex
]
;
const
FuncType
&
funcType
=
codeMeta_
-
>
getFuncType
(
fe
.
funcIndex
(
)
)
;
LazyFuncExport
lazyExport
(
fe
.
funcIndex
(
)
*
stubBlockIndex
codeRangeIndex
tierCodeBlock
.
kind
)
;
CodeRange
&
interpRange
=
stubCodeBlock
-
>
codeRanges
[
codeRangeIndex
]
;
MOZ_ASSERT
(
interpRange
.
isInterpEntry
(
)
)
;
MOZ_ASSERT
(
interpRange
.
funcIndex
(
)
=
=
fe
.
funcIndex
(
)
)
;
interpRange
.
offsetBy
(
offsetInSegment
)
;
codeRangeIndex
+
=
1
;
if
(
funcType
.
canHaveJitEntry
(
)
)
{
CodeRange
&
jitRange
=
stubCodeBlock
-
>
codeRanges
[
codeRangeIndex
]
;
MOZ_ASSERT
(
jitRange
.
isJitEntry
(
)
)
;
MOZ_ASSERT
(
jitRange
.
funcIndex
(
)
=
=
fe
.
funcIndex
(
)
)
;
codeRangeIndex
+
=
1
;
jitRange
.
offsetBy
(
offsetInSegment
)
;
}
size_t
exportIndex
;
const
uint32_t
targetFunctionIndex
=
fe
.
funcIndex
(
)
;
if
(
BinarySearchIf
(
guard
-
>
lazyExports
0
guard
-
>
lazyExports
.
length
(
)
[
targetFunctionIndex
]
(
const
LazyFuncExport
&
funcExport
)
{
return
targetFunctionIndex
-
funcExport
.
funcIndex
;
}
&
exportIndex
)
)
{
DebugOnly
<
CodeBlockKind
>
oldKind
=
guard
-
>
lazyExports
[
exportIndex
]
.
funcKind
;
MOZ_ASSERT
(
oldKind
=
=
CodeBlockKind
:
:
SharedStubs
|
|
oldKind
=
=
CodeBlockKind
:
:
BaselineTier
)
;
guard
-
>
lazyExports
[
exportIndex
]
=
std
:
:
move
(
lazyExport
)
;
}
else
if
(
!
guard
-
>
lazyExports
.
insert
(
guard
-
>
lazyExports
.
begin
(
)
+
exportIndex
std
:
:
move
(
lazyExport
)
)
)
{
return
false
;
}
}
return
addCodeBlock
(
guard
std
:
:
move
(
stubCodeBlock
)
nullptr
)
;
}
bool
Code
:
:
createOneLazyEntryStub
(
const
WriteGuard
&
guard
uint32_t
funcExportIndex
const
CodeBlock
&
tierCodeBlock
void
*
*
interpEntry
)
const
{
Uint32Vector
funcExportIndexes
;
if
(
!
funcExportIndexes
.
append
(
funcExportIndex
)
)
{
return
false
;
}
size_t
stubBlockIndex
;
if
(
!
createManyLazyEntryStubs
(
guard
funcExportIndexes
tierCodeBlock
&
stubBlockIndex
)
)
{
return
false
;
}
const
CodeBlock
&
block
=
*
guard
-
>
blocks
[
stubBlockIndex
]
;
const
CodeSegment
&
segment
=
*
block
.
segment
;
const
CodeRangeVector
&
codeRanges
=
block
.
codeRanges
;
const
FuncExport
&
fe
=
tierCodeBlock
.
funcExports
[
funcExportIndex
]
;
const
FuncType
&
funcType
=
codeMeta_
-
>
getFuncType
(
fe
.
funcIndex
(
)
)
;
uint32_t
funcEntryRanges
=
funcType
.
canHaveJitEntry
(
)
?
2
:
1
;
MOZ_ASSERT
(
codeRanges
.
length
(
)
>
=
funcEntryRanges
)
;
const
CodeRange
&
interpRange
=
codeRanges
[
codeRanges
.
length
(
)
-
funcEntryRanges
]
;
MOZ_ASSERT
(
interpRange
.
isInterpEntry
(
)
)
;
*
interpEntry
=
segment
.
base
(
)
+
interpRange
.
begin
(
)
;
if
(
funcType
.
canHaveJitEntry
(
)
)
{
const
CodeRange
&
jitRange
=
codeRanges
[
codeRanges
.
length
(
)
-
funcEntryRanges
+
1
]
;
MOZ_ASSERT
(
jitRange
.
isJitEntry
(
)
)
;
jumpTables_
.
setJitEntry
(
jitRange
.
funcIndex
(
)
segment
.
base
(
)
+
jitRange
.
begin
(
)
)
;
}
return
true
;
}
bool
Code
:
:
getOrCreateInterpEntry
(
uint32_t
funcIndex
const
FuncExport
*
*
funcExport
void
*
*
interpEntry
)
const
{
size_t
funcExportIndex
;
const
CodeBlock
&
codeBlock
=
funcCodeBlock
(
funcIndex
)
;
*
funcExport
=
&
codeBlock
.
lookupFuncExport
(
funcIndex
&
funcExportIndex
)
;
const
FuncExport
&
fe
=
*
*
funcExport
;
if
(
fe
.
hasEagerStubs
(
)
)
{
*
interpEntry
=
codeBlock
.
segment
-
>
base
(
)
+
fe
.
eagerInterpEntryOffset
(
)
;
return
true
;
}
MOZ_ASSERT
(
!
codeMetaForAsmJS_
"
only
wasm
can
lazily
export
functions
"
)
;
auto
guard
=
data_
.
writeLock
(
)
;
*
interpEntry
=
lookupLazyInterpEntry
(
guard
funcIndex
)
;
if
(
*
interpEntry
)
{
return
true
;
}
return
createOneLazyEntryStub
(
guard
funcExportIndex
codeBlock
interpEntry
)
;
}
bool
Code
:
:
createTier2LazyEntryStubs
(
const
WriteGuard
&
guard
const
CodeBlock
&
tier2Code
Maybe
<
size_t
>
*
outStubBlockIndex
)
const
{
if
(
!
guard
-
>
lazyExports
.
length
(
)
)
{
return
true
;
}
Uint32Vector
funcExportIndices
;
if
(
!
funcExportIndices
.
reserve
(
guard
-
>
lazyExports
.
length
(
)
)
)
{
return
false
;
}
for
(
size_t
i
=
0
;
i
<
tier2Code
.
funcExports
.
length
(
)
;
i
+
+
)
{
const
FuncExport
&
fe
=
tier2Code
.
funcExports
[
i
]
;
const
LazyFuncExport
*
lfe
=
lookupLazyFuncExport
(
guard
fe
.
funcIndex
(
)
)
;
if
(
lfe
)
{
MOZ_ASSERT
(
lfe
-
>
funcKind
=
=
CodeBlockKind
:
:
BaselineTier
)
;
funcExportIndices
.
infallibleAppend
(
i
)
;
}
}
if
(
funcExportIndices
.
length
(
)
=
=
0
)
{
return
true
;
}
size_t
stubBlockIndex
;
if
(
!
createManyLazyEntryStubs
(
guard
funcExportIndices
tier2Code
&
stubBlockIndex
)
)
{
return
false
;
}
outStubBlockIndex
-
>
emplace
(
stubBlockIndex
)
;
return
true
;
}
class
Module
:
:
PartialTier2CompileTaskImpl
:
public
PartialTier2CompileTask
{
const
SharedCode
code_
;
uint32_t
funcIndex_
;
Atomic
<
bool
>
cancelled_
;
public
:
PartialTier2CompileTaskImpl
(
const
Code
&
code
uint32_t
funcIndex
)
:
code_
(
&
code
)
funcIndex_
(
funcIndex
)
cancelled_
(
false
)
{
}
void
cancel
(
)
override
{
cancelled_
=
true
;
}
void
runHelperThreadTask
(
AutoLockHelperThreadState
&
locked
)
override
{
if
(
!
cancelled_
)
{
AutoUnlockHelperThreadState
unlock
(
locked
)
;
UniqueChars
error
;
UniqueCharsVector
warnings
;
bool
success
=
CompilePartialTier2
(
*
code_
funcIndex_
&
error
&
warnings
&
cancelled_
)
;
ReportTier2ResultsOffThread
(
success
mozilla
:
:
Some
(
funcIndex_
)
code_
-
>
codeMeta
(
)
.
scriptedCaller
(
)
error
warnings
)
;
}
js_delete
(
this
)
;
}
ThreadType
threadType
(
)
override
{
return
ThreadType
:
:
THREAD_TYPE_WASM_COMPILE_PARTIAL_TIER2
;
}
}
;
bool
Code
:
:
requestTierUp
(
uint32_t
funcIndex
)
const
{
MOZ_ASSERT
(
mode_
=
=
CompileMode
:
:
LazyTiering
)
;
FuncState
&
state
=
funcStates_
[
funcIndex
-
codeMeta_
-
>
numFuncImports
]
;
if
(
!
state
.
tierUpState
.
compareExchange
(
TierUpState
:
:
NotRequested
TierUpState
:
:
Requested
)
)
{
return
true
;
}
auto
task
=
js
:
:
MakeUnique
<
Module
:
:
PartialTier2CompileTaskImpl
>
(
*
this
funcIndex
)
;
if
(
!
task
)
{
return
false
;
}
StartOffThreadWasmPartialTier2Compile
(
std
:
:
move
(
task
)
)
;
return
true
;
}
bool
Code
:
:
finishTier2
(
UniqueCodeBlock
tier2CodeBlock
UniqueLinkData
tier2LinkData
)
const
{
MOZ_RELEASE_ASSERT
(
mode_
=
=
CompileMode
:
:
EagerTiering
|
|
mode_
=
=
CompileMode
:
:
LazyTiering
)
;
MOZ_RELEASE_ASSERT
(
hasCompleteTier2_
=
=
false
&
&
tier2CodeBlock
-
>
tier
(
)
=
=
Tier
:
:
Optimized
)
;
CodeBlock
*
tier2CodePointer
;
{
auto
guard
=
data_
.
writeLock
(
)
;
tier2CodePointer
=
tier2CodeBlock
.
get
(
)
;
if
(
!
addCodeBlock
(
guard
std
:
:
move
(
tier2CodeBlock
)
std
:
:
move
(
tier2LinkData
)
)
)
{
return
false
;
}
Maybe
<
size_t
>
stub2Index
;
if
(
!
createTier2LazyEntryStubs
(
guard
*
tier2CodePointer
&
stub2Index
)
)
{
return
false
;
}
jit
:
:
FlushExecutionContextForAllThreads
(
)
;
if
(
mode_
=
=
CompileMode
:
:
EagerTiering
)
{
completeTier2_
=
tier2CodePointer
;
hasCompleteTier2_
=
true
;
MOZ_ASSERT
(
!
funcStates_
.
get
(
)
)
;
}
else
{
for
(
const
CodeRange
&
cr
:
tier2CodePointer
-
>
codeRanges
)
{
if
(
!
cr
.
isFunction
(
)
)
{
continue
;
}
FuncState
&
state
=
funcStates_
.
get
(
)
[
cr
.
funcIndex
(
)
-
codeMeta_
-
>
numFuncImports
]
;
state
.
bestTier
=
tier2CodePointer
;
state
.
tierUpState
=
TierUpState
:
:
Finished
;
}
}
if
(
stub2Index
)
{
const
CodeBlock
&
block
=
*
guard
-
>
blocks
[
*
stub2Index
]
;
const
CodeSegment
&
segment
=
*
block
.
segment
;
for
(
const
CodeRange
&
cr
:
block
.
codeRanges
)
{
if
(
!
cr
.
isJitEntry
(
)
)
{
continue
;
}
jumpTables_
.
setJitEntry
(
cr
.
funcIndex
(
)
segment
.
base
(
)
+
cr
.
begin
(
)
)
;
}
}
}
uint8_t
*
base
=
tier2CodePointer
-
>
segment
-
>
base
(
)
;
for
(
const
CodeRange
&
cr
:
tier2CodePointer
-
>
codeRanges
)
{
if
(
cr
.
isFunction
(
)
)
{
jumpTables_
.
setTieringEntry
(
cr
.
funcIndex
(
)
base
+
cr
.
funcTierEntry
(
)
)
;
}
else
if
(
cr
.
isJitEntry
(
)
)
{
jumpTables_
.
setJitEntry
(
cr
.
funcIndex
(
)
base
+
cr
.
begin
(
)
)
;
}
}
return
true
;
}
bool
Code
:
:
addCodeBlock
(
const
WriteGuard
&
guard
UniqueCodeBlock
block
UniqueLinkData
maybeLinkData
)
const
{
if
(
maybeLinkData
&
&
!
block
-
>
isSerializable
(
)
)
{
maybeLinkData
=
nullptr
;
}
CodeBlock
*
blockPtr
=
block
.
get
(
)
;
size_t
codeBlockIndex
=
guard
-
>
blocks
.
length
(
)
;
return
guard
-
>
blocks
.
append
(
std
:
:
move
(
block
)
)
&
&
guard
-
>
blocksLinkData
.
append
(
std
:
:
move
(
maybeLinkData
)
)
&
&
blockMap_
.
insert
(
blockPtr
)
&
&
blockPtr
-
>
initialize
(
*
this
codeBlockIndex
)
;
}
SharedCodeSegment
Code
:
:
createFuncCodeSegmentFromPool
(
jit
:
:
MacroAssembler
&
masm
const
LinkData
&
linkData
bool
allowLastDitchGC
uint8_t
*
*
codeStartOut
uint32_t
*
codeLengthOut
)
const
{
uint32_t
codeLength
=
masm
.
bytesNeeded
(
)
;
uint8_t
*
allocationStart
;
uint8_t
*
codeStart
;
uint32_t
allocationLength
;
SharedCodeSegment
segment
;
{
auto
guard
=
data_
.
writeLock
(
)
;
segment
=
CodeSegment
:
:
claimSpaceFromPool
(
codeLength
&
guard
-
>
lazyFuncSegments
allowLastDitchGC
&
allocationStart
&
codeStart
&
allocationLength
)
;
if
(
!
segment
)
{
return
nullptr
;
}
}
{
auto
guard
=
codeMeta
(
)
.
stats
.
writeLock
(
)
;
guard
-
>
partialCodeBytesMapped
+
=
allocationLength
;
guard
-
>
partialCodeBytesUsed
+
=
codeLength
;
}
Maybe
<
AutoMarkJitCodeWritableForThread
>
writable
;
writable
.
emplace
(
)
;
masm
.
executableCopy
(
codeStart
)
;
if
(
!
segment
-
>
linkAndMakeExecutableSubRange
(
*
writable
linkData
this
allocationStart
codeStart
allocationLength
)
)
{
return
nullptr
;
}
*
codeStartOut
=
codeStart
;
*
codeLengthOut
=
codeLength
;
return
segment
;
}
const
LazyFuncExport
*
Code
:
:
lookupLazyFuncExport
(
const
WriteGuard
&
guard
uint32_t
funcIndex
)
const
{
size_t
match
;
if
(
!
BinarySearchIf
(
guard
-
>
lazyExports
0
guard
-
>
lazyExports
.
length
(
)
[
funcIndex
]
(
const
LazyFuncExport
&
funcExport
)
{
return
funcIndex
-
funcExport
.
funcIndex
;
}
&
match
)
)
{
return
nullptr
;
}
return
&
guard
-
>
lazyExports
[
match
]
;
}
void
*
Code
:
:
lookupLazyInterpEntry
(
const
WriteGuard
&
guard
uint32_t
funcIndex
)
const
{
const
LazyFuncExport
*
fe
=
lookupLazyFuncExport
(
guard
funcIndex
)
;
if
(
!
fe
)
{
return
nullptr
;
}
const
CodeBlock
&
block
=
*
guard
-
>
blocks
[
fe
-
>
lazyStubBlockIndex
]
;
const
CodeSegment
&
segment
=
*
block
.
segment
;
return
segment
.
base
(
)
+
block
.
codeRanges
[
fe
-
>
funcCodeRangeIndex
]
.
begin
(
)
;
}
CodeBlock
:
:
~
CodeBlock
(
)
{
if
(
unregisterOnDestroy_
)
{
UnregisterCodeBlock
(
this
)
;
}
}
bool
CodeBlock
:
:
initialize
(
const
Code
&
code
size_t
codeBlockIndex
)
{
MOZ_ASSERT
(
!
initialized
(
)
)
;
this
-
>
code
=
&
code
;
this
-
>
codeBlockIndex
=
codeBlockIndex
;
segment
-
>
setCode
(
code
)
;
SendCodeRangesToProfiler
(
segment
-
>
base
(
)
code
.
codeMeta
(
)
code
.
codeMetaForAsmJS
(
)
codeRanges
)
;
if
(
!
RegisterCodeBlock
(
this
)
)
{
return
false
;
}
MOZ_ASSERT
(
!
unregisterOnDestroy_
)
;
unregisterOnDestroy_
=
true
;
MOZ_ASSERT
(
initialized
(
)
)
;
return
true
;
}
void
CodeBlock
:
:
offsetMetadataBy
(
uint32_t
delta
)
{
if
(
delta
=
=
0
)
{
return
;
}
for
(
CodeRange
&
cr
:
codeRanges
)
{
cr
.
offsetBy
(
delta
)
;
}
callSites
.
offsetBy
(
delta
)
;
trapSites
.
offsetBy
(
delta
)
;
for
(
FuncExport
&
fe
:
funcExports
)
{
fe
.
offsetBy
(
delta
)
;
}
stackMaps
.
offsetBy
(
delta
)
;
for
(
TryNote
&
tn
:
tryNotes
)
{
tn
.
offsetBy
(
delta
)
;
}
for
(
CodeRangeUnwindInfo
&
crui
:
codeRangeUnwindInfos
)
{
crui
.
offsetBy
(
delta
)
;
}
}
void
CodeBlock
:
:
addSizeOfMisc
(
MallocSizeOf
mallocSizeOf
size_t
*
code
size_t
*
data
)
const
{
segment
-
>
addSizeOfMisc
(
mallocSizeOf
code
data
)
;
*
data
+
=
funcToCodeRange
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
codeRanges
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
callSites
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
tryNotes
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
codeRangeUnwindInfos
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
trapSites
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
stackMaps
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
funcExports
.
sizeOfExcludingThis
(
mallocSizeOf
)
;
;
}
const
CodeRange
*
CodeBlock
:
:
lookupRange
(
const
void
*
pc
)
const
{
CodeRange
:
:
OffsetInCode
target
(
(
uint8_t
*
)
pc
-
segment
-
>
base
(
)
)
;
return
LookupInSorted
(
codeRanges
target
)
;
}
bool
CodeBlock
:
:
lookupCallSite
(
void
*
pc
CallSite
*
callSite
)
const
{
uint32_t
target
=
(
(
uint8_t
*
)
pc
)
-
segment
-
>
base
(
)
;
return
callSites
.
lookup
(
target
callSite
)
;
}
const
StackMap
*
CodeBlock
:
:
lookupStackMap
(
uint8_t
*
pc
)
const
{
return
stackMaps
.
findMap
(
pc
)
;
}
const
wasm
:
:
TryNote
*
CodeBlock
:
:
lookupTryNote
(
const
void
*
pc
)
const
{
size_t
target
=
(
uint8_t
*
)
pc
-
segment
-
>
base
(
)
;
for
(
const
auto
&
tryNote
:
tryNotes
)
{
if
(
tryNote
.
offsetWithinTryBody
(
target
)
)
{
return
&
tryNote
;
}
}
return
nullptr
;
}
bool
CodeBlock
:
:
lookupTrap
(
void
*
pc
Trap
*
kindOut
TrapSiteDesc
*
trapOut
)
const
{
MOZ_ASSERT
(
containsCodePC
(
pc
)
)
;
uint32_t
target
=
(
(
uint8_t
*
)
pc
)
-
segment
-
>
base
(
)
;
return
trapSites
.
lookup
(
target
kindOut
trapOut
)
;
}
struct
UnwindInfoPCOffset
{
const
CodeRangeUnwindInfoVector
&
info
;
explicit
UnwindInfoPCOffset
(
const
CodeRangeUnwindInfoVector
&
info
)
:
info
(
info
)
{
}
uint32_t
operator
[
]
(
size_t
index
)
const
{
return
info
[
index
]
.
offset
(
)
;
}
}
;
const
CodeRangeUnwindInfo
*
CodeBlock
:
:
lookupUnwindInfo
(
void
*
pc
)
const
{
uint32_t
target
=
(
(
uint8_t
*
)
pc
)
-
segment
-
>
base
(
)
;
size_t
match
;
const
CodeRangeUnwindInfo
*
info
=
nullptr
;
if
(
BinarySearch
(
UnwindInfoPCOffset
(
codeRangeUnwindInfos
)
0
codeRangeUnwindInfos
.
length
(
)
target
&
match
)
)
{
info
=
&
codeRangeUnwindInfos
[
match
]
;
}
else
{
if
(
match
=
=
0
)
return
nullptr
;
if
(
match
=
=
codeRangeUnwindInfos
.
length
(
)
)
{
MOZ_ASSERT
(
codeRangeUnwindInfos
[
codeRangeUnwindInfos
.
length
(
)
-
1
]
.
unwindHow
(
)
=
=
CodeRangeUnwindInfo
:
:
Normal
)
;
return
nullptr
;
}
info
=
&
codeRangeUnwindInfos
[
match
-
1
]
;
}
return
info
-
>
unwindHow
(
)
=
=
CodeRangeUnwindInfo
:
:
Normal
?
nullptr
:
info
;
}
struct
ProjectFuncIndex
{
const
FuncExportVector
&
funcExports
;
explicit
ProjectFuncIndex
(
const
FuncExportVector
&
funcExports
)
:
funcExports
(
funcExports
)
{
}
uint32_t
operator
[
]
(
size_t
index
)
const
{
return
funcExports
[
index
]
.
funcIndex
(
)
;
}
}
;
FuncExport
&
CodeBlock
:
:
lookupFuncExport
(
uint32_t
funcIndex
size_t
*
funcExportIndex
)
{
size_t
match
;
if
(
!
BinarySearch
(
ProjectFuncIndex
(
funcExports
)
0
funcExports
.
length
(
)
funcIndex
&
match
)
)
{
MOZ_CRASH
(
"
missing
function
export
"
)
;
}
if
(
funcExportIndex
)
{
*
funcExportIndex
=
match
;
}
return
funcExports
[
match
]
;
}
const
FuncExport
&
CodeBlock
:
:
lookupFuncExport
(
uint32_t
funcIndex
size_t
*
funcExportIndex
)
const
{
return
const_cast
<
CodeBlock
*
>
(
this
)
-
>
lookupFuncExport
(
funcIndex
funcExportIndex
)
;
}
bool
JumpTables
:
:
initialize
(
CompileMode
mode
const
CodeMetadata
&
codeMeta
const
CodeBlock
&
sharedStubs
const
CodeBlock
&
tier1
)
{
static_assert
(
JSScript
:
:
offsetOfJitCodeRaw
(
)
=
=
0
"
wasm
fast
jit
entry
is
at
(
void
*
)
jit
[
funcIndex
]
"
)
;
mode_
=
mode
;
numFuncs_
=
codeMeta
.
numFuncs
(
)
;
if
(
mode_
!
=
CompileMode
:
:
Once
)
{
tiering_
=
TablePointer
(
js_pod_calloc
<
void
*
>
(
numFuncs_
)
)
;
if
(
!
tiering_
)
{
return
false
;
}
}
jit_
=
TablePointer
(
js_pod_calloc
<
void
*
>
(
numFuncs_
)
)
;
if
(
!
jit_
)
{
return
false
;
}
uint8_t
*
codeBase
=
sharedStubs
.
segment
-
>
base
(
)
;
for
(
const
CodeRange
&
cr
:
sharedStubs
.
codeRanges
)
{
if
(
cr
.
isFunction
(
)
)
{
setTieringEntry
(
cr
.
funcIndex
(
)
codeBase
+
cr
.
funcTierEntry
(
)
)
;
}
else
if
(
cr
.
isJitEntry
(
)
)
{
setJitEntry
(
cr
.
funcIndex
(
)
codeBase
+
cr
.
begin
(
)
)
;
}
}
codeBase
=
tier1
.
segment
-
>
base
(
)
;
for
(
const
CodeRange
&
cr
:
tier1
.
codeRanges
)
{
if
(
cr
.
isFunction
(
)
)
{
setTieringEntry
(
cr
.
funcIndex
(
)
codeBase
+
cr
.
funcTierEntry
(
)
)
;
}
else
if
(
cr
.
isJitEntry
(
)
)
{
setJitEntry
(
cr
.
funcIndex
(
)
codeBase
+
cr
.
begin
(
)
)
;
}
}
return
true
;
}
Code
:
:
Code
(
CompileMode
mode
const
CodeMetadata
&
codeMeta
const
CodeMetadataForAsmJS
*
codeMetaForAsmJS
)
:
mode_
(
mode
)
data_
(
mutexid
:
:
WasmCodeProtected
)
codeMeta_
(
&
codeMeta
)
codeMetaForAsmJS_
(
codeMetaForAsmJS
)
completeTier1_
(
nullptr
)
completeTier2_
(
nullptr
)
profilingLabels_
(
mutexid
:
:
WasmCodeProfilingLabels
CacheableCharsVector
(
)
)
trapCode_
(
nullptr
)
debugStubOffset_
(
0
)
requestTierUpStubOffset_
(
0
)
updateCallRefMetricsStubOffset_
(
0
)
{
}
bool
Code
:
:
initialize
(
FuncImportVector
&
&
funcImports
UniqueCodeBlock
sharedStubs
UniqueLinkData
sharedStubsLinkData
UniqueCodeBlock
tier1CodeBlock
UniqueLinkData
tier1LinkData
)
{
funcImports_
=
std
:
:
move
(
funcImports
)
;
auto
guard
=
data_
.
writeLock
(
)
;
sharedStubs_
=
sharedStubs
.
get
(
)
;
completeTier1_
=
tier1CodeBlock
.
get
(
)
;
trapCode_
=
sharedStubs_
-
>
segment
-
>
base
(
)
+
sharedStubsLinkData
-
>
trapOffset
;
if
(
!
jumpTables_
.
initialize
(
mode_
*
codeMeta_
*
sharedStubs_
*
completeTier1_
)
|
|
!
addCodeBlock
(
guard
std
:
:
move
(
sharedStubs
)
std
:
:
move
(
sharedStubsLinkData
)
)
|
|
!
addCodeBlock
(
guard
std
:
:
move
(
tier1CodeBlock
)
std
:
:
move
(
tier1LinkData
)
)
)
{
return
false
;
}
if
(
mode_
=
=
CompileMode
:
:
LazyTiering
)
{
uint32_t
numFuncDefs
=
codeMeta_
-
>
numFuncs
(
)
-
codeMeta_
-
>
numFuncImports
;
funcStates_
=
FuncStatesPointer
(
js_pod_calloc
<
FuncState
>
(
numFuncDefs
)
)
;
if
(
!
funcStates_
)
{
return
false
;
}
for
(
uint32_t
funcDefIndex
=
0
;
funcDefIndex
<
numFuncDefs
;
funcDefIndex
+
+
)
{
funcStates_
.
get
(
)
[
funcDefIndex
]
.
bestTier
=
completeTier1_
;
funcStates_
.
get
(
)
[
funcDefIndex
]
.
tierUpState
=
TierUpState
:
:
NotRequested
;
}
}
return
true
;
}
Tiers
Code
:
:
completeTiers
(
)
const
{
if
(
hasCompleteTier2_
)
{
return
Tiers
(
completeTier1_
-
>
tier
(
)
completeTier2_
-
>
tier
(
)
)
;
}
return
Tiers
(
completeTier1_
-
>
tier
(
)
)
;
}
bool
Code
:
:
hasCompleteTier
(
Tier
t
)
const
{
if
(
hasCompleteTier2_
&
&
completeTier2_
-
>
tier
(
)
=
=
t
)
{
return
true
;
}
return
completeTier1_
-
>
tier
(
)
=
=
t
;
}
Tier
Code
:
:
stableCompleteTier
(
)
const
{
return
completeTier1_
-
>
tier
(
)
;
}
Tier
Code
:
:
bestCompleteTier
(
)
const
{
if
(
hasCompleteTier2_
)
{
return
completeTier2_
-
>
tier
(
)
;
}
return
completeTier1_
-
>
tier
(
)
;
}
const
CodeBlock
&
Code
:
:
completeTierCodeBlock
(
Tier
tier
)
const
{
switch
(
tier
)
{
case
Tier
:
:
Baseline
:
if
(
completeTier1_
-
>
tier
(
)
=
=
Tier
:
:
Baseline
)
{
MOZ_ASSERT
(
completeTier1_
-
>
initialized
(
)
)
;
return
*
completeTier1_
;
}
MOZ_CRASH
(
"
No
code
segment
at
this
tier
"
)
;
case
Tier
:
:
Optimized
:
if
(
completeTier1_
-
>
tier
(
)
=
=
Tier
:
:
Optimized
)
{
MOZ_ASSERT
(
completeTier1_
-
>
initialized
(
)
)
;
return
*
completeTier1_
;
}
MOZ_RELEASE_ASSERT
(
hasCompleteTier2_
)
;
MOZ_ASSERT
(
completeTier2_
-
>
initialized
(
)
)
;
return
*
completeTier2_
;
}
MOZ_CRASH
(
)
;
}
const
LinkData
*
Code
:
:
codeBlockLinkData
(
const
CodeBlock
&
block
)
const
{
auto
guard
=
data_
.
readLock
(
)
;
MOZ_ASSERT
(
block
.
initialized
(
)
&
&
block
.
code
=
=
this
)
;
return
guard
-
>
blocksLinkData
[
block
.
codeBlockIndex
]
.
get
(
)
;
}
void
Code
:
:
clearLinkData
(
)
const
{
auto
guard
=
data_
.
writeLock
(
)
;
for
(
UniqueLinkData
&
linkData
:
guard
-
>
blocksLinkData
)
{
linkData
=
nullptr
;
}
}
bool
Code
:
:
lookupFunctionTier
(
const
CodeRange
*
codeRange
Tier
*
tier
)
const
{
MOZ_ASSERT
(
codeRange
-
>
isFunction
(
)
)
;
for
(
Tier
t
:
completeTiers
(
)
)
{
const
CodeBlock
&
code
=
completeTierCodeBlock
(
t
)
;
if
(
codeRange
>
=
code
.
codeRanges
.
begin
(
)
&
&
codeRange
<
code
.
codeRanges
.
end
(
)
)
{
*
tier
=
t
;
return
true
;
}
}
return
false
;
}
void
Code
:
:
ensureProfilingLabels
(
bool
profilingEnabled
)
const
{
auto
labels
=
profilingLabels_
.
lock
(
)
;
if
(
!
profilingEnabled
)
{
labels
-
>
clear
(
)
;
return
;
}
if
(
!
labels
-
>
empty
(
)
)
{
return
;
}
const
CodeBlock
&
sharedStubsCodeBlock
=
sharedStubs
(
)
;
const
CodeBlock
&
tier1CodeBlock
=
completeTierCodeBlock
(
stableCompleteTier
(
)
)
;
(
void
)
appendProfilingLabels
(
labels
sharedStubsCodeBlock
)
;
(
void
)
appendProfilingLabels
(
labels
tier1CodeBlock
)
;
}
bool
Code
:
:
appendProfilingLabels
(
const
ExclusiveData
<
CacheableCharsVector
>
:
:
Guard
&
labels
const
CodeBlock
&
codeBlock
)
const
{
for
(
const
CodeRange
&
codeRange
:
codeBlock
.
codeRanges
)
{
if
(
!
codeRange
.
isFunction
(
)
)
{
continue
;
}
Int32ToCStringBuf
cbuf
;
size_t
bytecodeStrLen
;
const
char
*
bytecodeStr
=
Uint32ToCString
(
&
cbuf
codeMeta
(
)
.
funcBytecodeOffset
(
codeRange
.
funcIndex
(
)
)
&
bytecodeStrLen
)
;
MOZ_ASSERT
(
bytecodeStr
)
;
UTF8Bytes
name
;
bool
ok
;
if
(
codeMetaForAsmJS
(
)
)
{
ok
=
codeMetaForAsmJS
(
)
-
>
getFuncNameForAsmJS
(
codeRange
.
funcIndex
(
)
&
name
)
;
}
else
{
ok
=
codeMeta
(
)
.
getFuncNameForWasm
(
NameContext
:
:
Standalone
codeRange
.
funcIndex
(
)
&
name
)
;
}
if
(
!
ok
|
|
!
name
.
append
(
"
(
"
2
)
)
{
return
false
;
}
if
(
const
char
*
filename
=
codeMeta
(
)
.
scriptedCaller
(
)
.
filename
.
get
(
)
)
{
if
(
!
name
.
append
(
filename
strlen
(
filename
)
)
)
{
return
false
;
}
}
else
{
if
(
!
name
.
append
(
'
?
'
)
)
{
return
false
;
}
}
if
(
!
name
.
append
(
'
:
'
)
|
|
!
name
.
append
(
bytecodeStr
bytecodeStrLen
)
|
|
!
name
.
append
(
"
)
\
0
"
2
)
)
{
return
false
;
}
UniqueChars
label
(
name
.
extractOrCopyRawBuffer
(
)
)
;
if
(
!
label
)
{
return
false
;
}
if
(
codeRange
.
funcIndex
(
)
>
=
labels
-
>
length
(
)
)
{
if
(
!
labels
-
>
resize
(
codeRange
.
funcIndex
(
)
+
1
)
)
{
return
false
;
}
}
(
(
CacheableCharsVector
&
)
labels
)
[
codeRange
.
funcIndex
(
)
]
=
std
:
:
move
(
label
)
;
}
return
true
;
}
const
char
*
Code
:
:
profilingLabel
(
uint32_t
funcIndex
)
const
{
auto
labels
=
profilingLabels_
.
lock
(
)
;
if
(
funcIndex
>
=
labels
-
>
length
(
)
|
|
!
(
(
CacheableCharsVector
&
)
labels
)
[
funcIndex
]
)
{
return
"
?
"
;
}
return
(
(
CacheableCharsVector
&
)
labels
)
[
funcIndex
]
.
get
(
)
;
}
void
Code
:
:
addSizeOfMiscIfNotSeen
(
MallocSizeOf
mallocSizeOf
CodeMetadata
:
:
SeenSet
*
seenCodeMeta
CodeMetadataForAsmJS
:
:
SeenSet
*
seenCodeMetaForAsmJS
Code
:
:
SeenSet
*
seenCode
size_t
*
code
size_t
*
data
)
const
{
auto
p
=
seenCode
-
>
lookupForAdd
(
this
)
;
if
(
p
)
{
return
;
}
bool
ok
=
seenCode
-
>
add
(
p
this
)
;
(
void
)
ok
;
auto
guard
=
data_
.
readLock
(
)
;
*
data
+
=
mallocSizeOf
(
this
)
+
guard
-
>
blocks
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
guard
-
>
blocksLinkData
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
guard
-
>
lazyExports
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
(
codeMetaForAsmJS
(
)
?
codeMetaForAsmJS
(
)
-
>
sizeOfIncludingThisIfNotSeen
(
mallocSizeOf
seenCodeMetaForAsmJS
)
:
0
)
+
funcImports_
.
sizeOfExcludingThis
(
mallocSizeOf
)
+
profilingLabels_
.
lock
(
)
-
>
sizeOfExcludingThis
(
mallocSizeOf
)
+
jumpTables_
.
sizeOfMiscExcludingThis
(
)
;
for
(
const
SharedCodeSegment
&
stub
:
guard
-
>
lazyStubSegments
)
{
stub
-
>
addSizeOfMisc
(
mallocSizeOf
code
data
)
;
}
sharedStubs
(
)
.
addSizeOfMisc
(
mallocSizeOf
code
data
)
;
for
(
auto
t
:
completeTiers
(
)
)
{
completeTierCodeBlock
(
t
)
.
addSizeOfMisc
(
mallocSizeOf
code
data
)
;
}
}
void
CodeBlock
:
:
disassemble
(
JSContext
*
cx
int
kindSelection
PrintCallback
printString
)
const
{
for
(
const
CodeRange
&
range
:
codeRanges
)
{
if
(
kindSelection
&
(
1
<
<
range
.
kind
(
)
)
)
{
MOZ_ASSERT
(
range
.
begin
(
)
<
segment
-
>
lengthBytes
(
)
)
;
MOZ_ASSERT
(
range
.
end
(
)
<
segment
-
>
lengthBytes
(
)
)
;
const
char
*
kind
;
char
kindbuf
[
128
]
;
switch
(
range
.
kind
(
)
)
{
case
CodeRange
:
:
Function
:
kind
=
"
Function
"
;
break
;
case
CodeRange
:
:
InterpEntry
:
kind
=
"
InterpEntry
"
;
break
;
case
CodeRange
:
:
JitEntry
:
kind
=
"
JitEntry
"
;
break
;
case
CodeRange
:
:
ImportInterpExit
:
kind
=
"
ImportInterpExit
"
;
break
;
case
CodeRange
:
:
ImportJitExit
:
kind
=
"
ImportJitExit
"
;
break
;
default
:
SprintfLiteral
(
kindbuf
"
CodeRange
:
:
Kind
(
%
d
)
"
range
.
kind
(
)
)
;
kind
=
kindbuf
;
break
;
}
const
char
*
separator
=
"
\
n
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
\
n
"
;
char
buf
[
4096
]
;
if
(
range
.
hasFuncIndex
(
)
)
{
const
char
*
funcName
=
"
(
unknown
)
"
;
UTF8Bytes
namebuf
;
bool
ok
;
if
(
code
-
>
codeMetaForAsmJS
(
)
)
{
ok
=
code
-
>
codeMetaForAsmJS
(
)
-
>
getFuncNameForAsmJS
(
range
.
funcIndex
(
)
&
namebuf
)
;
}
else
{
ok
=
code
-
>
codeMeta
(
)
.
getFuncNameForWasm
(
NameContext
:
:
Standalone
range
.
funcIndex
(
)
&
namebuf
)
;
}
if
(
ok
&
&
namebuf
.
append
(
'
\
0
'
)
)
{
funcName
=
namebuf
.
begin
(
)
;
}
SprintfLiteral
(
buf
"
%
sKind
=
%
s
index
=
%
d
name
=
%
s
:
\
n
"
separator
kind
range
.
funcIndex
(
)
funcName
)
;
}
else
{
SprintfLiteral
(
buf
"
%
sKind
=
%
s
\
n
"
separator
kind
)
;
}
printString
(
buf
)
;
uint8_t
*
theCode
=
segment
-
>
base
(
)
+
range
.
begin
(
)
;
jit
:
:
Disassemble
(
theCode
range
.
end
(
)
-
range
.
begin
(
)
printString
)
;
}
}
}
void
Code
:
:
disassemble
(
JSContext
*
cx
Tier
tier
int
kindSelection
PrintCallback
printString
)
const
{
this
-
>
sharedStubs
(
)
.
disassemble
(
cx
kindSelection
printString
)
;
this
-
>
completeTierCodeBlock
(
tier
)
.
disassemble
(
cx
kindSelection
printString
)
;
}
MetadataAnalysisHashMap
Code
:
:
metadataAnalysis
(
JSContext
*
cx
)
const
{
MetadataAnalysisHashMap
hashmap
;
if
(
!
hashmap
.
reserve
(
14
)
)
{
return
hashmap
;
}
for
(
auto
t
:
completeTiers
(
)
)
{
const
CodeBlock
&
codeBlock
=
completeTierCodeBlock
(
t
)
;
size_t
length
=
codeBlock
.
funcToCodeRange
.
numEntries
(
)
;
length
+
=
codeBlock
.
codeRanges
.
length
(
)
;
length
+
=
codeBlock
.
callSites
.
length
(
)
;
length
+
=
codeBlock
.
trapSites
.
sumOfLengths
(
)
;
length
+
=
codeBlock
.
funcExports
.
length
(
)
;
length
+
=
codeBlock
.
stackMaps
.
length
(
)
;
length
+
=
codeBlock
.
tryNotes
.
length
(
)
;
hashmap
.
putNewInfallible
(
"
metadata
length
"
length
)
;
size_t
code_size
=
0
;
for
(
const
CodeRange
&
codeRange
:
codeBlock
.
codeRanges
)
{
if
(
!
codeRange
.
isFunction
(
)
)
{
continue
;
}
code_size
+
=
codeRange
.
end
(
)
-
codeRange
.
begin
(
)
;
}
hashmap
.
putNewInfallible
(
"
stackmaps
number
"
codeBlock
.
stackMaps
.
length
(
)
)
;
hashmap
.
putNewInfallible
(
"
trapSites
number
"
codeBlock
.
trapSites
.
sumOfLengths
(
)
)
;
hashmap
.
putNewInfallible
(
"
codeRange
size
in
bytes
"
code_size
)
;
hashmap
.
putNewInfallible
(
"
code
segment
capacity
"
codeBlock
.
segment
-
>
capacityBytes
(
)
)
;
auto
mallocSizeOf
=
cx
-
>
runtime
(
)
-
>
debuggerMallocSizeOf
;
hashmap
.
putNewInfallible
(
"
funcToCodeRange
size
"
codeBlock
.
funcToCodeRange
.
sizeOfExcludingThis
(
mallocSizeOf
)
)
;
hashmap
.
putNewInfallible
(
"
codeRanges
size
"
codeBlock
.
codeRanges
.
sizeOfExcludingThis
(
mallocSizeOf
)
)
;
hashmap
.
putNewInfallible
(
"
callSites
size
"
codeBlock
.
callSites
.
sizeOfExcludingThis
(
mallocSizeOf
)
)
;
hashmap
.
putNewInfallible
(
"
tryNotes
size
"
codeBlock
.
tryNotes
.
sizeOfExcludingThis
(
mallocSizeOf
)
)
;
hashmap
.
putNewInfallible
(
"
trapSites
size
"
codeBlock
.
trapSites
.
sizeOfExcludingThis
(
mallocSizeOf
)
)
;
hashmap
.
putNewInfallible
(
"
stackMaps
size
"
codeBlock
.
stackMaps
.
sizeOfExcludingThis
(
mallocSizeOf
)
)
;
hashmap
.
putNewInfallible
(
"
funcExports
size
"
codeBlock
.
funcExports
.
sizeOfExcludingThis
(
mallocSizeOf
)
)
;
}
return
hashmap
;
}
void
wasm
:
:
PatchDebugSymbolicAccesses
(
uint8_t
*
codeBase
MacroAssembler
&
masm
)
{
#
ifdef
WASM_CODEGEN_DEBUG
for
(
auto
&
access
:
masm
.
symbolicAccesses
(
)
)
{
switch
(
access
.
target
)
{
case
SymbolicAddress
:
:
PrintI32
:
case
SymbolicAddress
:
:
PrintPtr
:
case
SymbolicAddress
:
:
PrintF32
:
case
SymbolicAddress
:
:
PrintF64
:
case
SymbolicAddress
:
:
PrintText
:
break
;
default
:
MOZ_CRASH
(
"
unexpected
symbol
in
PatchDebugSymbolicAccesses
"
)
;
}
ABIFunctionType
abiType
;
void
*
target
=
AddressOf
(
access
.
target
&
abiType
)
;
uint8_t
*
patchAt
=
codeBase
+
access
.
patchAt
.
offset
(
)
;
Assembler
:
:
PatchDataWithValueCheck
(
CodeLocationLabel
(
patchAt
)
PatchedImmPtr
(
target
)
PatchedImmPtr
(
(
void
*
)
-
1
)
)
;
}
#
else
MOZ_ASSERT
(
masm
.
symbolicAccesses
(
)
.
empty
(
)
)
;
#
endif
}
