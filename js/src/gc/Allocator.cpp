#
include
"
gc
/
Allocator
.
h
"
#
include
"
mozilla
/
DebugOnly
.
h
"
#
include
"
mozilla
/
OperatorNewExtensions
.
h
"
#
include
"
mozilla
/
TimeStamp
.
h
"
#
include
"
gc
/
GCInternals
.
h
"
#
include
"
gc
/
GCLock
.
h
"
#
include
"
gc
/
GCProbes
.
h
"
#
include
"
gc
/
Nursery
.
h
"
#
include
"
threading
/
CpuCount
.
h
"
#
include
"
util
/
Poison
.
h
"
#
include
"
vm
/
BigIntType
.
h
"
#
include
"
vm
/
JSContext
.
h
"
#
include
"
vm
/
Runtime
.
h
"
#
include
"
vm
/
StringType
.
h
"
#
include
"
gc
/
ArenaList
-
inl
.
h
"
#
include
"
gc
/
Heap
-
inl
.
h
"
#
include
"
gc
/
PrivateIterators
-
inl
.
h
"
#
include
"
vm
/
JSContext
-
inl
.
h
"
using
mozilla
:
:
TimeDuration
;
using
mozilla
:
:
TimeStamp
;
using
namespace
js
;
using
namespace
gc
;
void
Zone
:
:
updateNurseryAllocFlags
(
const
Nursery
&
nursery
)
{
allocNurseryObjects_
=
nursery
.
isEnabled
(
)
;
allocNurseryStrings_
=
nursery
.
isEnabled
(
)
&
&
nursery
.
canAllocateStrings
(
)
&
&
!
nurseryStringsDisabled
;
allocNurseryBigInts_
=
nursery
.
isEnabled
(
)
&
&
nursery
.
canAllocateBigInts
(
)
&
&
!
nurseryBigIntsDisabled
;
}
template
<
AllowGC
allowGC
>
void
*
gc
:
:
CellAllocator
:
:
AllocateObjectCell
(
JSContext
*
cx
AllocKind
kind
gc
:
:
InitialHeap
heap
const
JSClass
*
clasp
AllocSite
*
site
)
{
MOZ_ASSERT
(
!
cx
-
>
isHelperThreadContext
(
)
)
;
MOZ_ASSERT
(
IsObjectAllocKind
(
kind
)
)
;
size_t
thingSize
=
Arena
:
:
thingSize
(
kind
)
;
MOZ_ASSERT
(
thingSize
=
=
Arena
:
:
thingSize
(
kind
)
)
;
MOZ_ASSERT
(
thingSize
>
=
sizeof
(
JSObject_Slots0
)
)
;
static_assert
(
sizeof
(
JSObject_Slots0
)
>
=
MinCellSize
"
All
allocations
must
be
at
least
the
allocator
-
imposed
minimum
size
.
"
)
;
MOZ_ASSERT_IF
(
site
&
&
site
-
>
initialHeap
(
)
=
=
TenuredHeap
heap
=
=
TenuredHeap
)
;
JSRuntime
*
rt
=
cx
-
>
runtime
(
)
;
if
(
!
rt
-
>
gc
.
checkAllocatorState
<
allowGC
>
(
cx
kind
)
)
{
return
nullptr
;
}
if
(
heap
!
=
TenuredHeap
&
&
cx
-
>
zone
(
)
-
>
allocNurseryObjects
(
)
)
{
MOZ_ASSERT_IF
(
clasp
-
>
hasFinalize
(
)
&
&
!
clasp
-
>
isProxyObject
(
)
CanNurseryAllocateFinalizedClass
(
clasp
)
)
;
if
(
!
site
)
{
site
=
cx
-
>
zone
(
)
-
>
unknownAllocSite
(
JS
:
:
TraceKind
:
:
Object
)
;
}
void
*
obj
=
rt
-
>
gc
.
tryNewNurseryCell
<
JS
:
:
TraceKind
:
:
Object
allowGC
>
(
cx
thingSize
site
)
;
if
(
obj
)
{
return
obj
;
}
if
(
!
allowGC
)
{
return
nullptr
;
}
}
return
GCRuntime
:
:
tryNewTenuredThing
<
allowGC
>
(
cx
kind
thingSize
)
;
}
template
void
*
gc
:
:
CellAllocator
:
:
AllocateObjectCell
<
NoGC
>
(
JSContext
*
cx
gc
:
:
AllocKind
kind
gc
:
:
InitialHeap
heap
const
JSClass
*
clasp
gc
:
:
AllocSite
*
site
)
;
template
void
*
gc
:
:
CellAllocator
:
:
AllocateObjectCell
<
CanGC
>
(
JSContext
*
cx
gc
:
:
AllocKind
kind
gc
:
:
InitialHeap
heap
const
JSClass
*
clasp
gc
:
:
AllocSite
*
site
)
;
template
<
JS
:
:
TraceKind
kind
AllowGC
allowGC
>
void
*
GCRuntime
:
:
tryNewNurseryCell
(
JSContext
*
cx
size_t
thingSize
AllocSite
*
site
)
{
MOZ_ASSERT
(
cx
-
>
isNurseryAllocAllowed
(
)
)
;
MOZ_ASSERT
(
cx
-
>
zone
(
)
=
=
site
-
>
zone
(
)
)
;
MOZ_ASSERT
(
!
cx
-
>
zone
(
)
-
>
isAtomsZone
(
)
)
;
MOZ_ASSERT
(
cx
-
>
zone
(
)
-
>
allocKindInNursery
(
kind
)
)
;
void
*
ptr
=
cx
-
>
nursery
(
)
.
allocateCell
(
site
thingSize
kind
)
;
if
(
ptr
)
{
return
ptr
;
}
if
constexpr
(
allowGC
)
{
if
(
!
cx
-
>
suppressGC
)
{
cx
-
>
runtime
(
)
-
>
gc
.
minorGC
(
JS
:
:
GCReason
:
:
OUT_OF_NURSERY
)
;
if
(
cx
-
>
zone
(
)
-
>
allocKindInNursery
(
kind
)
)
{
return
cx
-
>
nursery
(
)
.
allocateCell
(
site
thingSize
kind
)
;
}
}
}
return
nullptr
;
}
template
<
AllowGC
allowGC
>
void
*
gc
:
:
CellAllocator
:
:
AllocateStringCell
(
JSContext
*
cx
AllocKind
kind
size_t
size
InitialHeap
heap
)
{
MOZ_ASSERT
(
!
cx
-
>
isHelperThreadContext
(
)
)
;
MOZ_ASSERT
(
size
=
=
Arena
:
:
thingSize
(
kind
)
)
;
MOZ_ASSERT
(
size
=
=
sizeof
(
JSString
)
|
|
size
=
=
sizeof
(
JSFatInlineString
)
)
;
MOZ_ASSERT
(
IsNurseryAllocable
(
kind
)
)
;
JSRuntime
*
rt
=
cx
-
>
runtime
(
)
;
if
(
!
rt
-
>
gc
.
checkAllocatorState
<
allowGC
>
(
cx
kind
)
)
{
return
nullptr
;
}
if
(
heap
!
=
TenuredHeap
&
&
cx
-
>
zone
(
)
-
>
allocNurseryStrings
(
)
)
{
AllocSite
*
site
=
cx
-
>
zone
(
)
-
>
unknownAllocSite
(
JS
:
:
TraceKind
:
:
String
)
;
void
*
ptr
=
rt
-
>
gc
.
tryNewNurseryCell
<
JS
:
:
TraceKind
:
:
String
allowGC
>
(
cx
size
site
)
;
if
(
ptr
)
{
return
ptr
;
}
if
(
!
allowGC
)
{
return
nullptr
;
}
}
return
GCRuntime
:
:
tryNewTenuredThing
<
allowGC
>
(
cx
kind
size
)
;
}
template
void
*
gc
:
:
CellAllocator
:
:
AllocateStringCell
<
NoGC
>
(
JSContext
*
AllocKind
size_t
InitialHeap
)
;
template
void
*
gc
:
:
CellAllocator
:
:
AllocateStringCell
<
CanGC
>
(
JSContext
*
AllocKind
size_t
InitialHeap
)
;
template
<
AllowGC
allowGC
>
void
*
gc
:
:
CellAllocator
:
:
AllocateBigIntCell
(
JSContext
*
cx
InitialHeap
heap
)
{
MOZ_ASSERT
(
!
cx
-
>
isHelperThreadContext
(
)
)
;
AllocKind
kind
=
MapTypeToAllocKind
<
JS
:
:
BigInt
>
:
:
kind
;
size_t
size
=
sizeof
(
JS
:
:
BigInt
)
;
MOZ_ASSERT
(
size
=
=
Arena
:
:
thingSize
(
kind
)
)
;
JSRuntime
*
rt
=
cx
-
>
runtime
(
)
;
if
(
!
rt
-
>
gc
.
checkAllocatorState
<
allowGC
>
(
cx
kind
)
)
{
return
nullptr
;
}
if
(
heap
!
=
TenuredHeap
&
&
cx
-
>
zone
(
)
-
>
allocNurseryBigInts
(
)
)
{
AllocSite
*
site
=
cx
-
>
zone
(
)
-
>
unknownAllocSite
(
JS
:
:
TraceKind
:
:
BigInt
)
;
void
*
ptr
=
rt
-
>
gc
.
tryNewNurseryCell
<
JS
:
:
TraceKind
:
:
BigInt
allowGC
>
(
cx
size
site
)
;
if
(
ptr
)
{
return
ptr
;
}
if
constexpr
(
!
allowGC
)
{
return
nullptr
;
}
}
return
GCRuntime
:
:
tryNewTenuredThing
<
allowGC
>
(
cx
kind
size
)
;
}
template
void
*
gc
:
:
CellAllocator
:
:
AllocateBigIntCell
<
NoGC
>
(
JSContext
*
cx
gc
:
:
InitialHeap
heap
)
;
template
void
*
gc
:
:
CellAllocator
:
:
AllocateBigIntCell
<
CanGC
>
(
JSContext
*
cx
gc
:
:
InitialHeap
heap
)
;
template
<
AllowGC
allowGC
>
void
*
gc
:
:
CellAllocator
:
:
AllocateTenuredCell
(
JSContext
*
cx
gc
:
:
AllocKind
kind
size_t
size
)
{
MOZ_ASSERT
(
!
cx
-
>
isHelperThreadContext
(
)
)
;
MOZ_ASSERT
(
!
IsNurseryAllocable
(
kind
)
)
;
MOZ_ASSERT
(
size
=
=
Arena
:
:
thingSize
(
kind
)
)
;
MOZ_ASSERT
(
size
>
=
gc
:
:
MinCellSize
"
All
allocations
must
be
at
least
the
allocator
-
imposed
minimum
size
.
"
)
;
if
(
!
cx
-
>
runtime
(
)
-
>
gc
.
checkAllocatorState
<
allowGC
>
(
cx
kind
)
)
{
return
nullptr
;
}
return
GCRuntime
:
:
tryNewTenuredThing
<
allowGC
>
(
cx
kind
size
)
;
}
template
void
*
gc
:
:
CellAllocator
:
:
AllocateTenuredCell
<
NoGC
>
(
JSContext
*
AllocKind
size_t
)
;
template
void
*
gc
:
:
CellAllocator
:
:
AllocateTenuredCell
<
CanGC
>
(
JSContext
*
AllocKind
size_t
)
;
template
<
AllowGC
allowGC
>
void
*
GCRuntime
:
:
tryNewTenuredThing
(
JSContext
*
cx
AllocKind
kind
size_t
thingSize
)
{
Zone
*
zone
=
cx
-
>
zone
(
)
;
void
*
ptr
=
zone
-
>
arenas
.
freeLists
(
)
.
allocate
(
kind
)
;
if
(
MOZ_UNLIKELY
(
!
ptr
)
)
{
ptr
=
refillFreeList
(
cx
kind
)
;
if
(
MOZ_UNLIKELY
(
!
ptr
)
)
{
if
constexpr
(
allowGC
)
{
cx
-
>
runtime
(
)
-
>
gc
.
attemptLastDitchGC
(
cx
)
;
ptr
=
tryNewTenuredThing
<
NoGC
>
(
cx
kind
thingSize
)
;
if
(
ptr
)
{
return
ptr
;
}
ReportOutOfMemory
(
cx
)
;
}
return
nullptr
;
}
}
#
ifdef
DEBUG
checkIncrementalZoneState
(
cx
ptr
)
;
#
endif
gcprobes
:
:
TenuredAlloc
(
ptr
kind
)
;
zone
-
>
noteTenuredAlloc
(
)
;
return
ptr
;
}
void
GCRuntime
:
:
attemptLastDitchGC
(
JSContext
*
cx
)
{
if
(
!
lastLastDitchTime
.
IsNull
(
)
&
&
TimeStamp
:
:
Now
(
)
-
lastLastDitchTime
<
=
tunables
.
minLastDitchGCPeriod
(
)
)
{
return
;
}
JS
:
:
PrepareForFullGC
(
cx
)
;
gc
(
JS
:
:
GCOptions
:
:
Shrink
JS
:
:
GCReason
:
:
LAST_DITCH
)
;
waitBackgroundAllocEnd
(
)
;
waitBackgroundFreeEnd
(
)
;
lastLastDitchTime
=
mozilla
:
:
TimeStamp
:
:
Now
(
)
;
}
template
<
AllowGC
allowGC
>
bool
GCRuntime
:
:
checkAllocatorState
(
JSContext
*
cx
AllocKind
kind
)
{
if
(
allowGC
)
{
if
(
!
gcIfNeededAtAllocation
(
cx
)
)
{
return
false
;
}
}
#
if
defined
(
JS_GC_ZEAL
)
|
|
defined
(
DEBUG
)
MOZ_ASSERT_IF
(
cx
-
>
zone
(
)
-
>
isAtomsZone
(
)
kind
=
=
AllocKind
:
:
ATOM
|
|
kind
=
=
AllocKind
:
:
FAT_INLINE_ATOM
|
|
kind
=
=
AllocKind
:
:
SYMBOL
|
|
kind
=
=
AllocKind
:
:
JITCODE
|
|
kind
=
=
AllocKind
:
:
SCOPE
)
;
MOZ_ASSERT_IF
(
!
cx
-
>
zone
(
)
-
>
isAtomsZone
(
)
kind
!
=
AllocKind
:
:
ATOM
&
&
kind
!
=
AllocKind
:
:
FAT_INLINE_ATOM
)
;
MOZ_ASSERT
(
!
JS
:
:
RuntimeHeapIsBusy
(
)
)
;
#
endif
if
(
allowGC
&
&
!
cx
-
>
suppressGC
)
{
cx
-
>
verifyIsSafeToGC
(
)
;
}
if
(
js
:
:
oom
:
:
ShouldFailWithOOM
(
)
)
{
if
(
allowGC
)
{
ReportOutOfMemory
(
cx
)
;
}
return
false
;
}
return
true
;
}
inline
bool
GCRuntime
:
:
gcIfNeededAtAllocation
(
JSContext
*
cx
)
{
#
ifdef
JS_GC_ZEAL
if
(
needZealousGC
(
)
)
{
runDebugGC
(
)
;
}
#
endif
if
(
cx
-
>
hasAnyPendingInterrupt
(
)
)
{
gcIfRequested
(
)
;
}
return
true
;
}
#
ifdef
DEBUG
void
GCRuntime
:
:
checkIncrementalZoneState
(
JSContext
*
cx
void
*
ptr
)
{
MOZ_ASSERT
(
ptr
)
;
TenuredCell
*
cell
=
reinterpret_cast
<
TenuredCell
*
>
(
ptr
)
;
TenuredChunkBase
*
chunk
=
detail
:
:
GetCellChunkBase
(
cell
)
;
if
(
cx
-
>
zone
(
)
-
>
isGCMarkingOrSweeping
(
)
)
{
MOZ_ASSERT
(
chunk
-
>
markBits
.
isMarkedBlack
(
cell
)
)
;
}
else
{
MOZ_ASSERT
(
!
chunk
-
>
markBits
.
isMarkedAny
(
cell
)
)
;
}
}
#
endif
void
*
js
:
:
gc
:
:
AllocateCellInGC
(
Zone
*
zone
AllocKind
thingKind
)
{
void
*
ptr
=
zone
-
>
arenas
.
allocateFromFreeList
(
thingKind
)
;
if
(
!
ptr
)
{
AutoEnterOOMUnsafeRegion
oomUnsafe
;
ptr
=
GCRuntime
:
:
refillFreeListInGC
(
zone
thingKind
)
;
if
(
!
ptr
)
{
oomUnsafe
.
crash
(
ChunkSize
"
Failed
to
allocate
new
chunk
during
GC
"
)
;
}
}
return
ptr
;
}
void
GCRuntime
:
:
startBackgroundAllocTaskIfIdle
(
)
{
AutoLockHelperThreadState
lock
;
if
(
!
allocTask
.
wasStarted
(
lock
)
)
{
allocTask
.
joinWithLockHeld
(
lock
)
;
allocTask
.
startWithLockHeld
(
lock
)
;
}
}
void
*
GCRuntime
:
:
refillFreeList
(
JSContext
*
cx
AllocKind
thingKind
)
{
MOZ_ASSERT
(
cx
-
>
zone
(
)
-
>
arenas
.
freeLists
(
)
.
isEmpty
(
thingKind
)
)
;
MOZ_ASSERT
(
!
cx
-
>
isHelperThreadContext
(
)
)
;
MOZ_ASSERT
(
!
JS
:
:
RuntimeHeapIsBusy
(
)
"
allocating
while
under
GC
"
)
;
return
cx
-
>
zone
(
)
-
>
arenas
.
refillFreeListAndAllocate
(
thingKind
ShouldCheckThresholds
:
:
CheckThresholds
)
;
}
void
*
GCRuntime
:
:
refillFreeListInGC
(
Zone
*
zone
AllocKind
thingKind
)
{
MOZ_ASSERT
(
JS
:
:
RuntimeHeapIsCollecting
(
)
)
;
MOZ_ASSERT_IF
(
!
JS
:
:
RuntimeHeapIsMinorCollecting
(
)
!
zone
-
>
runtimeFromMainThread
(
)
-
>
gc
.
isBackgroundSweeping
(
)
)
;
return
zone
-
>
arenas
.
refillFreeListAndAllocate
(
thingKind
ShouldCheckThresholds
:
:
DontCheckThresholds
)
;
}
void
*
ArenaLists
:
:
refillFreeListAndAllocate
(
AllocKind
thingKind
ShouldCheckThresholds
checkThresholds
)
{
MOZ_ASSERT
(
freeLists
(
)
.
isEmpty
(
thingKind
)
)
;
JSRuntime
*
rt
=
runtimeFromAnyThread
(
)
;
mozilla
:
:
Maybe
<
AutoLockGCBgAlloc
>
maybeLock
;
if
(
concurrentUse
(
thingKind
)
!
=
ConcurrentUse
:
:
None
)
{
maybeLock
.
emplace
(
rt
)
;
}
Arena
*
arena
=
arenaList
(
thingKind
)
.
takeNextArena
(
)
;
if
(
arena
)
{
MOZ_ASSERT
(
!
arena
-
>
isEmpty
(
)
)
;
return
freeLists
(
)
.
setArenaAndAllocate
(
arena
thingKind
)
;
}
if
(
maybeLock
.
isNothing
(
)
)
{
maybeLock
.
emplace
(
rt
)
;
}
TenuredChunk
*
chunk
=
rt
-
>
gc
.
pickChunk
(
maybeLock
.
ref
(
)
)
;
if
(
!
chunk
)
{
return
nullptr
;
}
arena
=
rt
-
>
gc
.
allocateArena
(
chunk
zone_
thingKind
checkThresholds
maybeLock
.
ref
(
)
)
;
if
(
!
arena
)
{
return
nullptr
;
}
ArenaList
&
al
=
arenaList
(
thingKind
)
;
MOZ_ASSERT
(
al
.
isCursorAtEnd
(
)
)
;
al
.
insertBeforeCursor
(
arena
)
;
return
freeLists
(
)
.
setArenaAndAllocate
(
arena
thingKind
)
;
}
inline
void
*
FreeLists
:
:
setArenaAndAllocate
(
Arena
*
arena
AllocKind
kind
)
{
#
ifdef
DEBUG
auto
old
=
freeLists_
[
kind
]
;
if
(
!
old
-
>
isEmpty
(
)
)
{
old
-
>
getArena
(
)
-
>
checkNoMarkedFreeCells
(
)
;
}
#
endif
FreeSpan
*
span
=
arena
-
>
getFirstFreeSpan
(
)
;
freeLists_
[
kind
]
=
span
;
Zone
*
zone
=
arena
-
>
zone
;
if
(
MOZ_UNLIKELY
(
zone
-
>
isGCMarkingOrSweeping
(
)
)
)
{
arena
-
>
arenaAllocatedDuringGC
(
)
;
}
TenuredCell
*
thing
=
span
-
>
allocate
(
Arena
:
:
thingSize
(
kind
)
)
;
MOZ_ASSERT
(
thing
)
;
return
thing
;
}
void
Arena
:
:
arenaAllocatedDuringGC
(
)
{
MOZ_ASSERT
(
zone
-
>
isGCMarkingOrSweeping
(
)
)
;
for
(
ArenaFreeCellIter
cell
(
this
)
;
!
cell
.
done
(
)
;
cell
.
next
(
)
)
{
MOZ_ASSERT
(
!
cell
-
>
isMarkedAny
(
)
)
;
cell
-
>
markBlack
(
)
;
}
}
bool
GCRuntime
:
:
wantBackgroundAllocation
(
const
AutoLockGC
&
lock
)
const
{
return
allocTask
.
enabled
(
)
&
&
emptyChunks
(
lock
)
.
count
(
)
<
minEmptyChunkCount
(
lock
)
&
&
(
fullChunks
(
lock
)
.
count
(
)
+
availableChunks
(
lock
)
.
count
(
)
)
>
=
4
;
}
Arena
*
GCRuntime
:
:
allocateArena
(
TenuredChunk
*
chunk
Zone
*
zone
AllocKind
thingKind
ShouldCheckThresholds
checkThresholds
const
AutoLockGC
&
lock
)
{
MOZ_ASSERT
(
chunk
-
>
hasAvailableArenas
(
)
)
;
if
(
(
checkThresholds
!
=
ShouldCheckThresholds
:
:
DontCheckThresholds
)
&
&
(
heapSize
.
bytes
(
)
>
=
tunables
.
gcMaxBytes
(
)
)
)
{
return
nullptr
;
}
Arena
*
arena
=
chunk
-
>
allocateArena
(
this
zone
thingKind
lock
)
;
zone
-
>
gcHeapSize
.
addGCArena
(
heapSize
)
;
if
(
checkThresholds
!
=
ShouldCheckThresholds
:
:
DontCheckThresholds
)
{
maybeTriggerGCAfterAlloc
(
zone
)
;
}
return
arena
;
}
Arena
*
TenuredChunk
:
:
allocateArena
(
GCRuntime
*
gc
Zone
*
zone
AllocKind
thingKind
const
AutoLockGC
&
lock
)
{
if
(
info
.
numArenasFreeCommitted
=
=
0
)
{
commitOnePage
(
gc
)
;
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
=
=
ArenasPerPage
)
;
}
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
>
0
)
;
Arena
*
arena
=
fetchNextFreeArena
(
gc
)
;
arena
-
>
init
(
zone
thingKind
lock
)
;
updateChunkListAfterAlloc
(
gc
lock
)
;
verify
(
)
;
return
arena
;
}
template
<
size_t
N
>
static
inline
size_t
FindFirstBitSet
(
const
mozilla
:
:
BitSet
<
N
uint32_t
>
&
bitset
)
{
MOZ_ASSERT
(
!
bitset
.
IsEmpty
(
)
)
;
const
auto
&
words
=
bitset
.
Storage
(
)
;
for
(
size_t
i
=
0
;
i
<
words
.
Length
(
)
;
i
+
+
)
{
uint32_t
word
=
words
[
i
]
;
if
(
word
)
{
return
i
*
32
+
mozilla
:
:
CountTrailingZeroes32
(
word
)
;
}
}
MOZ_CRASH
(
"
No
bits
found
"
)
;
}
void
TenuredChunk
:
:
commitOnePage
(
GCRuntime
*
gc
)
{
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
=
=
0
)
;
MOZ_ASSERT
(
info
.
numArenasFree
>
=
ArenasPerPage
)
;
uint32_t
pageIndex
=
FindFirstBitSet
(
decommittedPages
)
;
MOZ_ASSERT
(
decommittedPages
[
pageIndex
]
)
;
if
(
DecommitEnabled
(
)
)
{
MarkPagesInUseSoft
(
pageAddress
(
pageIndex
)
PageSize
)
;
}
decommittedPages
[
pageIndex
]
=
false
;
for
(
size_t
i
=
0
;
i
<
ArenasPerPage
;
i
+
+
)
{
size_t
arenaIndex
=
pageIndex
*
ArenasPerPage
+
i
;
MOZ_ASSERT
(
!
freeCommittedArenas
[
arenaIndex
]
)
;
freeCommittedArenas
[
arenaIndex
]
=
true
;
arenas
[
arenaIndex
]
.
setAsNotAllocated
(
)
;
+
+
info
.
numArenasFreeCommitted
;
gc
-
>
updateOnArenaFree
(
)
;
}
verify
(
)
;
}
inline
void
GCRuntime
:
:
updateOnFreeArenaAlloc
(
const
TenuredChunkInfo
&
info
)
{
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
<
=
numArenasFreeCommitted
)
;
-
-
numArenasFreeCommitted
;
}
Arena
*
TenuredChunk
:
:
fetchNextFreeArena
(
GCRuntime
*
gc
)
{
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
>
0
)
;
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
<
=
info
.
numArenasFree
)
;
size_t
index
=
FindFirstBitSet
(
freeCommittedArenas
)
;
MOZ_ASSERT
(
freeCommittedArenas
[
index
]
)
;
freeCommittedArenas
[
index
]
=
false
;
-
-
info
.
numArenasFreeCommitted
;
-
-
info
.
numArenasFree
;
gc
-
>
updateOnFreeArenaAlloc
(
info
)
;
return
&
arenas
[
index
]
;
}
TenuredChunk
*
GCRuntime
:
:
getOrAllocChunk
(
AutoLockGCBgAlloc
&
lock
)
{
TenuredChunk
*
chunk
=
emptyChunks
(
lock
)
.
pop
(
)
;
if
(
chunk
)
{
SetMemCheckKind
(
chunk
sizeof
(
ChunkBase
)
MemCheckKind
:
:
MakeUndefined
)
;
chunk
-
>
initBase
(
rt
nullptr
)
;
MOZ_ASSERT
(
chunk
-
>
unused
(
)
)
;
}
else
{
void
*
ptr
=
TenuredChunk
:
:
allocate
(
this
)
;
if
(
!
ptr
)
{
return
nullptr
;
}
chunk
=
TenuredChunk
:
:
emplace
(
ptr
this
true
)
;
MOZ_ASSERT
(
chunk
-
>
info
.
numArenasFreeCommitted
=
=
0
)
;
}
if
(
wantBackgroundAllocation
(
lock
)
)
{
lock
.
tryToStartBackgroundAllocation
(
)
;
}
return
chunk
;
}
void
GCRuntime
:
:
recycleChunk
(
TenuredChunk
*
chunk
const
AutoLockGC
&
lock
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
chunk
-
>
unused
(
)
)
;
chunk
-
>
verify
(
)
;
#
endif
AlwaysPoison
(
chunk
JS_FREED_CHUNK_PATTERN
sizeof
(
ChunkBase
)
MemCheckKind
:
:
MakeNoAccess
)
;
emptyChunks
(
lock
)
.
push
(
chunk
)
;
}
TenuredChunk
*
GCRuntime
:
:
pickChunk
(
AutoLockGCBgAlloc
&
lock
)
{
if
(
availableChunks
(
lock
)
.
count
(
)
)
{
return
availableChunks
(
lock
)
.
head
(
)
;
}
TenuredChunk
*
chunk
=
getOrAllocChunk
(
lock
)
;
if
(
!
chunk
)
{
return
nullptr
;
}
#
ifdef
DEBUG
chunk
-
>
verify
(
)
;
MOZ_ASSERT
(
chunk
-
>
unused
(
)
)
;
MOZ_ASSERT
(
!
fullChunks
(
lock
)
.
contains
(
chunk
)
)
;
MOZ_ASSERT
(
!
availableChunks
(
lock
)
.
contains
(
chunk
)
)
;
#
endif
availableChunks
(
lock
)
.
push
(
chunk
)
;
return
chunk
;
}
BackgroundAllocTask
:
:
BackgroundAllocTask
(
GCRuntime
*
gc
ChunkPool
&
pool
)
:
GCParallelTask
(
gc
gcstats
:
:
PhaseKind
:
:
NONE
)
chunkPool_
(
pool
)
enabled_
(
CanUseExtraThreads
(
)
&
&
GetCPUCount
(
)
>
=
2
)
{
}
void
BackgroundAllocTask
:
:
run
(
AutoLockHelperThreadState
&
lock
)
{
AutoUnlockHelperThreadState
unlock
(
lock
)
;
AutoLockGC
gcLock
(
gc
)
;
while
(
!
isCancelled
(
)
&
&
gc
-
>
wantBackgroundAllocation
(
gcLock
)
)
{
TenuredChunk
*
chunk
;
{
AutoUnlockGC
unlock
(
gcLock
)
;
void
*
ptr
=
TenuredChunk
:
:
allocate
(
gc
)
;
if
(
!
ptr
)
{
break
;
}
chunk
=
TenuredChunk
:
:
emplace
(
ptr
gc
true
)
;
}
chunkPool_
.
ref
(
)
.
push
(
chunk
)
;
}
}
void
*
TenuredChunk
:
:
allocate
(
GCRuntime
*
gc
)
{
void
*
chunk
=
MapAlignedPages
(
ChunkSize
ChunkSize
)
;
if
(
!
chunk
)
{
return
nullptr
;
}
gc
-
>
stats
(
)
.
count
(
gcstats
:
:
COUNT_NEW_CHUNK
)
;
return
chunk
;
}
static
inline
bool
ShouldDecommitNewChunk
(
bool
allMemoryCommitted
const
GCSchedulingState
&
state
)
{
if
(
!
DecommitEnabled
(
)
)
{
return
false
;
}
return
!
allMemoryCommitted
|
|
!
state
.
inHighFrequencyGCMode
(
)
;
}
TenuredChunk
*
TenuredChunk
:
:
emplace
(
void
*
ptr
GCRuntime
*
gc
bool
allMemoryCommitted
)
{
MOZ_MAKE_MEM_UNDEFINED
(
ptr
ChunkSize
)
;
Poison
(
ptr
JS_FRESH_TENURED_PATTERN
ChunkSize
MemCheckKind
:
:
MakeUndefined
)
;
TenuredChunk
*
chunk
=
new
(
mozilla
:
:
KnownNotNull
ptr
)
TenuredChunk
(
gc
-
>
rt
)
;
if
(
ShouldDecommitNewChunk
(
allMemoryCommitted
gc
-
>
schedulingState
)
)
{
chunk
-
>
decommitAllArenas
(
)
;
}
else
{
chunk
-
>
initAsDecommitted
(
)
;
}
chunk
-
>
verify
(
)
;
return
chunk
;
}
void
TenuredChunk
:
:
decommitAllArenas
(
)
{
MOZ_ASSERT
(
unused
(
)
)
;
MarkPagesUnusedSoft
(
&
arenas
[
0
]
ArenasPerChunk
*
ArenaSize
)
;
initAsDecommitted
(
)
;
}
void
TenuredChunkBase
:
:
initAsDecommitted
(
)
{
decommittedPages
.
SetAll
(
)
;
freeCommittedArenas
.
ResetAll
(
)
;
info
.
numArenasFree
=
ArenasPerChunk
;
info
.
numArenasFreeCommitted
=
0
;
}
