#
include
"
gc
/
Allocator
.
h
"
#
include
"
jscntxt
.
h
"
#
include
"
gc
/
GCInternals
.
h
"
#
include
"
gc
/
GCTrace
.
h
"
#
include
"
gc
/
Nursery
.
h
"
#
include
"
jit
/
JitCompartment
.
h
"
#
include
"
vm
/
Runtime
.
h
"
#
include
"
vm
/
String
.
h
"
#
include
"
jsobjinlines
.
h
"
#
include
"
gc
/
Heap
-
inl
.
h
"
using
namespace
js
;
using
namespace
gc
;
template
<
typename
T
AllowGC
allowGC
>
JSObject
*
js
:
:
Allocate
(
ExclusiveContext
*
cx
AllocKind
kind
size_t
nDynamicSlots
InitialHeap
heap
const
Class
*
clasp
)
{
static_assert
(
mozilla
:
:
IsConvertible
<
T
*
JSObject
*
>
:
:
value
"
must
be
JSObject
derived
"
)
;
MOZ_ASSERT
(
IsObjectAllocKind
(
kind
)
)
;
size_t
thingSize
=
Arena
:
:
thingSize
(
kind
)
;
MOZ_ASSERT
(
thingSize
=
=
Arena
:
:
thingSize
(
kind
)
)
;
MOZ_ASSERT
(
thingSize
>
=
sizeof
(
JSObject_Slots0
)
)
;
static_assert
(
sizeof
(
JSObject_Slots0
)
>
=
CellSize
"
All
allocations
must
be
at
least
the
allocator
-
imposed
minimum
size
.
"
)
;
MOZ_ASSERT_IF
(
nDynamicSlots
!
=
0
clasp
-
>
isNative
(
)
|
|
clasp
-
>
isProxy
(
)
)
;
if
(
!
cx
-
>
isJSContext
(
)
)
return
GCRuntime
:
:
tryNewTenuredObject
<
NoGC
>
(
cx
kind
thingSize
nDynamicSlots
)
;
JSContext
*
ncx
=
cx
-
>
asJSContext
(
)
;
JSRuntime
*
rt
=
ncx
-
>
runtime
(
)
;
if
(
!
rt
-
>
gc
.
checkAllocatorState
<
allowGC
>
(
ncx
kind
)
)
return
nullptr
;
if
(
ncx
-
>
nursery
(
)
.
isEnabled
(
)
&
&
heap
!
=
TenuredHeap
)
{
JSObject
*
obj
=
rt
-
>
gc
.
tryNewNurseryObject
<
allowGC
>
(
ncx
thingSize
nDynamicSlots
clasp
)
;
if
(
obj
)
return
obj
;
if
(
!
allowGC
)
return
nullptr
;
}
return
GCRuntime
:
:
tryNewTenuredObject
<
allowGC
>
(
cx
kind
thingSize
nDynamicSlots
)
;
}
template
JSObject
*
js
:
:
Allocate
<
JSObject
NoGC
>
(
ExclusiveContext
*
cx
gc
:
:
AllocKind
kind
size_t
nDynamicSlots
gc
:
:
InitialHeap
heap
const
Class
*
clasp
)
;
template
JSObject
*
js
:
:
Allocate
<
JSObject
CanGC
>
(
ExclusiveContext
*
cx
gc
:
:
AllocKind
kind
size_t
nDynamicSlots
gc
:
:
InitialHeap
heap
const
Class
*
clasp
)
;
template
<
AllowGC
allowGC
>
JSObject
*
GCRuntime
:
:
tryNewNurseryObject
(
JSContext
*
cx
size_t
thingSize
size_t
nDynamicSlots
const
Class
*
clasp
)
{
MOZ_ASSERT
(
isNurseryAllocAllowed
(
)
)
;
MOZ_ASSERT
(
!
cx
-
>
zone
(
)
-
>
usedByExclusiveThread
)
;
MOZ_ASSERT
(
!
IsAtomsCompartment
(
cx
-
>
compartment
(
)
)
)
;
JSObject
*
obj
=
nursery
.
allocateObject
(
cx
thingSize
nDynamicSlots
clasp
)
;
if
(
obj
)
return
obj
;
if
(
allowGC
&
&
!
rt
-
>
mainThread
.
suppressGC
)
{
minorGC
(
JS
:
:
gcreason
:
:
OUT_OF_NURSERY
)
;
if
(
nursery
.
isEnabled
(
)
)
{
JSObject
*
obj
=
nursery
.
allocateObject
(
cx
thingSize
nDynamicSlots
clasp
)
;
MOZ_ASSERT
(
obj
)
;
return
obj
;
}
}
return
nullptr
;
}
template
<
AllowGC
allowGC
>
JSObject
*
GCRuntime
:
:
tryNewTenuredObject
(
ExclusiveContext
*
cx
AllocKind
kind
size_t
thingSize
size_t
nDynamicSlots
)
{
HeapSlot
*
slots
=
nullptr
;
if
(
nDynamicSlots
)
{
slots
=
cx
-
>
zone
(
)
-
>
pod_malloc
<
HeapSlot
>
(
nDynamicSlots
)
;
if
(
MOZ_UNLIKELY
(
!
slots
)
)
{
if
(
allowGC
)
ReportOutOfMemory
(
cx
)
;
return
nullptr
;
}
Debug_SetSlotRangeToCrashOnTouch
(
slots
nDynamicSlots
)
;
}
JSObject
*
obj
=
tryNewTenuredThing
<
JSObject
allowGC
>
(
cx
kind
thingSize
)
;
if
(
obj
)
obj
-
>
setInitialSlotsMaybeNonNative
(
slots
)
;
else
js_free
(
slots
)
;
return
obj
;
}
template
<
typename
T
AllowGC
allowGC
>
T
*
js
:
:
Allocate
(
ExclusiveContext
*
cx
)
{
static_assert
(
!
mozilla
:
:
IsConvertible
<
T
*
JSObject
*
>
:
:
value
"
must
not
be
JSObject
derived
"
)
;
static_assert
(
sizeof
(
T
)
>
=
CellSize
"
All
allocations
must
be
at
least
the
allocator
-
imposed
minimum
size
.
"
)
;
AllocKind
kind
=
MapTypeToFinalizeKind
<
T
>
:
:
kind
;
size_t
thingSize
=
sizeof
(
T
)
;
MOZ_ASSERT
(
thingSize
=
=
Arena
:
:
thingSize
(
kind
)
)
;
if
(
cx
-
>
isJSContext
(
)
)
{
JSContext
*
ncx
=
cx
-
>
asJSContext
(
)
;
if
(
!
ncx
-
>
runtime
(
)
-
>
gc
.
checkAllocatorState
<
allowGC
>
(
ncx
kind
)
)
return
nullptr
;
}
return
GCRuntime
:
:
tryNewTenuredThing
<
T
allowGC
>
(
cx
kind
thingSize
)
;
}
#
define
DECL_ALLOCATOR_INSTANCES
(
allocKind
traceKind
type
sizedType
)
\
template
type
*
js
:
:
Allocate
<
type
NoGC
>
(
ExclusiveContext
*
cx
)
;
\
template
type
*
js
:
:
Allocate
<
type
CanGC
>
(
ExclusiveContext
*
cx
)
;
FOR_EACH_NONOBJECT_ALLOCKIND
(
DECL_ALLOCATOR_INSTANCES
)
#
undef
DECL_ALLOCATOR_INSTANCES
template
<
typename
T
AllowGC
allowGC
>
T
*
GCRuntime
:
:
tryNewTenuredThing
(
ExclusiveContext
*
cx
AllocKind
kind
size_t
thingSize
)
{
T
*
t
=
reinterpret_cast
<
T
*
>
(
cx
-
>
arenas
(
)
-
>
allocateFromFreeList
(
kind
thingSize
)
)
;
if
(
MOZ_UNLIKELY
(
!
t
)
)
{
t
=
reinterpret_cast
<
T
*
>
(
refillFreeListFromAnyThread
(
cx
kind
thingSize
)
)
;
if
(
MOZ_UNLIKELY
(
!
t
&
&
allowGC
&
&
cx
-
>
isJSContext
(
)
)
)
{
JS
:
:
PrepareForFullGC
(
cx
-
>
asJSContext
(
)
)
;
AutoKeepAtoms
keepAtoms
(
cx
-
>
perThreadData
)
;
cx
-
>
asJSContext
(
)
-
>
gc
.
gc
(
GC_SHRINK
JS
:
:
gcreason
:
:
LAST_DITCH
)
;
cx
-
>
asJSContext
(
)
-
>
gc
.
waitBackgroundSweepOrAllocEnd
(
)
;
t
=
tryNewTenuredThing
<
T
NoGC
>
(
cx
kind
thingSize
)
;
if
(
!
t
)
ReportOutOfMemory
(
cx
)
;
}
}
checkIncrementalZoneState
(
cx
t
)
;
TraceTenuredAlloc
(
t
kind
)
;
return
t
;
}
template
<
AllowGC
allowGC
>
bool
GCRuntime
:
:
checkAllocatorState
(
JSContext
*
cx
AllocKind
kind
)
{
if
(
allowGC
)
{
if
(
!
gcIfNeededPerAllocation
(
cx
)
)
return
false
;
}
#
if
defined
(
JS_GC_ZEAL
)
|
|
defined
(
DEBUG
)
MOZ_ASSERT_IF
(
rt
-
>
isAtomsCompartment
(
cx
-
>
compartment
(
)
)
kind
=
=
AllocKind
:
:
STRING
|
|
kind
=
=
AllocKind
:
:
FAT_INLINE_STRING
|
|
kind
=
=
AllocKind
:
:
SYMBOL
|
|
kind
=
=
AllocKind
:
:
JITCODE
|
|
kind
=
=
AllocKind
:
:
SCOPE
)
;
MOZ_ASSERT
(
!
rt
-
>
isHeapBusy
(
)
)
;
MOZ_ASSERT
(
isAllocAllowed
(
)
)
;
#
endif
if
(
allowGC
&
&
!
rt
-
>
mainThread
.
suppressGC
)
JS
:
:
AutoAssertOnGC
:
:
VerifyIsSafeToGC
(
rt
)
;
if
(
js
:
:
oom
:
:
ShouldFailWithOOM
(
)
)
{
if
(
allowGC
)
ReportOutOfMemory
(
cx
)
;
return
false
;
}
return
true
;
}
bool
GCRuntime
:
:
gcIfNeededPerAllocation
(
JSContext
*
cx
)
{
#
ifdef
JS_GC_ZEAL
if
(
needZealousGC
(
)
)
runDebugGC
(
)
;
#
endif
if
(
rt
-
>
hasPendingInterrupt
(
)
)
gcIfRequested
(
)
;
if
(
isIncrementalGCInProgress
(
)
&
&
cx
-
>
zone
(
)
-
>
usage
.
gcBytes
(
)
>
cx
-
>
zone
(
)
-
>
threshold
.
gcTriggerBytes
(
)
)
{
PrepareZoneForGC
(
cx
-
>
zone
(
)
)
;
AutoKeepAtoms
keepAtoms
(
cx
-
>
perThreadData
)
;
gc
(
GC_NORMAL
JS
:
:
gcreason
:
:
INCREMENTAL_TOO_SLOW
)
;
}
return
true
;
}
template
<
typename
T
>
void
GCRuntime
:
:
checkIncrementalZoneState
(
ExclusiveContext
*
cx
T
*
t
)
{
#
ifdef
DEBUG
if
(
!
cx
-
>
isJSContext
(
)
)
return
;
Zone
*
zone
=
cx
-
>
asJSContext
(
)
-
>
zone
(
)
;
MOZ_ASSERT_IF
(
t
&
&
zone
-
>
wasGCStarted
(
)
&
&
(
zone
-
>
isGCMarking
(
)
|
|
zone
-
>
isGCSweeping
(
)
)
t
-
>
asTenured
(
)
.
arena
(
)
-
>
allocatedDuringIncremental
)
;
#
endif
}
void
GCRuntime
:
:
startBackgroundAllocTaskIfIdle
(
)
{
AutoLockHelperThreadState
helperLock
;
if
(
allocTask
.
isRunningWithLockHeld
(
helperLock
)
)
return
;
allocTask
.
joinWithLockHeld
(
helperLock
)
;
allocTask
.
startWithLockHeld
(
helperLock
)
;
}
TenuredCell
*
GCRuntime
:
:
refillFreeListFromAnyThread
(
ExclusiveContext
*
cx
AllocKind
thingKind
size_t
thingSize
)
{
cx
-
>
arenas
(
)
-
>
checkEmptyFreeList
(
thingKind
)
;
if
(
cx
-
>
isJSContext
(
)
)
return
refillFreeListFromMainThread
(
cx
-
>
asJSContext
(
)
thingKind
thingSize
)
;
return
refillFreeListOffMainThread
(
cx
thingKind
)
;
}
TenuredCell
*
GCRuntime
:
:
refillFreeListFromMainThread
(
JSContext
*
cx
AllocKind
thingKind
size_t
thingSize
)
{
ArenaLists
*
arenas
=
cx
-
>
arenas
(
)
;
Zone
*
zone
=
cx
-
>
zone
(
)
;
MOZ_ASSERT
(
!
cx
-
>
runtime
(
)
-
>
isHeapBusy
(
)
"
allocating
while
under
GC
"
)
;
AutoMaybeStartBackgroundAllocation
maybeStartBGAlloc
;
return
arenas
-
>
allocateFromArena
(
zone
thingKind
maybeStartBGAlloc
)
;
}
TenuredCell
*
GCRuntime
:
:
refillFreeListOffMainThread
(
ExclusiveContext
*
cx
AllocKind
thingKind
)
{
ArenaLists
*
arenas
=
cx
-
>
arenas
(
)
;
Zone
*
zone
=
cx
-
>
zone
(
)
;
JSRuntime
*
rt
=
zone
-
>
runtimeFromAnyThread
(
)
;
AutoMaybeStartBackgroundAllocation
maybeStartBGAlloc
;
AutoLockHelperThreadState
lock
;
while
(
rt
-
>
isHeapCollecting
(
)
)
{
HelperThreadState
(
)
.
wait
(
lock
GlobalHelperThreadState
:
:
PRODUCER
)
;
HelperThreadState
(
)
.
notifyOne
(
GlobalHelperThreadState
:
:
PRODUCER
lock
)
;
}
return
arenas
-
>
allocateFromArena
(
zone
thingKind
maybeStartBGAlloc
)
;
}
TenuredCell
*
GCRuntime
:
:
refillFreeListInGC
(
Zone
*
zone
AllocKind
thingKind
)
{
zone
-
>
arenas
.
checkEmptyFreeList
(
thingKind
)
;
mozilla
:
:
DebugOnly
<
JSRuntime
*
>
rt
=
zone
-
>
runtimeFromMainThread
(
)
;
MOZ_ASSERT
(
rt
-
>
isHeapCollecting
(
)
)
;
MOZ_ASSERT_IF
(
!
rt
-
>
isHeapMinorCollecting
(
)
!
rt
-
>
gc
.
isBackgroundSweeping
(
)
)
;
AutoMaybeStartBackgroundAllocation
maybeStartBackgroundAllocation
;
return
zone
-
>
arenas
.
allocateFromArena
(
zone
thingKind
maybeStartBackgroundAllocation
)
;
}
TenuredCell
*
ArenaLists
:
:
allocateFromArena
(
JS
:
:
Zone
*
zone
AllocKind
thingKind
AutoMaybeStartBackgroundAllocation
&
maybeStartBGAlloc
)
{
JSRuntime
*
rt
=
zone
-
>
runtimeFromAnyThread
(
)
;
mozilla
:
:
Maybe
<
AutoLockGC
>
maybeLock
;
if
(
backgroundFinalizeState
[
thingKind
]
!
=
BFS_DONE
)
maybeLock
.
emplace
(
rt
)
;
ArenaList
&
al
=
arenaLists
[
thingKind
]
;
Arena
*
arena
=
al
.
takeNextArena
(
)
;
if
(
arena
)
{
MOZ_ASSERT
(
!
arena
-
>
isEmpty
(
)
)
;
return
allocateFromArenaInner
(
zone
arena
thingKind
)
;
}
if
(
maybeLock
.
isNothing
(
)
)
maybeLock
.
emplace
(
rt
)
;
Chunk
*
chunk
=
rt
-
>
gc
.
pickChunk
(
maybeLock
.
ref
(
)
maybeStartBGAlloc
)
;
if
(
!
chunk
)
return
nullptr
;
arena
=
rt
-
>
gc
.
allocateArena
(
chunk
zone
thingKind
maybeLock
.
ref
(
)
)
;
if
(
!
arena
)
return
nullptr
;
MOZ_ASSERT
(
al
.
isCursorAtEnd
(
)
)
;
al
.
insertBeforeCursor
(
arena
)
;
return
allocateFromArenaInner
(
zone
arena
thingKind
)
;
}
inline
TenuredCell
*
ArenaLists
:
:
allocateFromArenaInner
(
JS
:
:
Zone
*
zone
Arena
*
arena
AllocKind
kind
)
{
size_t
thingSize
=
Arena
:
:
thingSize
(
kind
)
;
freeLists
[
kind
]
=
arena
-
>
getFirstFreeSpan
(
)
;
if
(
MOZ_UNLIKELY
(
zone
-
>
wasGCStarted
(
)
)
)
zone
-
>
runtimeFromAnyThread
(
)
-
>
gc
.
arenaAllocatedDuringGC
(
zone
arena
)
;
TenuredCell
*
thing
=
freeLists
[
kind
]
-
>
allocate
(
thingSize
)
;
MOZ_ASSERT
(
thing
)
;
return
thing
;
}
void
GCRuntime
:
:
arenaAllocatedDuringGC
(
JS
:
:
Zone
*
zone
Arena
*
arena
)
{
if
(
zone
-
>
needsIncrementalBarrier
(
)
)
{
arena
-
>
allocatedDuringIncremental
=
true
;
marker
.
delayMarkingArena
(
arena
)
;
}
else
if
(
zone
-
>
isGCSweeping
(
)
)
{
arena
-
>
setNextAllocDuringSweep
(
arenasAllocatedDuringSweep
)
;
arenasAllocatedDuringSweep
=
arena
;
}
}
bool
GCRuntime
:
:
wantBackgroundAllocation
(
const
AutoLockGC
&
lock
)
const
{
return
allocTask
.
enabled
(
)
&
&
emptyChunks
(
lock
)
.
count
(
)
<
tunables
.
minEmptyChunkCount
(
lock
)
&
&
(
fullChunks
(
lock
)
.
count
(
)
+
availableChunks
(
lock
)
.
count
(
)
)
>
=
4
;
}
Arena
*
GCRuntime
:
:
allocateArena
(
Chunk
*
chunk
Zone
*
zone
AllocKind
thingKind
const
AutoLockGC
&
lock
)
{
MOZ_ASSERT
(
chunk
-
>
hasAvailableArenas
(
)
)
;
if
(
!
rt
-
>
isHeapMinorCollecting
(
)
&
&
!
isHeapCompacting
(
)
&
&
usage
.
gcBytes
(
)
>
=
tunables
.
gcMaxBytes
(
)
)
{
return
nullptr
;
}
Arena
*
arena
=
chunk
-
>
allocateArena
(
rt
zone
thingKind
lock
)
;
zone
-
>
usage
.
addGCArena
(
)
;
if
(
!
rt
-
>
isHeapMinorCollecting
(
)
&
&
!
isHeapCompacting
(
)
)
maybeAllocTriggerZoneGC
(
zone
lock
)
;
return
arena
;
}
Arena
*
Chunk
:
:
allocateArena
(
JSRuntime
*
rt
Zone
*
zone
AllocKind
thingKind
const
AutoLockGC
&
lock
)
{
Arena
*
arena
=
info
.
numArenasFreeCommitted
>
0
?
fetchNextFreeArena
(
rt
)
:
fetchNextDecommittedArena
(
)
;
arena
-
>
init
(
zone
thingKind
)
;
updateChunkListAfterAlloc
(
rt
lock
)
;
return
arena
;
}
inline
void
GCRuntime
:
:
updateOnFreeArenaAlloc
(
const
ChunkInfo
&
info
)
{
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
<
=
numArenasFreeCommitted
)
;
-
-
numArenasFreeCommitted
;
}
Arena
*
Chunk
:
:
fetchNextFreeArena
(
JSRuntime
*
rt
)
{
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
>
0
)
;
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
<
=
info
.
numArenasFree
)
;
Arena
*
arena
=
info
.
freeArenasHead
;
info
.
freeArenasHead
=
arena
-
>
next
;
-
-
info
.
numArenasFreeCommitted
;
-
-
info
.
numArenasFree
;
rt
-
>
gc
.
updateOnFreeArenaAlloc
(
info
)
;
return
arena
;
}
Arena
*
Chunk
:
:
fetchNextDecommittedArena
(
)
{
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
=
=
0
)
;
MOZ_ASSERT
(
info
.
numArenasFree
>
0
)
;
unsigned
offset
=
findDecommittedArenaOffset
(
)
;
info
.
lastDecommittedArenaOffset
=
offset
+
1
;
-
-
info
.
numArenasFree
;
decommittedArenas
.
unset
(
offset
)
;
Arena
*
arena
=
&
arenas
[
offset
]
;
MarkPagesInUse
(
arena
ArenaSize
)
;
arena
-
>
setAsNotAllocated
(
)
;
return
arena
;
}
uint32_t
Chunk
:
:
findDecommittedArenaOffset
(
)
{
for
(
unsigned
i
=
info
.
lastDecommittedArenaOffset
;
i
<
ArenasPerChunk
;
i
+
+
)
{
if
(
decommittedArenas
.
get
(
i
)
)
return
i
;
}
for
(
unsigned
i
=
0
;
i
<
info
.
lastDecommittedArenaOffset
;
i
+
+
)
{
if
(
decommittedArenas
.
get
(
i
)
)
return
i
;
}
MOZ_CRASH
(
"
No
decommitted
arenas
found
.
"
)
;
}
Chunk
*
GCRuntime
:
:
getOrAllocChunk
(
const
AutoLockGC
&
lock
AutoMaybeStartBackgroundAllocation
&
maybeStartBackgroundAllocation
)
{
Chunk
*
chunk
=
emptyChunks
(
lock
)
.
pop
(
)
;
if
(
!
chunk
)
{
chunk
=
Chunk
:
:
allocate
(
rt
)
;
if
(
!
chunk
)
return
nullptr
;
MOZ_ASSERT
(
chunk
-
>
info
.
numArenasFreeCommitted
=
=
0
)
;
}
if
(
wantBackgroundAllocation
(
lock
)
)
maybeStartBackgroundAllocation
.
tryToStartBackgroundAllocation
(
rt
-
>
gc
)
;
return
chunk
;
}
void
GCRuntime
:
:
recycleChunk
(
Chunk
*
chunk
const
AutoLockGC
&
lock
)
{
emptyChunks
(
lock
)
.
push
(
chunk
)
;
}
Chunk
*
GCRuntime
:
:
pickChunk
(
const
AutoLockGC
&
lock
AutoMaybeStartBackgroundAllocation
&
maybeStartBackgroundAllocation
)
{
if
(
availableChunks
(
lock
)
.
count
(
)
)
return
availableChunks
(
lock
)
.
head
(
)
;
Chunk
*
chunk
=
getOrAllocChunk
(
lock
maybeStartBackgroundAllocation
)
;
if
(
!
chunk
)
return
nullptr
;
chunk
-
>
init
(
rt
)
;
MOZ_ASSERT
(
chunk
-
>
info
.
numArenasFreeCommitted
=
=
0
)
;
MOZ_ASSERT
(
chunk
-
>
unused
(
)
)
;
MOZ_ASSERT
(
!
fullChunks
(
lock
)
.
contains
(
chunk
)
)
;
MOZ_ASSERT
(
!
availableChunks
(
lock
)
.
contains
(
chunk
)
)
;
chunkAllocationSinceLastGC
=
true
;
availableChunks
(
lock
)
.
push
(
chunk
)
;
return
chunk
;
}
BackgroundAllocTask
:
:
BackgroundAllocTask
(
JSRuntime
*
rt
ChunkPool
&
pool
)
:
runtime
(
rt
)
chunkPool_
(
pool
)
enabled_
(
CanUseExtraThreads
(
)
&
&
GetCPUCount
(
)
>
=
2
)
{
}
void
BackgroundAllocTask
:
:
run
(
)
{
TraceLoggerThread
*
logger
=
TraceLoggerForCurrentThread
(
)
;
AutoTraceLog
logAllocation
(
logger
TraceLogger_GCAllocation
)
;
AutoLockGC
lock
(
runtime
)
;
while
(
!
cancel_
&
&
runtime
-
>
gc
.
wantBackgroundAllocation
(
lock
)
)
{
Chunk
*
chunk
;
{
AutoUnlockGC
unlock
(
lock
)
;
chunk
=
Chunk
:
:
allocate
(
runtime
)
;
if
(
!
chunk
)
break
;
chunk
-
>
init
(
runtime
)
;
}
chunkPool_
.
push
(
chunk
)
;
}
}
Chunk
*
Chunk
:
:
allocate
(
JSRuntime
*
rt
)
{
Chunk
*
chunk
=
static_cast
<
Chunk
*
>
(
MapAlignedPages
(
ChunkSize
ChunkSize
)
)
;
if
(
!
chunk
)
return
nullptr
;
rt
-
>
gc
.
stats
.
count
(
gcstats
:
:
STAT_NEW_CHUNK
)
;
return
chunk
;
}
void
Chunk
:
:
init
(
JSRuntime
*
rt
)
{
JS_POISON
(
this
JS_FRESH_TENURED_PATTERN
ChunkSize
)
;
bitmap
.
clear
(
)
;
decommitAllArenas
(
rt
)
;
info
.
init
(
)
;
new
(
&
trailer
)
ChunkTrailer
(
rt
)
;
}
void
Chunk
:
:
decommitAllArenas
(
JSRuntime
*
rt
)
{
decommittedArenas
.
clear
(
true
)
;
MarkPagesUnused
(
&
arenas
[
0
]
ArenasPerChunk
*
ArenaSize
)
;
info
.
freeArenasHead
=
nullptr
;
info
.
lastDecommittedArenaOffset
=
0
;
info
.
numArenasFree
=
ArenasPerChunk
;
info
.
numArenasFreeCommitted
=
0
;
}
