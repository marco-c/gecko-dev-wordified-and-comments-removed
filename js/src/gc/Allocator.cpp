#
include
"
gc
/
Allocator
.
h
"
#
include
"
mozilla
/
DebugOnly
.
h
"
#
include
"
mozilla
/
OperatorNewExtensions
.
h
"
#
include
"
mozilla
/
TimeStamp
.
h
"
#
include
"
gc
/
GCInternals
.
h
"
#
include
"
gc
/
GCLock
.
h
"
#
include
"
gc
/
GCProbes
.
h
"
#
include
"
gc
/
Nursery
.
h
"
#
include
"
threading
/
CpuCount
.
h
"
#
include
"
util
/
Poison
.
h
"
#
include
"
vm
/
BigIntType
.
h
"
#
include
"
vm
/
FrameIter
.
h
"
#
include
"
vm
/
Runtime
.
h
"
#
include
"
vm
/
StringType
.
h
"
#
include
"
gc
/
ArenaList
-
inl
.
h
"
#
include
"
gc
/
Heap
-
inl
.
h
"
#
include
"
gc
/
PrivateIterators
-
inl
.
h
"
#
include
"
vm
/
JSContext
-
inl
.
h
"
#
include
"
vm
/
JSScript
-
inl
.
h
"
using
mozilla
:
:
TimeStamp
;
using
namespace
js
;
using
namespace
js
:
:
gc
;
static
Heap
MinHeapToTenure
(
bool
allowNurseryAlloc
)
{
static_assert
(
Heap
:
:
Tenured
>
Heap
:
:
Default
)
;
return
allowNurseryAlloc
?
Heap
:
:
Tenured
:
Heap
:
:
Default
;
}
void
Zone
:
:
setNurseryAllocFlags
(
bool
allocObjects
bool
allocStrings
bool
allocBigInts
)
{
allocNurseryObjects_
=
allocObjects
;
allocNurseryStrings_
=
allocStrings
;
allocNurseryBigInts_
=
allocBigInts
;
minObjectHeapToTenure_
=
MinHeapToTenure
(
allocNurseryObjects
(
)
)
;
minStringHeapToTenure_
=
MinHeapToTenure
(
allocNurseryStrings
(
)
)
;
minBigintHeapToTenure_
=
MinHeapToTenure
(
allocNurseryBigInts
(
)
)
;
}
#
define
INSTANTIATE_ALLOC_NURSERY_CELL
(
traceKind
allowGc
)
\
template
void
*
\
gc
:
:
CellAllocator
:
:
AllocNurseryOrTenuredCell
<
traceKind
allowGc
>
(
\
JSContext
*
AllocKind
size_t
gc
:
:
Heap
AllocSite
*
)
;
INSTANTIATE_ALLOC_NURSERY_CELL
(
JS
:
:
TraceKind
:
:
Object
NoGC
)
INSTANTIATE_ALLOC_NURSERY_CELL
(
JS
:
:
TraceKind
:
:
Object
CanGC
)
INSTANTIATE_ALLOC_NURSERY_CELL
(
JS
:
:
TraceKind
:
:
String
NoGC
)
INSTANTIATE_ALLOC_NURSERY_CELL
(
JS
:
:
TraceKind
:
:
String
CanGC
)
INSTANTIATE_ALLOC_NURSERY_CELL
(
JS
:
:
TraceKind
:
:
BigInt
NoGC
)
INSTANTIATE_ALLOC_NURSERY_CELL
(
JS
:
:
TraceKind
:
:
BigInt
CanGC
)
#
undef
INSTANTIATE_ALLOC_NURSERY_CELL
template
<
AllowGC
allowGC
>
MOZ_NEVER_INLINE
void
*
CellAllocator
:
:
RetryNurseryAlloc
(
JSContext
*
cx
JS
:
:
TraceKind
traceKind
AllocKind
allocKind
size_t
thingSize
AllocSite
*
site
)
{
MOZ_ASSERT
(
cx
-
>
isNurseryAllocAllowed
(
)
)
;
Zone
*
zone
=
site
-
>
zone
(
)
;
MOZ_ASSERT
(
!
zone
-
>
isAtomsZone
(
)
)
;
MOZ_ASSERT
(
zone
-
>
allocKindInNursery
(
traceKind
)
)
;
Nursery
&
nursery
=
cx
-
>
nursery
(
)
;
JS
:
:
GCReason
reason
=
nursery
.
handleAllocationFailure
(
)
;
if
(
reason
=
=
JS
:
:
GCReason
:
:
NO_REASON
)
{
void
*
ptr
=
nursery
.
tryAllocateCell
(
site
thingSize
traceKind
)
;
MOZ_ASSERT
(
ptr
)
;
return
ptr
;
}
if
constexpr
(
!
allowGC
)
{
return
nullptr
;
}
if
(
!
cx
-
>
suppressGC
)
{
cx
-
>
runtime
(
)
-
>
gc
.
minorGC
(
reason
)
;
if
(
zone
-
>
allocKindInNursery
(
traceKind
)
)
{
void
*
ptr
=
cx
-
>
nursery
(
)
.
allocateCell
(
site
thingSize
traceKind
)
;
if
(
ptr
)
{
return
ptr
;
}
}
}
return
AllocTenuredCellForNurseryAlloc
<
allowGC
>
(
cx
allocKind
)
;
}
template
void
*
CellAllocator
:
:
RetryNurseryAlloc
<
NoGC
>
(
JSContext
*
cx
JS
:
:
TraceKind
traceKind
AllocKind
allocKind
size_t
thingSize
AllocSite
*
site
)
;
template
void
*
CellAllocator
:
:
RetryNurseryAlloc
<
CanGC
>
(
JSContext
*
cx
JS
:
:
TraceKind
traceKind
AllocKind
allocKind
size_t
thingSize
AllocSite
*
site
)
;
static
inline
void
MajorGCIfRequested
(
JSContext
*
cx
)
{
if
(
cx
-
>
hasPendingInterrupt
(
InterruptReason
:
:
MajorGC
)
)
{
cx
-
>
runtime
(
)
-
>
gc
.
gcIfRequested
(
)
;
}
}
template
<
AllowGC
allowGC
>
MOZ_NEVER_INLINE
void
*
gc
:
:
CellAllocator
:
:
AllocTenuredCellForNurseryAlloc
(
JSContext
*
cx
gc
:
:
AllocKind
kind
)
{
if
constexpr
(
allowGC
)
{
MajorGCIfRequested
(
cx
)
;
}
return
AllocTenuredCellUnchecked
<
allowGC
>
(
cx
kind
)
;
}
template
void
*
gc
:
:
CellAllocator
:
:
AllocTenuredCellForNurseryAlloc
<
NoGC
>
(
JSContext
*
AllocKind
)
;
template
void
*
gc
:
:
CellAllocator
:
:
AllocTenuredCellForNurseryAlloc
<
CanGC
>
(
JSContext
*
AllocKind
)
;
template
<
AllowGC
allowGC
>
void
*
gc
:
:
CellAllocator
:
:
AllocTenuredCell
(
JSContext
*
cx
gc
:
:
AllocKind
kind
)
{
MOZ_ASSERT
(
!
IsNurseryAllocable
(
kind
)
)
;
if
(
!
PreAllocChecks
<
allowGC
>
(
cx
kind
)
)
{
return
nullptr
;
}
if
constexpr
(
allowGC
)
{
MajorGCIfRequested
(
cx
)
;
}
return
AllocTenuredCellUnchecked
<
allowGC
>
(
cx
kind
)
;
}
template
void
*
gc
:
:
CellAllocator
:
:
AllocTenuredCell
<
NoGC
>
(
JSContext
*
AllocKind
)
;
template
void
*
gc
:
:
CellAllocator
:
:
AllocTenuredCell
<
CanGC
>
(
JSContext
*
AllocKind
)
;
template
<
AllowGC
allowGC
>
void
*
CellAllocator
:
:
AllocTenuredCellUnchecked
(
JSContext
*
cx
AllocKind
kind
)
{
Zone
*
zone
=
cx
-
>
zone
(
)
;
void
*
ptr
=
zone
-
>
arenas
.
freeLists
(
)
.
allocate
(
kind
)
;
if
(
MOZ_UNLIKELY
(
!
ptr
)
)
{
ptr
=
GCRuntime
:
:
refillFreeList
(
cx
kind
)
;
if
(
MOZ_UNLIKELY
(
!
ptr
)
)
{
if
constexpr
(
allowGC
)
{
return
RetryTenuredAlloc
(
cx
kind
)
;
}
return
nullptr
;
}
}
#
ifdef
DEBUG
CheckIncrementalZoneState
(
cx
ptr
)
;
#
endif
gcprobes
:
:
TenuredAlloc
(
ptr
kind
)
;
zone
-
>
noteTenuredAlloc
(
)
;
return
ptr
;
}
template
void
*
CellAllocator
:
:
AllocTenuredCellUnchecked
<
NoGC
>
(
JSContext
*
cx
AllocKind
kind
)
;
template
void
*
CellAllocator
:
:
AllocTenuredCellUnchecked
<
CanGC
>
(
JSContext
*
cx
AllocKind
kind
)
;
MOZ_NEVER_INLINE
void
*
CellAllocator
:
:
RetryTenuredAlloc
(
JSContext
*
cx
AllocKind
kind
)
{
cx
-
>
runtime
(
)
-
>
gc
.
attemptLastDitchGC
(
cx
)
;
void
*
ptr
=
AllocTenuredCellUnchecked
<
NoGC
>
(
cx
kind
)
;
if
(
!
ptr
)
{
ReportOutOfMemory
(
cx
)
;
return
nullptr
;
}
return
ptr
;
}
void
GCRuntime
:
:
attemptLastDitchGC
(
JSContext
*
cx
)
{
if
(
!
lastLastDitchTime
.
IsNull
(
)
&
&
TimeStamp
:
:
Now
(
)
-
lastLastDitchTime
<
=
tunables
.
minLastDitchGCPeriod
(
)
)
{
return
;
}
JS
:
:
PrepareForFullGC
(
cx
)
;
gc
(
JS
:
:
GCOptions
:
:
Shrink
JS
:
:
GCReason
:
:
LAST_DITCH
)
;
waitBackgroundAllocEnd
(
)
;
waitBackgroundFreeEnd
(
)
;
lastLastDitchTime
=
mozilla
:
:
TimeStamp
:
:
Now
(
)
;
}
#
ifdef
DEBUG
static
bool
IsAtomsZoneKind
(
AllocKind
kind
)
{
return
kind
=
=
AllocKind
:
:
ATOM
|
|
kind
=
=
AllocKind
:
:
FAT_INLINE_ATOM
|
|
kind
=
=
AllocKind
:
:
SYMBOL
;
}
#
endif
#
if
defined
(
DEBUG
)
|
|
defined
(
JS_GC_ZEAL
)
|
|
defined
(
JS_OOM_BREAKPOINT
)
static
inline
void
CheckAllocZone
(
Zone
*
zone
AllocKind
kind
)
{
MOZ_ASSERT_IF
(
zone
-
>
isAtomsZone
(
)
IsAtomsZoneKind
(
kind
)
|
|
kind
=
=
AllocKind
:
:
JITCODE
)
;
MOZ_ASSERT_IF
(
!
zone
-
>
isAtomsZone
(
)
!
IsAtomsZoneKind
(
kind
)
)
;
}
template
<
AllowGC
allowGC
>
bool
CellAllocator
:
:
PreAllocChecks
(
JSContext
*
cx
AllocKind
kind
)
{
MOZ_ASSERT
(
CurrentThreadCanAccessRuntime
(
cx
-
>
runtime
(
)
)
)
;
CheckAllocZone
(
cx
-
>
zone
(
)
kind
)
;
if
(
allowGC
&
&
!
cx
-
>
suppressGC
)
{
cx
-
>
verifyIsSafeToGC
(
)
;
}
#
ifdef
JS_GC_ZEAL
if
constexpr
(
allowGC
)
{
GCRuntime
*
gc
=
&
cx
-
>
runtime
(
)
-
>
gc
;
if
(
gc
-
>
needZealousGC
(
)
)
{
gc
-
>
runDebugGC
(
)
;
}
}
#
endif
if
(
js
:
:
oom
:
:
ShouldFailWithOOM
(
)
)
{
if
constexpr
(
allowGC
)
{
ReportOutOfMemory
(
cx
)
;
}
return
false
;
}
return
true
;
}
template
bool
CellAllocator
:
:
PreAllocChecks
<
NoGC
>
(
JSContext
*
cx
AllocKind
kind
)
;
template
bool
CellAllocator
:
:
PreAllocChecks
<
CanGC
>
(
JSContext
*
cx
AllocKind
kind
)
;
#
endif
#
ifdef
JS_GC_ZEAL
AllocSite
*
CellAllocator
:
:
MaybeGenerateMissingAllocSite
(
JSContext
*
cx
JS
:
:
TraceKind
traceKind
AllocSite
*
site
)
{
MOZ_ASSERT
(
site
)
;
if
(
!
cx
-
>
runtime
(
)
-
>
gc
.
tunables
.
generateMissingAllocSites
(
)
)
{
return
site
;
}
if
(
!
site
-
>
isUnknown
(
)
)
{
return
site
;
}
if
(
cx
-
>
inUnsafeCallWithABI
)
{
return
site
;
}
FrameIter
frame
(
cx
)
;
if
(
frame
.
done
(
)
|
|
!
frame
.
isBaseline
(
)
)
{
return
site
;
}
MOZ_ASSERT
(
site
=
=
cx
-
>
zone
(
)
-
>
unknownAllocSite
(
traceKind
)
)
;
MOZ_ASSERT
(
frame
.
hasScript
(
)
)
;
JSScript
*
script
=
frame
.
script
(
)
;
if
(
cx
-
>
zone
(
)
!
=
script
-
>
zone
(
)
)
{
return
site
;
}
uint32_t
pcOffset
=
script
-
>
pcToOffset
(
frame
.
pc
(
)
)
;
if
(
!
script
-
>
hasBaselineScript
(
)
|
|
pcOffset
>
AllocSite
:
:
MaxValidPCOffset
)
{
return
site
;
}
AllocSite
*
missingSite
=
GetOrCreateMissingAllocSite
(
cx
script
pcOffset
traceKind
)
;
if
(
!
missingSite
)
{
return
site
;
}
return
missingSite
;
}
#
endif
#
ifdef
DEBUG
void
CellAllocator
:
:
CheckIncrementalZoneState
(
JSContext
*
cx
void
*
ptr
)
{
MOZ_ASSERT
(
ptr
)
;
TenuredCell
*
cell
=
reinterpret_cast
<
TenuredCell
*
>
(
ptr
)
;
TenuredChunkBase
*
chunk
=
detail
:
:
GetCellChunkBase
(
cell
)
;
if
(
cx
-
>
zone
(
)
-
>
isGCMarkingOrSweeping
(
)
)
{
MOZ_ASSERT
(
chunk
-
>
markBits
.
isMarkedBlack
(
cell
)
)
;
}
else
{
MOZ_ASSERT
(
!
chunk
-
>
markBits
.
isMarkedAny
(
cell
)
)
;
}
}
#
endif
void
*
js
:
:
gc
:
:
AllocateTenuredCellInGC
(
Zone
*
zone
AllocKind
thingKind
)
{
void
*
ptr
=
zone
-
>
arenas
.
allocateFromFreeList
(
thingKind
)
;
if
(
!
ptr
)
{
AutoEnterOOMUnsafeRegion
oomUnsafe
;
ptr
=
GCRuntime
:
:
refillFreeListInGC
(
zone
thingKind
)
;
if
(
!
ptr
)
{
oomUnsafe
.
crash
(
ChunkSize
"
Failed
to
allocate
new
chunk
during
GC
"
)
;
}
}
return
ptr
;
}
void
GCRuntime
:
:
startBackgroundAllocTaskIfIdle
(
)
{
AutoLockHelperThreadState
lock
;
if
(
!
allocTask
.
wasStarted
(
lock
)
)
{
allocTask
.
joinWithLockHeld
(
lock
)
;
allocTask
.
startWithLockHeld
(
lock
)
;
}
}
void
*
GCRuntime
:
:
refillFreeList
(
JSContext
*
cx
AllocKind
thingKind
)
{
MOZ_ASSERT
(
cx
-
>
zone
(
)
-
>
arenas
.
freeLists
(
)
.
isEmpty
(
thingKind
)
)
;
MOZ_ASSERT
(
!
JS
:
:
RuntimeHeapIsBusy
(
)
"
allocating
while
under
GC
"
)
;
return
cx
-
>
zone
(
)
-
>
arenas
.
refillFreeListAndAllocate
(
thingKind
ShouldCheckThresholds
:
:
CheckThresholds
)
;
}
void
*
GCRuntime
:
:
refillFreeListInGC
(
Zone
*
zone
AllocKind
thingKind
)
{
MOZ_ASSERT
(
JS
:
:
RuntimeHeapIsCollecting
(
)
)
;
MOZ_ASSERT_IF
(
!
JS
:
:
RuntimeHeapIsMinorCollecting
(
)
!
zone
-
>
runtimeFromMainThread
(
)
-
>
gc
.
isBackgroundSweeping
(
)
)
;
return
zone
-
>
arenas
.
refillFreeListAndAllocate
(
thingKind
ShouldCheckThresholds
:
:
DontCheckThresholds
)
;
}
void
*
ArenaLists
:
:
refillFreeListAndAllocate
(
AllocKind
thingKind
ShouldCheckThresholds
checkThresholds
)
{
MOZ_ASSERT
(
freeLists
(
)
.
isEmpty
(
thingKind
)
)
;
JSRuntime
*
rt
=
runtimeFromAnyThread
(
)
;
mozilla
:
:
Maybe
<
AutoLockGCBgAlloc
>
maybeLock
;
if
(
concurrentUse
(
thingKind
)
!
=
ConcurrentUse
:
:
None
)
{
maybeLock
.
emplace
(
rt
)
;
}
Arena
*
arena
=
arenaList
(
thingKind
)
.
takeNextArena
(
)
;
if
(
arena
)
{
MOZ_ASSERT
(
!
arena
-
>
isEmpty
(
)
)
;
return
freeLists
(
)
.
setArenaAndAllocate
(
arena
thingKind
)
;
}
if
(
maybeLock
.
isNothing
(
)
)
{
maybeLock
.
emplace
(
rt
)
;
}
TenuredChunk
*
chunk
=
rt
-
>
gc
.
pickChunk
(
maybeLock
.
ref
(
)
)
;
if
(
!
chunk
)
{
return
nullptr
;
}
arena
=
rt
-
>
gc
.
allocateArena
(
chunk
zone_
thingKind
checkThresholds
maybeLock
.
ref
(
)
)
;
if
(
!
arena
)
{
return
nullptr
;
}
ArenaList
&
al
=
arenaList
(
thingKind
)
;
MOZ_ASSERT
(
al
.
isCursorAtEnd
(
)
)
;
al
.
insertBeforeCursor
(
arena
)
;
return
freeLists
(
)
.
setArenaAndAllocate
(
arena
thingKind
)
;
}
inline
void
*
FreeLists
:
:
setArenaAndAllocate
(
Arena
*
arena
AllocKind
kind
)
{
#
ifdef
DEBUG
auto
*
old
=
freeLists_
[
kind
]
;
if
(
!
old
-
>
isEmpty
(
)
)
{
old
-
>
getArena
(
)
-
>
checkNoMarkedFreeCells
(
)
;
}
#
endif
FreeSpan
*
span
=
arena
-
>
getFirstFreeSpan
(
)
;
freeLists_
[
kind
]
=
span
;
Zone
*
zone
=
arena
-
>
zone
(
)
;
if
(
MOZ_UNLIKELY
(
zone
-
>
isGCMarkingOrSweeping
(
)
)
)
{
arena
-
>
arenaAllocatedDuringGC
(
)
;
}
TenuredCell
*
thing
=
span
-
>
allocate
(
Arena
:
:
thingSize
(
kind
)
)
;
MOZ_ASSERT
(
thing
)
;
return
thing
;
}
void
Arena
:
:
arenaAllocatedDuringGC
(
)
{
MOZ_ASSERT
(
zone
(
)
-
>
isGCMarkingOrSweeping
(
)
)
;
for
(
ArenaFreeCellIter
cell
(
this
)
;
!
cell
.
done
(
)
;
cell
.
next
(
)
)
{
MOZ_ASSERT
(
!
cell
-
>
isMarkedAny
(
)
)
;
cell
-
>
markBlack
(
)
;
}
}
bool
GCRuntime
:
:
wantBackgroundAllocation
(
const
AutoLockGC
&
lock
)
const
{
return
allocTask
.
enabled
(
)
&
&
emptyChunks
(
lock
)
.
count
(
)
<
minEmptyChunkCount
(
lock
)
&
&
(
fullChunks
(
lock
)
.
count
(
)
+
availableChunks
(
lock
)
.
count
(
)
)
>
=
4
;
}
Arena
*
GCRuntime
:
:
allocateArena
(
TenuredChunk
*
chunk
Zone
*
zone
AllocKind
thingKind
ShouldCheckThresholds
checkThresholds
const
AutoLockGC
&
lock
)
{
MOZ_ASSERT
(
chunk
-
>
hasAvailableArenas
(
)
)
;
if
(
(
checkThresholds
!
=
ShouldCheckThresholds
:
:
DontCheckThresholds
)
&
&
(
heapSize
.
bytes
(
)
>
=
tunables
.
gcMaxBytes
(
)
)
)
{
return
nullptr
;
}
Arena
*
arena
=
chunk
-
>
allocateArena
(
this
zone
thingKind
lock
)
;
zone
-
>
gcHeapSize
.
addGCArena
(
heapSize
)
;
if
(
checkThresholds
!
=
ShouldCheckThresholds
:
:
DontCheckThresholds
)
{
maybeTriggerGCAfterAlloc
(
zone
)
;
}
return
arena
;
}
Arena
*
TenuredChunk
:
:
allocateArena
(
GCRuntime
*
gc
Zone
*
zone
AllocKind
thingKind
const
AutoLockGC
&
lock
)
{
if
(
info
.
numArenasFreeCommitted
=
=
0
)
{
commitOnePage
(
gc
)
;
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
=
=
ArenasPerPage
)
;
}
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
>
0
)
;
Arena
*
arena
=
fetchNextFreeArena
(
gc
)
;
arena
-
>
init
(
gc
zone
thingKind
lock
)
;
updateChunkListAfterAlloc
(
gc
lock
)
;
verify
(
)
;
return
arena
;
}
template
<
size_t
N
>
static
inline
size_t
FindFirstBitSet
(
const
mozilla
:
:
BitSet
<
N
uint32_t
>
&
bitset
)
{
MOZ_ASSERT
(
!
bitset
.
IsEmpty
(
)
)
;
const
auto
&
words
=
bitset
.
Storage
(
)
;
for
(
size_t
i
=
0
;
i
<
words
.
Length
(
)
;
i
+
+
)
{
uint32_t
word
=
words
[
i
]
;
if
(
word
)
{
return
i
*
32
+
mozilla
:
:
CountTrailingZeroes32
(
word
)
;
}
}
MOZ_CRASH
(
"
No
bits
found
"
)
;
}
void
TenuredChunk
:
:
commitOnePage
(
GCRuntime
*
gc
)
{
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
=
=
0
)
;
MOZ_ASSERT
(
info
.
numArenasFree
>
=
ArenasPerPage
)
;
uint32_t
pageIndex
=
FindFirstBitSet
(
decommittedPages
)
;
MOZ_ASSERT
(
decommittedPages
[
pageIndex
]
)
;
if
(
DecommitEnabled
(
)
)
{
MarkPagesInUseSoft
(
pageAddress
(
pageIndex
)
PageSize
)
;
}
decommittedPages
[
pageIndex
]
=
false
;
for
(
size_t
i
=
0
;
i
<
ArenasPerPage
;
i
+
+
)
{
size_t
arenaIndex
=
pageIndex
*
ArenasPerPage
+
i
;
MOZ_ASSERT
(
!
freeCommittedArenas
[
arenaIndex
]
)
;
freeCommittedArenas
[
arenaIndex
]
=
true
;
arenas
[
arenaIndex
]
.
setAsNotAllocated
(
)
;
+
+
info
.
numArenasFreeCommitted
;
}
verify
(
)
;
}
Arena
*
TenuredChunk
:
:
fetchNextFreeArena
(
GCRuntime
*
gc
)
{
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
>
0
)
;
MOZ_ASSERT
(
info
.
numArenasFreeCommitted
<
=
info
.
numArenasFree
)
;
size_t
index
=
FindFirstBitSet
(
freeCommittedArenas
)
;
MOZ_ASSERT
(
freeCommittedArenas
[
index
]
)
;
freeCommittedArenas
[
index
]
=
false
;
-
-
info
.
numArenasFreeCommitted
;
-
-
info
.
numArenasFree
;
return
&
arenas
[
index
]
;
}
TenuredChunk
*
GCRuntime
:
:
getOrAllocChunk
(
AutoLockGCBgAlloc
&
lock
)
{
TenuredChunk
*
chunk
=
emptyChunks
(
lock
)
.
pop
(
)
;
if
(
chunk
)
{
SetMemCheckKind
(
chunk
sizeof
(
ChunkBase
)
MemCheckKind
:
:
MakeUndefined
)
;
chunk
-
>
initBaseForTenuredChunk
(
rt
)
;
MOZ_ASSERT
(
chunk
-
>
unused
(
)
)
;
}
else
{
void
*
ptr
=
TenuredChunk
:
:
allocate
(
this
)
;
if
(
!
ptr
)
{
return
nullptr
;
}
chunk
=
TenuredChunk
:
:
emplace
(
ptr
this
true
)
;
MOZ_ASSERT
(
chunk
-
>
info
.
numArenasFreeCommitted
=
=
0
)
;
}
if
(
wantBackgroundAllocation
(
lock
)
)
{
lock
.
tryToStartBackgroundAllocation
(
)
;
}
return
chunk
;
}
void
GCRuntime
:
:
recycleChunk
(
TenuredChunk
*
chunk
const
AutoLockGC
&
lock
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
chunk
-
>
unused
(
)
)
;
chunk
-
>
verify
(
)
;
#
endif
AlwaysPoison
(
chunk
JS_FREED_CHUNK_PATTERN
sizeof
(
ChunkBase
)
MemCheckKind
:
:
MakeNoAccess
)
;
emptyChunks
(
lock
)
.
push
(
chunk
)
;
}
TenuredChunk
*
GCRuntime
:
:
pickChunk
(
AutoLockGCBgAlloc
&
lock
)
{
if
(
availableChunks
(
lock
)
.
count
(
)
)
{
return
availableChunks
(
lock
)
.
head
(
)
;
}
TenuredChunk
*
chunk
=
getOrAllocChunk
(
lock
)
;
if
(
!
chunk
)
{
return
nullptr
;
}
#
ifdef
DEBUG
chunk
-
>
verify
(
)
;
MOZ_ASSERT
(
chunk
-
>
unused
(
)
)
;
MOZ_ASSERT
(
!
fullChunks
(
lock
)
.
contains
(
chunk
)
)
;
MOZ_ASSERT
(
!
availableChunks
(
lock
)
.
contains
(
chunk
)
)
;
#
endif
availableChunks
(
lock
)
.
push
(
chunk
)
;
return
chunk
;
}
BackgroundAllocTask
:
:
BackgroundAllocTask
(
GCRuntime
*
gc
ChunkPool
&
pool
)
:
GCParallelTask
(
gc
gcstats
:
:
PhaseKind
:
:
NONE
)
chunkPool_
(
pool
)
enabled_
(
CanUseExtraThreads
(
)
&
&
GetCPUCount
(
)
>
=
2
)
{
}
void
BackgroundAllocTask
:
:
run
(
AutoLockHelperThreadState
&
lock
)
{
AutoUnlockHelperThreadState
unlock
(
lock
)
;
AutoLockGC
gcLock
(
gc
)
;
while
(
!
isCancelled
(
)
&
&
gc
-
>
wantBackgroundAllocation
(
gcLock
)
)
{
TenuredChunk
*
chunk
;
{
AutoUnlockGC
unlock
(
gcLock
)
;
void
*
ptr
=
TenuredChunk
:
:
allocate
(
gc
)
;
if
(
!
ptr
)
{
break
;
}
chunk
=
TenuredChunk
:
:
emplace
(
ptr
gc
true
)
;
}
chunkPool_
.
ref
(
)
.
push
(
chunk
)
;
}
}
void
*
TenuredChunk
:
:
allocate
(
GCRuntime
*
gc
)
{
void
*
chunk
=
MapAlignedPages
(
ChunkSize
ChunkSize
)
;
if
(
!
chunk
)
{
return
nullptr
;
}
gc
-
>
stats
(
)
.
count
(
gcstats
:
:
COUNT_NEW_CHUNK
)
;
return
chunk
;
}
static
inline
bool
ShouldDecommitNewChunk
(
bool
allMemoryCommitted
const
GCSchedulingState
&
state
)
{
if
(
!
DecommitEnabled
(
)
)
{
return
false
;
}
return
!
allMemoryCommitted
|
|
!
state
.
inHighFrequencyGCMode
(
)
;
}
TenuredChunk
*
TenuredChunk
:
:
emplace
(
void
*
ptr
GCRuntime
*
gc
bool
allMemoryCommitted
)
{
MOZ_MAKE_MEM_UNDEFINED
(
ptr
ChunkSize
)
;
Poison
(
ptr
JS_FRESH_TENURED_PATTERN
ChunkSize
MemCheckKind
:
:
MakeUndefined
)
;
TenuredChunk
*
chunk
=
new
(
mozilla
:
:
KnownNotNull
ptr
)
TenuredChunk
(
gc
-
>
rt
)
;
if
(
ShouldDecommitNewChunk
(
allMemoryCommitted
gc
-
>
schedulingState
)
)
{
chunk
-
>
decommitAllArenas
(
)
;
}
else
{
chunk
-
>
initAsDecommitted
(
)
;
}
chunk
-
>
verify
(
)
;
return
chunk
;
}
void
TenuredChunk
:
:
decommitAllArenas
(
)
{
MOZ_ASSERT
(
unused
(
)
)
;
MarkPagesUnusedSoft
(
&
arenas
[
0
]
ArenasPerChunk
*
ArenaSize
)
;
initAsDecommitted
(
)
;
}
void
TenuredChunkBase
:
:
initAsDecommitted
(
)
{
decommittedPages
.
SetAll
(
)
;
freeCommittedArenas
.
ResetAll
(
)
;
info
.
numArenasFree
=
ArenasPerChunk
;
info
.
numArenasFreeCommitted
=
0
;
}
