#
include
"
gc
/
BufferAllocator
-
inl
.
h
"
#
include
"
mozilla
/
PodOperations
.
h
"
#
include
"
mozilla
/
ScopeExit
.
h
"
#
ifdef
XP_DARWIN
#
include
<
mach
/
mach_init
.
h
>
#
include
<
mach
/
vm_map
.
h
>
#
endif
#
include
"
gc
/
BufferAllocatorInternals
.
h
"
#
include
"
gc
/
GCInternals
.
h
"
#
include
"
gc
/
GCLock
.
h
"
#
include
"
gc
/
PublicIterators
.
h
"
#
include
"
gc
/
Zone
.
h
"
#
include
"
js
/
HeapAPI
.
h
"
#
include
"
util
/
Poison
.
h
"
#
include
"
gc
/
Heap
-
inl
.
h
"
#
include
"
gc
/
Marking
-
inl
.
h
"
using
namespace
js
;
using
namespace
js
:
:
gc
;
namespace
js
:
:
gc
{
static
constexpr
size_t
MinFreeRegionSize
=
1
<
<
BufferAllocator
:
:
MinSizeClassShift
;
static
constexpr
size_t
SmallRegionShift
=
14
;
static
constexpr
size_t
SmallRegionSize
=
1
<
<
SmallRegionShift
;
static
constexpr
uintptr_t
SmallRegionMask
=
SmallRegionSize
-
1
;
static_assert
(
SmallRegionSize
>
=
MinMediumAllocSize
)
;
static_assert
(
SmallRegionSize
<
=
MaxMediumAllocSize
)
;
struct
alignas
(
CellAlignBytes
)
LargeBuffer
:
public
SlimLinkedListElement
<
LargeBuffer
>
{
void
*
alloc
;
size_t
bytes
;
bool
isNurseryOwned
;
bool
allocatedDuringCollection
=
false
;
#
ifdef
DEBUG
uint32_t
checkValue
=
LargeBufferCheckValue
;
#
endif
LargeBuffer
(
void
*
alloc
size_t
bytes
bool
nurseryOwned
)
:
alloc
(
alloc
)
bytes
(
bytes
)
isNurseryOwned
(
nurseryOwned
)
{
MOZ_ASSERT
(
(
bytes
%
ChunkSize
)
=
=
0
)
;
}
inline
void
check
(
)
const
;
#
ifdef
DEBUG
inline
Zone
*
zone
(
)
;
inline
Zone
*
zoneFromAnyThread
(
)
;
#
endif
void
*
data
(
)
{
return
alloc
;
}
size_t
allocBytes
(
)
const
{
return
bytes
;
}
bool
isPointerWithinAllocation
(
void
*
ptr
)
const
;
}
;
inline
void
LargeBuffer
:
:
check
(
)
const
{
MOZ_ASSERT
(
checkValue
=
=
LargeBufferCheckValue
)
;
}
BufferAllocator
:
:
AutoLock
:
:
AutoLock
(
GCRuntime
*
gc
)
:
LockGuard
(
gc
-
>
bufferAllocatorLock
)
{
}
BufferAllocator
:
:
AutoLock
:
:
AutoLock
(
BufferAllocator
*
allocator
)
:
LockGuard
(
allocator
-
>
lock
(
)
)
{
}
struct
BufferAllocator
:
:
FreeRegion
:
public
SlimLinkedListElement
<
BufferAllocator
:
:
FreeRegion
>
{
uintptr_t
startAddr
;
bool
hasDecommittedPages
;
#
ifdef
DEBUG
uint32_t
checkValue
=
FreeRegionCheckValue
;
#
endif
explicit
FreeRegion
(
uintptr_t
startAddr
bool
decommitted
=
false
)
:
startAddr
(
startAddr
)
hasDecommittedPages
(
decommitted
)
{
}
static
FreeRegion
*
fromEndOffset
(
BufferChunk
*
chunk
uintptr_t
endOffset
)
{
MOZ_ASSERT
(
endOffset
<
=
ChunkSize
)
;
return
fromEndAddr
(
uintptr_t
(
chunk
)
+
endOffset
)
;
}
static
FreeRegion
*
fromEndOffset
(
SmallBufferRegion
*
region
uintptr_t
endOffset
)
{
MOZ_ASSERT
(
endOffset
<
=
SmallRegionSize
)
;
return
fromEndAddr
(
uintptr_t
(
region
)
+
endOffset
)
;
}
static
FreeRegion
*
fromEndAddr
(
uintptr_t
endAddr
)
{
MOZ_ASSERT
(
endAddr
%
SmallAllocGranularity
=
=
0
)
;
auto
*
region
=
reinterpret_cast
<
FreeRegion
*
>
(
endAddr
-
sizeof
(
FreeRegion
)
)
;
region
-
>
check
(
)
;
return
region
;
}
void
check
(
)
const
{
MOZ_ASSERT
(
checkValue
=
=
FreeRegionCheckValue
)
;
}
uintptr_t
getEnd
(
)
const
{
return
uintptr_t
(
this
+
1
)
;
}
size_t
size
(
)
const
{
return
getEnd
(
)
-
startAddr
;
}
}
;
template
<
size_t
N
>
class
BitSetIter
{
const
mozilla
:
:
BitSet
<
N
>
&
bitset
;
size_t
bit
=
0
;
public
:
explicit
BitSetIter
(
const
mozilla
:
:
BitSet
<
N
>
&
bitset
)
:
bitset
(
bitset
)
{
MOZ_ASSERT
(
!
done
(
)
)
;
if
(
!
bitset
[
bit
]
)
{
next
(
)
;
}
}
bool
done
(
)
const
{
MOZ_ASSERT
(
bit
<
=
N
|
|
bit
=
=
SIZE_MAX
)
;
return
bit
>
=
N
;
}
void
next
(
)
{
MOZ_ASSERT
(
!
done
(
)
)
;
bit
+
+
;
if
(
bit
!
=
N
)
{
bit
=
bitset
.
FindNext
(
bit
)
;
}
}
size_t
get
(
)
const
{
MOZ_ASSERT
(
!
done
(
)
)
;
return
bit
;
}
operator
size_t
(
)
const
{
return
get
(
)
;
}
}
;
template
<
typename
BitmapIter
size_t
Granularity
typename
T
=
void
>
class
BitmapToBlockIter
:
public
BitmapIter
{
uintptr_t
baseAddr
;
public
:
template
<
typename
S
>
BitmapToBlockIter
(
void
*
base
S
&
&
arg
)
:
BitmapIter
(
std
:
:
forward
<
S
>
(
arg
)
)
baseAddr
(
uintptr_t
(
base
)
)
{
}
size_t
getOffset
(
)
const
{
return
BitmapIter
:
:
get
(
)
*
Granularity
;
}
T
*
get
(
)
const
{
return
reinterpret_cast
<
T
*
>
(
baseAddr
+
getOffset
(
)
)
;
}
operator
T
*
(
)
const
{
return
get
(
)
;
}
}
;
struct
BufferChunk
:
public
ChunkBase
public
SlimLinkedListElement
<
BufferChunk
>
{
#
ifdef
DEBUG
MainThreadOrGCTaskData
<
Zone
*
>
zone
;
#
endif
MainThreadOrGCTaskData
<
bool
>
allocatedDuringCollection
;
MainThreadData
<
bool
>
hasNurseryOwnedAllocs
;
MainThreadOrGCTaskData
<
bool
>
hasNurseryOwnedAllocsAfterSweep
;
static
constexpr
size_t
MaxAllocsPerChunk
=
ChunkSize
/
MediumAllocGranularity
;
using
EncodedSizeArray
=
MediumBufferSize
[
MaxAllocsPerChunk
]
;
EncodedSizeArray
encodedSizeArray
;
MainThreadOrGCTaskData
<
AtomicBitmap
<
MaxAllocsPerChunk
>
>
markBits
;
using
PerAllocBitmap
=
mozilla
:
:
BitSet
<
MaxAllocsPerChunk
>
;
MainThreadOrGCTaskData
<
PerAllocBitmap
>
allocBitmap
;
MainThreadOrGCTaskData
<
PerAllocBitmap
>
nurseryOwnedBitmap
;
static
constexpr
size_t
PagesPerChunk
=
ChunkSize
/
PageSize
;
using
PerPageBitmap
=
mozilla
:
:
BitSet
<
PagesPerChunk
uint32_t
>
;
MainThreadOrGCTaskData
<
PerPageBitmap
>
decommittedPages
;
static
constexpr
size_t
SmallRegionsPerChunk
=
ChunkSize
/
SmallRegionSize
;
using
SmallRegionBitmap
=
AtomicBitmap
<
SmallRegionsPerChunk
>
;
MainThreadOrGCTaskData
<
SmallRegionBitmap
>
smallRegionBitmap
;
using
AllocIter
=
BitmapToBlockIter
<
BitSetIter
<
MaxAllocsPerChunk
>
MediumAllocGranularity
>
;
AllocIter
allocIter
(
)
{
return
{
this
allocBitmap
.
ref
(
)
}
;
}
using
SmallRegionIter
=
BitmapToBlockIter
<
SmallRegionBitmap
:
:
Iter
SmallRegionSize
SmallBufferRegion
>
;
SmallRegionIter
smallRegionIter
(
)
{
return
{
this
smallRegionBitmap
.
ref
(
)
}
;
}
static
BufferChunk
*
from
(
void
*
alloc
)
{
ChunkBase
*
chunk
=
js
:
:
gc
:
:
detail
:
:
GetGCAddressChunkBase
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
kind
=
=
ChunkKind
:
:
Buffers
)
;
return
static_cast
<
BufferChunk
*
>
(
chunk
)
;
}
explicit
BufferChunk
(
Zone
*
zone
)
;
~
BufferChunk
(
)
;
void
setAllocated
(
void
*
alloc
bool
allocated
)
;
bool
isAllocated
(
const
void
*
alloc
)
const
;
bool
isAllocated
(
uintptr_t
offset
)
const
;
void
setNurseryOwned
(
void
*
alloc
bool
nurseryOwned
)
;
bool
isNurseryOwned
(
const
void
*
alloc
)
const
;
void
setSmallBufferRegion
(
void
*
alloc
bool
smallAlloc
)
;
bool
isSmallBufferRegion
(
const
void
*
alloc
)
const
;
void
setAllocBytes
(
void
*
alloc
size_t
bytes
)
;
size_t
allocBytes
(
const
void
*
alloc
)
const
;
bool
setMarked
(
void
*
alloc
)
;
void
setUnmarked
(
void
*
alloc
)
;
bool
isMarked
(
const
void
*
alloc
)
const
;
size_t
findNextAllocated
(
uintptr_t
offset
)
const
;
size_t
findPrevAllocated
(
uintptr_t
offset
)
const
;
using
FreeRegion
=
BufferAllocator
:
:
FreeRegion
;
FreeRegion
*
findFollowingFreeRegion
(
uintptr_t
startAddr
)
;
FreeRegion
*
findPrecedingFreeRegion
(
uintptr_t
endAddr
)
;
bool
isPointerWithinAllocation
(
void
*
ptr
)
const
;
private
:
template
<
size_t
Divisor
=
MinMediumAllocSize
size_t
Align
=
Divisor
>
size_t
ptrToIndex
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
(
uintptr_t
(
alloc
)
&
~
ChunkMask
)
=
=
uintptr_t
(
this
)
)
;
uintptr_t
offset
=
uintptr_t
(
alloc
)
&
ChunkMask
;
return
offsetToIndex
<
Divisor
Align
>
(
offset
)
;
}
template
<
size_t
Divisor
=
MinMediumAllocSize
size_t
Align
=
Divisor
>
static
size_t
offsetToIndex
(
uintptr_t
offset
)
{
MOZ_ASSERT
(
isValidOffset
(
offset
)
)
;
MOZ_ASSERT
(
offset
%
Align
=
=
0
)
;
return
offset
/
Divisor
;
}
const
void
*
ptrFromOffset
(
uintptr_t
offset
)
const
{
MOZ_ASSERT
(
isValidOffset
(
offset
)
)
;
MOZ_ASSERT
(
offset
%
MediumAllocGranularity
=
=
0
)
;
return
reinterpret_cast
<
void
*
>
(
uintptr_t
(
this
)
+
offset
)
;
}
#
ifdef
DEBUG
static
bool
isValidOffset
(
uintptr_t
offset
)
;
#
endif
}
;
constexpr
size_t
FirstMediumAllocOffset
=
RoundUp
(
sizeof
(
BufferChunk
)
MediumAllocGranularity
)
;
#
ifdef
DEBUG
bool
BufferChunk
:
:
isValidOffset
(
uintptr_t
offset
)
{
return
offset
>
=
FirstMediumAllocOffset
&
&
offset
<
ChunkSize
;
}
#
endif
struct
SmallBufferRegion
{
static
constexpr
size_t
MaxAllocsPerRegion
=
SmallRegionSize
/
SmallAllocGranularity
;
using
EncodedSizeArray
=
SmallBufferSize
[
MaxAllocsPerRegion
]
;
EncodedSizeArray
encodedSizeArray
;
MainThreadOrGCTaskData
<
AtomicBitmap
<
MaxAllocsPerRegion
>
>
markBits
;
using
PerAllocBitmap
=
mozilla
:
:
BitSet
<
MaxAllocsPerRegion
>
;
MainThreadOrGCTaskData
<
PerAllocBitmap
>
allocBitmap
;
MainThreadOrGCTaskData
<
PerAllocBitmap
>
nurseryOwnedBitmap
;
MainThreadOrGCTaskData
<
bool
>
hasNurseryOwnedAllocs_
;
using
AllocIter
=
BitmapToBlockIter
<
BitSetIter
<
MaxAllocsPerRegion
>
SmallAllocGranularity
>
;
AllocIter
allocIter
(
)
{
return
{
this
allocBitmap
.
ref
(
)
}
;
}
static
SmallBufferRegion
*
from
(
void
*
alloc
)
{
uintptr_t
addr
=
uintptr_t
(
alloc
)
&
~
SmallRegionMask
;
auto
*
region
=
reinterpret_cast
<
SmallBufferRegion
*
>
(
addr
)
;
#
ifdef
DEBUG
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
region
)
;
MOZ_ASSERT
(
chunk
-
>
isAllocated
(
region
)
)
;
MOZ_ASSERT
(
chunk
-
>
isSmallBufferRegion
(
region
)
)
;
#
endif
return
region
;
}
SmallBufferRegion
(
)
;
const
void
*
ptrFromOffset
(
uintptr_t
offset
)
const
;
void
setAllocated
(
void
*
alloc
bool
allocated
)
;
bool
isAllocated
(
const
void
*
alloc
)
const
;
bool
isAllocated
(
uintptr_t
offset
)
const
;
void
setNurseryOwned
(
void
*
alloc
bool
nurseryOwned
)
;
bool
isNurseryOwned
(
const
void
*
alloc
)
const
;
void
setHasNurseryOwnedAllocs
(
bool
value
)
;
bool
hasNurseryOwnedAllocs
(
)
const
;
void
setAllocBytes
(
void
*
alloc
size_t
bytes
)
;
size_t
allocBytes
(
const
void
*
alloc
)
const
;
bool
setMarked
(
void
*
alloc
)
;
void
setUnmarked
(
void
*
alloc
)
;
bool
isMarked
(
const
void
*
alloc
)
const
;
size_t
findNextAllocated
(
uintptr_t
offset
)
const
;
size_t
findPrevAllocated
(
uintptr_t
offset
)
const
;
bool
isPointerWithinAllocation
(
void
*
ptr
)
const
;
private
:
size_t
ptrToIndex
(
const
void
*
alloc
)
const
;
size_t
offsetToIndex
(
uintptr_t
offset
)
const
;
}
;
static
constexpr
size_t
FirstSmallAllocOffset
=
RoundUp
(
sizeof
(
SmallBufferRegion
)
SmallAllocGranularity
)
;
static_assert
(
FirstSmallAllocOffset
<
SmallRegionSize
)
;
static
void
CheckHighBitsOfPointer
(
void
*
ptr
)
{
#
ifdef
JS_64BIT
MOZ_DIAGNOSTIC_ASSERT
(
(
uintptr_t
(
ptr
)
>
>
47
)
=
=
0
)
;
#
endif
}
BufferAllocator
:
:
FreeLists
:
:
FreeLists
(
FreeLists
&
&
other
)
{
MOZ_ASSERT
(
this
!
=
&
other
)
;
assertEmpty
(
)
;
std
:
:
swap
(
lists
other
.
lists
)
;
std
:
:
swap
(
available
other
.
available
)
;
other
.
assertEmpty
(
)
;
}
BufferAllocator
:
:
FreeLists
&
BufferAllocator
:
:
FreeLists
:
:
operator
=
(
FreeLists
&
&
other
)
{
MOZ_ASSERT
(
this
!
=
&
other
)
;
assertEmpty
(
)
;
std
:
:
swap
(
lists
other
.
lists
)
;
std
:
:
swap
(
available
other
.
available
)
;
other
.
assertEmpty
(
)
;
return
*
this
;
}
size_t
BufferAllocator
:
:
FreeLists
:
:
getFirstAvailableSizeClass
(
size_t
minSizeClass
size_t
maxSizeClass
)
const
{
MOZ_ASSERT
(
maxSizeClass
<
=
MaxMediumAllocClass
)
;
size_t
result
=
available
.
FindNext
(
minSizeClass
)
;
MOZ_ASSERT
(
result
>
=
minSizeClass
)
;
MOZ_ASSERT_IF
(
result
!
=
SIZE_MAX
!
lists
[
result
]
.
isEmpty
(
)
)
;
if
(
result
>
maxSizeClass
)
{
return
SIZE_MAX
;
}
return
result
;
}
BufferAllocator
:
:
FreeRegion
*
BufferAllocator
:
:
FreeLists
:
:
getFirstRegion
(
size_t
sizeClass
)
{
MOZ_ASSERT
(
!
lists
[
sizeClass
]
.
isEmpty
(
)
)
;
return
lists
[
sizeClass
]
.
getFirst
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
pushFront
(
size_t
sizeClass
FreeRegion
*
region
)
{
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
lists
[
sizeClass
]
.
pushFront
(
region
)
;
available
[
sizeClass
]
=
true
;
}
void
BufferAllocator
:
:
FreeLists
:
:
pushBack
(
size_t
sizeClass
FreeRegion
*
region
)
{
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
lists
[
sizeClass
]
.
pushBack
(
region
)
;
available
[
sizeClass
]
=
true
;
}
void
BufferAllocator
:
:
FreeLists
:
:
append
(
FreeLists
&
&
other
)
{
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
if
(
!
other
.
lists
[
i
]
.
isEmpty
(
)
)
{
lists
[
i
]
.
append
(
std
:
:
move
(
other
.
lists
[
i
]
)
)
;
available
[
i
]
=
true
;
}
}
other
.
available
.
ResetAll
(
)
;
other
.
assertEmpty
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
prepend
(
FreeLists
&
&
other
)
{
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
if
(
!
other
.
lists
[
i
]
.
isEmpty
(
)
)
{
lists
[
i
]
.
prepend
(
std
:
:
move
(
other
.
lists
[
i
]
)
)
;
available
[
i
]
=
true
;
}
}
other
.
available
.
ResetAll
(
)
;
other
.
assertEmpty
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
remove
(
size_t
sizeClass
FreeRegion
*
region
)
{
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
lists
[
sizeClass
]
.
remove
(
region
)
;
available
[
sizeClass
]
=
!
lists
[
sizeClass
]
.
isEmpty
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
clear
(
)
{
for
(
auto
&
freeList
:
lists
)
{
new
(
&
freeList
)
FreeList
;
}
available
.
ResetAll
(
)
;
}
template
<
typename
Pred
>
void
BufferAllocator
:
:
FreeLists
:
:
eraseIf
(
Pred
&
&
pred
)
{
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
FreeList
&
freeList
=
lists
[
i
]
;
freeList
.
eraseIf
(
std
:
:
forward
<
Pred
>
(
pred
)
)
;
available
[
i
]
=
!
freeList
.
isEmpty
(
)
;
}
}
inline
void
BufferAllocator
:
:
FreeLists
:
:
assertEmpty
(
)
const
{
#
ifdef
DEBUG
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
MOZ_ASSERT
(
lists
[
i
]
.
isEmpty
(
)
)
;
}
MOZ_ASSERT
(
available
.
IsEmpty
(
)
)
;
#
endif
}
inline
void
BufferAllocator
:
:
FreeLists
:
:
assertContains
(
size_t
sizeClass
FreeRegion
*
region
)
const
{
#
ifdef
DEBUG
MOZ_ASSERT
(
available
[
sizeClass
]
)
;
MOZ_ASSERT
(
lists
[
sizeClass
]
.
contains
(
region
)
)
;
#
endif
}
inline
void
BufferAllocator
:
:
FreeLists
:
:
checkAvailable
(
)
const
{
#
ifdef
DEBUG
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
MOZ_ASSERT
(
available
[
i
]
=
=
!
lists
[
i
]
.
isEmpty
(
)
)
;
}
#
endif
}
}
MOZ_ALWAYS_INLINE
void
PoisonAlloc
(
void
*
alloc
uint8_t
value
size_t
bytes
MemCheckKind
kind
)
{
#
ifndef
EARLY_BETA_OR_EARLIER
bytes
=
std
:
:
min
(
bytes
size_t
(
256
)
)
;
#
endif
AlwaysPoison
(
alloc
value
bytes
kind
)
;
}
BufferChunk
:
:
BufferChunk
(
Zone
*
zone
)
:
ChunkBase
(
zone
-
>
runtimeFromMainThread
(
)
ChunkKind
:
:
Buffers
)
{
mozilla
:
:
PodArrayZero
(
encodedSizeArray
)
;
#
ifdef
DEBUG
this
-
>
zone
=
zone
;
MOZ_ASSERT
(
allocBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
nurseryOwnedBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
decommittedPages
.
ref
(
)
.
IsEmpty
(
)
)
;
for
(
const
auto
&
encodedSize
:
encodedSizeArray
)
{
MOZ_ASSERT
(
encodedSize
.
get
(
)
=
=
0
)
;
}
#
endif
}
BufferChunk
:
:
~
BufferChunk
(
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
allocBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
nurseryOwnedBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
for
(
const
auto
&
encodedSize
:
encodedSizeArray
)
{
MOZ_ASSERT
(
encodedSize
.
get
(
)
=
=
0
)
;
}
#
endif
}
void
BufferChunk
:
:
setAllocated
(
void
*
alloc
bool
allocated
)
{
size_t
bit
=
ptrToIndex
(
alloc
)
;
MOZ_ASSERT
(
allocBitmap
.
ref
(
)
[
bit
]
!
=
allocated
)
;
allocBitmap
.
ref
(
)
[
bit
]
=
allocated
;
}
bool
BufferChunk
:
:
isAllocated
(
const
void
*
alloc
)
const
{
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
allocBitmap
.
ref
(
)
[
bit
]
;
}
bool
BufferChunk
:
:
isAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
return
allocBitmap
.
ref
(
)
[
bit
]
;
}
size_t
BufferChunk
:
:
findNextAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
size_t
next
=
allocBitmap
.
ref
(
)
.
FindNext
(
bit
)
;
if
(
next
=
=
SIZE_MAX
)
{
return
ChunkSize
;
}
return
next
*
MediumAllocGranularity
;
}
size_t
BufferChunk
:
:
findPrevAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
size_t
prev
=
allocBitmap
.
ref
(
)
.
FindPrev
(
bit
)
;
if
(
prev
=
=
SIZE_MAX
)
{
return
ChunkSize
;
}
return
prev
*
MediumAllocGranularity
;
}
void
BufferChunk
:
:
setNurseryOwned
(
void
*
alloc
bool
nurseryOwned
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
nurseryOwnedBitmap
.
ref
(
)
[
bit
]
=
nurseryOwned
;
}
bool
BufferChunk
:
:
isNurseryOwned
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
nurseryOwnedBitmap
.
ref
(
)
[
bit
]
;
}
void
BufferChunk
:
:
setSmallBufferRegion
(
void
*
alloc
bool
smallAlloc
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
<
SmallRegionSize
SmallRegionSize
>
(
alloc
)
;
smallRegionBitmap
.
ref
(
)
.
setBit
(
bit
smallAlloc
)
;
}
bool
BufferChunk
:
:
isSmallBufferRegion
(
const
void
*
alloc
)
const
{
size_t
bit
=
ptrToIndex
<
SmallRegionSize
SmallAllocGranularity
>
(
alloc
)
;
return
smallRegionBitmap
.
ref
(
)
.
getBit
(
bit
)
;
}
void
BufferChunk
:
:
setAllocBytes
(
void
*
alloc
size_t
bytes
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
index
=
ptrToIndex
(
alloc
)
;
MOZ_ASSERT
(
index
<
std
:
:
size
(
encodedSizeArray
)
)
;
encodedSizeArray
[
index
]
.
set
(
bytes
)
;
}
size_t
BufferChunk
:
:
allocBytes
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
index
=
ptrToIndex
(
alloc
)
;
MOZ_ASSERT
(
index
<
std
:
:
size
(
encodedSizeArray
)
)
;
return
encodedSizeArray
[
index
]
.
get
(
)
;
}
bool
BufferChunk
:
:
setMarked
(
void
*
alloc
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
if
(
markBits
.
ref
(
)
.
getBit
(
bit
)
)
{
return
false
;
}
markBits
.
ref
(
)
.
setBit
(
bit
true
)
;
return
true
;
}
void
BufferChunk
:
:
setUnmarked
(
void
*
alloc
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
markBits
.
ref
(
)
.
setBit
(
bit
false
)
;
}
bool
BufferChunk
:
:
isMarked
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
markBits
.
ref
(
)
.
getBit
(
bit
)
;
}
SmallBufferRegion
:
:
SmallBufferRegion
(
)
{
mozilla
:
:
PodArrayZero
(
encodedSizeArray
)
;
#
ifdef
DEBUG
MOZ_ASSERT
(
allocBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
nurseryOwnedBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
for
(
const
auto
&
encodedSize
:
encodedSizeArray
)
{
MOZ_ASSERT
(
encodedSize
.
get
(
)
=
=
0
)
;
}
#
endif
}
uintptr_t
SmallBufferRegion
:
:
ptrToIndex
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
(
uintptr_t
(
alloc
)
&
~
SmallRegionMask
)
=
=
uintptr_t
(
this
)
)
;
uintptr_t
offset
=
uintptr_t
(
alloc
)
&
SmallRegionMask
;
return
offsetToIndex
(
offset
)
;
}
uintptr_t
SmallBufferRegion
:
:
offsetToIndex
(
uintptr_t
offset
)
const
{
MOZ_ASSERT
(
offset
>
=
FirstSmallAllocOffset
)
;
MOZ_ASSERT
(
offset
<
SmallRegionSize
)
;
MOZ_ASSERT
(
offset
%
SmallAllocGranularity
=
=
0
)
;
return
offset
/
SmallAllocGranularity
;
}
const
void
*
SmallBufferRegion
:
:
ptrFromOffset
(
uintptr_t
offset
)
const
{
MOZ_ASSERT
(
offset
>
=
FirstSmallAllocOffset
)
;
MOZ_ASSERT
(
offset
<
SmallRegionSize
)
;
MOZ_ASSERT
(
offset
%
SmallAllocGranularity
=
=
0
)
;
return
reinterpret_cast
<
void
*
>
(
uintptr_t
(
this
)
+
offset
)
;
}
void
SmallBufferRegion
:
:
setAllocated
(
void
*
alloc
bool
allocated
)
{
size_t
bit
=
ptrToIndex
(
alloc
)
;
MOZ_ASSERT
(
allocBitmap
.
ref
(
)
[
bit
]
!
=
allocated
)
;
allocBitmap
.
ref
(
)
[
bit
]
=
allocated
;
}
bool
SmallBufferRegion
:
:
isAllocated
(
const
void
*
alloc
)
const
{
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
allocBitmap
.
ref
(
)
[
bit
]
;
}
bool
SmallBufferRegion
:
:
isAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
return
allocBitmap
.
ref
(
)
[
bit
]
;
}
void
SmallBufferRegion
:
:
setNurseryOwned
(
void
*
alloc
bool
nurseryOwned
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
nurseryOwnedBitmap
.
ref
(
)
[
bit
]
=
nurseryOwned
;
}
bool
SmallBufferRegion
:
:
isNurseryOwned
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
nurseryOwnedBitmap
.
ref
(
)
[
bit
]
;
}
void
SmallBufferRegion
:
:
setHasNurseryOwnedAllocs
(
bool
value
)
{
hasNurseryOwnedAllocs_
=
value
;
}
bool
SmallBufferRegion
:
:
hasNurseryOwnedAllocs
(
)
const
{
return
hasNurseryOwnedAllocs_
.
ref
(
)
;
}
void
SmallBufferRegion
:
:
setAllocBytes
(
void
*
alloc
size_t
bytes
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
index
=
ptrToIndex
(
alloc
)
;
MOZ_ASSERT
(
index
<
std
:
:
size
(
encodedSizeArray
)
)
;
encodedSizeArray
[
index
]
.
set
(
bytes
)
;
}
size_t
SmallBufferRegion
:
:
allocBytes
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
index
=
ptrToIndex
(
alloc
)
;
MOZ_ASSERT
(
index
<
std
:
:
size
(
encodedSizeArray
)
)
;
return
encodedSizeArray
[
index
]
.
get
(
)
;
}
bool
SmallBufferRegion
:
:
setMarked
(
void
*
alloc
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
if
(
markBits
.
ref
(
)
.
getBit
(
bit
)
)
{
return
false
;
}
markBits
.
ref
(
)
.
setBit
(
bit
true
)
;
return
true
;
}
void
SmallBufferRegion
:
:
setUnmarked
(
void
*
alloc
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
markBits
.
ref
(
)
.
setBit
(
bit
false
)
;
}
bool
SmallBufferRegion
:
:
isMarked
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
markBits
.
ref
(
)
.
getBit
(
bit
)
;
}
size_t
SmallBufferRegion
:
:
findNextAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
size_t
next
=
allocBitmap
.
ref
(
)
.
FindNext
(
bit
)
;
if
(
next
=
=
SIZE_MAX
)
{
return
SmallRegionSize
;
}
return
next
*
SmallAllocGranularity
;
}
size_t
SmallBufferRegion
:
:
findPrevAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
size_t
prev
=
allocBitmap
.
ref
(
)
.
FindPrev
(
bit
)
;
if
(
prev
=
=
SIZE_MAX
)
{
return
SmallRegionSize
;
}
return
prev
*
SmallAllocGranularity
;
}
BufferAllocator
:
:
BufferAllocator
(
Zone
*
zone
)
:
zone
(
zone
)
sweptMixedChunks
(
lock
(
)
)
sweptTenuredChunks
(
lock
(
)
)
sweptNurseryFreeLists
(
lock
(
)
)
sweptTenuredFreeLists
(
lock
(
)
)
sweptLargeTenuredAllocs
(
lock
(
)
)
minorState
(
State
:
:
NotCollecting
)
majorState
(
State
:
:
NotCollecting
)
minorSweepingFinished
(
lock
(
)
)
majorSweepingFinished
(
lock
(
)
)
{
MOZ_ASSERT
(
SmallBufferSize
(
MaxSmallAllocSize
)
.
get
(
)
<
MinMediumAllocSize
)
;
MOZ_ASSERT
(
MediumBufferSize
(
MaxMediumAllocSize
)
.
get
(
)
<
MinLargeAllocSize
)
;
MOZ_ASSERT
(
(
ChunkSize
-
MaxMediumAllocSize
)
>
=
sizeof
(
BufferChunk
)
)
;
}
BufferAllocator
:
:
~
BufferAllocator
(
)
{
#
ifdef
DEBUG
checkGCStateNotInUse
(
)
;
MOZ_ASSERT
(
mixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
tenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
freeLists
.
ref
(
)
.
assertEmpty
(
)
;
MOZ_ASSERT
(
largeNurseryAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
#
endif
}
bool
BufferAllocator
:
:
isEmpty
(
)
const
{
MOZ_ASSERT
(
!
zone
-
>
wasGCStarted
(
)
|
|
zone
-
>
isGCFinished
(
)
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
return
mixedChunks
.
ref
(
)
.
isEmpty
(
)
&
&
tenuredChunks
.
ref
(
)
.
isEmpty
(
)
&
&
largeNurseryAllocs
.
ref
(
)
.
isEmpty
(
)
&
&
largeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
;
}
Mutex
&
BufferAllocator
:
:
lock
(
)
const
{
return
zone
-
>
runtimeFromAnyThread
(
)
-
>
gc
.
bufferAllocatorLock
;
}
void
*
BufferAllocator
:
:
alloc
(
size_t
bytes
bool
nurseryOwned
)
{
MOZ_ASSERT_IF
(
zone
-
>
isGCMarkingOrSweeping
(
)
majorState
=
=
State
:
:
Marking
)
;
if
(
IsLargeAllocSize
(
bytes
)
)
{
return
allocLarge
(
bytes
nurseryOwned
false
)
;
}
if
(
IsSmallAllocSize
(
bytes
)
)
{
return
allocSmall
(
bytes
nurseryOwned
false
)
;
}
return
allocMedium
(
bytes
nurseryOwned
false
)
;
}
void
*
BufferAllocator
:
:
allocInGC
(
size_t
bytes
bool
nurseryOwned
)
{
MOZ_ASSERT
(
minorState
=
=
State
:
:
Marking
)
;
MOZ_ASSERT_IF
(
zone
-
>
isGCMarkingOrSweeping
(
)
majorState
=
=
State
:
:
Marking
)
;
void
*
result
;
if
(
IsLargeAllocSize
(
bytes
)
)
{
result
=
allocLarge
(
bytes
nurseryOwned
true
)
;
}
else
if
(
IsSmallAllocSize
(
bytes
)
)
{
result
=
allocSmall
(
bytes
nurseryOwned
true
)
;
}
else
{
result
=
allocMedium
(
bytes
nurseryOwned
true
)
;
}
if
(
!
result
)
{
return
nullptr
;
}
if
(
nurseryOwned
)
{
markNurseryOwnedAlloc
(
result
false
)
;
}
return
result
;
}
#
ifdef
DEBUG
inline
Zone
*
LargeBuffer
:
:
zone
(
)
{
Zone
*
zone
=
zoneFromAnyThread
(
)
;
MOZ_ASSERT
(
CurrentThreadCanAccessZone
(
zone
)
)
;
return
zone
;
}
inline
Zone
*
LargeBuffer
:
:
zoneFromAnyThread
(
)
{
return
BufferChunk
:
:
from
(
this
)
-
>
zone
;
}
#
endif
#
ifdef
XP_DARWIN
static
inline
void
VirtualCopyPages
(
void
*
dst
const
void
*
src
size_t
bytes
)
{
MOZ_ASSERT
(
(
uintptr_t
(
dst
)
&
PageMask
)
=
=
0
)
;
MOZ_ASSERT
(
(
uintptr_t
(
src
)
&
PageMask
)
=
=
0
)
;
MOZ_ASSERT
(
bytes
>
=
ChunkSize
)
;
kern_return_t
r
=
vm_copy
(
mach_task_self
(
)
vm_address_t
(
src
)
vm_size_t
(
bytes
)
vm_address_t
(
dst
)
)
;
if
(
r
!
=
KERN_SUCCESS
)
{
MOZ_CRASH
(
"
vm_copy
(
)
failed
"
)
;
}
}
#
endif
void
*
BufferAllocator
:
:
realloc
(
void
*
alloc
size_t
bytes
bool
nurseryOwned
)
{
if
(
!
alloc
)
{
return
this
-
>
alloc
(
bytes
nurseryOwned
)
;
}
MOZ_ASSERT
(
isNurseryOwned
(
alloc
)
=
=
nurseryOwned
)
;
MOZ_ASSERT_IF
(
zone
-
>
isGCMarkingOrSweeping
(
)
majorState
=
=
State
:
:
Marking
)
;
bytes
=
GetGoodAllocSize
(
bytes
)
;
size_t
currentBytes
;
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
currentBytes
=
buffer
-
>
allocBytes
(
)
;
if
(
bytes
<
buffer
-
>
allocBytes
(
)
&
&
IsLargeAllocSize
(
bytes
)
)
{
if
(
shrinkLarge
(
buffer
bytes
)
)
{
return
alloc
;
}
}
}
else
if
(
IsMediumAlloc
(
alloc
)
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
)
;
currentBytes
=
chunk
-
>
allocBytes
(
alloc
)
;
if
(
bytes
<
currentBytes
&
&
!
IsSmallAllocSize
(
bytes
)
)
{
if
(
shrinkMedium
(
alloc
bytes
)
)
{
return
alloc
;
}
}
if
(
bytes
>
currentBytes
&
&
!
IsLargeAllocSize
(
bytes
)
)
{
if
(
growMedium
(
alloc
bytes
)
)
{
return
alloc
;
}
}
}
else
{
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
currentBytes
=
region
-
>
allocBytes
(
alloc
)
;
}
if
(
bytes
=
=
currentBytes
)
{
return
alloc
;
}
void
*
newAlloc
=
this
-
>
alloc
(
bytes
nurseryOwned
)
;
if
(
!
newAlloc
)
{
return
nullptr
;
}
auto
freeGuard
=
mozilla
:
:
MakeScopeExit
(
[
&
]
(
)
{
free
(
alloc
)
;
}
)
;
size_t
bytesToCopy
=
std
:
:
min
(
bytes
currentBytes
)
;
#
ifdef
XP_DARWIN
if
(
bytesToCopy
>
=
ChunkSize
)
{
MOZ_ASSERT
(
IsLargeAlloc
(
alloc
)
)
;
MOZ_ASSERT
(
IsLargeAlloc
(
newAlloc
)
)
;
VirtualCopyPages
(
newAlloc
alloc
bytesToCopy
)
;
return
newAlloc
;
}
#
endif
memcpy
(
newAlloc
alloc
bytesToCopy
)
;
return
newAlloc
;
}
void
BufferAllocator
:
:
free
(
void
*
alloc
)
{
MOZ_ASSERT
(
alloc
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
freeLarge
(
alloc
)
;
return
;
}
if
(
IsMediumAlloc
(
alloc
)
)
{
freeMedium
(
alloc
)
;
return
;
}
}
bool
BufferAllocator
:
:
IsBufferAlloc
(
void
*
alloc
)
{
if
(
IsLargeAlloc
(
alloc
)
)
{
return
true
;
}
ChunkBase
*
chunk
=
detail
:
:
GetGCAddressChunkBase
(
alloc
)
;
return
chunk
-
>
getKind
(
)
=
=
ChunkKind
:
:
Buffers
;
}
size_t
BufferAllocator
:
:
getAllocSize
(
void
*
alloc
)
{
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
return
buffer
-
>
allocBytes
(
)
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
return
region
-
>
allocBytes
(
alloc
)
;
}
MOZ_ASSERT
(
IsMediumAlloc
(
alloc
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
allocBytes
(
alloc
)
;
}
bool
BufferAllocator
:
:
isNurseryOwned
(
void
*
alloc
)
{
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
return
buffer
-
>
isNurseryOwned
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
return
region
-
>
isNurseryOwned
(
alloc
)
;
}
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
isNurseryOwned
(
alloc
)
;
}
void
BufferAllocator
:
:
markNurseryOwnedAlloc
(
void
*
alloc
bool
ownerWasTenured
)
{
MOZ_ASSERT
(
alloc
)
;
MOZ_ASSERT
(
isNurseryOwned
(
alloc
)
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
Marking
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
MOZ_ASSERT
(
buffer
-
>
zone
(
)
=
=
zone
)
;
markLargeNurseryOwnedBuffer
(
buffer
ownerWasTenured
)
;
return
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
markSmallNurseryOwnedBuffer
(
alloc
ownerWasTenured
)
;
return
;
}
MOZ_ASSERT
(
IsMediumAlloc
(
alloc
)
)
;
markMediumNurseryOwnedBuffer
(
alloc
ownerWasTenured
)
;
}
void
BufferAllocator
:
:
markSmallNurseryOwnedBuffer
(
void
*
alloc
bool
ownerWasTenured
)
{
#
ifdef
DEBUG
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
)
;
#
endif
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
region
-
>
hasNurseryOwnedAllocs
(
)
)
;
MOZ_ASSERT
(
region
-
>
isNurseryOwned
(
alloc
)
)
;
if
(
ownerWasTenured
)
{
region
-
>
setNurseryOwned
(
alloc
false
)
;
return
;
}
region
-
>
setMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
markMediumNurseryOwnedBuffer
(
void
*
alloc
bool
ownerWasTenured
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
)
;
MOZ_ASSERT
(
chunk
-
>
isAllocated
(
alloc
)
)
;
MOZ_ASSERT
(
chunk
-
>
isNurseryOwned
(
alloc
)
)
;
if
(
ownerWasTenured
)
{
chunk
-
>
setNurseryOwned
(
alloc
false
)
;
size_t
size
=
chunk
-
>
allocBytes
(
alloc
)
;
updateHeapSize
(
size
false
false
)
;
return
;
}
chunk
-
>
setMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
markLargeNurseryOwnedBuffer
(
LargeBuffer
*
buffer
bool
ownerWasTenured
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
)
;
auto
*
region
=
SmallBufferRegion
:
:
from
(
buffer
)
;
if
(
region
-
>
isNurseryOwned
(
buffer
)
)
{
markSmallNurseryOwnedBuffer
(
buffer
ownerWasTenured
)
;
}
largeNurseryAllocsToSweep
.
ref
(
)
.
remove
(
buffer
)
;
if
(
ownerWasTenured
)
{
buffer
-
>
isNurseryOwned
=
false
;
buffer
-
>
allocatedDuringCollection
=
majorState
!
=
State
:
:
NotCollecting
;
largeTenuredAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
size_t
usableSize
=
buffer
-
>
allocBytes
(
)
;
updateHeapSize
(
usableSize
false
false
)
;
return
;
}
largeNurseryAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
}
bool
BufferAllocator
:
:
isMarkedBlack
(
void
*
alloc
)
{
if
(
IsLargeAlloc
(
alloc
)
)
{
alloc
=
lookupLargeBuffer
(
alloc
)
;
}
else
if
(
!
IsSmallAlloc
(
alloc
)
)
{
MOZ_ASSERT
(
IsMediumAlloc
(
alloc
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
isMarked
(
alloc
)
;
}
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
return
region
-
>
isMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
traceEdge
(
JSTracer
*
trc
Cell
*
owner
void
*
*
bufferp
const
char
*
name
)
{
MOZ_ASSERT
(
owner
)
;
MOZ_ASSERT
(
bufferp
)
;
void
*
buffer
=
*
bufferp
;
MOZ_ASSERT
(
buffer
)
;
if
(
!
IsLargeAlloc
(
buffer
)
&
&
js
:
:
gc
:
:
detail
:
:
GetGCAddressChunkBase
(
buffer
)
-
>
isNurseryChunk
(
)
)
{
return
;
}
MOZ_ASSERT
(
IsBufferAlloc
(
buffer
)
)
;
if
(
IsLargeAlloc
(
buffer
)
)
{
traceLargeAlloc
(
trc
owner
bufferp
name
)
;
return
;
}
if
(
IsSmallAlloc
(
buffer
)
)
{
traceSmallAlloc
(
trc
owner
bufferp
name
)
;
return
;
}
traceMediumAlloc
(
trc
owner
bufferp
name
)
;
}
void
BufferAllocator
:
:
traceSmallAlloc
(
JSTracer
*
trc
Cell
*
owner
void
*
*
allocp
const
char
*
name
)
{
void
*
alloc
=
*
allocp
;
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
if
(
trc
-
>
isTenuringTracer
(
)
)
{
if
(
region
-
>
isNurseryOwned
(
alloc
)
)
{
markSmallNurseryOwnedBuffer
(
alloc
owner
-
>
isTenured
(
)
)
;
}
return
;
}
if
(
trc
-
>
isMarkingTracer
(
)
)
{
if
(
!
region
-
>
isNurseryOwned
(
alloc
)
)
{
markSmallTenuredAlloc
(
alloc
)
;
}
return
;
}
}
void
BufferAllocator
:
:
traceMediumAlloc
(
JSTracer
*
trc
Cell
*
owner
void
*
*
allocp
const
char
*
name
)
{
void
*
alloc
=
*
allocp
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
if
(
trc
-
>
isTenuringTracer
(
)
)
{
if
(
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
markMediumNurseryOwnedBuffer
(
alloc
owner
-
>
isTenured
(
)
)
;
}
return
;
}
if
(
trc
-
>
isMarkingTracer
(
)
)
{
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
markMediumTenuredAlloc
(
alloc
)
;
}
return
;
}
}
void
BufferAllocator
:
:
traceLargeAlloc
(
JSTracer
*
trc
Cell
*
owner
void
*
*
allocp
const
char
*
name
)
{
void
*
alloc
=
*
allocp
;
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
traceSmallAlloc
(
trc
owner
reinterpret_cast
<
void
*
*
>
(
&
buffer
)
"
LargeBuffer
"
)
;
if
(
trc
-
>
isTenuringTracer
(
)
)
{
if
(
isNurseryOwned
(
alloc
)
)
{
markLargeNurseryOwnedBuffer
(
buffer
owner
-
>
isTenured
(
)
)
;
}
return
;
}
if
(
trc
-
>
isMarkingTracer
(
)
)
{
if
(
!
isNurseryOwned
(
alloc
)
)
{
markLargeTenuredBuffer
(
buffer
)
;
}
return
;
}
}
bool
BufferAllocator
:
:
markTenuredAlloc
(
void
*
alloc
)
{
MOZ_ASSERT
(
alloc
)
;
MOZ_ASSERT
(
!
isNurseryOwned
(
alloc
)
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
return
markLargeTenuredBuffer
(
buffer
)
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
return
markSmallTenuredAlloc
(
alloc
)
;
}
return
markMediumTenuredAlloc
(
alloc
)
;
}
bool
BufferAllocator
:
:
markSmallTenuredAlloc
(
void
*
alloc
)
{
auto
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
if
(
chunk
-
>
allocatedDuringCollection
)
{
return
false
;
}
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
region
-
>
isAllocated
(
alloc
)
)
;
return
region
-
>
setMarked
(
alloc
)
;
}
bool
BufferAllocator
:
:
markMediumTenuredAlloc
(
void
*
alloc
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
isAllocated
(
alloc
)
)
;
if
(
chunk
-
>
allocatedDuringCollection
)
{
return
false
;
}
return
chunk
-
>
setMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
startMinorCollection
(
MaybeLock
&
lock
)
{
maybeMergeSweptData
(
lock
)
;
#
ifdef
DEBUG
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
if
(
majorState
=
=
State
:
:
NotCollecting
)
{
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromMainThread
(
)
-
>
gc
;
if
(
gc
-
>
hasZealMode
(
ZealMode
:
:
CheckHeapBeforeMinorGC
)
)
{
checkGCStateNotInUse
(
lock
)
;
}
}
#
endif
MOZ_ASSERT
(
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
std
:
:
swap
(
largeNurseryAllocs
.
ref
(
)
largeNurseryAllocsToSweep
.
ref
(
)
)
;
minorState
=
State
:
:
Marking
;
}
bool
BufferAllocator
:
:
startMinorSweeping
(
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
minorState
=
=
State
:
:
Marking
)
;
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT
(
!
minorSweepingFinished
)
;
MOZ_ASSERT
(
sweptMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
}
for
(
LargeBuffer
*
buffer
:
largeNurseryAllocs
.
ref
(
)
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
)
;
}
for
(
LargeBuffer
*
buffer
:
largeNurseryAllocsToSweep
.
ref
(
)
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
)
;
}
#
endif
if
(
mixedChunks
.
ref
(
)
.
isEmpty
(
)
&
&
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
minorState
=
State
:
:
NotCollecting
;
return
false
;
}
freeLists
.
ref
(
)
.
eraseIf
(
[
]
(
FreeRegion
*
region
)
{
return
BufferChunk
:
:
from
(
region
)
-
>
hasNurseryOwnedAllocs
;
}
)
;
mixedChunksToSweep
.
ref
(
)
=
std
:
:
move
(
mixedChunks
.
ref
(
)
)
;
minorState
=
State
:
:
Sweeping
;
return
true
;
}
struct
LargeAllocToFree
{
size_t
bytes
;
LargeAllocToFree
*
next
=
nullptr
;
explicit
LargeAllocToFree
(
size_t
bytes
)
:
bytes
(
bytes
)
{
}
}
;
static
void
PushLargeAllocToFree
(
LargeAllocToFree
*
*
listHead
LargeBuffer
*
buffer
)
{
auto
*
alloc
=
new
(
buffer
-
>
data
(
)
)
LargeAllocToFree
(
buffer
-
>
bytes
)
;
alloc
-
>
next
=
*
listHead
;
*
listHead
=
alloc
;
}
static
void
FreeLargeAllocs
(
LargeAllocToFree
*
listHead
)
{
while
(
listHead
)
{
LargeAllocToFree
*
alloc
=
listHead
;
LargeAllocToFree
*
next
=
alloc
-
>
next
;
UnmapPages
(
alloc
alloc
-
>
bytes
)
;
listHead
=
next
;
}
}
void
BufferAllocator
:
:
sweepForMinorCollection
(
)
{
MOZ_ASSERT
(
minorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
)
;
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT
(
sweptMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
}
LargeAllocToFree
*
largeAllocsToFree
=
nullptr
;
while
(
!
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
LargeBuffer
*
buffer
=
largeNurseryAllocsToSweep
.
ref
(
)
.
popFirst
(
)
;
PushLargeAllocToFree
(
&
largeAllocsToFree
buffer
)
;
MaybeLock
lock
(
std
:
:
in_place
this
)
;
unregisterLarge
(
buffer
true
lock
)
;
}
while
(
!
mixedChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
BufferChunk
*
chunk
=
mixedChunksToSweep
.
ref
(
)
.
popFirst
(
)
;
FreeLists
sweptFreeLists
;
if
(
sweepChunk
(
chunk
SweepKind
:
:
SweepNursery
false
sweptFreeLists
)
)
{
{
AutoLock
lock
(
this
)
;
sweptMixedChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
if
(
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
)
{
sweptNurseryFreeLists
.
ref
(
)
.
append
(
std
:
:
move
(
sweptFreeLists
)
)
;
}
else
{
sweptTenuredFreeLists
.
ref
(
)
.
append
(
std
:
:
move
(
sweptFreeLists
)
)
;
}
}
hasMinorSweepDataToMerge
=
true
;
}
}
FreeLargeAllocs
(
largeAllocsToFree
)
;
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT
(
!
minorSweepingFinished
)
;
minorSweepingFinished
=
true
;
hasMinorSweepDataToMerge
=
true
;
}
}
void
BufferAllocator
:
:
startMajorCollection
(
MaybeLock
&
lock
)
{
maybeMergeSweptData
(
lock
)
;
#
ifdef
DEBUG
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
checkGCStateNotInUse
(
lock
)
;
MOZ_ASSERT
(
mixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeNurseryAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
#
endif
tenuredChunksToSweep
.
ref
(
)
=
std
:
:
move
(
tenuredChunks
.
ref
(
)
)
;
largeTenuredAllocsToSweep
.
ref
(
)
=
std
:
:
move
(
largeTenuredAllocs
.
ref
(
)
)
;
freeLists
.
ref
(
)
.
clear
(
)
;
if
(
minorState
=
=
State
:
:
Sweeping
)
{
majorStartedWhileMinorSweeping
=
true
;
}
#
ifdef
DEBUG
MOZ_ASSERT
(
tenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
freeLists
.
ref
(
)
.
assertEmpty
(
)
;
MOZ_ASSERT
(
largeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
#
endif
majorState
=
State
:
:
Marking
;
}
void
BufferAllocator
:
:
startMajorSweeping
(
MaybeLock
&
lock
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
majorState
=
=
State
:
:
Marking
)
;
MOZ_ASSERT
(
zone
-
>
isGCFinished
(
)
)
;
MOZ_ASSERT
(
!
majorSweepingFinished
.
refNoCheck
(
)
)
;
#
endif
maybeMergeSweptData
(
lock
)
;
MOZ_ASSERT
(
!
majorStartedWhileMinorSweeping
)
;
majorState
=
State
:
:
Sweeping
;
}
void
BufferAllocator
:
:
sweepForMajorCollection
(
bool
shouldDecommit
)
{
MOZ_ASSERT
(
majorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
)
;
LargeAllocList
sweptLargeAllocs
;
LargeAllocToFree
*
largeAllocsToFree
=
nullptr
;
while
(
!
largeTenuredAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
LargeBuffer
*
buffer
=
largeTenuredAllocsToSweep
.
ref
(
)
.
popFirst
(
)
;
if
(
isLargeTenuredMarked
(
buffer
)
)
{
sweptLargeAllocs
.
pushBack
(
buffer
)
;
}
else
{
PushLargeAllocToFree
(
&
largeAllocsToFree
buffer
)
;
MaybeLock
lock
(
std
:
:
in_place
this
)
;
unregisterLarge
(
buffer
true
lock
)
;
}
}
while
(
!
tenuredChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
BufferChunk
*
chunk
=
tenuredChunksToSweep
.
ref
(
)
.
popFirst
(
)
;
FreeLists
sweptFreeLists
;
if
(
sweepChunk
(
chunk
SweepKind
:
:
SweepTenured
shouldDecommit
sweptFreeLists
)
)
{
{
AutoLock
lock
(
this
)
;
sweptTenuredChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
sweptTenuredFreeLists
.
ref
(
)
.
append
(
std
:
:
move
(
sweptFreeLists
)
)
;
}
hasMinorSweepDataToMerge
=
true
;
}
}
FreeLargeAllocs
(
largeAllocsToFree
)
;
AutoLock
lock
(
this
)
;
sweptLargeTenuredAllocs
.
ref
(
)
=
std
:
:
move
(
sweptLargeAllocs
)
;
MOZ_ASSERT
(
!
majorSweepingFinished
)
;
majorSweepingFinished
=
true
;
}
static
void
ClearAllocatedDuringCollection
(
SlimLinkedList
<
BufferChunk
>
&
list
)
{
for
(
auto
*
buffer
:
list
)
{
buffer
-
>
allocatedDuringCollection
=
false
;
}
}
static
void
ClearAllocatedDuringCollection
(
SlimLinkedList
<
LargeBuffer
>
&
list
)
{
for
(
auto
*
element
:
list
)
{
element
-
>
allocatedDuringCollection
=
false
;
}
}
void
BufferAllocator
:
:
finishMajorCollection
(
const
AutoLock
&
lock
)
{
MOZ_ASSERT_IF
(
majorState
=
=
State
:
:
Sweeping
majorSweepingFinished
)
;
if
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
{
mergeSweptData
(
lock
)
;
}
if
(
majorState
=
=
State
:
:
Marking
)
{
abortMajorSweeping
(
lock
)
;
}
#
ifdef
DEBUG
checkGCStateNotInUse
(
lock
)
;
#
endif
}
void
BufferAllocator
:
:
abortMajorSweeping
(
const
AutoLock
&
lock
)
{
MOZ_ASSERT
(
majorState
=
=
State
:
:
Marking
)
;
MOZ_ASSERT
(
sweptTenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
clearAllocatedDuringCollectionState
(
lock
)
;
for
(
BufferChunk
*
chunk
:
tenuredChunksToSweep
.
ref
(
)
)
{
MOZ_ALWAYS_TRUE
(
sweepChunk
(
chunk
SweepKind
:
:
RebuildFreeLists
false
freeLists
.
ref
(
)
)
)
;
}
tenuredChunks
.
ref
(
)
.
prepend
(
std
:
:
move
(
tenuredChunksToSweep
.
ref
(
)
)
)
;
largeTenuredAllocs
.
ref
(
)
.
prepend
(
std
:
:
move
(
largeTenuredAllocsToSweep
.
ref
(
)
)
)
;
majorState
=
State
:
:
NotCollecting
;
}
void
BufferAllocator
:
:
clearAllocatedDuringCollectionState
(
const
AutoLock
&
lock
)
{
ClearAllocatedDuringCollection
(
mixedChunks
.
ref
(
)
)
;
ClearAllocatedDuringCollection
(
tenuredChunks
.
ref
(
)
)
;
ClearAllocatedDuringCollection
(
largeTenuredAllocs
.
ref
(
)
)
;
}
void
BufferAllocator
:
:
maybeMergeSweptData
(
)
{
if
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
{
mergeSweptData
(
)
;
}
}
void
BufferAllocator
:
:
mergeSweptData
(
)
{
AutoLock
lock
(
this
)
;
mergeSweptData
(
lock
)
;
}
void
BufferAllocator
:
:
maybeMergeSweptData
(
MaybeLock
&
lock
)
{
if
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
{
if
(
lock
.
isNothing
(
)
)
{
lock
.
emplace
(
this
)
;
}
mergeSweptData
(
lock
.
ref
(
)
)
;
}
}
void
BufferAllocator
:
:
mergeSweptData
(
const
AutoLock
&
lock
)
{
MOZ_ASSERT
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
;
if
(
majorSweepingFinished
)
{
clearAllocatedDuringCollectionState
(
lock
)
;
if
(
minorState
=
=
State
:
:
Sweeping
)
{
majorFinishedWhileMinorSweeping
=
true
;
}
}
while
(
!
sweptMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
{
BufferChunk
*
chunk
=
sweptMixedChunks
.
ref
(
)
.
popLast
(
)
;
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
)
;
chunk
-
>
hasNurseryOwnedAllocs
=
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
;
MOZ_ASSERT_IF
(
majorState
=
=
State
:
:
NotCollecting
&
&
!
majorFinishedWhileMinorSweeping
!
chunk
-
>
allocatedDuringCollection
)
;
if
(
majorFinishedWhileMinorSweeping
)
{
chunk
-
>
allocatedDuringCollection
=
false
;
}
if
(
chunk
-
>
hasNurseryOwnedAllocs
)
{
mixedChunks
.
ref
(
)
.
pushFront
(
chunk
)
;
}
else
if
(
majorStartedWhileMinorSweeping
)
{
tenuredChunksToSweep
.
ref
(
)
.
pushFront
(
chunk
)
;
}
else
{
tenuredChunks
.
ref
(
)
.
pushFront
(
chunk
)
;
}
}
#
ifdef
DEBUG
for
(
BufferChunk
*
chunk
:
sweptTenuredChunks
.
ref
(
)
)
{
MOZ_ASSERT
(
!
chunk
-
>
hasNurseryOwnedAllocs
)
;
MOZ_ASSERT
(
!
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
)
;
MOZ_ASSERT
(
!
chunk
-
>
allocatedDuringCollection
)
;
}
#
endif
tenuredChunks
.
ref
(
)
.
prepend
(
std
:
:
move
(
sweptTenuredChunks
.
ref
(
)
)
)
;
freeLists
.
ref
(
)
.
prepend
(
std
:
:
move
(
sweptNurseryFreeLists
.
ref
(
)
)
)
;
if
(
!
majorStartedWhileMinorSweeping
)
{
freeLists
.
ref
(
)
.
prepend
(
std
:
:
move
(
sweptTenuredFreeLists
.
ref
(
)
)
)
;
}
else
{
sweptTenuredFreeLists
.
ref
(
)
.
clear
(
)
;
}
largeTenuredAllocs
.
ref
(
)
.
prepend
(
std
:
:
move
(
sweptLargeTenuredAllocs
.
ref
(
)
)
)
;
hasMinorSweepDataToMerge
=
false
;
if
(
minorSweepingFinished
)
{
MOZ_ASSERT
(
minorState
=
=
State
:
:
Sweeping
)
;
minorState
=
State
:
:
NotCollecting
;
minorSweepingFinished
=
false
;
majorStartedWhileMinorSweeping
=
false
;
majorFinishedWhileMinorSweeping
=
false
;
#
ifdef
DEBUG
for
(
BufferChunk
*
chunk
:
mixedChunks
.
ref
(
)
)
{
verifyChunk
(
chunk
true
)
;
}
for
(
BufferChunk
*
chunk
:
tenuredChunks
.
ref
(
)
)
{
verifyChunk
(
chunk
false
)
;
}
#
endif
}
if
(
majorSweepingFinished
)
{
MOZ_ASSERT
(
majorState
=
=
State
:
:
Sweeping
)
;
majorState
=
State
:
:
NotCollecting
;
majorSweepingFinished
=
false
;
MOZ_ASSERT
(
tenuredChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
}
}
void
BufferAllocator
:
:
clearMarkStateAfterBarrierVerification
(
)
{
MOZ_ASSERT
(
!
zone
-
>
wasGCStarted
(
)
)
;
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
for
(
auto
*
chunks
:
{
&
mixedChunks
.
ref
(
)
&
tenuredChunks
.
ref
(
)
}
)
{
for
(
auto
*
chunk
:
*
chunks
)
{
chunk
-
>
markBits
.
ref
(
)
.
clear
(
)
;
for
(
auto
iter
=
chunk
-
>
smallRegionIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
SmallBufferRegion
*
region
=
iter
.
get
(
)
;
region
-
>
markBits
.
ref
(
)
.
clear
(
)
;
}
}
}
}
bool
BufferAllocator
:
:
isPointerWithinBuffer
(
void
*
ptr
)
{
maybeMergeSweptData
(
)
;
for
(
const
auto
*
chunks
:
{
&
mixedChunks
.
ref
(
)
&
tenuredChunks
.
ref
(
)
}
)
{
for
(
auto
*
chunk
:
*
chunks
)
{
if
(
chunk
-
>
isPointerWithinAllocation
(
ptr
)
)
{
return
true
;
}
}
}
if
(
majorState
=
=
State
:
:
Marking
)
{
for
(
auto
*
chunk
:
tenuredChunksToSweep
.
ref
(
)
)
{
if
(
chunk
-
>
isPointerWithinAllocation
(
ptr
)
)
{
return
true
;
}
}
}
for
(
const
auto
*
allocs
:
{
&
largeNurseryAllocs
.
ref
(
)
&
largeTenuredAllocs
.
ref
(
)
}
)
{
for
(
auto
*
alloc
:
*
allocs
)
{
if
(
alloc
-
>
isPointerWithinAllocation
(
ptr
)
)
{
return
true
;
}
}
}
return
false
;
}
bool
BufferChunk
:
:
isPointerWithinAllocation
(
void
*
ptr
)
const
{
uintptr_t
offset
=
uintptr_t
(
ptr
)
-
uintptr_t
(
this
)
;
if
(
offset
>
=
ChunkSize
|
|
offset
<
FirstMediumAllocOffset
)
{
return
false
;
}
if
(
smallRegionBitmap
.
ref
(
)
.
getBit
(
offset
/
SmallRegionSize
)
)
{
auto
*
region
=
SmallBufferRegion
:
:
from
(
ptr
)
;
return
region
-
>
isPointerWithinAllocation
(
ptr
)
;
}
uintptr_t
allocOffset
=
findPrevAllocated
(
RoundDown
(
offset
MinMediumAllocSize
)
)
;
MOZ_ASSERT
(
allocOffset
<
=
ChunkSize
)
;
if
(
allocOffset
=
=
ChunkSize
)
{
return
false
;
}
const
void
*
alloc
=
ptrFromOffset
(
allocOffset
)
;
size_t
size
=
allocBytes
(
alloc
)
;
return
offset
<
allocOffset
+
size
;
}
bool
SmallBufferRegion
:
:
isPointerWithinAllocation
(
void
*
ptr
)
const
{
uintptr_t
offset
=
uintptr_t
(
ptr
)
-
uintptr_t
(
this
)
;
MOZ_ASSERT
(
offset
<
SmallRegionSize
)
;
uintptr_t
allocOffset
=
findPrevAllocated
(
RoundDown
(
offset
SmallAllocGranularity
)
)
;
MOZ_ASSERT
(
allocOffset
<
=
SmallRegionSize
)
;
if
(
allocOffset
=
=
SmallRegionSize
)
{
return
false
;
}
const
void
*
alloc
=
ptrFromOffset
(
allocOffset
)
;
size_t
size
=
allocBytes
(
alloc
)
;
return
offset
<
allocOffset
+
size
;
}
bool
LargeBuffer
:
:
isPointerWithinAllocation
(
void
*
ptr
)
const
{
return
uintptr_t
(
ptr
)
-
uintptr_t
(
alloc
)
<
bytes
;
}
#
ifdef
DEBUG
void
BufferAllocator
:
:
checkGCStateNotInUse
(
)
{
maybeMergeSweptData
(
)
;
AutoLock
lock
(
this
)
;
checkGCStateNotInUse
(
lock
)
;
}
void
BufferAllocator
:
:
checkGCStateNotInUse
(
MaybeLock
&
maybeLock
)
{
if
(
maybeLock
.
isNothing
(
)
)
{
maybeLock
.
emplace
(
this
)
;
}
checkGCStateNotInUse
(
maybeLock
.
ref
(
)
)
;
}
void
BufferAllocator
:
:
checkGCStateNotInUse
(
const
AutoLock
&
lock
)
{
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
bool
isNurserySweeping
=
minorState
=
=
State
:
:
Sweeping
;
checkChunkListGCStateNotInUse
(
mixedChunks
.
ref
(
)
true
false
)
;
checkChunkListGCStateNotInUse
(
tenuredChunks
.
ref
(
)
false
false
)
;
if
(
isNurserySweeping
)
{
checkChunkListGCStateNotInUse
(
sweptMixedChunks
.
ref
(
)
true
majorFinishedWhileMinorSweeping
)
;
checkChunkListGCStateNotInUse
(
sweptTenuredChunks
.
ref
(
)
false
false
)
;
}
else
{
MOZ_ASSERT
(
mixedChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
sweptMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
sweptTenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
sweptNurseryFreeLists
.
ref
(
)
.
assertEmpty
(
)
;
sweptTenuredFreeLists
.
ref
(
)
.
assertEmpty
(
)
;
MOZ_ASSERT
(
!
majorStartedWhileMinorSweeping
)
;
MOZ_ASSERT
(
!
majorFinishedWhileMinorSweeping
)
;
MOZ_ASSERT
(
!
hasMinorSweepDataToMerge
)
;
MOZ_ASSERT
(
!
minorSweepingFinished
)
;
MOZ_ASSERT
(
!
majorSweepingFinished
)
;
}
MOZ_ASSERT
(
tenuredChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
checkAllocListGCStateNotInUse
(
largeNurseryAllocs
.
ref
(
)
true
)
;
checkAllocListGCStateNotInUse
(
largeTenuredAllocs
.
ref
(
)
false
)
;
MOZ_ASSERT
(
largeTenuredAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
sweptLargeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
}
void
BufferAllocator
:
:
checkChunkListGCStateNotInUse
(
BufferChunkList
&
chunks
bool
hasNurseryOwnedAllocs
bool
allowAllocatedDuringCollection
)
{
for
(
BufferChunk
*
chunk
:
chunks
)
{
checkChunkGCStateNotInUse
(
chunk
allowAllocatedDuringCollection
)
;
verifyChunk
(
chunk
hasNurseryOwnedAllocs
)
;
}
}
void
BufferAllocator
:
:
checkChunkGCStateNotInUse
(
BufferChunk
*
chunk
bool
allowAllocatedDuringCollection
)
{
MOZ_ASSERT_IF
(
!
allowAllocatedDuringCollection
!
chunk
-
>
allocatedDuringCollection
)
;
MOZ_ASSERT
(
chunk
-
>
markBits
.
ref
(
)
.
isEmpty
(
)
)
;
}
void
BufferAllocator
:
:
verifyChunk
(
BufferChunk
*
chunk
bool
hasNurseryOwnedAllocs
)
{
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
=
=
hasNurseryOwnedAllocs
)
;
static
constexpr
size_t
StepBytes
=
MediumAllocGranularity
;
size_t
freeOffset
=
FirstMediumAllocOffset
;
for
(
auto
iter
=
chunk
-
>
allocIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
size_t
offset
=
iter
.
getOffset
(
)
;
MOZ_ASSERT
(
offset
>
=
FirstMediumAllocOffset
)
;
if
(
offset
>
freeOffset
)
{
verifyFreeRegion
(
chunk
offset
offset
-
freeOffset
)
;
}
void
*
alloc
=
iter
.
get
(
)
;
MOZ_ASSERT_IF
(
chunk
-
>
isNurseryOwned
(
alloc
)
hasNurseryOwnedAllocs
)
;
size_t
bytes
=
chunk
-
>
allocBytes
(
alloc
)
;
uintptr_t
endOffset
=
offset
+
bytes
;
MOZ_ASSERT
(
endOffset
<
=
ChunkSize
)
;
for
(
size_t
i
=
offset
+
StepBytes
;
i
<
endOffset
;
i
+
=
StepBytes
)
{
MOZ_ASSERT
(
!
chunk
-
>
isAllocated
(
i
)
)
;
}
if
(
chunk
-
>
isSmallBufferRegion
(
alloc
)
)
{
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
MOZ_ASSERT_IF
(
region
-
>
hasNurseryOwnedAllocs
(
)
hasNurseryOwnedAllocs
)
;
verifySmallBufferRegion
(
region
)
;
}
freeOffset
=
endOffset
;
}
if
(
freeOffset
<
ChunkSize
)
{
verifyFreeRegion
(
chunk
ChunkSize
ChunkSize
-
freeOffset
)
;
}
}
void
BufferAllocator
:
:
verifyFreeRegion
(
BufferChunk
*
chunk
uintptr_t
endOffset
size_t
expectedSize
)
{
MOZ_ASSERT
(
expectedSize
>
=
MinFreeRegionSize
)
;
auto
*
freeRegion
=
FreeRegion
:
:
fromEndOffset
(
chunk
endOffset
)
;
MOZ_ASSERT
(
freeRegion
-
>
isInList
(
)
)
;
MOZ_ASSERT
(
freeRegion
-
>
size
(
)
=
=
expectedSize
)
;
}
void
BufferAllocator
:
:
verifySmallBufferRegion
(
SmallBufferRegion
*
region
)
{
bool
foundNurseryOwnedAllocs
=
false
;
static
constexpr
size_t
StepBytes
=
SmallAllocGranularity
;
size_t
freeOffset
=
FirstSmallAllocOffset
;
for
(
auto
iter
=
region
-
>
allocIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
size_t
offset
=
iter
.
getOffset
(
)
;
MOZ_ASSERT
(
offset
>
=
FirstSmallAllocOffset
)
;
if
(
offset
>
freeOffset
)
{
verifyFreeRegion
(
region
offset
offset
-
freeOffset
)
;
}
void
*
alloc
=
iter
.
get
(
)
;
MOZ_ASSERT_IF
(
region
-
>
isNurseryOwned
(
alloc
)
region
-
>
hasNurseryOwnedAllocs
(
)
)
;
size_t
bytes
=
region
-
>
allocBytes
(
alloc
)
;
uintptr_t
endOffset
=
offset
+
bytes
;
MOZ_ASSERT
(
endOffset
<
=
SmallRegionSize
)
;
for
(
size_t
i
=
offset
+
StepBytes
;
i
<
endOffset
;
i
+
=
StepBytes
)
{
MOZ_ASSERT
(
!
region
-
>
isAllocated
(
i
)
)
;
}
if
(
region
-
>
isNurseryOwned
(
alloc
)
)
{
foundNurseryOwnedAllocs
=
true
;
}
freeOffset
=
endOffset
;
}
MOZ_ASSERT
(
foundNurseryOwnedAllocs
=
=
region
-
>
hasNurseryOwnedAllocs
(
)
)
;
if
(
freeOffset
<
SmallRegionSize
)
{
verifyFreeRegion
(
region
SmallRegionSize
SmallRegionSize
-
freeOffset
)
;
}
}
void
BufferAllocator
:
:
verifyFreeRegion
(
SmallBufferRegion
*
region
uintptr_t
endOffset
size_t
expectedSize
)
{
if
(
expectedSize
<
MinFreeRegionSize
)
{
return
;
}
auto
*
freeRegion
=
FreeRegion
:
:
fromEndOffset
(
region
endOffset
)
;
MOZ_ASSERT
(
freeRegion
-
>
isInList
(
)
)
;
MOZ_ASSERT
(
freeRegion
-
>
size
(
)
=
=
expectedSize
)
;
}
void
BufferAllocator
:
:
checkAllocListGCStateNotInUse
(
LargeAllocList
&
list
bool
isNurseryOwned
)
{
for
(
LargeBuffer
*
buffer
:
list
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
=
=
isNurseryOwned
)
;
MOZ_ASSERT_IF
(
!
isNurseryOwned
!
buffer
-
>
allocatedDuringCollection
)
;
}
}
#
endif
void
*
BufferAllocator
:
:
allocSmall
(
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
MOZ_ASSERT
(
IsSmallAllocSize
(
bytes
)
)
;
bytes
=
SmallBufferSize
(
bytes
)
.
get
(
)
;
size_t
sizeClass
=
SizeClassForSmallAlloc
(
bytes
)
;
void
*
alloc
=
bumpAlloc
(
bytes
sizeClass
MaxSmallAllocClass
)
;
if
(
MOZ_UNLIKELY
(
!
alloc
)
)
{
alloc
=
retrySmallAlloc
(
bytes
sizeClass
inGC
)
;
if
(
!
alloc
)
{
return
nullptr
;
}
}
SmallBufferRegion
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
region
-
>
setAllocated
(
alloc
true
)
;
MOZ_ASSERT
(
!
region
-
>
isNurseryOwned
(
alloc
)
)
;
region
-
>
setNurseryOwned
(
alloc
nurseryOwned
)
;
MOZ_ASSERT
(
region
-
>
allocBytes
(
alloc
)
=
=
0
)
;
region
-
>
setAllocBytes
(
alloc
bytes
)
;
MOZ_ASSERT
(
region
-
>
allocBytes
(
alloc
)
=
=
bytes
)
;
auto
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
if
(
nurseryOwned
&
&
!
region
-
>
hasNurseryOwnedAllocs
(
)
)
{
region
-
>
setHasNurseryOwnedAllocs
(
true
)
;
if
(
!
chunk
-
>
hasNurseryOwnedAllocs
)
{
tenuredChunks
.
ref
(
)
.
remove
(
chunk
)
;
chunk
-
>
hasNurseryOwnedAllocs
=
true
;
mixedChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
}
}
MOZ_ASSERT
(
!
region
-
>
isMarked
(
alloc
)
)
;
MOZ_ASSERT
(
IsSmallAlloc
(
alloc
)
)
;
return
alloc
;
}
MOZ_NEVER_INLINE
void
*
BufferAllocator
:
:
retrySmallAlloc
(
size_t
bytes
size_t
sizeClass
bool
inGC
)
{
if
(
hasMinorSweepDataToMerge
)
{
mergeSweptData
(
)
;
void
*
ptr
=
bumpAlloc
(
bytes
sizeClass
MaxSmallAllocClass
)
;
if
(
ptr
)
{
return
ptr
;
}
}
if
(
!
allocNewSmallRegion
(
inGC
)
)
{
return
nullptr
;
}
void
*
ptr
=
bumpAlloc
(
bytes
sizeClass
MaxSmallAllocClass
)
;
MOZ_ASSERT
(
ptr
)
;
return
ptr
;
}
bool
BufferAllocator
:
:
allocNewSmallRegion
(
bool
inGC
)
{
void
*
ptr
=
allocMediumAligned
(
SmallRegionSize
inGC
)
;
if
(
!
ptr
)
{
return
false
;
}
auto
*
region
=
new
(
ptr
)
SmallBufferRegion
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
region
)
;
chunk
-
>
setSmallBufferRegion
(
region
true
)
;
uintptr_t
freeStart
=
uintptr_t
(
region
)
+
FirstSmallAllocOffset
;
uintptr_t
freeEnd
=
uintptr_t
(
region
)
+
SmallRegionSize
;
size_t
sizeClass
=
SizeClassForFreeRegion
(
freeEnd
-
freeStart
SizeKind
:
:
Small
)
;
ptr
=
reinterpret_cast
<
void
*
>
(
freeEnd
-
sizeof
(
FreeRegion
)
)
;
FreeRegion
*
freeRegion
=
new
(
ptr
)
FreeRegion
(
freeStart
)
;
MOZ_ASSERT
(
freeRegion
-
>
getEnd
(
)
=
=
freeEnd
)
;
freeLists
.
ref
(
)
.
pushFront
(
sizeClass
freeRegion
)
;
return
true
;
}
bool
BufferAllocator
:
:
IsSmallAlloc
(
void
*
alloc
)
{
MOZ_ASSERT
(
IsBufferAlloc
(
alloc
)
)
;
MOZ_ASSERT
(
!
IsLargeAlloc
(
alloc
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
isSmallBufferRegion
(
alloc
)
;
}
void
*
BufferAllocator
:
:
allocMedium
(
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
MOZ_ASSERT
(
!
IsSmallAllocSize
(
bytes
)
)
;
MOZ_ASSERT
(
!
IsLargeAllocSize
(
bytes
)
)
;
bytes
=
MediumBufferSize
(
bytes
)
.
get
(
)
;
size_t
sizeClass
=
SizeClassForMediumAlloc
(
bytes
)
;
void
*
alloc
=
bumpAlloc
(
bytes
sizeClass
MaxMediumAllocClass
)
;
if
(
MOZ_UNLIKELY
(
!
alloc
)
)
{
alloc
=
retryMediumAlloc
(
bytes
sizeClass
inGC
)
;
if
(
!
alloc
)
{
return
nullptr
;
}
}
setAllocated
(
alloc
bytes
nurseryOwned
inGC
)
;
return
alloc
;
}
MOZ_NEVER_INLINE
void
*
BufferAllocator
:
:
retryMediumAlloc
(
size_t
bytes
size_t
sizeClass
bool
inGC
)
{
if
(
hasMinorSweepDataToMerge
)
{
mergeSweptData
(
)
;
void
*
ptr
=
bumpAlloc
(
bytes
sizeClass
MaxMediumAllocClass
)
;
if
(
ptr
)
{
return
ptr
;
}
}
if
(
!
allocNewChunk
(
inGC
)
)
{
return
nullptr
;
}
void
*
ptr
=
bumpAlloc
(
bytes
sizeClass
MaxMediumAllocClass
)
;
MOZ_ASSERT
(
ptr
)
;
return
ptr
;
}
static
bool
IsMediumSizeClass
(
size_t
sizeClass
)
{
MOZ_ASSERT
(
sizeClass
<
BufferAllocator
:
:
AllocSizeClasses
)
;
return
sizeClass
>
=
MinMediumAllocClass
;
}
BufferAllocator
:
:
SizeKind
BufferAllocator
:
:
SizeClassKind
(
size_t
sizeClass
)
{
return
IsMediumSizeClass
(
sizeClass
)
?
SizeKind
:
:
Medium
:
SizeKind
:
:
Small
;
}
void
*
BufferAllocator
:
:
bumpAlloc
(
size_t
bytes
size_t
sizeClass
size_t
maxSizeClass
)
{
MOZ_ASSERT
(
SizeClassKind
(
sizeClass
)
=
=
SizeClassKind
(
maxSizeClass
)
)
;
freeLists
.
ref
(
)
.
checkAvailable
(
)
;
sizeClass
=
freeLists
.
ref
(
)
.
getFirstAvailableSizeClass
(
sizeClass
maxSizeClass
)
;
if
(
sizeClass
=
=
SIZE_MAX
)
{
return
nullptr
;
}
FreeRegion
*
region
=
freeLists
.
ref
(
)
.
getFirstRegion
(
sizeClass
)
;
MOZ_ASSERT
(
region
-
>
size
(
)
>
=
bytes
)
;
void
*
ptr
=
allocFromRegion
(
region
bytes
sizeClass
)
;
updateFreeListsAfterAlloc
(
&
freeLists
.
ref
(
)
region
sizeClass
)
;
return
ptr
;
}
#
ifdef
DEBUG
static
size_t
GranularityForSizeClass
(
size_t
sizeClass
)
{
return
IsMediumSizeClass
(
sizeClass
)
?
MediumAllocGranularity
:
SmallAllocGranularity
;
}
#
endif
void
*
BufferAllocator
:
:
allocFromRegion
(
FreeRegion
*
region
size_t
bytes
size_t
sizeClass
)
{
uintptr_t
start
=
region
-
>
startAddr
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
>
start
)
;
MOZ_ASSERT_IF
(
sizeClass
!
=
MaxMediumAllocClass
region
-
>
size
(
)
>
=
SizeClassBytes
(
sizeClass
)
)
;
MOZ_ASSERT_IF
(
sizeClass
=
=
MaxMediumAllocClass
region
-
>
size
(
)
>
=
MaxMediumAllocSize
)
;
MOZ_ASSERT
(
start
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
MOZ_ASSERT
(
region
-
>
size
(
)
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
if
(
region
-
>
hasDecommittedPages
)
{
recommitRegion
(
region
)
;
}
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
start
)
;
start
+
=
bytes
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
>
=
start
)
;
region
-
>
startAddr
=
start
;
return
ptr
;
}
void
*
BufferAllocator
:
:
allocMediumAligned
(
size_t
bytes
bool
inGC
)
{
MOZ_ASSERT
(
bytes
>
=
MinMediumAllocSize
)
;
MOZ_ASSERT
(
bytes
<
=
MaxAlignedAllocSize
)
;
MOZ_ASSERT
(
mozilla
:
:
IsPowerOfTwo
(
bytes
)
)
;
size_t
sizeClass
=
SizeClassForMediumAlloc
(
bytes
)
;
void
*
alloc
=
alignedAlloc
(
sizeClass
)
;
if
(
MOZ_UNLIKELY
(
!
alloc
)
)
{
alloc
=
retryAlignedAlloc
(
sizeClass
inGC
)
;
if
(
!
alloc
)
{
return
nullptr
;
}
}
setAllocated
(
alloc
bytes
false
inGC
)
;
return
alloc
;
}
MOZ_NEVER_INLINE
void
*
BufferAllocator
:
:
retryAlignedAlloc
(
size_t
sizeClass
bool
inGC
)
{
if
(
hasMinorSweepDataToMerge
)
{
mergeSweptData
(
)
;
void
*
ptr
=
alignedAlloc
(
sizeClass
)
;
if
(
ptr
)
{
return
ptr
;
}
}
if
(
!
allocNewChunk
(
inGC
)
)
{
return
nullptr
;
}
void
*
ptr
=
alignedAlloc
(
sizeClass
)
;
MOZ_ASSERT
(
ptr
)
;
return
ptr
;
}
void
*
BufferAllocator
:
:
alignedAlloc
(
size_t
sizeClass
)
{
freeLists
.
ref
(
)
.
checkAvailable
(
)
;
size_t
allocClass
=
freeLists
.
ref
(
)
.
getFirstAvailableSizeClass
(
sizeClass
MaxMediumAllocClass
)
;
MOZ_ASSERT
(
allocClass
>
=
sizeClass
)
;
if
(
allocClass
=
=
SIZE_MAX
)
{
return
nullptr
;
}
FreeRegion
*
region
=
freeLists
.
ref
(
)
.
getFirstRegion
(
allocClass
)
;
void
*
ptr
=
alignedAllocFromRegion
(
region
sizeClass
)
;
if
(
ptr
)
{
updateFreeListsAfterAlloc
(
&
freeLists
.
ref
(
)
region
allocClass
)
;
return
ptr
;
}
MOZ_ASSERT
(
allocClass
=
=
sizeClass
)
;
allocClass
=
freeLists
.
ref
(
)
.
getFirstAvailableSizeClass
(
sizeClass
+
1
MaxMediumAllocClass
)
;
if
(
allocClass
=
=
SIZE_MAX
)
{
return
nullptr
;
}
region
=
freeLists
.
ref
(
)
.
getFirstRegion
(
allocClass
)
;
ptr
=
alignedAllocFromRegion
(
region
sizeClass
)
;
MOZ_ASSERT
(
ptr
)
;
updateFreeListsAfterAlloc
(
&
freeLists
.
ref
(
)
region
allocClass
)
;
return
ptr
;
}
void
*
BufferAllocator
:
:
alignedAllocFromRegion
(
FreeRegion
*
region
size_t
sizeClass
)
{
uintptr_t
start
=
region
-
>
startAddr
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
>
start
)
;
MOZ_ASSERT
(
region
-
>
size
(
)
>
=
SizeClassBytes
(
sizeClass
)
)
;
MOZ_ASSERT
(
(
region
-
>
size
(
)
%
MinMediumAllocSize
)
=
=
0
)
;
size_t
bytes
=
SizeClassBytes
(
sizeClass
)
;
size_t
alignedStart
=
RoundUp
(
start
bytes
)
;
size_t
end
=
alignedStart
+
bytes
;
if
(
end
>
region
-
>
getEnd
(
)
)
{
return
nullptr
;
}
if
(
alignedStart
!
=
start
)
{
size_t
alignBytes
=
alignedStart
-
start
;
void
*
prefix
=
allocFromRegion
(
region
alignBytes
sizeClass
)
;
MOZ_ASSERT
(
uintptr_t
(
prefix
)
=
=
start
)
;
(
void
)
prefix
;
MOZ_ASSERT
(
!
region
-
>
hasDecommittedPages
)
;
addFreeRegion
(
&
freeLists
.
ref
(
)
start
alignBytes
SizeKind
:
:
Medium
false
ListPosition
:
:
Back
)
;
}
MOZ_ASSERT
(
region
-
>
startAddr
%
bytes
=
=
0
)
;
return
allocFromRegion
(
region
bytes
sizeClass
)
;
}
void
BufferAllocator
:
:
setAllocated
(
void
*
alloc
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
chunk
-
>
setAllocated
(
alloc
true
)
;
MOZ_ASSERT
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
;
chunk
-
>
setNurseryOwned
(
alloc
nurseryOwned
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
0
)
;
chunk
-
>
setAllocBytes
(
alloc
bytes
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
bytes
)
;
if
(
nurseryOwned
&
&
!
chunk
-
>
hasNurseryOwnedAllocs
)
{
tenuredChunks
.
ref
(
)
.
remove
(
chunk
)
;
chunk
-
>
hasNurseryOwnedAllocs
=
true
;
mixedChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
}
MOZ_ASSERT
(
!
chunk
-
>
isMarked
(
alloc
)
)
;
if
(
!
nurseryOwned
)
{
bool
checkThresholds
=
!
inGC
;
updateHeapSize
(
bytes
checkThresholds
false
)
;
}
MOZ_ASSERT
(
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
)
;
}
void
BufferAllocator
:
:
updateFreeListsAfterAlloc
(
FreeLists
*
freeLists
FreeRegion
*
region
size_t
sizeClass
)
{
freeLists
-
>
assertContains
(
sizeClass
region
)
;
size_t
classBytes
=
SizeClassBytes
(
sizeClass
)
;
size_t
newSize
=
region
-
>
size
(
)
;
MOZ_ASSERT
(
newSize
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
if
(
newSize
>
=
classBytes
)
{
return
;
}
freeLists
-
>
remove
(
sizeClass
region
)
;
if
(
newSize
=
=
0
)
{
return
;
}
if
(
newSize
<
MinFreeRegionSize
)
{
return
;
}
size_t
newSizeClass
=
SizeClassForFreeRegion
(
newSize
SizeClassKind
(
sizeClass
)
)
;
MOZ_ASSERT_IF
(
newSizeClass
!
=
MaxMediumAllocClass
newSize
>
=
SizeClassBytes
(
newSizeClass
)
)
;
MOZ_ASSERT
(
newSizeClass
<
=
sizeClass
)
;
MOZ_ASSERT_IF
(
newSizeClass
!
=
MaxMediumAllocClass
newSizeClass
<
sizeClass
)
;
MOZ_ASSERT
(
SizeClassKind
(
newSizeClass
)
=
=
SizeClassKind
(
sizeClass
)
)
;
freeLists
-
>
pushFront
(
newSizeClass
region
)
;
}
void
BufferAllocator
:
:
recommitRegion
(
FreeRegion
*
region
)
{
MOZ_ASSERT
(
region
-
>
hasDecommittedPages
)
;
MOZ_ASSERT
(
DecommitEnabled
(
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
region
)
;
uintptr_t
startAddr
=
RoundUp
(
region
-
>
startAddr
PageSize
)
;
uintptr_t
endAddr
=
RoundDown
(
uintptr_t
(
region
)
PageSize
)
;
size_t
startPage
=
(
startAddr
-
uintptr_t
(
chunk
)
)
/
PageSize
;
size_t
endPage
=
(
endAddr
-
uintptr_t
(
chunk
)
)
/
PageSize
;
MOZ_ASSERT_IF
(
(
region
-
>
startAddr
%
PageSize
)
!
=
0
!
chunk
-
>
decommittedPages
.
ref
(
)
[
startPage
-
1
]
)
;
MOZ_ASSERT
(
!
chunk
-
>
decommittedPages
.
ref
(
)
[
endPage
]
)
;
MarkPagesInUseSoft
(
reinterpret_cast
<
void
*
>
(
startAddr
)
endAddr
-
startAddr
)
;
for
(
size_t
i
=
startPage
;
i
!
=
endPage
;
i
+
+
)
{
chunk
-
>
decommittedPages
.
ref
(
)
[
i
]
=
false
;
}
region
-
>
hasDecommittedPages
=
false
;
}
static
inline
StallAndRetry
ShouldStallAndRetry
(
bool
inGC
)
{
return
inGC
?
StallAndRetry
:
:
Yes
:
StallAndRetry
:
:
No
;
}
bool
BufferAllocator
:
:
allocNewChunk
(
bool
inGC
)
{
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromMainThread
(
)
-
>
gc
;
AutoLockGCBgAlloc
lock
(
gc
)
;
ArenaChunk
*
baseChunk
=
gc
-
>
getOrAllocChunk
(
ShouldStallAndRetry
(
inGC
)
lock
)
;
if
(
!
baseChunk
)
{
return
false
;
}
CheckHighBitsOfPointer
(
baseChunk
)
;
if
(
!
baseChunk
-
>
decommittedPages
.
IsEmpty
(
)
)
{
MOZ_ASSERT
(
DecommitEnabled
(
)
)
;
MarkPagesInUseSoft
(
baseChunk
ChunkSize
)
;
}
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
uintptr_t
(
baseChunk
)
+
sizeof
(
ChunkBase
)
)
;
size_t
size
=
ChunkSize
-
sizeof
(
ChunkBase
)
;
SetMemCheckKind
(
ptr
size
MemCheckKind
:
:
MakeUndefined
)
;
BufferChunk
*
chunk
=
new
(
baseChunk
)
BufferChunk
(
zone
)
;
chunk
-
>
allocatedDuringCollection
=
majorState
!
=
State
:
:
NotCollecting
;
tenuredChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
uintptr_t
freeStart
=
uintptr_t
(
chunk
)
+
FirstMediumAllocOffset
;
uintptr_t
freeEnd
=
uintptr_t
(
chunk
)
+
ChunkSize
;
size_t
sizeClass
=
SizeClassForFreeRegion
(
freeEnd
-
freeStart
SizeKind
:
:
Medium
)
;
MOZ_ASSERT
(
sizeClass
>
MaxSmallAllocClass
)
;
MOZ_ASSERT
(
sizeClass
<
=
MaxMediumAllocClass
)
;
ptr
=
reinterpret_cast
<
void
*
>
(
freeEnd
-
sizeof
(
FreeRegion
)
)
;
FreeRegion
*
region
=
new
(
ptr
)
FreeRegion
(
freeStart
)
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
=
=
freeEnd
)
;
freeLists
.
ref
(
)
.
pushFront
(
sizeClass
region
)
;
return
true
;
}
static
void
SetDeallocated
(
BufferChunk
*
chunk
void
*
alloc
)
{
MOZ_ASSERT
(
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
)
;
chunk
-
>
setNurseryOwned
(
alloc
false
)
;
chunk
-
>
setAllocBytes
(
alloc
0
)
;
chunk
-
>
setAllocated
(
alloc
false
)
;
}
static
void
SetDeallocated
(
SmallBufferRegion
*
region
void
*
alloc
)
{
region
-
>
setNurseryOwned
(
alloc
false
)
;
region
-
>
setAllocBytes
(
alloc
0
)
;
region
-
>
setAllocated
(
alloc
false
)
;
}
bool
BufferAllocator
:
:
sweepChunk
(
BufferChunk
*
chunk
SweepKind
sweepKind
bool
shouldDecommit
FreeLists
&
freeLists
)
{
if
(
sweepKind
=
=
SweepKind
:
:
RebuildFreeLists
)
{
chunk
-
>
markBits
.
ref
(
)
.
clear
(
)
;
}
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromAnyThread
(
)
-
>
gc
;
bool
hasNurseryOwnedAllocs
=
false
;
size_t
freeStart
=
FirstMediumAllocOffset
;
bool
sweptAny
=
false
;
size_t
mallocHeapBytesFreed
=
0
;
for
(
auto
iter
=
chunk
-
>
smallRegionIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
SmallBufferRegion
*
region
=
iter
.
get
(
)
;
MOZ_ASSERT
(
!
chunk
-
>
isMarked
(
region
)
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
region
)
=
=
SmallRegionSize
)
;
if
(
!
sweepSmallBufferRegion
(
chunk
region
sweepKind
freeLists
)
)
{
MOZ_ASSERT
(
sweepKind
=
=
SweepKind
:
:
SweepNursery
|
|
sweepKind
=
=
SweepKind
:
:
SweepTenured
)
;
chunk
-
>
setSmallBufferRegion
(
region
false
)
;
SetDeallocated
(
chunk
region
)
;
PoisonAlloc
(
region
JS_SWEPT_TENURED_PATTERN
sizeof
(
SmallBufferRegion
)
MemCheckKind
:
:
MakeUndefined
)
;
mallocHeapBytesFreed
+
=
SmallRegionSize
;
sweptAny
=
true
;
}
else
if
(
region
-
>
hasNurseryOwnedAllocs
(
)
)
{
hasNurseryOwnedAllocs
=
true
;
}
}
for
(
auto
iter
=
chunk
-
>
allocIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
void
*
alloc
=
iter
.
get
(
)
;
size_t
bytes
=
chunk
-
>
allocBytes
(
alloc
)
;
uintptr_t
allocEnd
=
iter
.
getOffset
(
)
+
bytes
;
bool
nurseryOwned
=
chunk
-
>
isNurseryOwned
(
alloc
)
;
bool
canSweep
=
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
&
&
CanSweepAlloc
(
nurseryOwned
sweepKind
)
;
bool
shouldSweep
=
canSweep
&
&
!
chunk
-
>
isMarked
(
alloc
)
;
if
(
shouldSweep
)
{
if
(
!
nurseryOwned
)
{
mallocHeapBytesFreed
+
=
bytes
;
}
SetDeallocated
(
chunk
alloc
)
;
PoisonAlloc
(
alloc
JS_SWEPT_TENURED_PATTERN
bytes
MemCheckKind
:
:
MakeUndefined
)
;
sweptAny
=
true
;
}
else
{
uintptr_t
allocStart
=
iter
.
getOffset
(
)
;
if
(
freeStart
!
=
allocStart
)
{
addSweptRegion
(
chunk
freeStart
allocStart
shouldDecommit
!
sweptAny
freeLists
)
;
}
freeStart
=
allocEnd
;
if
(
canSweep
)
{
chunk
-
>
setUnmarked
(
alloc
)
;
}
if
(
nurseryOwned
)
{
MOZ_ASSERT
(
sweepKind
=
=
SweepKind
:
:
SweepNursery
)
;
hasNurseryOwnedAllocs
=
true
;
}
}
}
if
(
mallocHeapBytesFreed
)
{
bool
inMajorGC
=
sweepKind
=
=
SweepKind
:
:
SweepTenured
;
zone
-
>
mallocHeapSize
.
removeBytes
(
mallocHeapBytesFreed
inMajorGC
)
;
}
if
(
freeStart
=
=
FirstMediumAllocOffset
&
&
sweepKind
!
=
SweepKind
:
:
RebuildFreeLists
)
{
bool
allMemoryCommitted
=
chunk
-
>
decommittedPages
.
ref
(
)
.
IsEmpty
(
)
;
chunk
-
>
~
BufferChunk
(
)
;
ArenaChunk
*
tenuredChunk
=
ArenaChunk
:
:
init
(
chunk
gc
allMemoryCommitted
)
;
AutoLockGC
lock
(
gc
)
;
gc
-
>
recycleChunk
(
tenuredChunk
lock
)
;
return
false
;
}
if
(
freeStart
!
=
ChunkSize
)
{
addSweptRegion
(
chunk
freeStart
ChunkSize
shouldDecommit
!
sweptAny
freeLists
)
;
}
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
=
hasNurseryOwnedAllocs
;
return
true
;
}
bool
BufferAllocator
:
:
CanSweepAlloc
(
bool
nurseryOwned
BufferAllocator
:
:
SweepKind
sweepKind
)
{
static_assert
(
SweepKind
:
:
SweepNursery
=
=
SweepKind
(
uint8_t
(
true
)
)
)
;
static_assert
(
SweepKind
:
:
SweepTenured
=
=
SweepKind
(
uint8_t
(
false
)
)
)
;
SweepKind
requiredKind
=
SweepKind
(
uint8_t
(
nurseryOwned
)
)
;
return
sweepKind
=
=
requiredKind
;
}
void
BufferAllocator
:
:
addSweptRegion
(
BufferChunk
*
chunk
uintptr_t
freeStart
uintptr_t
freeEnd
bool
shouldDecommit
bool
expectUnchanged
FreeLists
&
freeLists
)
{
MOZ_ASSERT
(
freeStart
>
=
FirstMediumAllocOffset
)
;
MOZ_ASSERT
(
freeStart
<
freeEnd
)
;
MOZ_ASSERT
(
freeEnd
<
=
ChunkSize
)
;
MOZ_ASSERT
(
(
freeStart
%
MediumAllocGranularity
)
=
=
0
)
;
MOZ_ASSERT
(
(
freeEnd
%
MediumAllocGranularity
)
=
=
0
)
;
MOZ_ASSERT_IF
(
shouldDecommit
DecommitEnabled
(
)
)
;
bool
anyDecommitted
=
false
;
uintptr_t
decommitStart
=
RoundUp
(
freeStart
PageSize
)
;
uintptr_t
decommitEnd
=
RoundDown
(
freeEnd
-
sizeof
(
FreeRegion
)
PageSize
)
;
size_t
endPage
=
decommitEnd
/
PageSize
;
if
(
shouldDecommit
&
&
decommitEnd
>
decommitStart
)
{
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
decommitStart
+
uintptr_t
(
chunk
)
)
;
MarkPagesUnusedSoft
(
ptr
decommitEnd
-
decommitStart
)
;
size_t
startPage
=
decommitStart
/
PageSize
;
for
(
size_t
i
=
startPage
;
i
!
=
endPage
;
i
+
+
)
{
chunk
-
>
decommittedPages
.
ref
(
)
[
i
]
=
true
;
}
anyDecommitted
=
true
;
}
else
{
uintptr_t
startPage
=
RoundDown
(
freeStart
PageSize
)
/
PageSize
;
for
(
size_t
i
=
startPage
;
i
!
=
endPage
;
i
+
+
)
{
if
(
chunk
-
>
decommittedPages
.
ref
(
)
[
i
]
)
{
anyDecommitted
=
true
;
}
}
}
MOZ_ASSERT
(
!
chunk
-
>
decommittedPages
.
ref
(
)
[
endPage
]
)
;
freeStart
+
=
uintptr_t
(
chunk
)
;
freeEnd
+
=
uintptr_t
(
chunk
)
;
size_t
bytes
=
freeEnd
-
freeStart
;
addFreeRegion
(
&
freeLists
freeStart
bytes
SizeKind
:
:
Medium
anyDecommitted
ListPosition
:
:
Back
expectUnchanged
)
;
}
bool
BufferAllocator
:
:
sweepSmallBufferRegion
(
BufferChunk
*
chunk
SmallBufferRegion
*
region
SweepKind
sweepKind
FreeLists
&
freeLists
)
{
bool
hasNurseryOwnedAllocs
=
false
;
if
(
sweepKind
=
=
SweepKind
:
:
RebuildFreeLists
)
{
region
-
>
markBits
.
ref
(
)
.
clear
(
)
;
}
size_t
freeStart
=
FirstSmallAllocOffset
;
bool
sweptAny
=
false
;
for
(
auto
iter
=
region
-
>
allocIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
void
*
alloc
=
iter
.
get
(
)
;
size_t
bytes
=
region
-
>
allocBytes
(
alloc
)
;
uintptr_t
allocEnd
=
iter
.
getOffset
(
)
+
bytes
;
bool
nurseryOwned
=
region
-
>
isNurseryOwned
(
alloc
)
;
bool
canSweep
=
CanSweepAlloc
(
nurseryOwned
sweepKind
)
;
bool
shouldSweep
=
canSweep
&
&
!
region
-
>
isMarked
(
alloc
)
;
if
(
shouldSweep
)
{
SetDeallocated
(
region
alloc
)
;
PoisonAlloc
(
alloc
JS_SWEPT_TENURED_PATTERN
bytes
MemCheckKind
:
:
MakeUndefined
)
;
sweptAny
=
true
;
}
else
{
uintptr_t
allocStart
=
iter
.
getOffset
(
)
;
if
(
freeStart
!
=
allocStart
)
{
addSweptRegion
(
region
freeStart
allocStart
!
sweptAny
freeLists
)
;
}
freeStart
=
allocEnd
;
if
(
canSweep
)
{
region
-
>
setUnmarked
(
alloc
)
;
}
if
(
nurseryOwned
)
{
MOZ_ASSERT
(
sweepKind
=
=
SweepKind
:
:
SweepNursery
)
;
hasNurseryOwnedAllocs
=
true
;
}
sweptAny
=
false
;
}
}
if
(
freeStart
=
=
FirstSmallAllocOffset
&
&
sweepKind
!
=
SweepKind
:
:
RebuildFreeLists
)
{
return
false
;
}
if
(
freeStart
!
=
SmallRegionSize
)
{
addSweptRegion
(
region
freeStart
SmallRegionSize
!
sweptAny
freeLists
)
;
}
region
-
>
setHasNurseryOwnedAllocs
(
hasNurseryOwnedAllocs
)
;
return
true
;
}
void
BufferAllocator
:
:
addSweptRegion
(
SmallBufferRegion
*
region
uintptr_t
freeStart
uintptr_t
freeEnd
bool
expectUnchanged
FreeLists
&
freeLists
)
{
MOZ_ASSERT
(
freeStart
>
=
FirstSmallAllocOffset
)
;
MOZ_ASSERT
(
freeStart
<
freeEnd
)
;
MOZ_ASSERT
(
freeEnd
<
=
SmallRegionSize
)
;
MOZ_ASSERT
(
freeStart
%
SmallAllocGranularity
=
=
0
)
;
MOZ_ASSERT
(
freeEnd
%
SmallAllocGranularity
=
=
0
)
;
freeStart
+
=
uintptr_t
(
region
)
;
freeEnd
+
=
uintptr_t
(
region
)
;
size_t
bytes
=
freeEnd
-
freeStart
;
addFreeRegion
(
&
freeLists
freeStart
bytes
SizeKind
:
:
Small
false
ListPosition
:
:
Back
expectUnchanged
)
;
}
void
BufferAllocator
:
:
freeMedium
(
void
*
alloc
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
size_t
bytes
=
chunk
-
>
allocBytes
(
alloc
)
;
PoisonAlloc
(
alloc
JS_FREED_BUFFER_PATTERN
bytes
MemCheckKind
:
:
MakeUndefined
)
;
if
(
isSweepingChunk
(
chunk
)
)
{
return
;
}
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bool
updateRetained
=
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
;
zone
-
>
mallocHeapSize
.
removeBytes
(
bytes
updateRetained
)
;
}
chunk
-
>
setUnmarked
(
alloc
)
;
SetDeallocated
(
chunk
alloc
)
;
FreeLists
*
freeLists
=
getChunkFreeLists
(
chunk
)
;
uintptr_t
startAddr
=
uintptr_t
(
alloc
)
;
uintptr_t
endAddr
=
startAddr
+
bytes
;
FreeRegion
*
region
;
uintptr_t
endOffset
=
endAddr
&
ChunkMask
;
if
(
endOffset
=
=
0
|
|
chunk
-
>
isAllocated
(
endOffset
)
)
{
region
=
addFreeRegion
(
freeLists
startAddr
bytes
SizeKind
:
:
Medium
false
ListPosition
:
:
Front
)
;
MOZ_ASSERT
(
region
)
;
}
else
{
region
=
chunk
-
>
findFollowingFreeRegion
(
endAddr
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
endAddr
)
;
updateFreeRegionStart
(
freeLists
region
startAddr
SizeKind
:
:
Medium
)
;
}
FreeRegion
*
precRegion
=
chunk
-
>
findPrecedingFreeRegion
(
startAddr
)
;
if
(
precRegion
)
{
if
(
freeLists
)
{
size_t
sizeClass
=
SizeClassForFreeRegion
(
precRegion
-
>
size
(
)
SizeKind
:
:
Medium
)
;
freeLists
-
>
remove
(
sizeClass
precRegion
)
;
}
updateFreeRegionStart
(
freeLists
region
precRegion
-
>
startAddr
SizeKind
:
:
Medium
)
;
if
(
precRegion
-
>
hasDecommittedPages
)
{
region
-
>
hasDecommittedPages
=
true
;
}
}
}
bool
BufferAllocator
:
:
isSweepingChunk
(
BufferChunk
*
chunk
)
{
if
(
minorState
=
=
State
:
:
Sweeping
&
&
chunk
-
>
hasNurseryOwnedAllocs
)
{
if
(
!
hasMinorSweepDataToMerge
)
{
#
ifdef
DEBUG
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT_IF
(
!
hasMinorSweepDataToMerge
!
minorSweepingFinished
)
;
}
#
endif
return
true
;
}
mergeSweptData
(
)
;
if
(
minorState
=
=
State
:
:
Sweeping
&
&
chunk
-
>
hasNurseryOwnedAllocs
)
{
return
true
;
}
}
if
(
majorState
=
=
State
:
:
Sweeping
&
&
!
chunk
-
>
allocatedDuringCollection
)
{
return
true
;
}
return
false
;
}
BufferAllocator
:
:
FreeRegion
*
BufferAllocator
:
:
addFreeRegion
(
FreeLists
*
freeLists
uintptr_t
start
size_t
bytes
SizeKind
kind
bool
anyDecommitted
ListPosition
position
bool
expectUnchanged
)
{
static_assert
(
sizeof
(
FreeRegion
)
<
=
MinFreeRegionSize
)
;
if
(
bytes
<
MinFreeRegionSize
)
{
return
nullptr
;
}
size_t
sizeClass
=
SizeClassForFreeRegion
(
bytes
kind
)
;
MOZ_ASSERT_IF
(
sizeClass
!
=
MaxMediumAllocClass
bytes
>
=
SizeClassBytes
(
sizeClass
)
)
;
MOZ_ASSERT
(
start
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
MOZ_ASSERT
(
bytes
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
uintptr_t
end
=
start
+
bytes
;
#
ifdef
DEBUG
if
(
expectUnchanged
)
{
auto
*
region
=
FreeRegion
:
:
fromEndAddr
(
end
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
start
)
;
}
#
endif
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
end
-
sizeof
(
FreeRegion
)
)
;
FreeRegion
*
region
=
new
(
ptr
)
FreeRegion
(
start
anyDecommitted
)
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
=
=
end
)
;
if
(
freeLists
)
{
if
(
position
=
=
ListPosition
:
:
Front
)
{
freeLists
-
>
pushFront
(
sizeClass
region
)
;
}
else
{
freeLists
-
>
pushBack
(
sizeClass
region
)
;
}
}
return
region
;
}
void
BufferAllocator
:
:
updateFreeRegionStart
(
FreeLists
*
freeLists
FreeRegion
*
region
uintptr_t
newStart
SizeKind
kind
)
{
MOZ_ASSERT
(
(
newStart
&
~
ChunkMask
)
=
=
(
uintptr_t
(
region
)
&
~
ChunkMask
)
)
;
MOZ_ASSERT
(
region
-
>
startAddr
!
=
newStart
)
;
MOZ_ASSERT
(
kind
=
=
SizeKind
:
:
Medium
)
;
size_t
oldSize
=
region
-
>
size
(
)
;
region
-
>
startAddr
=
newStart
;
if
(
!
freeLists
)
{
return
;
}
size_t
currentSizeClass
=
SizeClassForFreeRegion
(
oldSize
kind
)
;
size_t
newSizeClass
=
SizeClassForFreeRegion
(
region
-
>
size
(
)
kind
)
;
MOZ_ASSERT
(
SizeClassKind
(
newSizeClass
)
=
=
SizeClassKind
(
currentSizeClass
)
)
;
if
(
currentSizeClass
!
=
newSizeClass
)
{
freeLists
-
>
remove
(
currentSizeClass
region
)
;
freeLists
-
>
pushFront
(
newSizeClass
region
)
;
}
}
bool
BufferAllocator
:
:
growMedium
(
void
*
alloc
size_t
newBytes
)
{
MOZ_ASSERT
(
!
IsSmallAllocSize
(
newBytes
)
)
;
MOZ_ASSERT
(
!
IsLargeAllocSize
(
newBytes
)
)
;
newBytes
=
std
:
:
max
(
newBytes
MinMediumAllocSize
)
;
MOZ_ASSERT
(
newBytes
=
=
GetGoodAllocSize
(
newBytes
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
if
(
isSweepingChunk
(
chunk
)
)
{
return
false
;
}
size_t
currentBytes
=
chunk
-
>
allocBytes
(
alloc
)
;
MOZ_ASSERT
(
newBytes
>
currentBytes
)
;
uintptr_t
endOffset
=
(
uintptr_t
(
alloc
)
&
ChunkMask
)
+
currentBytes
;
MOZ_ASSERT
(
endOffset
<
=
ChunkSize
)
;
if
(
endOffset
=
=
ChunkSize
)
{
return
false
;
}
size_t
endAddr
=
uintptr_t
(
chunk
)
+
endOffset
;
if
(
chunk
-
>
isAllocated
(
endOffset
)
)
{
return
false
;
}
FreeRegion
*
region
=
chunk
-
>
findFollowingFreeRegion
(
endAddr
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
endAddr
)
;
size_t
extraBytes
=
newBytes
-
currentBytes
;
if
(
region
-
>
size
(
)
<
extraBytes
)
{
return
false
;
}
size_t
sizeClass
=
SizeClassForFreeRegion
(
region
-
>
size
(
)
SizeKind
:
:
Medium
)
;
allocFromRegion
(
region
extraBytes
sizeClass
)
;
if
(
FreeLists
*
freeLists
=
getChunkFreeLists
(
chunk
)
)
{
updateFreeListsAfterAlloc
(
freeLists
region
sizeClass
)
;
}
chunk
-
>
setAllocBytes
(
alloc
newBytes
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
newBytes
)
;
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bool
updateRetained
=
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
;
updateHeapSize
(
extraBytes
true
updateRetained
)
;
}
return
true
;
}
bool
BufferAllocator
:
:
shrinkMedium
(
void
*
alloc
size_t
newBytes
)
{
MOZ_ASSERT
(
!
IsSmallAllocSize
(
newBytes
)
)
;
MOZ_ASSERT
(
!
IsLargeAllocSize
(
newBytes
)
)
;
newBytes
=
std
:
:
max
(
newBytes
MinMediumAllocSize
)
;
MOZ_ASSERT
(
newBytes
=
=
GetGoodAllocSize
(
newBytes
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
if
(
isSweepingChunk
(
chunk
)
)
{
return
false
;
}
size_t
currentBytes
=
chunk
-
>
allocBytes
(
alloc
)
;
if
(
newBytes
=
=
currentBytes
)
{
return
false
;
}
MOZ_ASSERT
(
newBytes
<
currentBytes
)
;
size_t
sizeChange
=
currentBytes
-
newBytes
;
chunk
-
>
setAllocBytes
(
alloc
newBytes
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
newBytes
)
;
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bool
updateRetained
=
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
;
zone
-
>
mallocHeapSize
.
removeBytes
(
sizeChange
updateRetained
)
;
}
uintptr_t
startOffset
=
uintptr_t
(
alloc
)
&
ChunkMask
;
uintptr_t
oldEndOffset
=
startOffset
+
currentBytes
;
uintptr_t
newEndOffset
=
startOffset
+
newBytes
;
MOZ_ASSERT
(
oldEndOffset
<
=
ChunkSize
)
;
uintptr_t
chunkAddr
=
uintptr_t
(
chunk
)
;
PoisonAlloc
(
reinterpret_cast
<
void
*
>
(
chunkAddr
+
newEndOffset
)
JS_SWEPT_TENURED_PATTERN
sizeChange
MemCheckKind
:
:
MakeUndefined
)
;
FreeLists
*
freeLists
=
getChunkFreeLists
(
chunk
)
;
if
(
oldEndOffset
=
=
ChunkSize
|
|
chunk
-
>
isAllocated
(
oldEndOffset
)
)
{
uintptr_t
freeStart
=
chunkAddr
+
newEndOffset
;
addFreeRegion
(
freeLists
freeStart
sizeChange
SizeKind
:
:
Medium
false
ListPosition
:
:
Front
)
;
return
true
;
}
FreeRegion
*
region
=
chunk
-
>
findFollowingFreeRegion
(
chunkAddr
+
oldEndOffset
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
chunkAddr
+
oldEndOffset
)
;
updateFreeRegionStart
(
freeLists
region
chunkAddr
+
newEndOffset
SizeKind
:
:
Medium
)
;
return
true
;
}
BufferAllocator
:
:
FreeLists
*
BufferAllocator
:
:
getChunkFreeLists
(
BufferChunk
*
chunk
)
{
MOZ_ASSERT_IF
(
majorState
=
=
State
:
:
Sweeping
chunk
-
>
allocatedDuringCollection
)
;
if
(
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
)
{
return
nullptr
;
}
return
&
freeLists
.
ref
(
)
;
}
BufferAllocator
:
:
FreeRegion
*
BufferChunk
:
:
findFollowingFreeRegion
(
uintptr_t
startAddr
)
{
uintptr_t
offset
=
uintptr_t
(
startAddr
)
&
ChunkMask
;
MOZ_ASSERT
(
isValidOffset
(
offset
)
)
;
MOZ_ASSERT
(
(
offset
%
MediumAllocGranularity
)
=
=
0
)
;
MOZ_ASSERT
(
!
isAllocated
(
offset
)
)
;
offset
=
findNextAllocated
(
offset
)
;
MOZ_ASSERT
(
offset
<
=
ChunkSize
)
;
auto
*
region
=
FreeRegion
:
:
fromEndOffset
(
this
offset
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
startAddr
)
;
return
region
;
}
BufferAllocator
:
:
FreeRegion
*
BufferChunk
:
:
findPrecedingFreeRegion
(
uintptr_t
endAddr
)
{
uintptr_t
offset
=
uintptr_t
(
endAddr
)
&
ChunkMask
;
MOZ_ASSERT
(
isValidOffset
(
offset
)
)
;
MOZ_ASSERT
(
(
offset
%
MediumAllocGranularity
)
=
=
0
)
;
if
(
offset
=
=
FirstMediumAllocOffset
)
{
return
nullptr
;
}
MOZ_ASSERT
(
!
isAllocated
(
offset
)
)
;
offset
=
findPrevAllocated
(
offset
)
;
if
(
offset
!
=
ChunkSize
)
{
const
void
*
alloc
=
ptrFromOffset
(
offset
)
;
size_t
bytes
=
allocBytes
(
alloc
)
;
MOZ_ASSERT
(
uintptr_t
(
alloc
)
+
bytes
<
=
endAddr
)
;
if
(
uintptr_t
(
alloc
)
+
bytes
=
=
endAddr
)
{
return
nullptr
;
}
}
auto
*
region
=
FreeRegion
:
:
fromEndAddr
(
endAddr
)
;
#
ifdef
DEBUG
region
-
>
check
(
)
;
if
(
offset
!
=
ChunkSize
)
{
const
void
*
alloc
=
ptrFromOffset
(
offset
)
;
size_t
bytes
=
allocBytes
(
alloc
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
uintptr_t
(
alloc
)
+
bytes
)
;
}
else
{
MOZ_ASSERT
(
region
-
>
startAddr
=
=
uintptr_t
(
this
)
+
FirstMediumAllocOffset
)
;
}
#
endif
return
region
;
}
size_t
BufferAllocator
:
:
SizeClassForSmallAlloc
(
size_t
bytes
)
{
MOZ_ASSERT
(
bytes
>
=
MinSmallAllocSize
)
;
MOZ_ASSERT
(
bytes
<
=
MaxSmallAllocSize
)
;
size_t
log2Size
=
mozilla
:
:
CeilingLog2
(
bytes
)
;
MOZ_ASSERT
(
(
size_t
(
1
)
<
<
log2Size
)
>
=
bytes
)
;
MOZ_ASSERT
(
MinSizeClassShift
=
=
mozilla
:
:
CeilingLog2
(
MinFreeRegionSize
)
)
;
if
(
log2Size
<
MinSizeClassShift
)
{
return
0
;
}
size_t
sizeClass
=
log2Size
-
MinSizeClassShift
;
MOZ_ASSERT
(
sizeClass
<
=
MaxSmallAllocClass
)
;
return
sizeClass
;
}
size_t
BufferAllocator
:
:
SizeClassForMediumAlloc
(
size_t
bytes
)
{
MOZ_ASSERT
(
bytes
>
=
MinMediumAllocSize
)
;
MOZ_ASSERT
(
bytes
<
=
MaxMediumAllocSize
)
;
size_t
log2Size
=
mozilla
:
:
CeilingLog2
(
bytes
)
;
MOZ_ASSERT
(
(
size_t
(
1
)
<
<
log2Size
)
>
=
bytes
)
;
MOZ_ASSERT
(
log2Size
>
=
MinMediumAllocShift
)
;
size_t
sizeClass
=
log2Size
-
MinMediumAllocShift
+
MinMediumAllocClass
;
MOZ_ASSERT
(
sizeClass
>
=
MinMediumAllocClass
)
;
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
return
sizeClass
;
}
size_t
BufferAllocator
:
:
SizeClassForFreeRegion
(
size_t
bytes
SizeKind
kind
)
{
MOZ_ASSERT
(
bytes
>
=
MinFreeRegionSize
)
;
MOZ_ASSERT
(
bytes
<
ChunkSize
)
;
if
(
kind
=
=
SizeKind
:
:
Medium
&
&
bytes
>
=
MaxMediumAllocSize
)
{
return
MaxMediumAllocClass
;
}
size_t
log2Size
=
mozilla
:
:
FloorLog2
(
bytes
)
;
MOZ_ASSERT
(
(
size_t
(
1
)
<
<
log2Size
)
<
=
bytes
)
;
MOZ_ASSERT
(
log2Size
>
=
MinSizeClassShift
)
;
size_t
sizeClass
=
std
:
:
min
(
log2Size
-
MinSizeClassShift
AllocSizeClasses
-
1
)
;
if
(
kind
=
=
SizeKind
:
:
Small
)
{
return
std
:
:
min
(
sizeClass
MaxSmallAllocClass
)
;
}
sizeClass
+
+
;
MOZ_ASSERT
(
sizeClass
>
=
MinMediumAllocClass
)
;
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
return
sizeClass
;
}
inline
size_t
BufferAllocator
:
:
SizeClassBytes
(
size_t
sizeClass
)
{
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
if
(
sizeClass
>
=
MinMediumAllocClass
)
{
sizeClass
-
-
;
}
return
1
<
<
(
sizeClass
+
MinSizeClassShift
)
;
}
bool
BufferAllocator
:
:
IsMediumAlloc
(
void
*
alloc
)
{
MOZ_ASSERT
(
IsBufferAlloc
(
alloc
)
)
;
MOZ_ASSERT
(
!
IsLargeAlloc
(
alloc
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
;
}
bool
BufferAllocator
:
:
needLockToAccessBufferMap
(
)
const
{
MOZ_ASSERT
(
CurrentThreadCanAccessZone
(
zone
)
|
|
CurrentThreadIsPerformingGC
(
)
)
;
return
minorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
|
|
majorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
;
}
LargeBuffer
*
BufferAllocator
:
:
lookupLargeBuffer
(
void
*
alloc
)
{
MaybeLock
lock
;
return
lookupLargeBuffer
(
alloc
lock
)
;
}
LargeBuffer
*
BufferAllocator
:
:
lookupLargeBuffer
(
void
*
alloc
MaybeLock
&
lock
)
{
MOZ_ASSERT
(
lock
.
isNothing
(
)
)
;
if
(
needLockToAccessBufferMap
(
)
)
{
lock
.
emplace
(
this
)
;
}
auto
ptr
=
largeAllocMap
.
ref
(
)
.
readonlyThreadsafeLookup
(
alloc
)
;
MOZ_ASSERT
(
ptr
)
;
LargeBuffer
*
buffer
=
ptr
-
>
value
(
)
;
MOZ_ASSERT
(
buffer
-
>
data
(
)
=
=
alloc
)
;
MOZ_ASSERT
(
buffer
-
>
zoneFromAnyThread
(
)
=
=
zone
)
;
return
buffer
;
}
void
*
BufferAllocator
:
:
allocLarge
(
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
bytes
=
RoundUp
(
bytes
ChunkSize
)
;
MOZ_ASSERT
(
bytes
>
MaxMediumAllocSize
)
;
MOZ_ASSERT
(
bytes
>
=
bytes
)
;
static_assert
(
sizeof
(
LargeBuffer
)
<
=
MaxSmallAllocSize
)
;
void
*
bufferPtr
=
allocSmall
(
sizeof
(
LargeBuffer
)
nurseryOwned
inGC
)
;
if
(
!
bufferPtr
)
{
return
nullptr
;
}
void
*
alloc
=
MapAlignedPages
(
bytes
ChunkSize
ShouldStallAndRetry
(
inGC
)
)
;
if
(
!
alloc
)
{
return
nullptr
;
}
auto
freeGuard
=
mozilla
:
:
MakeScopeExit
(
[
&
]
(
)
{
UnmapPages
(
alloc
bytes
)
;
}
)
;
CheckHighBitsOfPointer
(
alloc
)
;
auto
*
buffer
=
new
(
bufferPtr
)
LargeBuffer
(
alloc
bytes
nurseryOwned
)
;
{
MaybeLock
lock
;
if
(
needLockToAccessBufferMap
(
)
)
{
lock
.
emplace
(
this
)
;
}
if
(
!
largeAllocMap
.
ref
(
)
.
putNew
(
alloc
buffer
)
)
{
return
nullptr
;
}
}
freeGuard
.
release
(
)
;
if
(
nurseryOwned
)
{
largeNurseryAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
}
else
{
buffer
-
>
allocatedDuringCollection
=
majorState
!
=
State
:
:
NotCollecting
;
largeTenuredAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
}
if
(
!
nurseryOwned
)
{
bool
checkThresholds
=
!
inGC
;
updateHeapSize
(
bytes
checkThresholds
false
)
;
}
MOZ_ASSERT
(
IsLargeAlloc
(
alloc
)
)
;
return
alloc
;
}
void
BufferAllocator
:
:
updateHeapSize
(
size_t
bytes
bool
checkThresholds
bool
updateRetainedSize
)
{
zone
-
>
mallocHeapSize
.
addBytes
(
bytes
updateRetainedSize
)
;
if
(
checkThresholds
)
{
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromAnyThread
(
)
-
>
gc
;
gc
-
>
maybeTriggerGCAfterMalloc
(
zone
)
;
}
}
bool
BufferAllocator
:
:
IsLargeAlloc
(
void
*
alloc
)
{
return
(
uintptr_t
(
alloc
)
&
ChunkMask
)
=
=
0
;
}
bool
BufferAllocator
:
:
markLargeTenuredBuffer
(
LargeBuffer
*
buffer
)
{
MOZ_ASSERT
(
!
buffer
-
>
isNurseryOwned
)
;
if
(
buffer
-
>
allocatedDuringCollection
)
{
return
false
;
}
auto
*
region
=
SmallBufferRegion
:
:
from
(
buffer
)
;
return
region
-
>
setMarked
(
buffer
)
;
}
bool
BufferAllocator
:
:
isLargeTenuredMarked
(
LargeBuffer
*
buffer
)
{
MOZ_ASSERT
(
!
buffer
-
>
isNurseryOwned
)
;
MOZ_ASSERT
(
buffer
-
>
zoneFromAnyThread
(
)
=
=
zone
)
;
MOZ_ASSERT
(
!
buffer
-
>
isInList
(
)
)
;
auto
*
region
=
SmallBufferRegion
:
:
from
(
buffer
)
;
return
region
-
>
isMarked
(
buffer
)
;
}
void
BufferAllocator
:
:
freeLarge
(
void
*
alloc
)
{
MaybeLock
lock
;
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
lock
)
;
MOZ_ASSERT
(
buffer
-
>
zone
(
)
=
=
zone
)
;
DebugOnlyPoison
(
alloc
JS_FREED_BUFFER_PATTERN
buffer
-
>
allocBytes
(
)
MemCheckKind
:
:
MakeUndefined
)
;
if
(
!
buffer
-
>
isNurseryOwned
&
&
majorState
=
=
State
:
:
Sweeping
&
&
!
buffer
-
>
allocatedDuringCollection
)
{
return
;
}
MOZ_ASSERT
(
buffer
-
>
isInList
(
)
)
;
if
(
buffer
-
>
isNurseryOwned
)
{
largeNurseryAllocs
.
ref
(
)
.
remove
(
buffer
)
;
}
else
if
(
majorState
=
=
State
:
:
Marking
&
&
!
buffer
-
>
allocatedDuringCollection
)
{
largeTenuredAllocsToSweep
.
ref
(
)
.
remove
(
buffer
)
;
}
else
{
largeTenuredAllocs
.
ref
(
)
.
remove
(
buffer
)
;
}
unmapLarge
(
buffer
false
lock
)
;
}
bool
BufferAllocator
:
:
shrinkLarge
(
LargeBuffer
*
buffer
size_t
newBytes
)
{
MOZ_ASSERT
(
IsLargeAllocSize
(
newBytes
)
)
;
#
ifdef
XP_WIN
return
false
;
#
else
MOZ_ASSERT
(
buffer
-
>
zone
(
)
=
=
zone
)
;
if
(
!
buffer
-
>
isNurseryOwned
&
&
majorState
=
=
State
:
:
Sweeping
&
&
!
buffer
-
>
allocatedDuringCollection
)
{
return
false
;
}
MOZ_ASSERT
(
buffer
-
>
isInList
(
)
)
;
newBytes
=
RoundUp
(
newBytes
ChunkSize
)
;
size_t
oldBytes
=
buffer
-
>
bytes
;
MOZ_ASSERT
(
oldBytes
>
newBytes
)
;
size_t
shrinkBytes
=
oldBytes
-
newBytes
;
if
(
!
buffer
-
>
isNurseryOwned
)
{
zone
-
>
mallocHeapSize
.
removeBytes
(
shrinkBytes
false
)
;
}
buffer
-
>
bytes
=
newBytes
;
void
*
endPtr
=
reinterpret_cast
<
void
*
>
(
uintptr_t
(
buffer
-
>
data
(
)
)
+
newBytes
)
;
UnmapPages
(
endPtr
shrinkBytes
)
;
return
true
;
#
endif
}
void
BufferAllocator
:
:
unmapLarge
(
LargeBuffer
*
buffer
bool
isSweeping
MaybeLock
&
lock
)
{
unregisterLarge
(
buffer
isSweeping
lock
)
;
UnmapPages
(
buffer
-
>
data
(
)
buffer
-
>
bytes
)
;
}
void
BufferAllocator
:
:
unregisterLarge
(
LargeBuffer
*
buffer
bool
isSweeping
MaybeLock
&
lock
)
{
MOZ_ASSERT
(
buffer
-
>
zoneFromAnyThread
(
)
=
=
zone
)
;
MOZ_ASSERT
(
!
buffer
-
>
isInList
(
)
)
;
MOZ_ASSERT_IF
(
isSweeping
|
|
needLockToAccessBufferMap
(
)
lock
.
isSome
(
)
)
;
#
ifdef
DEBUG
auto
ptr
=
largeAllocMap
.
ref
(
)
.
lookup
(
buffer
-
>
data
(
)
)
;
MOZ_ASSERT
(
ptr
&
&
ptr
-
>
value
(
)
=
=
buffer
)
;
#
endif
largeAllocMap
.
ref
(
)
.
remove
(
buffer
-
>
data
(
)
)
;
lock
.
reset
(
)
;
if
(
!
buffer
-
>
isNurseryOwned
)
{
zone
-
>
mallocHeapSize
.
removeBytes
(
buffer
-
>
bytes
isSweeping
)
;
}
}
#
include
"
js
/
Printer
.
h
"
#
include
"
util
/
GetPidProvider
.
h
"
static
const
char
*
const
BufferAllocatorStatsPrefix
=
"
BufAllc
:
"
;
#
define
FOR_EACH_BUFFER_STATS_FIELD
(
_
)
\
_
(
"
PID
"
7
"
%
7zu
"
pid
)
\
_
(
"
Runtime
"
14
"
0x
%
12p
"
runtime
)
\
_
(
"
Timestamp
"
10
"
%
10
.
6f
"
timestamp
.
ToSeconds
(
)
)
\
_
(
"
Reason
"
20
"
%
-
20
.
20s
"
reason
)
\
_
(
"
"
2
"
%
2s
"
"
"
)
\
_
(
"
TotalKB
"
8
"
%
8zu
"
totalBytes
/
1024
)
\
_
(
"
UsedKB
"
8
"
%
8zu
"
stats
.
usedBytes
/
1024
)
\
_
(
"
FreeKB
"
8
"
%
8zu
"
stats
.
freeBytes
/
1024
)
\
_
(
"
Zs
"
3
"
%
3zu
"
zoneCount
)
\
_
(
"
"
7
"
%
7s
"
"
"
)
\
_
(
"
MixSRs
"
6
"
%
6zu
"
stats
.
mixedSmallRegions
)
\
_
(
"
TnrSRs
"
6
"
%
6zu
"
stats
.
tenuredSmallRegions
)
\
_
(
"
MixCs
"
6
"
%
6zu
"
stats
.
mixedChunks
)
\
_
(
"
TnrCs
"
6
"
%
6zu
"
stats
.
tenuredChunks
)
\
_
(
"
FreeRs
"
6
"
%
6zu
"
stats
.
freeRegions
)
\
_
(
"
LNurAs
"
6
"
%
6zu
"
stats
.
largeNurseryAllocs
)
\
_
(
"
LTnrAs
"
6
"
%
6zu
"
stats
.
largeTenuredAllocs
)
void
BufferAllocator
:
:
printStatsHeader
(
FILE
*
file
)
{
Sprinter
sprinter
;
if
(
!
sprinter
.
init
(
)
)
{
return
;
}
sprinter
.
put
(
BufferAllocatorStatsPrefix
)
;
#
define
PRINT_METADATA_NAME
(
name
width
_1
_2
)
\
sprinter
.
printf
(
"
%
-
*
s
"
width
name
)
;
FOR_EACH_BUFFER_STATS_FIELD
(
PRINT_METADATA_NAME
)
#
undef
PRINT_METADATA_NAME
sprinter
.
put
(
"
\
n
"
)
;
JS
:
:
UniqueChars
str
=
sprinter
.
release
(
)
;
if
(
!
str
)
{
return
;
}
fputs
(
str
.
get
(
)
file
)
;
}
void
BufferAllocator
:
:
printStats
(
GCRuntime
*
gc
mozilla
:
:
TimeStamp
creationTime
bool
isMajorGC
FILE
*
file
)
{
Sprinter
sprinter
;
if
(
!
sprinter
.
init
(
)
)
{
return
;
}
sprinter
.
put
(
BufferAllocatorStatsPrefix
)
;
size_t
pid
=
getpid
(
)
;
JSRuntime
*
runtime
=
gc
-
>
rt
;
mozilla
:
:
TimeDuration
timestamp
=
mozilla
:
:
TimeStamp
:
:
Now
(
)
-
creationTime
;
const
char
*
reason
=
isMajorGC
?
"
post
major
slice
"
:
"
pre
minor
GC
"
;
size_t
zoneCount
=
0
;
Stats
stats
;
for
(
AllZonesIter
zone
(
gc
)
;
!
zone
.
done
(
)
;
zone
.
next
(
)
)
{
zoneCount
+
+
;
zone
-
>
bufferAllocator
.
getStats
(
stats
)
;
}
size_t
totalBytes
=
stats
.
usedBytes
+
stats
.
freeBytes
+
stats
.
adminBytes
;
#
define
PRINT_FIELD_VALUE
(
_1
_2
format
value
)
\
sprinter
.
printf
(
"
"
format
value
)
;
FOR_EACH_BUFFER_STATS_FIELD
(
PRINT_FIELD_VALUE
)
#
undef
PRINT_FIELD_VALUE
sprinter
.
put
(
"
\
n
"
)
;
JS
:
:
UniqueChars
str
=
sprinter
.
release
(
)
;
if
(
!
str
)
{
return
;
}
fputs
(
str
.
get
(
)
file
)
;
}
size_t
BufferAllocator
:
:
getSizeOfNurseryBuffers
(
)
{
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
size_t
bytes
=
0
;
for
(
BufferChunk
*
chunk
:
mixedChunks
.
ref
(
)
)
{
for
(
auto
alloc
=
chunk
-
>
allocIter
(
)
;
!
alloc
.
done
(
)
;
alloc
.
next
(
)
)
{
if
(
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bytes
+
=
chunk
-
>
allocBytes
(
alloc
)
;
}
}
}
for
(
const
LargeBuffer
*
buffer
:
largeNurseryAllocs
.
ref
(
)
)
{
bytes
+
=
buffer
-
>
allocBytes
(
)
;
}
return
bytes
;
}
void
BufferAllocator
:
:
addSizeOfExcludingThis
(
size_t
*
usedBytesOut
size_t
*
freeBytesOut
size_t
*
adminBytesOut
)
{
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
Stats
stats
;
getStats
(
stats
)
;
*
usedBytesOut
+
=
stats
.
usedBytes
;
*
freeBytesOut
+
=
stats
.
freeBytes
;
*
adminBytesOut
+
=
stats
.
adminBytes
;
}
static
void
GetChunkStats
(
BufferChunk
*
chunk
BufferAllocator
:
:
Stats
&
stats
)
{
stats
.
usedBytes
+
=
ChunkSize
-
FirstMediumAllocOffset
;
stats
.
adminBytes
+
=
FirstMediumAllocOffset
;
for
(
auto
iter
=
chunk
-
>
smallRegionIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
SmallBufferRegion
*
region
=
iter
.
get
(
)
;
if
(
region
-
>
hasNurseryOwnedAllocs
(
)
)
{
stats
.
mixedSmallRegions
+
+
;
}
else
{
stats
.
tenuredSmallRegions
+
+
;
}
stats
.
adminBytes
+
=
FirstSmallAllocOffset
;
}
}
void
BufferAllocator
:
:
getStats
(
Stats
&
stats
)
{
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
for
(
BufferChunk
*
chunk
:
mixedChunks
.
ref
(
)
)
{
stats
.
mixedChunks
+
+
;
GetChunkStats
(
chunk
stats
)
;
}
for
(
BufferChunk
*
chunk
:
tenuredChunks
.
ref
(
)
)
{
(
void
)
chunk
;
stats
.
tenuredChunks
+
+
;
GetChunkStats
(
chunk
stats
)
;
}
for
(
const
LargeBuffer
*
buffer
:
largeNurseryAllocs
.
ref
(
)
)
{
stats
.
largeNurseryAllocs
+
+
;
stats
.
usedBytes
+
=
buffer
-
>
allocBytes
(
)
;
stats
.
adminBytes
+
=
sizeof
(
LargeBuffer
)
;
}
for
(
const
LargeBuffer
*
buffer
:
largeTenuredAllocs
.
ref
(
)
)
{
stats
.
largeTenuredAllocs
+
+
;
stats
.
usedBytes
+
=
buffer
-
>
allocBytes
(
)
;
stats
.
adminBytes
+
=
sizeof
(
LargeBuffer
)
;
}
for
(
const
FreeList
&
freeList
:
freeLists
.
ref
(
)
)
{
for
(
const
FreeRegion
*
region
:
freeList
)
{
stats
.
freeRegions
+
+
;
size_t
size
=
region
-
>
size
(
)
;
MOZ_ASSERT
(
stats
.
usedBytes
>
=
size
)
;
stats
.
usedBytes
-
=
size
;
stats
.
freeBytes
+
=
size
;
}
}
}
