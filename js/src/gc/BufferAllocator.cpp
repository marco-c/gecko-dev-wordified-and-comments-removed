#
include
"
gc
/
BufferAllocator
-
inl
.
h
"
#
include
"
mozilla
/
PodOperations
.
h
"
#
include
"
mozilla
/
ScopeExit
.
h
"
#
ifdef
XP_DARWIN
#
include
<
mach
/
mach_init
.
h
>
#
include
<
mach
/
vm_map
.
h
>
#
endif
#
include
"
gc
/
BufferAllocatorInternals
.
h
"
#
include
"
gc
/
GCInternals
.
h
"
#
include
"
gc
/
GCLock
.
h
"
#
include
"
gc
/
PublicIterators
.
h
"
#
include
"
gc
/
Zone
.
h
"
#
include
"
js
/
HeapAPI
.
h
"
#
include
"
util
/
Poison
.
h
"
#
include
"
gc
/
Heap
-
inl
.
h
"
#
include
"
gc
/
Marking
-
inl
.
h
"
using
namespace
js
;
using
namespace
js
:
:
gc
;
namespace
js
:
:
gc
{
struct
alignas
(
CellAlignBytes
)
LargeBuffer
:
public
SlimLinkedListElement
<
LargeBuffer
>
{
void
*
alloc
;
size_t
bytes
;
bool
isNurseryOwned
;
bool
allocatedDuringCollection
=
false
;
#
ifdef
DEBUG
uint32_t
checkValue
=
LargeBufferCheckValue
;
#
endif
LargeBuffer
(
void
*
alloc
size_t
bytes
bool
nurseryOwned
)
:
alloc
(
alloc
)
bytes
(
bytes
)
isNurseryOwned
(
nurseryOwned
)
{
MOZ_ASSERT
(
(
bytes
%
ChunkSize
)
=
=
0
)
;
}
inline
void
check
(
)
const
;
inline
SmallBuffer
*
headerCell
(
)
;
#
ifdef
DEBUG
inline
Zone
*
zone
(
)
;
inline
Zone
*
zoneFromAnyThread
(
)
;
#
endif
void
*
data
(
)
{
return
alloc
;
}
size_t
allocBytes
(
)
const
{
return
bytes
;
}
bool
isPointerWithinAllocation
(
void
*
ptr
)
const
;
}
;
bool
SmallBuffer
:
:
isNurseryOwned
(
)
const
{
return
header_
.
get
(
)
&
NURSERY_OWNED_BIT
;
}
void
SmallBuffer
:
:
setNurseryOwned
(
bool
value
)
{
header_
.
set
(
value
?
NURSERY_OWNED_BIT
:
0
)
;
}
size_t
SmallBuffer
:
:
allocBytes
(
)
const
{
return
arena
(
)
-
>
getThingSize
(
)
-
sizeof
(
SmallBuffer
)
;
}
inline
void
LargeBuffer
:
:
check
(
)
const
{
MOZ_ASSERT
(
checkValue
=
=
LargeBufferCheckValue
)
;
}
BufferAllocator
:
:
AutoLock
:
:
AutoLock
(
GCRuntime
*
gc
)
:
LockGuard
(
gc
-
>
bufferAllocatorLock
)
{
}
BufferAllocator
:
:
AutoLock
:
:
AutoLock
(
BufferAllocator
*
allocator
)
:
LockGuard
(
allocator
-
>
lock
(
)
)
{
}
struct
BufferAllocator
:
:
FreeRegion
:
public
SlimLinkedListElement
<
BufferAllocator
:
:
FreeRegion
>
{
uintptr_t
startAddr
;
bool
hasDecommittedPages
;
#
ifdef
DEBUG
uint32_t
checkValue
=
FreeRegionCheckValue
;
#
endif
explicit
FreeRegion
(
uintptr_t
startAddr
bool
decommitted
=
false
)
:
startAddr
(
startAddr
)
hasDecommittedPages
(
decommitted
)
{
}
static
FreeRegion
*
fromEndOffset
(
BufferChunk
*
chunk
uintptr_t
endOffset
)
{
MOZ_ASSERT
(
endOffset
<
=
ChunkSize
)
;
return
fromEndAddr
(
uintptr_t
(
chunk
)
+
endOffset
)
;
}
static
FreeRegion
*
fromEndAddr
(
uintptr_t
endAddr
)
{
MOZ_ASSERT
(
(
endAddr
%
MediumAllocGranularity
)
=
=
0
)
;
auto
*
region
=
reinterpret_cast
<
FreeRegion
*
>
(
endAddr
-
sizeof
(
FreeRegion
)
)
;
region
-
>
check
(
)
;
return
region
;
}
void
check
(
)
const
{
MOZ_ASSERT
(
checkValue
=
=
FreeRegionCheckValue
)
;
}
uintptr_t
getEnd
(
)
const
{
return
uintptr_t
(
this
+
1
)
;
}
size_t
size
(
)
const
{
return
getEnd
(
)
-
startAddr
;
}
}
;
struct
BufferChunk
:
public
ChunkBase
public
SlimLinkedListElement
<
BufferChunk
>
{
#
ifdef
DEBUG
MainThreadData
<
Zone
*
>
zone
;
#
endif
MainThreadOrGCTaskData
<
bool
>
allocatedDuringCollection
;
MainThreadData
<
bool
>
hasNurseryOwnedAllocs
;
MainThreadOrGCTaskData
<
bool
>
hasNurseryOwnedAllocsAfterSweep
;
static
constexpr
size_t
MaxAllocsPerChunk
=
ChunkSize
/
MediumAllocGranularity
;
using
EncodedSizeArray
=
MediumBufferSize
[
MaxAllocsPerChunk
]
;
EncodedSizeArray
encodedSizeArray
;
MainThreadOrGCTaskData
<
AtomicBitmap
<
MaxAllocsPerChunk
>
>
markBits
;
using
PerAllocBitmap
=
mozilla
:
:
BitSet
<
MaxAllocsPerChunk
>
;
MainThreadOrGCTaskData
<
PerAllocBitmap
>
allocBitmap
;
MainThreadOrGCTaskData
<
PerAllocBitmap
>
nurseryOwnedBitmap
;
static
constexpr
size_t
PagesPerChunk
=
ChunkSize
/
PageSize
;
using
PerPageBitmap
=
mozilla
:
:
BitSet
<
PagesPerChunk
uint32_t
>
;
MainThreadOrGCTaskData
<
PerPageBitmap
>
decommittedPages
;
class
AllocIter
;
static
BufferChunk
*
from
(
void
*
alloc
)
{
ChunkBase
*
chunk
=
js
:
:
gc
:
:
detail
:
:
GetGCAddressChunkBase
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
kind
=
=
ChunkKind
:
:
MediumBuffers
)
;
return
static_cast
<
BufferChunk
*
>
(
chunk
)
;
}
explicit
BufferChunk
(
Zone
*
zone
)
;
~
BufferChunk
(
)
;
void
setAllocated
(
void
*
alloc
bool
allocated
)
;
bool
isAllocated
(
const
void
*
alloc
)
const
;
bool
isAllocated
(
uintptr_t
offset
)
const
;
void
setNurseryOwned
(
void
*
alloc
bool
nurseryOwned
)
;
bool
isNurseryOwned
(
const
void
*
alloc
)
const
;
void
setAllocBytes
(
void
*
alloc
size_t
bytes
)
;
size_t
allocBytes
(
const
void
*
alloc
)
const
;
bool
setMarked
(
void
*
alloc
)
;
void
setUnmarked
(
void
*
alloc
)
;
bool
isMarked
(
const
void
*
alloc
)
const
;
size_t
findNextAllocated
(
uintptr_t
offset
)
const
;
size_t
findPrevAllocated
(
uintptr_t
offset
)
const
;
using
FreeRegion
=
BufferAllocator
:
:
FreeRegion
;
FreeRegion
*
findFollowingFreeRegion
(
uintptr_t
startAddr
)
;
FreeRegion
*
findPrecedingFreeRegion
(
uintptr_t
endAddr
)
;
bool
isPointerWithinAllocation
(
void
*
ptr
)
const
;
private
:
size_t
ptrToIndex
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
(
uintptr_t
(
alloc
)
&
~
ChunkMask
)
=
=
uintptr_t
(
this
)
)
;
uintptr_t
offset
=
uintptr_t
(
alloc
)
&
ChunkMask
;
return
offsetToIndex
(
offset
)
;
}
static
size_t
offsetToIndex
(
uintptr_t
offset
)
{
MOZ_ASSERT
(
isValidOffset
(
offset
)
)
;
MOZ_ASSERT
(
offset
%
MediumAllocGranularity
=
=
0
)
;
return
offset
/
MediumAllocGranularity
;
}
const
void
*
ptrFromOffset
(
uintptr_t
offset
)
const
{
MOZ_ASSERT
(
isValidOffset
(
offset
)
)
;
MOZ_ASSERT
(
offset
%
MediumAllocGranularity
=
=
0
)
;
return
reinterpret_cast
<
void
*
>
(
uintptr_t
(
this
)
+
offset
)
;
}
#
ifdef
DEBUG
static
bool
isValidOffset
(
uintptr_t
offset
)
;
#
endif
}
;
constexpr
size_t
FirstMediumAllocOffset
=
RoundUp
(
sizeof
(
BufferChunk
)
MediumAllocGranularity
)
;
#
ifdef
DEBUG
bool
BufferChunk
:
:
isValidOffset
(
uintptr_t
offset
)
{
return
offset
>
=
FirstMediumAllocOffset
&
&
offset
<
ChunkSize
;
}
#
endif
class
BufferChunk
:
:
AllocIter
{
BufferChunk
*
chunk
;
size_t
bit
=
0
;
public
:
explicit
AllocIter
(
BufferChunk
*
chunk
)
:
chunk
(
chunk
)
{
MOZ_ASSERT
(
!
chunk
-
>
allocBitmap
.
ref
(
)
[
bit
]
)
;
next
(
)
;
}
bool
done
(
)
const
{
MOZ_ASSERT
(
bit
<
=
MaxAllocsPerChunk
|
|
bit
=
=
SIZE_MAX
)
;
return
bit
>
=
MaxAllocsPerChunk
;
}
void
next
(
)
{
MOZ_ASSERT
(
!
done
(
)
)
;
bit
+
+
;
if
(
bit
!
=
MaxAllocsPerChunk
)
{
bit
=
chunk
-
>
allocBitmap
.
ref
(
)
.
FindNext
(
bit
)
;
}
}
size_t
getOffset
(
)
const
{
MOZ_ASSERT
(
!
done
(
)
)
;
return
bit
*
MediumAllocGranularity
;
}
void
*
get
(
)
const
{
return
reinterpret_cast
<
void
*
>
(
uintptr_t
(
chunk
)
+
getOffset
(
)
)
;
}
operator
void
*
(
)
{
return
get
(
)
;
}
}
;
static
void
CheckHighBitsOfPointer
(
void
*
ptr
)
{
#
ifdef
JS_64BIT
MOZ_DIAGNOSTIC_ASSERT
(
(
uintptr_t
(
ptr
)
>
>
47
)
=
=
0
)
;
#
endif
}
BufferAllocator
:
:
FreeLists
:
:
FreeLists
(
FreeLists
&
&
other
)
{
MOZ_ASSERT
(
this
!
=
&
other
)
;
assertEmpty
(
)
;
std
:
:
swap
(
lists
other
.
lists
)
;
std
:
:
swap
(
available
other
.
available
)
;
other
.
assertEmpty
(
)
;
}
BufferAllocator
:
:
FreeLists
&
BufferAllocator
:
:
FreeLists
:
:
operator
=
(
FreeLists
&
&
other
)
{
MOZ_ASSERT
(
this
!
=
&
other
)
;
assertEmpty
(
)
;
std
:
:
swap
(
lists
other
.
lists
)
;
std
:
:
swap
(
available
other
.
available
)
;
other
.
assertEmpty
(
)
;
return
*
this
;
}
size_t
BufferAllocator
:
:
FreeLists
:
:
getFirstAvailableSizeClass
(
size_t
minSizeClass
)
const
{
size_t
result
=
available
.
FindNext
(
minSizeClass
)
;
MOZ_ASSERT
(
result
>
=
minSizeClass
)
;
MOZ_ASSERT_IF
(
result
!
=
SIZE_MAX
!
lists
[
result
]
.
isEmpty
(
)
)
;
return
result
;
}
BufferAllocator
:
:
FreeRegion
*
BufferAllocator
:
:
FreeLists
:
:
getFirstRegion
(
size_t
sizeClass
)
{
MOZ_ASSERT
(
!
lists
[
sizeClass
]
.
isEmpty
(
)
)
;
return
lists
[
sizeClass
]
.
getFirst
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
pushFront
(
size_t
sizeClass
FreeRegion
*
region
)
{
MOZ_ASSERT
(
sizeClass
<
MediumAllocClasses
)
;
lists
[
sizeClass
]
.
pushFront
(
region
)
;
available
[
sizeClass
]
=
true
;
}
void
BufferAllocator
:
:
FreeLists
:
:
pushBack
(
size_t
sizeClass
FreeRegion
*
region
)
{
MOZ_ASSERT
(
sizeClass
<
MediumAllocClasses
)
;
lists
[
sizeClass
]
.
pushBack
(
region
)
;
available
[
sizeClass
]
=
true
;
}
void
BufferAllocator
:
:
FreeLists
:
:
append
(
FreeLists
&
&
other
)
{
for
(
size_t
i
=
0
;
i
<
MediumAllocClasses
;
i
+
+
)
{
if
(
!
other
.
lists
[
i
]
.
isEmpty
(
)
)
{
lists
[
i
]
.
append
(
std
:
:
move
(
other
.
lists
[
i
]
)
)
;
available
[
i
]
=
true
;
}
}
other
.
available
.
ResetAll
(
)
;
other
.
assertEmpty
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
prepend
(
FreeLists
&
&
other
)
{
for
(
size_t
i
=
0
;
i
<
MediumAllocClasses
;
i
+
+
)
{
if
(
!
other
.
lists
[
i
]
.
isEmpty
(
)
)
{
lists
[
i
]
.
prepend
(
std
:
:
move
(
other
.
lists
[
i
]
)
)
;
available
[
i
]
=
true
;
}
}
other
.
available
.
ResetAll
(
)
;
other
.
assertEmpty
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
remove
(
size_t
sizeClass
FreeRegion
*
region
)
{
MOZ_ASSERT
(
sizeClass
<
MediumAllocClasses
)
;
lists
[
sizeClass
]
.
remove
(
region
)
;
available
[
sizeClass
]
=
!
lists
[
sizeClass
]
.
isEmpty
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
clear
(
)
{
for
(
auto
&
freeList
:
lists
)
{
new
(
&
freeList
)
FreeList
;
}
available
.
ResetAll
(
)
;
}
template
<
typename
Pred
>
void
BufferAllocator
:
:
FreeLists
:
:
eraseIf
(
Pred
&
&
pred
)
{
for
(
size_t
i
=
0
;
i
<
MediumAllocClasses
;
i
+
+
)
{
FreeList
&
freeList
=
lists
[
i
]
;
freeList
.
eraseIf
(
std
:
:
forward
<
Pred
>
(
pred
)
)
;
available
[
i
]
=
!
freeList
.
isEmpty
(
)
;
}
}
inline
void
BufferAllocator
:
:
FreeLists
:
:
assertEmpty
(
)
const
{
#
ifdef
DEBUG
for
(
size_t
i
=
0
;
i
<
MediumAllocClasses
;
i
+
+
)
{
MOZ_ASSERT
(
lists
[
i
]
.
isEmpty
(
)
)
;
}
MOZ_ASSERT
(
available
.
IsEmpty
(
)
)
;
#
endif
}
inline
void
BufferAllocator
:
:
FreeLists
:
:
assertContains
(
size_t
sizeClass
FreeRegion
*
region
)
const
{
#
ifdef
DEBUG
MOZ_ASSERT
(
available
[
sizeClass
]
)
;
MOZ_ASSERT
(
lists
[
sizeClass
]
.
contains
(
region
)
)
;
#
endif
}
inline
void
BufferAllocator
:
:
FreeLists
:
:
checkAvailable
(
)
const
{
#
ifdef
DEBUG
for
(
size_t
i
=
0
;
i
<
MediumAllocClasses
;
i
+
+
)
{
MOZ_ASSERT
(
available
[
i
]
=
=
!
lists
[
i
]
.
isEmpty
(
)
)
;
}
#
endif
}
}
MOZ_ALWAYS_INLINE
void
PoisonAlloc
(
void
*
alloc
uint8_t
value
size_t
bytes
MemCheckKind
kind
)
{
#
ifndef
EARLY_BETA_OR_EARLIER
bytes
=
std
:
:
min
(
bytes
size_t
(
256
)
)
;
#
endif
AlwaysPoison
(
alloc
value
bytes
kind
)
;
}
BufferChunk
:
:
BufferChunk
(
Zone
*
zone
)
:
ChunkBase
(
zone
-
>
runtimeFromMainThread
(
)
ChunkKind
:
:
MediumBuffers
)
{
mozilla
:
:
PodArrayZero
(
encodedSizeArray
)
;
#
ifdef
DEBUG
this
-
>
zone
=
zone
;
MOZ_ASSERT
(
allocBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
nurseryOwnedBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
decommittedPages
.
ref
(
)
.
IsEmpty
(
)
)
;
for
(
const
auto
&
encodedSize
:
encodedSizeArray
)
{
MOZ_ASSERT
(
encodedSize
.
get
(
)
=
=
0
)
;
}
#
endif
}
BufferChunk
:
:
~
BufferChunk
(
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
allocBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
nurseryOwnedBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
for
(
const
auto
&
encodedSize
:
encodedSizeArray
)
{
MOZ_ASSERT
(
encodedSize
.
get
(
)
=
=
0
)
;
}
#
endif
}
void
BufferChunk
:
:
setAllocated
(
void
*
alloc
bool
allocated
)
{
size_t
bit
=
ptrToIndex
(
alloc
)
;
MOZ_ASSERT
(
allocBitmap
.
ref
(
)
[
bit
]
!
=
allocated
)
;
allocBitmap
.
ref
(
)
[
bit
]
=
allocated
;
}
bool
BufferChunk
:
:
isAllocated
(
const
void
*
alloc
)
const
{
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
allocBitmap
.
ref
(
)
[
bit
]
;
}
bool
BufferChunk
:
:
isAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
return
allocBitmap
.
ref
(
)
[
bit
]
;
}
size_t
BufferChunk
:
:
findNextAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
size_t
next
=
allocBitmap
.
ref
(
)
.
FindNext
(
bit
)
;
if
(
next
=
=
SIZE_MAX
)
{
return
ChunkSize
;
}
return
next
*
MediumAllocGranularity
;
}
size_t
BufferChunk
:
:
findPrevAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
size_t
prev
=
allocBitmap
.
ref
(
)
.
FindPrev
(
bit
)
;
if
(
prev
=
=
SIZE_MAX
)
{
return
ChunkSize
;
}
return
prev
*
MediumAllocGranularity
;
}
void
BufferChunk
:
:
setNurseryOwned
(
void
*
alloc
bool
nurseryOwned
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
nurseryOwnedBitmap
.
ref
(
)
[
bit
]
=
nurseryOwned
;
}
bool
BufferChunk
:
:
isNurseryOwned
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
nurseryOwnedBitmap
.
ref
(
)
[
bit
]
;
}
void
BufferChunk
:
:
setAllocBytes
(
void
*
alloc
size_t
bytes
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
index
=
ptrToIndex
(
alloc
)
;
MOZ_ASSERT
(
index
<
std
:
:
size
(
encodedSizeArray
)
)
;
encodedSizeArray
[
index
]
.
set
(
bytes
)
;
}
size_t
BufferChunk
:
:
allocBytes
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
index
=
ptrToIndex
(
alloc
)
;
MOZ_ASSERT
(
index
<
std
:
:
size
(
encodedSizeArray
)
)
;
return
encodedSizeArray
[
index
]
.
get
(
)
;
}
bool
BufferChunk
:
:
setMarked
(
void
*
alloc
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
if
(
markBits
.
ref
(
)
.
getBit
(
bit
)
)
{
return
false
;
}
markBits
.
ref
(
)
.
setBit
(
bit
true
)
;
return
true
;
}
void
BufferChunk
:
:
setUnmarked
(
void
*
alloc
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
markBits
.
ref
(
)
.
setBit
(
bit
false
)
;
}
bool
BufferChunk
:
:
isMarked
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
markBits
.
ref
(
)
.
getBit
(
bit
)
;
}
BufferAllocator
:
:
BufferAllocator
(
Zone
*
zone
)
:
zone
(
zone
)
sweptMediumMixedChunks
(
lock
(
)
)
sweptMediumTenuredChunks
(
lock
(
)
)
sweptMediumNurseryFreeLists
(
lock
(
)
)
sweptMediumTenuredFreeLists
(
lock
(
)
)
sweptLargeTenuredAllocs
(
lock
(
)
)
minorState
(
State
:
:
NotCollecting
)
majorState
(
State
:
:
NotCollecting
)
minorSweepingFinished
(
lock
(
)
)
majorSweepingFinished
(
lock
(
)
)
{
}
BufferAllocator
:
:
~
BufferAllocator
(
)
{
#
ifdef
DEBUG
checkGCStateNotInUse
(
)
;
MOZ_ASSERT
(
mediumMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
mediumTenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
mediumFreeLists
.
ref
(
)
.
assertEmpty
(
)
;
MOZ_ASSERT
(
largeNurseryAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
#
endif
}
bool
BufferAllocator
:
:
isEmpty
(
)
const
{
MOZ_ASSERT
(
!
zone
-
>
wasGCStarted
(
)
|
|
zone
-
>
isGCFinished
(
)
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
return
mediumMixedChunks
.
ref
(
)
.
isEmpty
(
)
&
&
mediumTenuredChunks
.
ref
(
)
.
isEmpty
(
)
&
&
largeNurseryAllocs
.
ref
(
)
.
isEmpty
(
)
&
&
largeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
;
}
Mutex
&
BufferAllocator
:
:
lock
(
)
const
{
return
zone
-
>
runtimeFromAnyThread
(
)
-
>
gc
.
bufferAllocatorLock
;
}
void
*
BufferAllocator
:
:
alloc
(
size_t
bytes
bool
nurseryOwned
)
{
MOZ_ASSERT_IF
(
zone
-
>
isGCMarkingOrSweeping
(
)
majorState
=
=
State
:
:
Marking
)
;
if
(
IsLargeAllocSize
(
bytes
)
)
{
return
allocLarge
(
bytes
nurseryOwned
false
)
;
}
if
(
IsSmallAllocSize
(
bytes
)
)
{
return
allocSmall
(
bytes
nurseryOwned
false
)
;
}
return
allocMedium
(
bytes
nurseryOwned
false
)
;
}
void
*
BufferAllocator
:
:
allocInGC
(
size_t
bytes
bool
nurseryOwned
)
{
MOZ_ASSERT
(
minorState
=
=
State
:
:
Marking
)
;
MOZ_ASSERT_IF
(
zone
-
>
isGCMarkingOrSweeping
(
)
majorState
=
=
State
:
:
Marking
)
;
void
*
result
;
if
(
IsLargeAllocSize
(
bytes
)
)
{
result
=
allocLarge
(
bytes
nurseryOwned
true
)
;
}
else
if
(
IsSmallAllocSize
(
bytes
)
)
{
result
=
allocSmall
(
bytes
nurseryOwned
true
)
;
}
else
{
result
=
allocMedium
(
bytes
nurseryOwned
true
)
;
}
if
(
!
result
)
{
return
nullptr
;
}
if
(
nurseryOwned
)
{
markNurseryOwnedAlloc
(
result
false
)
;
}
return
result
;
}
SmallBuffer
*
BufferAllocator
:
:
GetSmallBuffer
(
void
*
alloc
)
{
MOZ_ASSERT
(
BufferAllocator
:
:
IsSmallAlloc
(
alloc
)
)
;
auto
*
buffer
=
reinterpret_cast
<
SmallBuffer
*
>
(
uintptr_t
(
alloc
)
-
sizeof
(
SmallBuffer
)
)
;
buffer
-
>
check
(
)
;
return
buffer
;
}
inline
SmallBuffer
*
LargeBuffer
:
:
headerCell
(
)
{
return
BufferAllocator
:
:
GetSmallBuffer
(
this
)
;
}
#
ifdef
DEBUG
inline
Zone
*
LargeBuffer
:
:
zone
(
)
{
return
headerCell
(
)
-
>
zone
(
)
;
}
inline
Zone
*
LargeBuffer
:
:
zoneFromAnyThread
(
)
{
return
headerCell
(
)
-
>
zoneFromAnyThread
(
)
;
}
#
endif
#
ifdef
XP_DARWIN
static
inline
void
VirtualCopyPages
(
void
*
dst
const
void
*
src
size_t
bytes
)
{
MOZ_ASSERT
(
(
uintptr_t
(
dst
)
&
PageMask
)
=
=
0
)
;
MOZ_ASSERT
(
(
uintptr_t
(
src
)
&
PageMask
)
=
=
0
)
;
MOZ_ASSERT
(
bytes
>
=
ChunkSize
)
;
kern_return_t
r
=
vm_copy
(
mach_task_self
(
)
vm_address_t
(
src
)
vm_size_t
(
bytes
)
vm_address_t
(
dst
)
)
;
if
(
r
!
=
KERN_SUCCESS
)
{
MOZ_CRASH
(
"
vm_copy
(
)
failed
"
)
;
}
}
#
endif
void
*
BufferAllocator
:
:
realloc
(
void
*
alloc
size_t
bytes
bool
nurseryOwned
)
{
if
(
!
alloc
)
{
return
this
-
>
alloc
(
bytes
nurseryOwned
)
;
}
MOZ_ASSERT
(
isNurseryOwned
(
alloc
)
=
=
nurseryOwned
)
;
MOZ_ASSERT_IF
(
zone
-
>
isGCMarkingOrSweeping
(
)
majorState
=
=
State
:
:
Marking
)
;
bytes
=
GetGoodAllocSize
(
bytes
)
;
size_t
currentBytes
;
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
currentBytes
=
buffer
-
>
allocBytes
(
)
;
if
(
bytes
<
buffer
-
>
allocBytes
(
)
&
&
IsLargeAllocSize
(
bytes
)
)
{
if
(
shrinkLarge
(
buffer
bytes
)
)
{
return
alloc
;
}
}
}
else
if
(
IsMediumAlloc
(
alloc
)
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
currentBytes
=
chunk
-
>
allocBytes
(
alloc
)
;
if
(
bytes
<
currentBytes
&
&
!
IsSmallAllocSize
(
bytes
)
)
{
if
(
shrinkMedium
(
alloc
bytes
)
)
{
return
alloc
;
}
}
if
(
bytes
>
currentBytes
&
&
!
IsLargeAllocSize
(
bytes
)
)
{
if
(
growMedium
(
alloc
bytes
)
)
{
return
alloc
;
}
}
}
else
{
SmallBuffer
*
buffer
=
GetSmallBuffer
(
alloc
)
;
currentBytes
=
buffer
-
>
allocBytes
(
)
;
}
if
(
bytes
=
=
currentBytes
)
{
return
alloc
;
}
void
*
newAlloc
=
this
-
>
alloc
(
bytes
nurseryOwned
)
;
if
(
!
newAlloc
)
{
return
nullptr
;
}
auto
freeGuard
=
mozilla
:
:
MakeScopeExit
(
[
&
]
(
)
{
free
(
alloc
)
;
}
)
;
size_t
bytesToCopy
=
std
:
:
min
(
bytes
currentBytes
)
;
#
ifdef
XP_DARWIN
if
(
bytesToCopy
>
=
ChunkSize
)
{
MOZ_ASSERT
(
IsLargeAlloc
(
alloc
)
)
;
MOZ_ASSERT
(
IsLargeAlloc
(
newAlloc
)
)
;
VirtualCopyPages
(
newAlloc
alloc
bytesToCopy
)
;
return
newAlloc
;
}
#
endif
memcpy
(
newAlloc
alloc
bytesToCopy
)
;
return
newAlloc
;
}
void
BufferAllocator
:
:
free
(
void
*
alloc
)
{
MOZ_ASSERT
(
alloc
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
freeLarge
(
alloc
)
;
return
;
}
if
(
IsMediumAlloc
(
alloc
)
)
{
freeMedium
(
alloc
)
;
return
;
}
}
bool
BufferAllocator
:
:
IsBufferAlloc
(
void
*
alloc
)
{
if
(
IsLargeAlloc
(
alloc
)
)
{
return
true
;
}
ChunkKind
chunkKind
=
detail
:
:
GetGCAddressChunkBase
(
alloc
)
-
>
getKind
(
)
;
if
(
chunkKind
=
=
ChunkKind
:
:
MediumBuffers
)
{
return
true
;
}
if
(
chunkKind
=
=
ChunkKind
:
:
TenuredArenas
)
{
auto
*
arena
=
reinterpret_cast
<
Arena
*
>
(
uintptr_t
(
alloc
)
&
~
ArenaMask
)
;
return
IsBufferAllocKind
(
arena
-
>
getAllocKind
(
)
)
;
}
return
false
;
}
size_t
BufferAllocator
:
:
getAllocSize
(
void
*
alloc
)
{
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
return
buffer
-
>
allocBytes
(
)
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
SmallBuffer
*
cell
=
GetSmallBuffer
(
alloc
)
;
return
cell
-
>
allocBytes
(
)
;
}
MOZ_ASSERT
(
IsMediumAlloc
(
alloc
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
allocBytes
(
alloc
)
;
}
bool
BufferAllocator
:
:
isNurseryOwned
(
void
*
alloc
)
{
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
return
buffer
-
>
isNurseryOwned
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
SmallBuffer
*
cell
=
GetSmallBuffer
(
alloc
)
;
return
cell
-
>
isNurseryOwned
(
)
;
}
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
isNurseryOwned
(
alloc
)
;
}
void
BufferAllocator
:
:
markNurseryOwnedAlloc
(
void
*
alloc
bool
ownerWasTenured
)
{
MOZ_ASSERT
(
alloc
)
;
MOZ_ASSERT
(
isNurseryOwned
(
alloc
)
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
Marking
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
MOZ_ASSERT
(
buffer
-
>
zone
(
)
=
=
zone
)
;
markLargeNurseryOwnedBuffer
(
buffer
ownerWasTenured
)
;
return
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
SmallBuffer
*
buffer
=
GetSmallBuffer
(
alloc
)
;
markSmallNurseryOwnedBuffer
(
buffer
ownerWasTenured
)
;
return
;
}
MOZ_ASSERT
(
IsMediumAlloc
(
alloc
)
)
;
markMediumNurseryOwnedBuffer
(
alloc
ownerWasTenured
)
;
}
void
BufferAllocator
:
:
markSmallNurseryOwnedBuffer
(
SmallBuffer
*
buffer
bool
ownerWasTenured
)
{
MOZ_ASSERT
(
buffer
-
>
zone
(
)
=
=
zone
)
;
if
(
ownerWasTenured
)
{
buffer
-
>
setNurseryOwned
(
false
)
;
}
}
void
BufferAllocator
:
:
markMediumNurseryOwnedBuffer
(
void
*
alloc
bool
ownerWasTenured
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
)
;
MOZ_ASSERT
(
chunk
-
>
isAllocated
(
alloc
)
)
;
if
(
ownerWasTenured
)
{
chunk
-
>
setNurseryOwned
(
alloc
false
)
;
size_t
size
=
chunk
-
>
allocBytes
(
alloc
)
;
updateHeapSize
(
size
false
false
)
;
return
;
}
chunk
-
>
setMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
markLargeNurseryOwnedBuffer
(
LargeBuffer
*
buffer
bool
ownerWasTenured
)
{
largeNurseryAllocsToSweep
.
ref
(
)
.
remove
(
buffer
)
;
if
(
ownerWasTenured
)
{
buffer
-
>
isNurseryOwned
=
false
;
buffer
-
>
allocatedDuringCollection
=
majorState
!
=
State
:
:
NotCollecting
;
largeTenuredAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
size_t
usableSize
=
buffer
-
>
allocBytes
(
)
;
updateHeapSize
(
usableSize
false
false
)
;
return
;
}
largeNurseryAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
}
bool
BufferAllocator
:
:
isMarkedBlack
(
void
*
alloc
)
{
if
(
IsLargeAlloc
(
alloc
)
)
{
return
isLargeAllocMarked
(
alloc
)
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
SmallBuffer
*
cell
=
GetSmallBuffer
(
alloc
)
;
MOZ_ASSERT
(
!
cell
-
>
isMarkedGray
(
)
)
;
return
cell
-
>
isMarkedBlack
(
)
;
}
MOZ_ASSERT
(
IsMediumAlloc
(
alloc
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
isMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
traceEdge
(
JSTracer
*
trc
Cell
*
owner
void
*
*
bufferp
const
char
*
name
)
{
MOZ_ASSERT
(
owner
)
;
MOZ_ASSERT
(
bufferp
)
;
void
*
buffer
=
*
bufferp
;
MOZ_ASSERT
(
buffer
)
;
if
(
!
IsLargeAlloc
(
buffer
)
&
&
js
:
:
gc
:
:
detail
:
:
GetGCAddressChunkBase
(
buffer
)
-
>
isNurseryChunk
(
)
)
{
return
;
}
MOZ_ASSERT
(
IsBufferAlloc
(
buffer
)
)
;
if
(
IsLargeAlloc
(
buffer
)
)
{
traceLargeAlloc
(
trc
owner
bufferp
name
)
;
return
;
}
if
(
IsSmallAlloc
(
buffer
)
)
{
traceSmallAlloc
(
trc
owner
bufferp
name
)
;
return
;
}
traceMediumAlloc
(
trc
owner
bufferp
name
)
;
}
void
BufferAllocator
:
:
traceSmallAlloc
(
JSTracer
*
trc
Cell
*
owner
void
*
*
allocp
const
char
*
name
)
{
void
*
alloc
=
*
allocp
;
SmallBuffer
*
cell
=
GetSmallBuffer
(
alloc
)
;
TraceManuallyBarrieredEdge
(
trc
&
cell
name
)
;
if
(
cell
!
=
GetSmallBuffer
(
alloc
)
)
{
*
allocp
=
cell
-
>
data
(
)
;
}
}
void
BufferAllocator
:
:
traceMediumAlloc
(
JSTracer
*
trc
Cell
*
owner
void
*
*
allocp
const
char
*
name
)
{
void
*
alloc
=
*
allocp
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
if
(
trc
-
>
isTenuringTracer
(
)
)
{
if
(
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
markMediumNurseryOwnedBuffer
(
alloc
owner
-
>
isTenured
(
)
)
;
}
return
;
}
if
(
trc
-
>
isMarkingTracer
(
)
)
{
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
markMediumTenuredAlloc
(
alloc
)
;
}
return
;
}
}
void
BufferAllocator
:
:
traceLargeAlloc
(
JSTracer
*
trc
Cell
*
owner
void
*
*
allocp
const
char
*
name
)
{
void
*
alloc
=
*
allocp
;
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
traceSmallAlloc
(
trc
owner
reinterpret_cast
<
void
*
*
>
(
&
buffer
)
"
LargeBuffer
"
)
;
if
(
trc
-
>
isTenuringTracer
(
)
)
{
if
(
isNurseryOwned
(
alloc
)
)
{
markLargeNurseryOwnedBuffer
(
buffer
owner
-
>
isTenured
(
)
)
;
}
return
;
}
if
(
trc
-
>
isMarkingTracer
(
)
)
{
if
(
!
isNurseryOwned
(
alloc
)
)
{
markLargeTenuredBuffer
(
buffer
)
;
}
return
;
}
}
bool
BufferAllocator
:
:
markTenuredAlloc
(
void
*
alloc
)
{
MOZ_ASSERT
(
alloc
)
;
MOZ_ASSERT
(
!
isNurseryOwned
(
alloc
)
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
return
markLargeTenuredBuffer
(
buffer
)
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
return
markSmallTenuredAlloc
(
alloc
)
;
}
return
markMediumTenuredAlloc
(
alloc
)
;
}
bool
BufferAllocator
:
:
markSmallTenuredAlloc
(
void
*
alloc
)
{
SmallBuffer
*
cell
=
GetSmallBuffer
(
alloc
)
;
return
cell
-
>
markIfUnmarkedThreadSafe
(
MarkColor
:
:
Black
)
;
}
bool
BufferAllocator
:
:
markMediumTenuredAlloc
(
void
*
alloc
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
isAllocated
(
alloc
)
)
;
if
(
chunk
-
>
allocatedDuringCollection
)
{
return
false
;
}
return
chunk
-
>
setMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
startMinorCollection
(
MaybeLock
&
lock
)
{
maybeMergeSweptData
(
lock
)
;
#
ifdef
DEBUG
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
if
(
majorState
=
=
State
:
:
NotCollecting
)
{
checkGCStateNotInUse
(
lock
)
;
}
#
endif
MOZ_ASSERT
(
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
std
:
:
swap
(
largeNurseryAllocs
.
ref
(
)
largeNurseryAllocsToSweep
.
ref
(
)
)
;
minorState
=
State
:
:
Marking
;
}
bool
BufferAllocator
:
:
startMinorSweeping
(
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
minorState
=
=
State
:
:
Marking
)
;
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT
(
!
minorSweepingFinished
)
;
MOZ_ASSERT
(
sweptMediumMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
}
for
(
LargeBuffer
*
buffer
:
largeNurseryAllocs
.
ref
(
)
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
)
;
}
for
(
LargeBuffer
*
buffer
:
largeNurseryAllocsToSweep
.
ref
(
)
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
)
;
}
#
endif
if
(
mediumMixedChunks
.
ref
(
)
.
isEmpty
(
)
&
&
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
minorState
=
State
:
:
NotCollecting
;
return
false
;
}
mediumFreeLists
.
ref
(
)
.
eraseIf
(
[
]
(
FreeRegion
*
region
)
{
return
BufferChunk
:
:
from
(
region
)
-
>
hasNurseryOwnedAllocs
;
}
)
;
mediumMixedChunksToSweep
.
ref
(
)
=
std
:
:
move
(
mediumMixedChunks
.
ref
(
)
)
;
minorState
=
State
:
:
Sweeping
;
return
true
;
}
void
BufferAllocator
:
:
sweepForMinorCollection
(
)
{
MOZ_ASSERT
(
minorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
)
;
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT
(
sweptMediumMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
}
while
(
!
mediumMixedChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
BufferChunk
*
chunk
=
mediumMixedChunksToSweep
.
ref
(
)
.
popFirst
(
)
;
FreeLists
sweptFreeLists
;
if
(
sweepChunk
(
chunk
SweepKind
:
:
SweepNursery
false
sweptFreeLists
)
)
{
{
AutoLock
lock
(
this
)
;
sweptMediumMixedChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
if
(
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
)
{
sweptMediumNurseryFreeLists
.
ref
(
)
.
append
(
std
:
:
move
(
sweptFreeLists
)
)
;
}
else
{
sweptMediumTenuredFreeLists
.
ref
(
)
.
append
(
std
:
:
move
(
sweptFreeLists
)
)
;
}
}
hasMinorSweepDataToMerge
=
true
;
}
}
while
(
!
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
LargeBuffer
*
buffer
=
largeNurseryAllocsToSweep
.
ref
(
)
.
popFirst
(
)
;
MaybeLock
lock
(
std
:
:
in_place
this
)
;
unmapLarge
(
buffer
true
lock
)
;
}
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT
(
!
minorSweepingFinished
)
;
minorSweepingFinished
=
true
;
hasMinorSweepDataToMerge
=
true
;
}
}
void
BufferAllocator
:
:
startMajorCollection
(
MaybeLock
&
lock
)
{
maybeMergeSweptData
(
lock
)
;
#
ifdef
DEBUG
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
checkGCStateNotInUse
(
lock
)
;
MOZ_ASSERT
(
mediumMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeNurseryAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
#
endif
mediumTenuredChunksToSweep
.
ref
(
)
=
std
:
:
move
(
mediumTenuredChunks
.
ref
(
)
)
;
largeTenuredAllocsToSweep
.
ref
(
)
=
std
:
:
move
(
largeTenuredAllocs
.
ref
(
)
)
;
mediumFreeLists
.
ref
(
)
.
clear
(
)
;
if
(
minorState
=
=
State
:
:
Sweeping
)
{
majorStartedWhileMinorSweeping
=
true
;
}
#
ifdef
DEBUG
MOZ_ASSERT
(
mediumTenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
mediumFreeLists
.
ref
(
)
.
assertEmpty
(
)
;
MOZ_ASSERT
(
largeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
#
endif
majorState
=
State
:
:
Marking
;
}
void
BufferAllocator
:
:
startMajorSweeping
(
MaybeLock
&
lock
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
majorState
=
=
State
:
:
Marking
)
;
MOZ_ASSERT
(
zone
-
>
isGCFinished
(
)
)
;
MOZ_ASSERT
(
!
majorSweepingFinished
.
refNoCheck
(
)
)
;
#
endif
maybeMergeSweptData
(
lock
)
;
MOZ_ASSERT
(
!
majorStartedWhileMinorSweeping
)
;
majorState
=
State
:
:
Sweeping
;
}
void
BufferAllocator
:
:
sweepForMajorCollection
(
bool
shouldDecommit
)
{
MOZ_ASSERT
(
majorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
)
;
while
(
!
mediumTenuredChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
BufferChunk
*
chunk
=
mediumTenuredChunksToSweep
.
ref
(
)
.
popFirst
(
)
;
FreeLists
sweptFreeLists
;
if
(
sweepChunk
(
chunk
SweepKind
:
:
SweepTenured
shouldDecommit
sweptFreeLists
)
)
{
{
AutoLock
lock
(
this
)
;
sweptMediumTenuredChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
sweptMediumTenuredFreeLists
.
ref
(
)
.
append
(
std
:
:
move
(
sweptFreeLists
)
)
;
}
hasMinorSweepDataToMerge
=
true
;
}
}
LargeAllocList
sweptList
;
while
(
!
largeTenuredAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
LargeBuffer
*
buffer
=
largeTenuredAllocsToSweep
.
ref
(
)
.
popFirst
(
)
;
if
(
sweepLargeTenured
(
buffer
)
)
{
sweptList
.
pushBack
(
buffer
)
;
}
}
AutoLock
lock
(
this
)
;
sweptLargeTenuredAllocs
.
ref
(
)
=
std
:
:
move
(
sweptList
)
;
MOZ_ASSERT
(
!
majorSweepingFinished
)
;
majorSweepingFinished
=
true
;
}
static
void
ClearAllocatedDuringCollection
(
SlimLinkedList
<
BufferChunk
>
&
list
)
{
for
(
auto
*
buffer
:
list
)
{
buffer
-
>
allocatedDuringCollection
=
false
;
}
}
static
void
ClearAllocatedDuringCollection
(
SlimLinkedList
<
LargeBuffer
>
&
list
)
{
for
(
auto
*
element
:
list
)
{
element
-
>
allocatedDuringCollection
=
false
;
}
}
void
BufferAllocator
:
:
finishMajorCollection
(
const
AutoLock
&
lock
)
{
MOZ_ASSERT_IF
(
majorState
=
=
State
:
:
Sweeping
majorSweepingFinished
)
;
if
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
{
mergeSweptData
(
lock
)
;
}
if
(
majorState
=
=
State
:
:
Marking
)
{
abortMajorSweeping
(
lock
)
;
}
#
ifdef
DEBUG
checkGCStateNotInUse
(
lock
)
;
#
endif
}
void
BufferAllocator
:
:
abortMajorSweeping
(
const
AutoLock
&
lock
)
{
MOZ_ASSERT
(
majorState
=
=
State
:
:
Marking
)
;
MOZ_ASSERT
(
sweptMediumTenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
clearAllocatedDuringCollectionState
(
lock
)
;
for
(
BufferChunk
*
chunk
:
mediumTenuredChunksToSweep
.
ref
(
)
)
{
chunk
-
>
markBits
.
ref
(
)
.
clear
(
)
;
}
for
(
BufferChunk
*
chunk
:
mediumTenuredChunksToSweep
.
ref
(
)
)
{
MOZ_ALWAYS_TRUE
(
sweepChunk
(
chunk
SweepKind
:
:
RebuildFreeLists
false
mediumFreeLists
.
ref
(
)
)
)
;
}
mediumTenuredChunks
.
ref
(
)
.
prepend
(
std
:
:
move
(
mediumTenuredChunksToSweep
.
ref
(
)
)
)
;
largeTenuredAllocs
.
ref
(
)
.
prepend
(
std
:
:
move
(
largeTenuredAllocsToSweep
.
ref
(
)
)
)
;
majorState
=
State
:
:
NotCollecting
;
}
void
BufferAllocator
:
:
clearAllocatedDuringCollectionState
(
const
AutoLock
&
lock
)
{
ClearAllocatedDuringCollection
(
mediumMixedChunks
.
ref
(
)
)
;
ClearAllocatedDuringCollection
(
mediumTenuredChunks
.
ref
(
)
)
;
ClearAllocatedDuringCollection
(
largeTenuredAllocs
.
ref
(
)
)
;
}
void
BufferAllocator
:
:
maybeMergeSweptData
(
)
{
if
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
{
mergeSweptData
(
)
;
}
}
void
BufferAllocator
:
:
mergeSweptData
(
)
{
AutoLock
lock
(
this
)
;
mergeSweptData
(
lock
)
;
}
void
BufferAllocator
:
:
maybeMergeSweptData
(
MaybeLock
&
lock
)
{
if
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
{
if
(
lock
.
isNothing
(
)
)
{
lock
.
emplace
(
this
)
;
}
mergeSweptData
(
lock
.
ref
(
)
)
;
}
}
void
BufferAllocator
:
:
mergeSweptData
(
const
AutoLock
&
lock
)
{
MOZ_ASSERT
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
;
if
(
majorSweepingFinished
)
{
clearAllocatedDuringCollectionState
(
lock
)
;
if
(
minorState
=
=
State
:
:
Sweeping
)
{
majorFinishedWhileMinorSweeping
=
true
;
}
}
while
(
!
sweptMediumMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
{
BufferChunk
*
chunk
=
sweptMediumMixedChunks
.
ref
(
)
.
popLast
(
)
;
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
)
;
chunk
-
>
hasNurseryOwnedAllocs
=
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
;
MOZ_ASSERT_IF
(
majorState
=
=
State
:
:
NotCollecting
&
&
!
majorFinishedWhileMinorSweeping
!
chunk
-
>
allocatedDuringCollection
)
;
if
(
majorFinishedWhileMinorSweeping
)
{
chunk
-
>
allocatedDuringCollection
=
false
;
}
if
(
chunk
-
>
hasNurseryOwnedAllocs
)
{
mediumMixedChunks
.
ref
(
)
.
pushFront
(
chunk
)
;
}
else
if
(
majorStartedWhileMinorSweeping
)
{
mediumTenuredChunksToSweep
.
ref
(
)
.
pushFront
(
chunk
)
;
}
else
{
mediumTenuredChunks
.
ref
(
)
.
pushFront
(
chunk
)
;
}
}
#
ifdef
DEBUG
for
(
BufferChunk
*
chunk
:
sweptMediumTenuredChunks
.
ref
(
)
)
{
MOZ_ASSERT
(
!
chunk
-
>
hasNurseryOwnedAllocs
)
;
MOZ_ASSERT
(
!
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
)
;
MOZ_ASSERT
(
!
chunk
-
>
allocatedDuringCollection
)
;
}
#
endif
mediumTenuredChunks
.
ref
(
)
.
prepend
(
std
:
:
move
(
sweptMediumTenuredChunks
.
ref
(
)
)
)
;
mediumFreeLists
.
ref
(
)
.
prepend
(
std
:
:
move
(
sweptMediumNurseryFreeLists
.
ref
(
)
)
)
;
if
(
!
majorStartedWhileMinorSweeping
)
{
mediumFreeLists
.
ref
(
)
.
prepend
(
std
:
:
move
(
sweptMediumTenuredFreeLists
.
ref
(
)
)
)
;
}
else
{
sweptMediumTenuredFreeLists
.
ref
(
)
.
clear
(
)
;
}
largeTenuredAllocs
.
ref
(
)
.
prepend
(
std
:
:
move
(
sweptLargeTenuredAllocs
.
ref
(
)
)
)
;
hasMinorSweepDataToMerge
=
false
;
if
(
minorSweepingFinished
)
{
MOZ_ASSERT
(
minorState
=
=
State
:
:
Sweeping
)
;
minorState
=
State
:
:
NotCollecting
;
minorSweepingFinished
=
false
;
majorStartedWhileMinorSweeping
=
false
;
majorFinishedWhileMinorSweeping
=
false
;
#
ifdef
DEBUG
for
(
BufferChunk
*
chunk
:
mediumMixedChunks
.
ref
(
)
)
{
verifyChunk
(
chunk
true
)
;
}
for
(
BufferChunk
*
chunk
:
mediumTenuredChunks
.
ref
(
)
)
{
verifyChunk
(
chunk
false
)
;
}
#
endif
}
if
(
majorSweepingFinished
)
{
MOZ_ASSERT
(
majorState
=
=
State
:
:
Sweeping
)
;
majorState
=
State
:
:
NotCollecting
;
majorSweepingFinished
=
false
;
MOZ_ASSERT
(
mediumTenuredChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
}
}
void
BufferAllocator
:
:
prepareForMovingGC
(
)
{
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeTenuredAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
#
ifdef
DEBUG
MOZ_ASSERT
(
!
movingGCInProgress
)
;
movingGCInProgress
=
true
;
#
endif
for
(
auto
i
=
largeAllocMap
.
ref
(
)
.
iter
(
)
;
!
i
.
done
(
)
;
i
.
next
(
)
)
{
LargeBuffer
*
buffer
=
i
.
get
(
)
.
value
(
)
;
if
(
buffer
-
>
isNurseryOwned
)
{
largeNurseryAllocs
.
ref
(
)
.
remove
(
buffer
)
;
}
else
{
largeTenuredAllocs
.
ref
(
)
.
remove
(
buffer
)
;
}
}
MOZ_ASSERT
(
largeNurseryAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
}
void
BufferAllocator
:
:
fixupAfterMovingGC
(
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
movingGCInProgress
)
;
movingGCInProgress
=
false
;
#
endif
for
(
auto
i
=
largeAllocMap
.
ref
(
)
.
iter
(
)
;
!
i
.
done
(
)
;
i
.
next
(
)
)
{
LargeBuffer
*
buffer
=
i
.
get
(
)
.
value
(
)
;
SmallBuffer
*
headerCell
=
buffer
-
>
headerCell
(
)
;
if
(
IsForwarded
(
headerCell
)
)
{
headerCell
=
Forwarded
(
headerCell
)
;
buffer
=
static_cast
<
LargeBuffer
*
>
(
headerCell
-
>
data
(
)
)
;
i
.
get
(
)
.
value
(
)
=
buffer
;
}
MOZ_ASSERT
(
!
buffer
-
>
isInList
(
)
)
;
if
(
buffer
-
>
isNurseryOwned
)
{
largeNurseryAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
}
else
{
largeTenuredAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
}
}
}
void
BufferAllocator
:
:
clearMarkStateAfterBarrierVerification
(
)
{
MOZ_ASSERT
(
!
zone
-
>
wasGCStarted
(
)
)
;
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
for
(
auto
*
chunks
:
{
&
mediumMixedChunks
.
ref
(
)
&
mediumTenuredChunks
.
ref
(
)
}
)
{
for
(
auto
*
chunk
:
*
chunks
)
{
chunk
-
>
markBits
.
ref
(
)
.
clear
(
)
;
}
}
}
bool
BufferAllocator
:
:
isPointerWithinMediumOrLargeBuffer
(
void
*
ptr
)
{
maybeMergeSweptData
(
)
;
for
(
const
auto
*
chunks
:
{
&
mediumMixedChunks
.
ref
(
)
&
mediumTenuredChunks
.
ref
(
)
}
)
{
for
(
auto
*
chunk
:
*
chunks
)
{
if
(
chunk
-
>
isPointerWithinAllocation
(
ptr
)
)
{
return
true
;
}
}
}
if
(
majorState
=
=
State
:
:
Marking
)
{
for
(
auto
*
chunk
:
mediumTenuredChunksToSweep
.
ref
(
)
)
{
if
(
chunk
-
>
isPointerWithinAllocation
(
ptr
)
)
{
return
true
;
}
}
}
for
(
const
auto
*
allocs
:
{
&
largeNurseryAllocs
.
ref
(
)
&
largeTenuredAllocs
.
ref
(
)
}
)
{
for
(
auto
*
alloc
:
*
allocs
)
{
if
(
alloc
-
>
isPointerWithinAllocation
(
ptr
)
)
{
return
true
;
}
}
}
return
false
;
}
bool
BufferChunk
:
:
isPointerWithinAllocation
(
void
*
ptr
)
const
{
uintptr_t
offset
=
uintptr_t
(
ptr
)
-
uintptr_t
(
this
)
;
if
(
offset
>
=
ChunkSize
|
|
offset
<
FirstMediumAllocOffset
)
{
return
false
;
}
uintptr_t
allocOffset
=
findPrevAllocated
(
offset
)
;
MOZ_ASSERT
(
allocOffset
<
=
ChunkSize
)
;
if
(
allocOffset
=
=
ChunkSize
)
{
return
false
;
}
const
void
*
alloc
=
ptrFromOffset
(
allocOffset
)
;
size_t
size
=
allocBytes
(
alloc
)
;
return
offset
<
allocOffset
+
size
;
}
bool
LargeBuffer
:
:
isPointerWithinAllocation
(
void
*
ptr
)
const
{
return
uintptr_t
(
ptr
)
-
uintptr_t
(
alloc
)
<
bytes
;
}
#
ifdef
DEBUG
void
BufferAllocator
:
:
checkGCStateNotInUse
(
)
{
maybeMergeSweptData
(
)
;
AutoLock
lock
(
this
)
;
checkGCStateNotInUse
(
lock
)
;
}
void
BufferAllocator
:
:
checkGCStateNotInUse
(
MaybeLock
&
maybeLock
)
{
if
(
maybeLock
.
isNothing
(
)
)
{
maybeLock
.
emplace
(
this
)
;
}
checkGCStateNotInUse
(
maybeLock
.
ref
(
)
)
;
}
void
BufferAllocator
:
:
checkGCStateNotInUse
(
const
AutoLock
&
lock
)
{
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
bool
isNurserySweeping
=
minorState
=
=
State
:
:
Sweeping
;
checkChunkListGCStateNotInUse
(
mediumMixedChunks
.
ref
(
)
true
false
)
;
checkChunkListGCStateNotInUse
(
mediumTenuredChunks
.
ref
(
)
false
false
)
;
if
(
isNurserySweeping
)
{
checkChunkListGCStateNotInUse
(
sweptMediumMixedChunks
.
ref
(
)
true
majorFinishedWhileMinorSweeping
)
;
checkChunkListGCStateNotInUse
(
sweptMediumTenuredChunks
.
ref
(
)
false
false
)
;
}
else
{
MOZ_ASSERT
(
mediumMixedChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
sweptMediumMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
sweptMediumTenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
sweptMediumNurseryFreeLists
.
ref
(
)
.
assertEmpty
(
)
;
sweptMediumTenuredFreeLists
.
ref
(
)
.
assertEmpty
(
)
;
MOZ_ASSERT
(
!
majorStartedWhileMinorSweeping
)
;
MOZ_ASSERT
(
!
majorFinishedWhileMinorSweeping
)
;
MOZ_ASSERT
(
!
hasMinorSweepDataToMerge
)
;
MOZ_ASSERT
(
!
minorSweepingFinished
)
;
MOZ_ASSERT
(
!
majorSweepingFinished
)
;
}
MOZ_ASSERT
(
mediumTenuredChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
checkAllocListGCStateNotInUse
(
largeNurseryAllocs
.
ref
(
)
true
)
;
checkAllocListGCStateNotInUse
(
largeTenuredAllocs
.
ref
(
)
false
)
;
MOZ_ASSERT
(
largeTenuredAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
sweptLargeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
!
movingGCInProgress
)
;
}
void
BufferAllocator
:
:
checkChunkListGCStateNotInUse
(
BufferChunkList
&
chunks
bool
hasNurseryOwnedAllocs
bool
allowAllocatedDuringCollection
)
{
for
(
BufferChunk
*
chunk
:
chunks
)
{
checkChunkGCStateNotInUse
(
chunk
allowAllocatedDuringCollection
)
;
verifyChunk
(
chunk
hasNurseryOwnedAllocs
)
;
}
}
void
BufferAllocator
:
:
checkChunkGCStateNotInUse
(
BufferChunk
*
chunk
bool
allowAllocatedDuringCollection
)
{
MOZ_ASSERT_IF
(
!
allowAllocatedDuringCollection
!
chunk
-
>
allocatedDuringCollection
)
;
MOZ_ASSERT
(
chunk
-
>
markBits
.
ref
(
)
.
isEmpty
(
)
)
;
}
void
BufferAllocator
:
:
verifyChunk
(
BufferChunk
*
chunk
bool
hasNurseryOwnedAllocs
)
{
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
=
=
hasNurseryOwnedAllocs
)
;
static
constexpr
size_t
StepBytes
=
MediumAllocGranularity
;
size_t
freeOffset
=
FirstMediumAllocOffset
;
for
(
BufferChunk
:
:
AllocIter
iter
(
chunk
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
size_t
offset
=
iter
.
getOffset
(
)
;
MOZ_ASSERT
(
offset
>
=
FirstMediumAllocOffset
)
;
if
(
offset
>
freeOffset
)
{
verifyFreeRegion
(
chunk
offset
offset
-
freeOffset
)
;
}
void
*
alloc
=
iter
.
get
(
)
;
MOZ_ASSERT_IF
(
chunk
-
>
isNurseryOwned
(
alloc
)
hasNurseryOwnedAllocs
)
;
size_t
bytes
=
chunk
-
>
allocBytes
(
alloc
)
;
uintptr_t
endOffset
=
offset
+
bytes
;
MOZ_ASSERT
(
endOffset
<
=
ChunkSize
)
;
for
(
size_t
i
=
offset
+
StepBytes
;
i
<
endOffset
;
i
+
=
StepBytes
)
{
MOZ_ASSERT
(
!
chunk
-
>
isAllocated
(
i
)
)
;
}
freeOffset
=
endOffset
;
}
if
(
freeOffset
<
ChunkSize
)
{
verifyFreeRegion
(
chunk
ChunkSize
ChunkSize
-
freeOffset
)
;
}
}
void
BufferAllocator
:
:
verifyFreeRegion
(
BufferChunk
*
chunk
uintptr_t
endOffset
size_t
expectedSize
)
{
auto
*
freeRegion
=
FreeRegion
:
:
fromEndOffset
(
chunk
endOffset
)
;
MOZ_ASSERT
(
freeRegion
-
>
isInList
(
)
)
;
MOZ_ASSERT
(
freeRegion
-
>
size
(
)
=
=
expectedSize
)
;
}
void
BufferAllocator
:
:
checkAllocListGCStateNotInUse
(
LargeAllocList
&
list
bool
isNurseryOwned
)
{
for
(
LargeBuffer
*
buffer
:
list
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
=
=
isNurseryOwned
)
;
MOZ_ASSERT_IF
(
!
isNurseryOwned
!
buffer
-
>
allocatedDuringCollection
)
;
}
}
#
endif
void
*
BufferAllocator
:
:
allocSmall
(
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
AllocKind
kind
=
AllocKindForSmallAlloc
(
bytes
)
;
void
*
ptr
;
if
(
inGC
)
{
ptr
=
AllocateTenuredCellInGC
(
zone
kind
)
;
}
else
{
ptr
=
CellAllocator
:
:
AllocTenuredCellUnchecked
<
NoGC
>
(
zone
kind
)
;
}
if
(
!
ptr
)
{
return
nullptr
;
}
auto
*
cell
=
new
(
ptr
)
SmallBuffer
(
)
;
cell
-
>
setNurseryOwned
(
nurseryOwned
)
;
MOZ_ASSERT
(
cell
-
>
isNurseryOwned
(
)
=
=
nurseryOwned
)
;
void
*
alloc
=
cell
-
>
data
(
)
;
MOZ_ASSERT
(
IsSmallAlloc
(
alloc
)
)
;
MOZ_ASSERT
(
getAllocSize
(
alloc
)
>
=
bytes
)
;
MOZ_ASSERT
(
getAllocSize
(
alloc
)
<
2
*
bytes
)
;
return
alloc
;
}
AllocKind
BufferAllocator
:
:
AllocKindForSmallAlloc
(
size_t
bytes
)
{
bytes
=
std
:
:
max
(
bytes
MinAllocSize
)
;
MOZ_ASSERT
(
bytes
<
=
MaxSmallAllocSize
)
;
size_t
logBytes
=
mozilla
:
:
CeilingLog2
(
bytes
)
;
MOZ_ASSERT
(
bytes
<
=
(
size_t
(
1
)
<
<
logBytes
)
)
;
MOZ_ASSERT
(
logBytes
>
=
mozilla
:
:
CeilingLog2
(
MinAllocSize
)
)
;
size_t
kindIndex
=
logBytes
-
mozilla
:
:
CeilingLog2
(
MinAllocSize
)
;
AllocKind
kind
=
AllocKind
(
size_t
(
AllocKind
:
:
BUFFER_FIRST
)
+
kindIndex
)
;
MOZ_ASSERT
(
IsBufferAllocKind
(
kind
)
)
;
return
kind
;
}
bool
BufferAllocator
:
:
IsSmallAlloc
(
void
*
alloc
)
{
MOZ_ASSERT
(
IsBufferAlloc
(
alloc
)
)
;
MOZ_ASSERT
(
!
IsLargeAlloc
(
alloc
)
)
;
ChunkBase
*
chunk
=
detail
:
:
GetGCAddressChunkBase
(
alloc
)
;
return
chunk
-
>
getKind
(
)
=
=
ChunkKind
:
:
TenuredArenas
;
}
void
*
BufferAllocator
:
:
allocMedium
(
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
MOZ_ASSERT
(
!
IsSmallAllocSize
(
bytes
)
)
;
MOZ_ASSERT
(
!
IsLargeAllocSize
(
bytes
)
)
;
bytes
=
MediumBufferSize
(
bytes
)
.
get
(
)
;
size_t
sizeClass
=
SizeClassForAlloc
(
bytes
)
;
void
*
alloc
=
bumpAlloc
(
bytes
sizeClass
)
;
if
(
MOZ_UNLIKELY
(
!
alloc
)
)
{
alloc
=
retryBumpAlloc
(
bytes
sizeClass
inGC
)
;
if
(
!
alloc
)
{
return
nullptr
;
}
}
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
chunk
-
>
setAllocated
(
alloc
true
)
;
MOZ_ASSERT
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
;
chunk
-
>
setNurseryOwned
(
alloc
nurseryOwned
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
0
)
;
chunk
-
>
setAllocBytes
(
alloc
bytes
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
bytes
)
;
if
(
nurseryOwned
&
&
!
chunk
-
>
hasNurseryOwnedAllocs
)
{
mediumTenuredChunks
.
ref
(
)
.
remove
(
chunk
)
;
chunk
-
>
hasNurseryOwnedAllocs
=
true
;
mediumMixedChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
}
MOZ_ASSERT
(
!
chunk
-
>
isMarked
(
alloc
)
)
;
if
(
!
nurseryOwned
)
{
bool
checkThresholds
=
!
inGC
;
updateHeapSize
(
bytes
checkThresholds
false
)
;
}
return
alloc
;
}
MOZ_NEVER_INLINE
void
*
BufferAllocator
:
:
retryBumpAlloc
(
size_t
bytes
size_t
sizeClass
bool
inGC
)
{
if
(
hasMinorSweepDataToMerge
)
{
mergeSweptData
(
)
;
void
*
ptr
=
bumpAlloc
(
bytes
sizeClass
)
;
if
(
ptr
)
{
return
ptr
;
}
}
if
(
!
allocNewChunk
(
inGC
)
)
{
return
nullptr
;
}
void
*
ptr
=
bumpAlloc
(
bytes
sizeClass
)
;
MOZ_ASSERT
(
ptr
)
;
return
ptr
;
}
void
*
BufferAllocator
:
:
bumpAlloc
(
size_t
bytes
size_t
sizeClass
)
{
mediumFreeLists
.
ref
(
)
.
checkAvailable
(
)
;
sizeClass
=
mediumFreeLists
.
ref
(
)
.
getFirstAvailableSizeClass
(
sizeClass
)
;
if
(
sizeClass
=
=
SIZE_MAX
)
{
return
nullptr
;
}
FreeRegion
*
region
=
mediumFreeLists
.
ref
(
)
.
getFirstRegion
(
sizeClass
)
;
void
*
ptr
=
allocFromRegion
(
region
bytes
sizeClass
)
;
updateFreeListsAfterAlloc
(
&
mediumFreeLists
.
ref
(
)
region
sizeClass
)
;
return
ptr
;
}
void
*
BufferAllocator
:
:
allocFromRegion
(
FreeRegion
*
region
size_t
bytes
size_t
sizeClass
)
{
uintptr_t
start
=
region
-
>
startAddr
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
>
start
)
;
MOZ_ASSERT
(
region
-
>
size
(
)
>
=
SizeClassBytes
(
sizeClass
)
)
;
MOZ_ASSERT
(
(
region
-
>
size
(
)
%
MediumAllocGranularity
)
=
=
0
)
;
if
(
region
-
>
hasDecommittedPages
)
{
recommitRegion
(
region
)
;
}
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
start
)
;
start
+
=
bytes
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
>
=
start
)
;
region
-
>
startAddr
=
start
;
return
ptr
;
}
void
BufferAllocator
:
:
updateFreeListsAfterAlloc
(
FreeLists
*
freeLists
FreeRegion
*
region
size_t
sizeClass
)
{
freeLists
-
>
assertContains
(
sizeClass
region
)
;
size_t
classBytes
=
SizeClassBytes
(
sizeClass
)
;
size_t
newSize
=
region
-
>
size
(
)
;
MOZ_ASSERT
(
(
newSize
%
MediumAllocGranularity
)
=
=
0
)
;
if
(
newSize
>
=
classBytes
)
{
return
;
}
freeLists
-
>
remove
(
sizeClass
region
)
;
if
(
newSize
=
=
0
)
{
return
;
}
size_t
newSizeClass
=
SizeClassForFreeRegion
(
newSize
)
;
MOZ_ASSERT
(
newSize
>
=
SizeClassBytes
(
newSizeClass
)
)
;
MOZ_ASSERT
(
newSizeClass
<
sizeClass
)
;
freeLists
-
>
pushFront
(
newSizeClass
region
)
;
}
void
BufferAllocator
:
:
recommitRegion
(
FreeRegion
*
region
)
{
MOZ_ASSERT
(
region
-
>
hasDecommittedPages
)
;
MOZ_ASSERT
(
DecommitEnabled
(
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
region
)
;
uintptr_t
startAddr
=
RoundUp
(
region
-
>
startAddr
PageSize
)
;
uintptr_t
endAddr
=
RoundDown
(
uintptr_t
(
region
)
PageSize
)
;
size_t
startPage
=
(
startAddr
-
uintptr_t
(
chunk
)
)
/
PageSize
;
size_t
endPage
=
(
endAddr
-
uintptr_t
(
chunk
)
)
/
PageSize
;
MOZ_ASSERT_IF
(
(
region
-
>
startAddr
%
PageSize
)
!
=
0
!
chunk
-
>
decommittedPages
.
ref
(
)
[
startPage
-
1
]
)
;
MOZ_ASSERT
(
!
chunk
-
>
decommittedPages
.
ref
(
)
[
endPage
]
)
;
MarkPagesInUseSoft
(
reinterpret_cast
<
void
*
>
(
startAddr
)
endAddr
-
startAddr
)
;
for
(
size_t
i
=
startPage
;
i
!
=
endPage
;
i
+
+
)
{
chunk
-
>
decommittedPages
.
ref
(
)
[
i
]
=
false
;
}
region
-
>
hasDecommittedPages
=
false
;
}
static
inline
StallAndRetry
ShouldStallAndRetry
(
bool
inGC
)
{
return
inGC
?
StallAndRetry
:
:
Yes
:
StallAndRetry
:
:
No
;
}
bool
BufferAllocator
:
:
allocNewChunk
(
bool
inGC
)
{
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromMainThread
(
)
-
>
gc
;
AutoLockGCBgAlloc
lock
(
gc
)
;
ArenaChunk
*
baseChunk
=
gc
-
>
getOrAllocChunk
(
ShouldStallAndRetry
(
inGC
)
lock
)
;
if
(
!
baseChunk
)
{
return
false
;
}
CheckHighBitsOfPointer
(
baseChunk
)
;
if
(
!
baseChunk
-
>
decommittedPages
.
IsEmpty
(
)
)
{
MOZ_ASSERT
(
DecommitEnabled
(
)
)
;
MarkPagesInUseSoft
(
baseChunk
ChunkSize
)
;
}
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
uintptr_t
(
baseChunk
)
+
sizeof
(
ChunkBase
)
)
;
size_t
size
=
ChunkSize
-
sizeof
(
ChunkBase
)
;
SetMemCheckKind
(
ptr
size
MemCheckKind
:
:
MakeUndefined
)
;
BufferChunk
*
chunk
=
new
(
baseChunk
)
BufferChunk
(
zone
)
;
chunk
-
>
allocatedDuringCollection
=
majorState
!
=
State
:
:
NotCollecting
;
mediumTenuredChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
uintptr_t
freeStart
=
uintptr_t
(
chunk
)
+
FirstMediumAllocOffset
;
uintptr_t
freeEnd
=
uintptr_t
(
chunk
)
+
ChunkSize
;
size_t
sizeClass
=
SizeClassForFreeRegion
(
freeEnd
-
freeStart
)
;
ptr
=
reinterpret_cast
<
void
*
>
(
freeEnd
-
sizeof
(
FreeRegion
)
)
;
FreeRegion
*
region
=
new
(
ptr
)
FreeRegion
(
freeStart
)
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
=
=
freeEnd
)
;
mediumFreeLists
.
ref
(
)
.
pushFront
(
sizeClass
region
)
;
return
true
;
}
bool
BufferAllocator
:
:
sweepChunk
(
BufferChunk
*
chunk
SweepKind
sweepKind
bool
shouldDecommit
FreeLists
&
freeLists
)
{
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromAnyThread
(
)
-
>
gc
;
bool
hasNurseryOwnedAllocs
=
false
;
size_t
freeStart
=
FirstMediumAllocOffset
;
bool
sweptAny
=
false
;
size_t
mallocHeapBytesFreed
=
0
;
for
(
BufferChunk
:
:
AllocIter
iter
(
chunk
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
void
*
alloc
=
iter
.
get
(
)
;
size_t
bytes
=
chunk
-
>
allocBytes
(
alloc
)
;
uintptr_t
allocEnd
=
iter
.
getOffset
(
)
+
bytes
;
bool
nurseryOwned
=
chunk
-
>
isNurseryOwned
(
alloc
)
;
bool
canSweep
=
CanSweepAlloc
(
nurseryOwned
sweepKind
)
;
bool
shouldSweep
=
canSweep
&
&
!
chunk
-
>
isMarked
(
alloc
)
;
if
(
shouldSweep
)
{
if
(
!
nurseryOwned
)
{
mallocHeapBytesFreed
+
=
bytes
;
}
chunk
-
>
setNurseryOwned
(
alloc
false
)
;
chunk
-
>
setAllocBytes
(
alloc
0
)
;
chunk
-
>
setAllocated
(
alloc
false
)
;
PoisonAlloc
(
alloc
JS_SWEPT_TENURED_PATTERN
bytes
MemCheckKind
:
:
MakeUndefined
)
;
sweptAny
=
true
;
}
else
{
uintptr_t
allocStart
=
iter
.
getOffset
(
)
;
if
(
freeStart
!
=
allocStart
)
{
addSweptRegion
(
chunk
freeStart
allocStart
shouldDecommit
!
sweptAny
freeLists
)
;
}
freeStart
=
allocEnd
;
if
(
canSweep
)
{
chunk
-
>
setUnmarked
(
alloc
)
;
}
if
(
nurseryOwned
)
{
MOZ_ASSERT
(
sweepKind
=
=
SweepKind
:
:
SweepNursery
)
;
hasNurseryOwnedAllocs
=
true
;
}
sweptAny
=
false
;
}
}
if
(
mallocHeapBytesFreed
)
{
zone
-
>
mallocHeapSize
.
removeBytes
(
mallocHeapBytesFreed
true
)
;
}
if
(
freeStart
=
=
FirstMediumAllocOffset
&
&
sweepKind
!
=
SweepKind
:
:
RebuildFreeLists
)
{
bool
allMemoryCommitted
=
chunk
-
>
decommittedPages
.
ref
(
)
.
IsEmpty
(
)
;
chunk
-
>
~
BufferChunk
(
)
;
ArenaChunk
*
tenuredChunk
=
ArenaChunk
:
:
init
(
chunk
gc
allMemoryCommitted
)
;
AutoLockGC
lock
(
gc
)
;
gc
-
>
recycleChunk
(
tenuredChunk
lock
)
;
return
false
;
}
if
(
freeStart
!
=
ChunkSize
)
{
addSweptRegion
(
chunk
freeStart
ChunkSize
shouldDecommit
!
sweptAny
freeLists
)
;
}
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
=
hasNurseryOwnedAllocs
;
return
true
;
}
bool
BufferAllocator
:
:
CanSweepAlloc
(
bool
nurseryOwned
BufferAllocator
:
:
SweepKind
sweepKind
)
{
static_assert
(
SweepKind
:
:
SweepNursery
=
=
SweepKind
(
uint8_t
(
true
)
)
)
;
static_assert
(
SweepKind
:
:
SweepTenured
=
=
SweepKind
(
uint8_t
(
false
)
)
)
;
SweepKind
requiredKind
=
SweepKind
(
uint8_t
(
nurseryOwned
)
)
;
return
sweepKind
=
=
requiredKind
;
}
void
BufferAllocator
:
:
addSweptRegion
(
BufferChunk
*
chunk
uintptr_t
freeStart
uintptr_t
freeEnd
bool
shouldDecommit
bool
expectUnchanged
FreeLists
&
freeLists
)
{
MOZ_ASSERT
(
freeStart
>
=
FirstMediumAllocOffset
)
;
MOZ_ASSERT
(
freeStart
<
freeEnd
)
;
MOZ_ASSERT
(
freeEnd
<
=
ChunkSize
)
;
MOZ_ASSERT
(
(
freeStart
%
MediumAllocGranularity
)
=
=
0
)
;
MOZ_ASSERT
(
(
freeEnd
%
MediumAllocGranularity
)
=
=
0
)
;
MOZ_ASSERT_IF
(
shouldDecommit
DecommitEnabled
(
)
)
;
bool
anyDecommitted
=
false
;
uintptr_t
decommitStart
=
RoundUp
(
freeStart
PageSize
)
;
uintptr_t
decommitEnd
=
RoundDown
(
freeEnd
-
sizeof
(
FreeRegion
)
PageSize
)
;
size_t
endPage
=
decommitEnd
/
PageSize
;
if
(
shouldDecommit
&
&
decommitEnd
>
decommitStart
)
{
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
decommitStart
+
uintptr_t
(
chunk
)
)
;
MarkPagesUnusedSoft
(
ptr
decommitEnd
-
decommitStart
)
;
size_t
startPage
=
decommitStart
/
PageSize
;
for
(
size_t
i
=
startPage
;
i
!
=
endPage
;
i
+
+
)
{
chunk
-
>
decommittedPages
.
ref
(
)
[
i
]
=
true
;
}
anyDecommitted
=
true
;
}
else
{
uintptr_t
startPage
=
RoundDown
(
freeStart
PageSize
)
/
PageSize
;
for
(
size_t
i
=
startPage
;
i
!
=
endPage
;
i
+
+
)
{
if
(
chunk
-
>
decommittedPages
.
ref
(
)
[
i
]
)
{
anyDecommitted
=
true
;
}
}
}
MOZ_ASSERT
(
!
chunk
-
>
decommittedPages
.
ref
(
)
[
endPage
]
)
;
freeStart
+
=
uintptr_t
(
chunk
)
;
freeEnd
+
=
uintptr_t
(
chunk
)
;
addFreeRegion
(
&
freeLists
freeStart
freeEnd
-
freeStart
anyDecommitted
ListPosition
:
:
Back
expectUnchanged
)
;
}
void
BufferAllocator
:
:
freeMedium
(
void
*
alloc
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
size_t
bytes
=
chunk
-
>
allocBytes
(
alloc
)
;
PoisonAlloc
(
alloc
JS_FREED_BUFFER_PATTERN
bytes
MemCheckKind
:
:
MakeUndefined
)
;
if
(
isSweepingChunk
(
chunk
)
)
{
return
;
}
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bool
updateRetained
=
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
;
zone
-
>
mallocHeapSize
.
removeBytes
(
bytes
updateRetained
)
;
}
chunk
-
>
setNurseryOwned
(
alloc
false
)
;
chunk
-
>
setAllocBytes
(
alloc
0
)
;
chunk
-
>
setUnmarked
(
alloc
)
;
chunk
-
>
setAllocated
(
alloc
false
)
;
FreeLists
*
freeLists
=
getChunkFreeLists
(
chunk
)
;
uintptr_t
startAddr
=
uintptr_t
(
alloc
)
;
uintptr_t
endAddr
=
startAddr
+
bytes
;
FreeRegion
*
region
;
uintptr_t
endOffset
=
endAddr
&
ChunkMask
;
if
(
endOffset
=
=
0
|
|
chunk
-
>
isAllocated
(
endOffset
)
)
{
region
=
addFreeRegion
(
freeLists
startAddr
bytes
false
ListPosition
:
:
Front
)
;
}
else
{
region
=
chunk
-
>
findFollowingFreeRegion
(
endAddr
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
endAddr
)
;
updateFreeRegionStart
(
freeLists
region
startAddr
)
;
}
FreeRegion
*
precRegion
=
chunk
-
>
findPrecedingFreeRegion
(
startAddr
)
;
if
(
precRegion
)
{
if
(
freeLists
)
{
size_t
sizeClass
=
SizeClassForFreeRegion
(
precRegion
-
>
size
(
)
)
;
freeLists
-
>
remove
(
sizeClass
precRegion
)
;
}
updateFreeRegionStart
(
freeLists
region
precRegion
-
>
startAddr
)
;
if
(
precRegion
-
>
hasDecommittedPages
)
{
region
-
>
hasDecommittedPages
=
true
;
}
}
}
bool
BufferAllocator
:
:
isSweepingChunk
(
BufferChunk
*
chunk
)
{
if
(
minorState
=
=
State
:
:
Sweeping
&
&
chunk
-
>
hasNurseryOwnedAllocs
)
{
if
(
!
hasMinorSweepDataToMerge
)
{
#
ifdef
DEBUG
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT_IF
(
!
hasMinorSweepDataToMerge
!
minorSweepingFinished
)
;
}
#
endif
return
true
;
}
mergeSweptData
(
)
;
if
(
minorState
=
=
State
:
:
Sweeping
&
&
chunk
-
>
hasNurseryOwnedAllocs
)
{
return
true
;
}
}
if
(
majorState
=
=
State
:
:
Sweeping
&
&
!
chunk
-
>
allocatedDuringCollection
)
{
return
true
;
}
return
false
;
}
BufferAllocator
:
:
FreeRegion
*
BufferAllocator
:
:
addFreeRegion
(
FreeLists
*
freeLists
uintptr_t
start
size_t
bytes
bool
anyDecommitted
ListPosition
position
bool
expectUnchanged
)
{
uintptr_t
end
=
start
+
bytes
;
#
ifdef
DEBUG
if
(
expectUnchanged
)
{
auto
*
region
=
FreeRegion
:
:
fromEndAddr
(
end
)
;
region
-
>
check
(
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
start
)
;
}
#
endif
size_t
sizeClass
=
SizeClassForFreeRegion
(
bytes
)
;
MOZ_ASSERT
(
bytes
>
=
SizeClassBytes
(
sizeClass
)
)
;
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
end
-
sizeof
(
FreeRegion
)
)
;
FreeRegion
*
region
=
new
(
ptr
)
FreeRegion
(
start
anyDecommitted
)
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
=
=
end
)
;
if
(
freeLists
)
{
if
(
position
=
=
ListPosition
:
:
Front
)
{
freeLists
-
>
pushFront
(
sizeClass
region
)
;
}
else
{
freeLists
-
>
pushBack
(
sizeClass
region
)
;
}
}
return
region
;
}
void
BufferAllocator
:
:
updateFreeRegionStart
(
FreeLists
*
freeLists
FreeRegion
*
region
uintptr_t
newStart
)
{
MOZ_ASSERT
(
(
newStart
&
~
ChunkMask
)
=
=
(
uintptr_t
(
region
)
&
~
ChunkMask
)
)
;
MOZ_ASSERT
(
region
-
>
startAddr
!
=
newStart
)
;
size_t
oldSize
=
region
-
>
size
(
)
;
region
-
>
startAddr
=
newStart
;
if
(
!
freeLists
)
{
return
;
}
size_t
currentSizeClass
=
SizeClassForFreeRegion
(
oldSize
)
;
size_t
newSizeClass
=
SizeClassForFreeRegion
(
region
-
>
size
(
)
)
;
if
(
currentSizeClass
!
=
newSizeClass
)
{
freeLists
-
>
remove
(
currentSizeClass
region
)
;
freeLists
-
>
pushFront
(
newSizeClass
region
)
;
}
}
bool
BufferAllocator
:
:
growMedium
(
void
*
alloc
size_t
newBytes
)
{
MOZ_ASSERT
(
!
IsSmallAllocSize
(
newBytes
)
)
;
MOZ_ASSERT
(
!
IsLargeAllocSize
(
newBytes
)
)
;
newBytes
=
std
:
:
max
(
newBytes
MinMediumAllocSize
)
;
MOZ_ASSERT
(
newBytes
=
=
GetGoodAllocSize
(
newBytes
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
if
(
isSweepingChunk
(
chunk
)
)
{
return
false
;
}
size_t
currentBytes
=
chunk
-
>
allocBytes
(
alloc
)
;
MOZ_ASSERT
(
newBytes
>
currentBytes
)
;
uintptr_t
endOffset
=
(
uintptr_t
(
alloc
)
&
ChunkMask
)
+
currentBytes
;
MOZ_ASSERT
(
endOffset
<
=
ChunkSize
)
;
if
(
endOffset
=
=
ChunkSize
)
{
return
false
;
}
size_t
endAddr
=
uintptr_t
(
chunk
)
+
endOffset
;
if
(
chunk
-
>
isAllocated
(
endOffset
)
)
{
return
false
;
}
FreeRegion
*
region
=
chunk
-
>
findFollowingFreeRegion
(
endAddr
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
endAddr
)
;
size_t
extraBytes
=
newBytes
-
currentBytes
;
if
(
region
-
>
size
(
)
<
extraBytes
)
{
return
false
;
}
size_t
sizeClass
=
SizeClassForFreeRegion
(
region
-
>
size
(
)
)
;
allocFromRegion
(
region
extraBytes
sizeClass
)
;
if
(
FreeLists
*
freeLists
=
getChunkFreeLists
(
chunk
)
)
{
updateFreeListsAfterAlloc
(
freeLists
region
sizeClass
)
;
}
chunk
-
>
setAllocBytes
(
alloc
newBytes
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
newBytes
)
;
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bool
updateRetained
=
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
;
updateHeapSize
(
extraBytes
true
updateRetained
)
;
}
return
true
;
}
bool
BufferAllocator
:
:
shrinkMedium
(
void
*
alloc
size_t
newBytes
)
{
MOZ_ASSERT
(
!
IsSmallAllocSize
(
newBytes
)
)
;
MOZ_ASSERT
(
!
IsLargeAllocSize
(
newBytes
)
)
;
newBytes
=
std
:
:
max
(
newBytes
MinMediumAllocSize
)
;
MOZ_ASSERT
(
newBytes
=
=
GetGoodAllocSize
(
newBytes
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
if
(
isSweepingChunk
(
chunk
)
)
{
return
false
;
}
size_t
currentBytes
=
chunk
-
>
allocBytes
(
alloc
)
;
if
(
newBytes
=
=
currentBytes
)
{
return
false
;
}
MOZ_ASSERT
(
newBytes
<
currentBytes
)
;
size_t
sizeChange
=
currentBytes
-
newBytes
;
chunk
-
>
setAllocBytes
(
alloc
newBytes
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
newBytes
)
;
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bool
updateRetained
=
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
;
zone
-
>
mallocHeapSize
.
removeBytes
(
sizeChange
updateRetained
)
;
}
uintptr_t
startOffset
=
uintptr_t
(
alloc
)
&
ChunkMask
;
uintptr_t
oldEndOffset
=
startOffset
+
currentBytes
;
uintptr_t
newEndOffset
=
startOffset
+
newBytes
;
MOZ_ASSERT
(
oldEndOffset
<
=
ChunkSize
)
;
uintptr_t
chunkAddr
=
uintptr_t
(
chunk
)
;
PoisonAlloc
(
reinterpret_cast
<
void
*
>
(
chunkAddr
+
newEndOffset
)
JS_SWEPT_TENURED_PATTERN
sizeChange
MemCheckKind
:
:
MakeUndefined
)
;
FreeLists
*
freeLists
=
getChunkFreeLists
(
chunk
)
;
if
(
oldEndOffset
=
=
ChunkSize
|
|
chunk
-
>
isAllocated
(
oldEndOffset
)
)
{
uintptr_t
freeStart
=
chunkAddr
+
newEndOffset
;
addFreeRegion
(
freeLists
freeStart
sizeChange
false
ListPosition
:
:
Front
)
;
return
true
;
}
FreeRegion
*
region
=
chunk
-
>
findFollowingFreeRegion
(
chunkAddr
+
oldEndOffset
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
chunkAddr
+
oldEndOffset
)
;
updateFreeRegionStart
(
freeLists
region
chunkAddr
+
newEndOffset
)
;
return
true
;
}
BufferAllocator
:
:
FreeLists
*
BufferAllocator
:
:
getChunkFreeLists
(
BufferChunk
*
chunk
)
{
MOZ_ASSERT_IF
(
majorState
=
=
State
:
:
Sweeping
chunk
-
>
allocatedDuringCollection
)
;
if
(
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
)
{
return
nullptr
;
}
return
&
mediumFreeLists
.
ref
(
)
;
}
BufferAllocator
:
:
FreeRegion
*
BufferChunk
:
:
findFollowingFreeRegion
(
uintptr_t
startAddr
)
{
uintptr_t
offset
=
uintptr_t
(
startAddr
)
&
ChunkMask
;
MOZ_ASSERT
(
isValidOffset
(
offset
)
)
;
MOZ_ASSERT
(
(
offset
%
MediumAllocGranularity
)
=
=
0
)
;
MOZ_ASSERT
(
!
isAllocated
(
offset
)
)
;
offset
=
findNextAllocated
(
offset
)
;
MOZ_ASSERT
(
offset
<
=
ChunkSize
)
;
auto
*
region
=
FreeRegion
:
:
fromEndOffset
(
this
offset
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
startAddr
)
;
return
region
;
}
BufferAllocator
:
:
FreeRegion
*
BufferChunk
:
:
findPrecedingFreeRegion
(
uintptr_t
endAddr
)
{
uintptr_t
offset
=
uintptr_t
(
endAddr
)
&
ChunkMask
;
MOZ_ASSERT
(
isValidOffset
(
offset
)
)
;
MOZ_ASSERT
(
(
offset
%
MediumAllocGranularity
)
=
=
0
)
;
if
(
offset
=
=
FirstMediumAllocOffset
)
{
return
nullptr
;
}
MOZ_ASSERT
(
!
isAllocated
(
offset
)
)
;
offset
=
findPrevAllocated
(
offset
)
;
if
(
offset
!
=
ChunkSize
)
{
const
void
*
alloc
=
ptrFromOffset
(
offset
)
;
size_t
bytes
=
allocBytes
(
alloc
)
;
MOZ_ASSERT
(
uintptr_t
(
alloc
)
+
bytes
<
=
endAddr
)
;
if
(
uintptr_t
(
alloc
)
+
bytes
=
=
endAddr
)
{
return
nullptr
;
}
}
auto
*
region
=
FreeRegion
:
:
fromEndAddr
(
endAddr
)
;
#
ifdef
DEBUG
region
-
>
check
(
)
;
if
(
offset
!
=
ChunkSize
)
{
const
void
*
alloc
=
ptrFromOffset
(
offset
)
;
size_t
bytes
=
allocBytes
(
alloc
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
uintptr_t
(
alloc
)
+
bytes
)
;
}
else
{
MOZ_ASSERT
(
region
-
>
startAddr
=
=
uintptr_t
(
this
)
+
FirstMediumAllocOffset
)
;
}
#
endif
return
region
;
}
size_t
BufferAllocator
:
:
SizeClassForAlloc
(
size_t
bytes
)
{
MOZ_ASSERT
(
bytes
>
=
MinMediumAllocSize
)
;
MOZ_ASSERT
(
bytes
<
=
MaxMediumAllocSize
)
;
size_t
log2Size
=
mozilla
:
:
CeilingLog2
(
bytes
)
;
MOZ_ASSERT
(
(
size_t
(
1
)
<
<
log2Size
)
>
=
bytes
)
;
MOZ_ASSERT
(
MinMediumAllocShift
=
=
mozilla
:
:
CeilingLog2
(
MinMediumAllocSize
)
)
;
MOZ_ASSERT
(
log2Size
>
=
MinMediumAllocShift
)
;
size_t
sizeClass
=
log2Size
-
MinMediumAllocShift
;
MOZ_ASSERT
(
sizeClass
<
MediumAllocClasses
)
;
return
sizeClass
;
}
size_t
BufferAllocator
:
:
SizeClassForFreeRegion
(
size_t
bytes
)
{
MOZ_ASSERT
(
bytes
>
=
MinMediumAllocSize
)
;
MOZ_ASSERT
(
bytes
<
ChunkSize
)
;
size_t
log2Size
=
mozilla
:
:
FloorLog2
(
bytes
)
;
MOZ_ASSERT
(
(
size_t
(
1
)
<
<
log2Size
)
<
=
bytes
)
;
MOZ_ASSERT
(
log2Size
>
=
MinMediumAllocShift
)
;
size_t
sizeClass
=
std
:
:
min
(
log2Size
-
MinMediumAllocShift
MediumAllocClasses
-
1
)
;
MOZ_ASSERT
(
sizeClass
<
MediumAllocClasses
)
;
return
sizeClass
;
}
inline
size_t
BufferAllocator
:
:
SizeClassBytes
(
size_t
sizeClass
)
{
MOZ_ASSERT
(
sizeClass
<
MediumAllocClasses
)
;
return
1
<
<
(
sizeClass
+
MinMediumAllocShift
)
;
}
bool
BufferAllocator
:
:
IsMediumAlloc
(
void
*
alloc
)
{
ChunkBase
*
chunk
=
js
:
:
gc
:
:
detail
:
:
GetGCAddressChunkBase
(
alloc
)
;
return
chunk
-
>
getKind
(
)
=
=
ChunkKind
:
:
MediumBuffers
;
}
bool
BufferAllocator
:
:
needLockToAccessBufferMap
(
)
const
{
MOZ_ASSERT
(
CurrentThreadCanAccessZone
(
zone
)
|
|
CurrentThreadIsPerformingGC
(
)
)
;
return
minorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
|
|
majorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
;
}
LargeBuffer
*
BufferAllocator
:
:
lookupLargeBuffer
(
void
*
alloc
)
{
MaybeLock
lock
;
return
lookupLargeBuffer
(
alloc
lock
)
;
}
LargeBuffer
*
BufferAllocator
:
:
lookupLargeBuffer
(
void
*
alloc
MaybeLock
&
lock
)
{
MOZ_ASSERT
(
lock
.
isNothing
(
)
)
;
if
(
needLockToAccessBufferMap
(
)
)
{
lock
.
emplace
(
this
)
;
}
auto
ptr
=
largeAllocMap
.
ref
(
)
.
readonlyThreadsafeLookup
(
alloc
)
;
MOZ_ASSERT
(
ptr
)
;
LargeBuffer
*
buffer
=
ptr
-
>
value
(
)
;
MOZ_ASSERT
(
buffer
-
>
data
(
)
=
=
alloc
)
;
MOZ_ASSERT
(
buffer
-
>
zoneFromAnyThread
(
)
=
=
zone
)
;
return
buffer
;
}
void
*
BufferAllocator
:
:
allocLarge
(
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
bytes
=
RoundUp
(
bytes
ChunkSize
)
;
MOZ_ASSERT
(
bytes
>
MaxMediumAllocSize
)
;
MOZ_ASSERT
(
bytes
>
=
bytes
)
;
static_assert
(
sizeof
(
LargeBuffer
)
<
=
MaxSmallAllocSize
)
;
void
*
bufferPtr
=
allocSmall
(
sizeof
(
LargeBuffer
)
nurseryOwned
inGC
)
;
if
(
!
bufferPtr
)
{
return
nullptr
;
}
void
*
alloc
=
MapAlignedPages
(
bytes
ChunkSize
ShouldStallAndRetry
(
inGC
)
)
;
if
(
!
alloc
)
{
return
nullptr
;
}
auto
freeGuard
=
mozilla
:
:
MakeScopeExit
(
[
&
]
(
)
{
UnmapPages
(
alloc
bytes
)
;
}
)
;
CheckHighBitsOfPointer
(
alloc
)
;
auto
*
buffer
=
new
(
bufferPtr
)
LargeBuffer
(
alloc
bytes
nurseryOwned
)
;
{
MaybeLock
lock
;
if
(
needLockToAccessBufferMap
(
)
)
{
lock
.
emplace
(
this
)
;
}
if
(
!
largeAllocMap
.
ref
(
)
.
putNew
(
alloc
buffer
)
)
{
return
nullptr
;
}
}
freeGuard
.
release
(
)
;
if
(
nurseryOwned
)
{
largeNurseryAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
}
else
{
buffer
-
>
allocatedDuringCollection
=
majorState
!
=
State
:
:
NotCollecting
;
largeTenuredAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
}
if
(
!
nurseryOwned
)
{
bool
checkThresholds
=
!
inGC
;
updateHeapSize
(
bytes
checkThresholds
false
)
;
}
MOZ_ASSERT
(
IsLargeAlloc
(
alloc
)
)
;
return
alloc
;
}
void
BufferAllocator
:
:
updateHeapSize
(
size_t
bytes
bool
checkThresholds
bool
updateRetainedSize
)
{
zone
-
>
mallocHeapSize
.
addBytes
(
bytes
updateRetainedSize
)
;
if
(
checkThresholds
)
{
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromAnyThread
(
)
-
>
gc
;
gc
-
>
maybeTriggerGCAfterMalloc
(
zone
)
;
}
}
bool
BufferAllocator
:
:
IsLargeAlloc
(
void
*
alloc
)
{
return
(
uintptr_t
(
alloc
)
&
ChunkMask
)
=
=
0
;
}
bool
BufferAllocator
:
:
isLargeAllocMarked
(
void
*
alloc
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
return
buffer
-
>
headerCell
(
)
-
>
isMarkedAny
(
)
;
}
bool
BufferAllocator
:
:
markLargeTenuredBuffer
(
LargeBuffer
*
buffer
)
{
MOZ_ASSERT
(
!
buffer
-
>
isNurseryOwned
)
;
if
(
buffer
-
>
allocatedDuringCollection
)
{
return
false
;
}
return
buffer
-
>
headerCell
(
)
-
>
markIfUnmarkedThreadSafe
(
MarkColor
:
:
Black
)
;
}
bool
BufferAllocator
:
:
sweepLargeTenured
(
LargeBuffer
*
buffer
)
{
MOZ_ASSERT
(
!
buffer
-
>
isNurseryOwned
)
;
MOZ_ASSERT
(
buffer
-
>
zoneFromAnyThread
(
)
=
=
zone
)
;
MOZ_ASSERT
(
!
buffer
-
>
isInList
(
)
)
;
if
(
buffer
-
>
headerCell
(
)
-
>
isMarkedAny
(
)
)
{
return
true
;
}
MaybeLock
lock
(
std
:
:
in_place
this
)
;
unmapLarge
(
buffer
true
lock
)
;
return
false
;
}
void
BufferAllocator
:
:
freeLarge
(
void
*
alloc
)
{
MaybeLock
lock
;
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
lock
)
;
MOZ_ASSERT
(
buffer
-
>
zone
(
)
=
=
zone
)
;
DebugOnlyPoison
(
alloc
JS_FREED_BUFFER_PATTERN
buffer
-
>
allocBytes
(
)
MemCheckKind
:
:
MakeUndefined
)
;
if
(
!
buffer
-
>
isNurseryOwned
&
&
majorState
=
=
State
:
:
Sweeping
&
&
!
buffer
-
>
allocatedDuringCollection
)
{
return
;
}
MOZ_ASSERT
(
buffer
-
>
isInList
(
)
)
;
if
(
buffer
-
>
isNurseryOwned
)
{
largeNurseryAllocs
.
ref
(
)
.
remove
(
buffer
)
;
}
else
if
(
majorState
=
=
State
:
:
Marking
&
&
!
buffer
-
>
allocatedDuringCollection
)
{
largeTenuredAllocsToSweep
.
ref
(
)
.
remove
(
buffer
)
;
}
else
{
largeTenuredAllocs
.
ref
(
)
.
remove
(
buffer
)
;
}
unmapLarge
(
buffer
false
lock
)
;
}
bool
BufferAllocator
:
:
shrinkLarge
(
LargeBuffer
*
buffer
size_t
newBytes
)
{
MOZ_ASSERT
(
IsLargeAllocSize
(
newBytes
)
)
;
#
ifdef
XP_WIN
return
false
;
#
else
MOZ_ASSERT
(
buffer
-
>
zone
(
)
=
=
zone
)
;
if
(
!
buffer
-
>
isNurseryOwned
&
&
majorState
=
=
State
:
:
Sweeping
&
&
!
buffer
-
>
allocatedDuringCollection
)
{
return
false
;
}
MOZ_ASSERT
(
buffer
-
>
isInList
(
)
)
;
newBytes
=
RoundUp
(
newBytes
ChunkSize
)
;
size_t
oldBytes
=
buffer
-
>
bytes
;
MOZ_ASSERT
(
oldBytes
>
newBytes
)
;
size_t
shrinkBytes
=
oldBytes
-
newBytes
;
if
(
!
buffer
-
>
isNurseryOwned
)
{
zone
-
>
mallocHeapSize
.
removeBytes
(
shrinkBytes
false
)
;
}
buffer
-
>
bytes
=
newBytes
;
void
*
endPtr
=
reinterpret_cast
<
void
*
>
(
uintptr_t
(
buffer
-
>
data
(
)
)
+
newBytes
)
;
UnmapPages
(
endPtr
shrinkBytes
)
;
return
true
;
#
endif
}
void
BufferAllocator
:
:
unmapLarge
(
LargeBuffer
*
buffer
bool
isSweeping
MaybeLock
&
lock
)
{
MOZ_ASSERT
(
buffer
-
>
zoneFromAnyThread
(
)
=
=
zone
)
;
MOZ_ASSERT
(
!
buffer
-
>
isInList
(
)
)
;
MOZ_ASSERT_IF
(
isSweeping
|
|
needLockToAccessBufferMap
(
)
lock
.
isSome
(
)
)
;
#
ifdef
DEBUG
auto
ptr
=
largeAllocMap
.
ref
(
)
.
lookup
(
buffer
-
>
data
(
)
)
;
MOZ_ASSERT
(
ptr
&
&
ptr
-
>
value
(
)
=
=
buffer
)
;
#
endif
largeAllocMap
.
ref
(
)
.
remove
(
buffer
-
>
data
(
)
)
;
lock
.
reset
(
)
;
size_t
bytes
=
buffer
-
>
bytes
;
if
(
!
buffer
-
>
isNurseryOwned
)
{
zone
-
>
mallocHeapSize
.
removeBytes
(
bytes
isSweeping
)
;
}
UnmapPages
(
buffer
-
>
data
(
)
bytes
)
;
}
#
include
"
js
/
Printer
.
h
"
#
include
"
util
/
GetPidProvider
.
h
"
static
const
char
*
const
BufferAllocatorStatsPrefix
=
"
BufAllc
:
"
;
#
define
FOR_EACH_BUFFER_STATS_FIELD
(
_
)
\
_
(
"
PID
"
7
"
%
7zu
"
pid
)
\
_
(
"
Runtime
"
14
"
0x
%
12p
"
runtime
)
\
_
(
"
Timestamp
"
10
"
%
10
.
6f
"
timestamp
.
ToSeconds
(
)
)
\
_
(
"
Reason
"
20
"
%
-
20
.
20s
"
reason
)
\
_
(
"
"
2
"
%
2s
"
"
"
)
\
_
(
"
TotalKB
"
8
"
%
8zu
"
totalBytes
/
1024
)
\
_
(
"
UsedKB
"
8
"
%
8zu
"
usedBytes
/
1024
)
\
_
(
"
FreeKB
"
8
"
%
8zu
"
freeBytes
/
1024
)
\
_
(
"
Zs
"
3
"
%
3zu
"
zoneCount
)
\
_
(
"
"
7
"
%
7s
"
"
"
)
\
_
(
"
MNCs
"
6
"
%
6zu
"
mediumMixedChunks
)
\
_
(
"
MTCs
"
6
"
%
6zu
"
mediumTenuredChunks
)
\
_
(
"
FRs
"
6
"
%
6zu
"
freeRegions
)
\
_
(
"
LNAs
"
6
"
%
6zu
"
largeNurseryAllocs
)
\
_
(
"
LTAs
"
6
"
%
6zu
"
largeTenuredAllocs
)
void
BufferAllocator
:
:
printStatsHeader
(
FILE
*
file
)
{
Sprinter
sprinter
;
if
(
!
sprinter
.
init
(
)
)
{
return
;
}
sprinter
.
put
(
BufferAllocatorStatsPrefix
)
;
#
define
PRINT_METADATA_NAME
(
name
width
_1
_2
)
\
sprinter
.
printf
(
"
%
-
*
s
"
width
name
)
;
FOR_EACH_BUFFER_STATS_FIELD
(
PRINT_METADATA_NAME
)
#
undef
PRINT_METADATA_NAME
sprinter
.
put
(
"
\
n
"
)
;
JS
:
:
UniqueChars
str
=
sprinter
.
release
(
)
;
if
(
!
str
)
{
return
;
}
fputs
(
str
.
get
(
)
file
)
;
}
void
BufferAllocator
:
:
printStats
(
GCRuntime
*
gc
mozilla
:
:
TimeStamp
creationTime
bool
isMajorGC
FILE
*
file
)
{
Sprinter
sprinter
;
if
(
!
sprinter
.
init
(
)
)
{
return
;
}
sprinter
.
put
(
BufferAllocatorStatsPrefix
)
;
size_t
pid
=
getpid
(
)
;
JSRuntime
*
runtime
=
gc
-
>
rt
;
mozilla
:
:
TimeDuration
timestamp
=
mozilla
:
:
TimeStamp
:
:
Now
(
)
-
creationTime
;
const
char
*
reason
=
isMajorGC
?
"
post
major
slice
"
:
"
pre
minor
GC
"
;
size_t
zoneCount
=
0
;
size_t
usedBytes
=
0
;
size_t
freeBytes
=
0
;
size_t
adminBytes
=
0
;
size_t
mediumMixedChunks
=
0
;
size_t
mediumTenuredChunks
=
0
;
size_t
freeRegions
=
0
;
size_t
largeNurseryAllocs
=
0
;
size_t
largeTenuredAllocs
=
0
;
for
(
AllZonesIter
zone
(
gc
)
;
!
zone
.
done
(
)
;
zone
.
next
(
)
)
{
zoneCount
+
+
;
zone
-
>
bufferAllocator
.
getStats
(
usedBytes
freeBytes
adminBytes
mediumMixedChunks
mediumTenuredChunks
freeRegions
largeNurseryAllocs
largeTenuredAllocs
)
;
}
size_t
totalBytes
=
usedBytes
+
freeBytes
+
adminBytes
;
#
define
PRINT_FIELD_VALUE
(
_1
_2
format
value
)
\
sprinter
.
printf
(
"
"
format
value
)
;
FOR_EACH_BUFFER_STATS_FIELD
(
PRINT_FIELD_VALUE
)
#
undef
PRINT_FIELD_VALUE
sprinter
.
put
(
"
\
n
"
)
;
JS
:
:
UniqueChars
str
=
sprinter
.
release
(
)
;
if
(
!
str
)
{
return
;
}
fputs
(
str
.
get
(
)
file
)
;
}
size_t
BufferAllocator
:
:
getSizeOfNurseryBuffers
(
)
{
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
size_t
bytes
=
0
;
for
(
BufferChunk
*
chunk
:
mediumMixedChunks
.
ref
(
)
)
{
for
(
BufferChunk
:
:
AllocIter
alloc
(
chunk
)
;
!
alloc
.
done
(
)
;
alloc
.
next
(
)
)
{
if
(
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bytes
+
=
chunk
-
>
allocBytes
(
alloc
)
;
}
}
}
for
(
const
LargeBuffer
*
buffer
:
largeNurseryAllocs
.
ref
(
)
)
{
bytes
+
=
buffer
-
>
allocBytes
(
)
;
}
return
bytes
;
}
void
BufferAllocator
:
:
addSizeOfExcludingThis
(
size_t
*
usedBytesOut
size_t
*
freeBytesOut
size_t
*
adminBytesOut
)
{
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
size_t
usedBytes
=
0
;
size_t
freeBytes
=
0
;
size_t
adminBytes
=
0
;
size_t
mediumMixedChunks
=
0
;
size_t
mediumTenuredChunks
=
0
;
size_t
freeRegions
=
0
;
size_t
largeNurseryAllocs
=
0
;
size_t
largeTenuredAllocs
=
0
;
getStats
(
usedBytes
freeBytes
adminBytes
mediumMixedChunks
mediumTenuredChunks
freeRegions
largeNurseryAllocs
largeTenuredAllocs
)
;
*
usedBytesOut
+
=
usedBytes
;
*
freeBytesOut
+
=
freeBytes
;
*
adminBytesOut
+
=
adminBytes
;
}
void
BufferAllocator
:
:
getStats
(
size_t
&
usedBytes
size_t
&
freeBytes
size_t
&
adminBytes
size_t
&
mediumNurseryChunkCount
size_t
&
mediumTenuredChunkCount
size_t
&
freeRegions
size_t
&
largeNurseryAllocCount
size_t
&
largeTenuredAllocCount
)
{
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
for
(
const
BufferChunk
*
chunk
:
mediumMixedChunks
.
ref
(
)
)
{
(
void
)
chunk
;
mediumNurseryChunkCount
+
+
;
usedBytes
+
=
ChunkSize
-
FirstMediumAllocOffset
;
adminBytes
+
=
FirstMediumAllocOffset
;
}
for
(
const
BufferChunk
*
chunk
:
mediumTenuredChunks
.
ref
(
)
)
{
(
void
)
chunk
;
mediumTenuredChunkCount
+
+
;
usedBytes
+
=
ChunkSize
-
FirstMediumAllocOffset
;
adminBytes
+
=
FirstMediumAllocOffset
;
}
for
(
const
LargeBuffer
*
buffer
:
largeNurseryAllocs
.
ref
(
)
)
{
largeNurseryAllocCount
+
+
;
usedBytes
+
=
buffer
-
>
allocBytes
(
)
;
adminBytes
+
=
sizeof
(
LargeBuffer
)
;
}
for
(
const
LargeBuffer
*
buffer
:
largeTenuredAllocs
.
ref
(
)
)
{
largeTenuredAllocCount
+
+
;
usedBytes
+
=
buffer
-
>
allocBytes
(
)
;
adminBytes
+
=
sizeof
(
LargeBuffer
)
;
}
for
(
const
FreeList
&
freeList
:
mediumFreeLists
.
ref
(
)
)
{
for
(
const
FreeRegion
*
region
:
freeList
)
{
freeRegions
+
+
;
size_t
size
=
region
-
>
size
(
)
;
MOZ_ASSERT
(
usedBytes
>
=
size
)
;
usedBytes
-
=
size
;
freeBytes
+
=
size
;
}
}
}
JS
:
:
ubi
:
:
Node
:
:
Size
JS
:
:
ubi
:
:
Concrete
<
SmallBuffer
>
:
:
size
(
mozilla
:
:
MallocSizeOf
mallocSizeOf
)
const
{
return
get
(
)
.
allocBytes
(
)
;
}
const
char16_t
JS
:
:
ubi
:
:
Concrete
<
SmallBuffer
>
:
:
concreteTypeName
[
]
=
u
"
SmallBuffer
"
;
