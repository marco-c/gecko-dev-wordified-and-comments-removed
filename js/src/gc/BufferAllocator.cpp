#
include
"
gc
/
BufferAllocator
-
inl
.
h
"
#
include
"
mozilla
/
PodOperations
.
h
"
#
include
"
mozilla
/
ScopeExit
.
h
"
#
ifdef
XP_DARWIN
#
include
<
mach
/
mach_init
.
h
>
#
include
<
mach
/
vm_map
.
h
>
#
endif
#
include
"
gc
/
BufferAllocatorInternals
.
h
"
#
include
"
gc
/
GCInternals
.
h
"
#
include
"
gc
/
GCLock
.
h
"
#
include
"
gc
/
PublicIterators
.
h
"
#
include
"
gc
/
Zone
.
h
"
#
include
"
js
/
HeapAPI
.
h
"
#
include
"
util
/
Poison
.
h
"
#
include
"
gc
/
Heap
-
inl
.
h
"
#
include
"
gc
/
Marking
-
inl
.
h
"
using
namespace
js
;
using
namespace
js
:
:
gc
;
namespace
js
:
:
gc
{
BufferAllocator
:
:
AutoLock
:
:
AutoLock
(
GCRuntime
*
gc
)
:
LockGuard
(
gc
-
>
bufferAllocatorLock
)
{
}
BufferAllocator
:
:
AutoLock
:
:
AutoLock
(
BufferAllocator
*
allocator
)
:
LockGuard
(
allocator
-
>
lock
(
)
)
{
}
static
void
CheckHighBitsOfPointer
(
void
*
ptr
)
{
#
ifdef
JS_64BIT
MOZ_DIAGNOSTIC_ASSERT
(
(
uintptr_t
(
ptr
)
>
>
47
)
=
=
0
)
;
#
endif
}
BufferAllocator
:
:
FreeLists
:
:
FreeLists
(
FreeLists
&
&
other
)
{
MOZ_ASSERT
(
this
!
=
&
other
)
;
assertEmpty
(
)
;
std
:
:
swap
(
lists
other
.
lists
)
;
std
:
:
swap
(
available
other
.
available
)
;
other
.
assertEmpty
(
)
;
}
BufferAllocator
:
:
FreeLists
&
BufferAllocator
:
:
FreeLists
:
:
operator
=
(
FreeLists
&
&
other
)
{
MOZ_ASSERT
(
this
!
=
&
other
)
;
assertEmpty
(
)
;
std
:
:
swap
(
lists
other
.
lists
)
;
std
:
:
swap
(
available
other
.
available
)
;
other
.
assertEmpty
(
)
;
return
*
this
;
}
BufferAllocator
:
:
FreeLists
:
:
FreeListIter
BufferAllocator
:
:
FreeLists
:
:
freeListIter
(
)
{
return
FreeListIter
(
*
this
)
;
}
BufferAllocator
:
:
FreeLists
:
:
FreeRegionIter
BufferAllocator
:
:
FreeLists
:
:
freeRegionIter
(
)
{
return
FreeRegionIter
(
*
this
)
;
}
bool
BufferAllocator
:
:
FreeLists
:
:
hasSizeClass
(
size_t
sizeClass
)
const
{
MOZ_ASSERT
(
sizeClass
<
=
MaxMediumAllocClass
)
;
return
available
[
sizeClass
]
;
}
size_t
BufferAllocator
:
:
FreeLists
:
:
getFirstAvailableSizeClass
(
size_t
minSizeClass
size_t
maxSizeClass
)
const
{
MOZ_ASSERT
(
maxSizeClass
<
=
MaxMediumAllocClass
)
;
size_t
result
=
available
.
FindNext
(
minSizeClass
)
;
MOZ_ASSERT
(
result
>
=
minSizeClass
)
;
MOZ_ASSERT_IF
(
result
!
=
SIZE_MAX
!
lists
[
result
]
.
isEmpty
(
)
)
;
if
(
result
>
maxSizeClass
)
{
return
SIZE_MAX
;
}
return
result
;
}
size_t
BufferAllocator
:
:
FreeLists
:
:
getLastAvailableSizeClass
(
size_t
minSizeClass
size_t
maxSizeClass
)
const
{
MOZ_ASSERT
(
maxSizeClass
<
=
MaxMediumAllocClass
)
;
size_t
result
=
available
.
FindPrev
(
maxSizeClass
)
;
MOZ_ASSERT
(
result
<
=
maxSizeClass
|
|
result
=
=
SIZE_MAX
)
;
MOZ_ASSERT_IF
(
result
!
=
SIZE_MAX
!
lists
[
result
]
.
isEmpty
(
)
)
;
if
(
result
<
minSizeClass
)
{
return
SIZE_MAX
;
}
return
result
;
}
BufferAllocator
:
:
FreeRegion
*
BufferAllocator
:
:
FreeLists
:
:
getFirstRegion
(
size_t
sizeClass
)
{
MOZ_ASSERT
(
!
lists
[
sizeClass
]
.
isEmpty
(
)
)
;
return
lists
[
sizeClass
]
.
getFirst
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
pushFront
(
size_t
sizeClass
FreeRegion
*
region
)
{
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
lists
[
sizeClass
]
.
pushFront
(
region
)
;
available
[
sizeClass
]
=
true
;
}
void
BufferAllocator
:
:
FreeLists
:
:
pushBack
(
size_t
sizeClass
FreeRegion
*
region
)
{
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
lists
[
sizeClass
]
.
pushBack
(
region
)
;
available
[
sizeClass
]
=
true
;
}
void
BufferAllocator
:
:
FreeLists
:
:
append
(
FreeLists
&
&
other
)
{
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
if
(
!
other
.
lists
[
i
]
.
isEmpty
(
)
)
{
lists
[
i
]
.
append
(
std
:
:
move
(
other
.
lists
[
i
]
)
)
;
available
[
i
]
=
true
;
}
}
other
.
available
.
ResetAll
(
)
;
other
.
assertEmpty
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
prepend
(
FreeLists
&
&
other
)
{
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
if
(
!
other
.
lists
[
i
]
.
isEmpty
(
)
)
{
lists
[
i
]
.
prepend
(
std
:
:
move
(
other
.
lists
[
i
]
)
)
;
available
[
i
]
=
true
;
}
}
other
.
available
.
ResetAll
(
)
;
other
.
assertEmpty
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
remove
(
size_t
sizeClass
FreeRegion
*
region
)
{
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
lists
[
sizeClass
]
.
remove
(
region
)
;
available
[
sizeClass
]
=
!
lists
[
sizeClass
]
.
isEmpty
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
clear
(
)
{
for
(
auto
freeList
=
freeListIter
(
)
;
!
freeList
.
done
(
)
;
freeList
.
next
(
)
)
{
new
(
&
freeList
.
get
(
)
)
FreeList
;
}
available
.
ResetAll
(
)
;
}
template
<
typename
Func
>
void
BufferAllocator
:
:
FreeLists
:
:
forEachRegion
(
Func
&
&
func
)
{
for
(
size_t
i
=
0
;
i
<
=
MaxMediumAllocClass
;
i
+
+
)
{
FreeList
&
freeList
=
lists
[
i
]
;
FreeRegion
*
region
=
freeList
.
getFirst
(
)
;
while
(
region
)
{
FreeRegion
*
next
=
region
-
>
getNext
(
)
;
func
(
freeList
i
region
)
;
region
=
next
;
}
available
[
i
]
=
!
freeList
.
isEmpty
(
)
;
}
}
inline
void
BufferAllocator
:
:
FreeLists
:
:
assertEmpty
(
)
const
{
#
ifdef
DEBUG
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
MOZ_ASSERT
(
lists
[
i
]
.
isEmpty
(
)
)
;
}
MOZ_ASSERT
(
available
.
IsEmpty
(
)
)
;
#
endif
}
inline
void
BufferAllocator
:
:
FreeLists
:
:
assertContains
(
size_t
sizeClass
FreeRegion
*
region
)
const
{
#
ifdef
DEBUG
MOZ_ASSERT
(
available
[
sizeClass
]
)
;
MOZ_ASSERT
(
lists
[
sizeClass
]
.
contains
(
region
)
)
;
#
endif
}
inline
void
BufferAllocator
:
:
FreeLists
:
:
checkAvailable
(
)
const
{
#
ifdef
DEBUG
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
MOZ_ASSERT
(
available
[
i
]
=
=
!
lists
[
i
]
.
isEmpty
(
)
)
;
}
#
endif
}
BufferAllocator
:
:
ChunkLists
:
:
ChunkListIter
BufferAllocator
:
:
ChunkLists
:
:
chunkListIter
(
)
{
return
ChunkListIter
(
*
this
)
;
}
BufferAllocator
:
:
ChunkLists
:
:
ChunkIter
BufferAllocator
:
:
ChunkLists
:
:
chunkIter
(
)
{
return
ChunkIter
(
*
this
)
;
}
size_t
BufferAllocator
:
:
ChunkLists
:
:
getFirstAvailableSizeClass
(
size_t
minSizeClass
size_t
maxSizeClass
)
const
{
MOZ_ASSERT
(
maxSizeClass
<
=
MaxMediumAllocClass
)
;
size_t
result
=
available
.
FindNext
(
minSizeClass
)
;
MOZ_ASSERT
(
result
>
=
minSizeClass
)
;
MOZ_ASSERT_IF
(
result
!
=
SIZE_MAX
!
lists
[
result
]
.
isEmpty
(
)
)
;
if
(
result
>
maxSizeClass
)
{
return
SIZE_MAX
;
}
return
result
;
}
BufferChunk
*
BufferAllocator
:
:
ChunkLists
:
:
popFirstChunk
(
size_t
sizeClass
)
{
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
MOZ_ASSERT
(
!
lists
[
sizeClass
]
.
isEmpty
(
)
)
;
BufferChunk
*
chunk
=
lists
[
sizeClass
]
.
popFirst
(
)
;
if
(
lists
[
sizeClass
]
.
isEmpty
(
)
)
{
available
[
sizeClass
]
=
false
;
}
return
chunk
;
}
void
BufferAllocator
:
:
ChunkLists
:
:
remove
(
size_t
sizeClass
BufferChunk
*
chunk
)
{
MOZ_ASSERT
(
sizeClass
<
=
AllocSizeClasses
)
;
lists
[
sizeClass
]
.
remove
(
chunk
)
;
available
[
sizeClass
]
=
!
lists
[
sizeClass
]
.
isEmpty
(
)
;
}
void
BufferAllocator
:
:
ChunkLists
:
:
pushFront
(
size_t
sizeClass
BufferChunk
*
chunk
)
{
MOZ_ASSERT
(
sizeClass
<
=
AllocSizeClasses
)
;
lists
[
sizeClass
]
.
pushFront
(
chunk
)
;
available
[
sizeClass
]
=
true
;
}
void
BufferAllocator
:
:
ChunkLists
:
:
pushBack
(
BufferChunk
*
chunk
)
{
MOZ_ASSERT
(
chunk
-
>
ownsFreeLists
)
;
pushBack
(
chunk
-
>
sizeClassForAvailableLists
(
)
chunk
)
;
}
void
BufferAllocator
:
:
ChunkLists
:
:
pushBack
(
size_t
sizeClass
BufferChunk
*
chunk
)
{
MOZ_ASSERT
(
sizeClass
<
=
AllocSizeClasses
)
;
MOZ_ASSERT
(
sizeClass
=
=
chunk
-
>
sizeClassForAvailableLists
(
)
)
;
lists
[
sizeClass
]
.
pushBack
(
chunk
)
;
available
[
sizeClass
]
=
true
;
}
BufferAllocator
:
:
BufferChunkList
BufferAllocator
:
:
ChunkLists
:
:
extractAllChunks
(
)
{
BufferChunkList
result
;
for
(
auto
list
=
chunkListIter
(
)
;
!
list
.
done
(
)
;
list
.
next
(
)
)
{
result
.
append
(
std
:
:
move
(
list
.
get
(
)
)
)
;
}
available
.
ResetAll
(
)
;
return
result
;
}
inline
bool
BufferAllocator
:
:
ChunkLists
:
:
isEmpty
(
)
const
{
checkAvailable
(
)
;
return
available
.
IsEmpty
(
)
;
}
inline
void
BufferAllocator
:
:
ChunkLists
:
:
checkAvailable
(
)
const
{
#
ifdef
DEBUG
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
MOZ_ASSERT
(
available
[
i
]
=
=
!
lists
[
i
]
.
isEmpty
(
)
)
;
}
#
endif
}
}
MOZ_ALWAYS_INLINE
void
PoisonAlloc
(
void
*
alloc
uint8_t
value
size_t
bytes
MemCheckKind
kind
)
{
#
ifndef
EARLY_BETA_OR_EARLIER
bytes
=
std
:
:
min
(
bytes
size_t
(
256
)
)
;
#
endif
AlwaysPoison
(
alloc
value
bytes
kind
)
;
}
template
<
typename
D
size_t
S
size_t
G
>
void
AllocSpace
<
D
S
G
>
:
:
setAllocated
(
void
*
alloc
size_t
bytes
bool
allocated
)
{
size_t
startBit
=
ptrToIndex
(
alloc
)
;
MOZ_ASSERT
(
bytes
%
GranularityBytes
=
=
0
)
;
size_t
endBit
=
startBit
+
bytes
/
GranularityBytes
;
MOZ_ASSERT
(
endBit
<
=
MaxAllocCount
)
;
MOZ_ASSERT
(
allocStartBitmap
.
ref
(
)
[
startBit
]
!
=
allocated
)
;
MOZ_ASSERT_IF
(
endBit
!
=
MaxAllocCount
allocStartBitmap
.
ref
(
)
[
startBit
]
=
=
allocEndBitmap
.
ref
(
)
[
endBit
]
)
;
MOZ_ASSERT_IF
(
startBit
+
1
<
MaxAllocCount
allocStartBitmap
.
ref
(
)
.
FindNext
(
startBit
+
1
)
>
=
endBit
)
;
MOZ_ASSERT
(
findEndBit
(
startBit
)
>
=
endBit
)
;
allocStartBitmap
.
ref
(
)
[
startBit
]
=
allocated
;
if
(
endBit
!
=
MaxAllocCount
)
{
allocEndBitmap
.
ref
(
)
[
endBit
]
=
allocated
;
}
}
template
<
typename
D
size_t
S
size_t
G
>
void
AllocSpace
<
D
S
G
>
:
:
updateEndOffset
(
void
*
alloc
size_t
oldBytes
size_t
newBytes
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
MOZ_ASSERT
(
oldBytes
%
GranularityBytes
=
=
0
)
;
MOZ_ASSERT
(
newBytes
%
GranularityBytes
=
=
0
)
;
size_t
startBit
=
ptrToIndex
(
alloc
)
;
size_t
oldEndBit
=
startBit
+
oldBytes
/
GranularityBytes
;
MOZ_ASSERT
(
oldEndBit
<
=
MaxAllocCount
)
;
if
(
oldEndBit
!
=
MaxAllocCount
)
{
MOZ_ASSERT
(
allocEndBitmap
.
ref
(
)
[
oldEndBit
]
)
;
allocEndBitmap
.
ref
(
)
[
oldEndBit
]
=
false
;
}
size_t
newEndBit
=
startBit
+
newBytes
/
GranularityBytes
;
MOZ_ASSERT
(
newEndBit
<
=
MaxAllocCount
)
;
MOZ_ASSERT_IF
(
startBit
+
1
<
MaxAllocCount
allocStartBitmap
.
ref
(
)
.
FindNext
(
startBit
+
1
)
>
=
newEndBit
)
;
MOZ_ASSERT
(
findEndBit
(
startBit
)
>
=
newEndBit
)
;
if
(
newEndBit
!
=
MaxAllocCount
)
{
allocEndBitmap
.
ref
(
)
[
newEndBit
]
=
true
;
}
}
template
<
typename
D
size_t
S
size_t
G
>
size_t
AllocSpace
<
D
S
G
>
:
:
allocBytes
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
startBit
=
ptrToIndex
(
alloc
)
;
size_t
endBit
=
findEndBit
(
startBit
)
;
MOZ_ASSERT
(
endBit
>
startBit
)
;
MOZ_ASSERT
(
endBit
<
=
MaxAllocCount
)
;
return
(
endBit
-
startBit
)
*
GranularityBytes
;
}
template
<
typename
D
size_t
S
size_t
G
>
bool
AllocSpace
<
D
S
G
>
:
:
setMarked
(
void
*
alloc
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
if
(
markBits
.
ref
(
)
.
getBit
(
bit
)
)
{
return
false
;
}
markBits
.
ref
(
)
.
setBit
(
bit
true
)
;
return
true
;
}
template
<
typename
D
size_t
S
size_t
G
>
size_t
AllocSpace
<
D
S
G
>
:
:
findNextAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
size_t
next
=
allocStartBitmap
.
ref
(
)
.
FindNext
(
bit
)
;
if
(
next
=
=
SIZE_MAX
)
{
return
SizeBytes
;
}
return
next
*
GranularityBytes
;
}
template
<
typename
D
size_t
S
size_t
G
>
size_t
AllocSpace
<
D
S
G
>
:
:
findPrevAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
size_t
prev
=
allocStartBitmap
.
ref
(
)
.
FindPrev
(
bit
)
;
if
(
prev
=
=
SIZE_MAX
)
{
return
SizeBytes
;
}
return
prev
*
GranularityBytes
;
}
template
<
typename
D
size_t
S
size_t
G
>
BufferAllocator
:
:
FreeRegion
*
AllocSpace
<
D
S
G
>
:
:
findFollowingFreeRegion
(
uintptr_t
startAddr
)
{
uintptr_t
offset
=
uintptr_t
(
startAddr
)
&
AddressMask
;
MOZ_ASSERT
(
isValidOffset
(
offset
)
)
;
MOZ_ASSERT
(
(
offset
%
GranularityBytes
)
=
=
0
)
;
MOZ_ASSERT
(
!
isAllocated
(
offset
)
)
;
offset
=
findNextAllocated
(
offset
)
;
MOZ_ASSERT
(
offset
<
=
SizeBytes
)
;
auto
*
region
=
FreeRegion
:
:
fromEndAddr
(
startAddress
(
)
+
offset
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
startAddr
)
;
return
region
;
}
template
<
typename
D
size_t
S
size_t
G
>
BufferAllocator
:
:
FreeRegion
*
AllocSpace
<
D
S
G
>
:
:
findPrecedingFreeRegion
(
uintptr_t
endAddr
)
{
uintptr_t
offset
=
uintptr_t
(
endAddr
)
&
AddressMask
;
MOZ_ASSERT
(
isValidOffset
(
offset
)
)
;
MOZ_ASSERT
(
(
offset
%
GranularityBytes
)
=
=
0
)
;
if
(
offset
=
=
firstAllocOffset
(
)
)
{
return
nullptr
;
}
MOZ_ASSERT
(
!
isAllocated
(
offset
)
)
;
offset
=
findPrevAllocated
(
offset
)
;
if
(
offset
!
=
SizeBytes
)
{
const
void
*
alloc
=
ptrFromOffset
(
offset
)
;
size_t
bytes
=
allocBytes
(
alloc
)
;
MOZ_ASSERT
(
uintptr_t
(
alloc
)
+
bytes
<
=
endAddr
)
;
if
(
uintptr_t
(
alloc
)
+
bytes
=
=
endAddr
)
{
return
nullptr
;
}
}
auto
*
region
=
FreeRegion
:
:
fromEndAddr
(
endAddr
)
;
#
ifdef
DEBUG
region
-
>
check
(
)
;
if
(
offset
!
=
SizeBytes
)
{
const
void
*
alloc
=
ptrFromOffset
(
offset
)
;
size_t
bytes
=
allocBytes
(
alloc
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
uintptr_t
(
alloc
)
+
bytes
)
;
}
else
{
MOZ_ASSERT
(
region
-
>
startAddr
=
=
startAddress
(
)
+
firstAllocOffset
(
)
)
;
}
#
endif
return
region
;
}
BufferChunk
:
:
BufferChunk
(
Zone
*
zone
)
:
ChunkBase
(
zone
-
>
runtimeFromMainThread
(
)
ChunkKind
:
:
Buffers
)
{
#
ifdef
DEBUG
this
-
>
zone
=
zone
;
MOZ_ASSERT
(
decommittedPages
.
ref
(
)
.
IsEmpty
(
)
)
;
#
endif
}
BufferChunk
:
:
~
BufferChunk
(
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
allocStartBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
allocEndBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
nurseryOwnedBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
#
endif
}
void
BufferChunk
:
:
setSmallBufferRegion
(
void
*
alloc
bool
smallAlloc
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
<
SmallRegionSize
SmallRegionSize
>
(
alloc
)
;
smallRegionBitmap
.
ref
(
)
.
setBit
(
bit
smallAlloc
)
;
}
bool
BufferChunk
:
:
isSmallBufferRegion
(
const
void
*
alloc
)
const
{
size_t
bit
=
ptrToIndex
<
SmallRegionSize
SmallAllocGranularity
>
(
alloc
)
;
return
smallRegionBitmap
.
ref
(
)
.
getBit
(
bit
)
;
}
size_t
BufferChunk
:
:
sizeClassForAvailableLists
(
)
const
{
MOZ_ASSERT
(
ownsFreeLists
)
;
size_t
sizeClass
=
freeLists
.
ref
(
)
.
getLastAvailableSizeClass
(
0
MaxMediumAllocClass
)
;
if
(
sizeClass
=
=
SIZE_MAX
)
{
return
BufferAllocator
:
:
FullChunkSizeClass
;
}
return
sizeClass
;
}
void
SmallBufferRegion
:
:
setHasNurseryOwnedAllocs
(
bool
value
)
{
hasNurseryOwnedAllocs_
=
value
;
}
bool
SmallBufferRegion
:
:
hasNurseryOwnedAllocs
(
)
const
{
return
hasNurseryOwnedAllocs_
.
ref
(
)
;
}
BufferAllocator
:
:
BufferAllocator
(
Zone
*
zone
)
:
zone
(
zone
)
sweptMixedChunks
(
lock
(
)
)
sweptTenuredChunks
(
lock
(
)
)
sweptLargeTenuredAllocs
(
lock
(
)
)
minorState
(
State
:
:
NotCollecting
)
majorState
(
State
:
:
NotCollecting
)
minorSweepingFinished
(
lock
(
)
)
majorSweepingFinished
(
lock
(
)
)
{
}
BufferAllocator
:
:
~
BufferAllocator
(
)
{
#
ifdef
DEBUG
checkGCStateNotInUse
(
)
;
MOZ_ASSERT
(
mixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
tenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
freeLists
.
ref
(
)
.
assertEmpty
(
)
;
MOZ_ASSERT
(
availableMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
availableTenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeNurseryAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
#
endif
}
bool
BufferAllocator
:
:
isEmpty
(
)
const
{
MOZ_ASSERT
(
!
zone
-
>
wasGCStarted
(
)
|
|
zone
-
>
isGCFinished
(
)
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
return
mixedChunks
.
ref
(
)
.
isEmpty
(
)
&
&
availableMixedChunks
.
ref
(
)
.
isEmpty
(
)
&
&
tenuredChunks
.
ref
(
)
.
isEmpty
(
)
&
&
availableTenuredChunks
.
ref
(
)
.
isEmpty
(
)
&
&
largeNurseryAllocs
.
ref
(
)
.
isEmpty
(
)
&
&
largeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
;
}
Mutex
&
BufferAllocator
:
:
lock
(
)
const
{
return
zone
-
>
runtimeFromAnyThread
(
)
-
>
gc
.
bufferAllocatorLock
;
}
void
*
BufferAllocator
:
:
alloc
(
size_t
bytes
bool
nurseryOwned
)
{
MOZ_ASSERT_IF
(
zone
-
>
isGCMarkingOrSweeping
(
)
majorState
=
=
State
:
:
Marking
)
;
if
(
IsLargeAllocSize
(
bytes
)
)
{
return
allocLarge
(
bytes
nurseryOwned
false
)
;
}
if
(
IsSmallAllocSize
(
bytes
)
)
{
return
allocSmall
(
bytes
nurseryOwned
false
)
;
}
return
allocMedium
(
bytes
nurseryOwned
false
)
;
}
void
*
BufferAllocator
:
:
allocInGC
(
size_t
bytes
bool
nurseryOwned
)
{
MOZ_ASSERT
(
minorState
=
=
State
:
:
Marking
)
;
MOZ_ASSERT_IF
(
zone
-
>
isGCMarkingOrSweeping
(
)
majorState
=
=
State
:
:
Marking
)
;
void
*
result
;
if
(
IsLargeAllocSize
(
bytes
)
)
{
result
=
allocLarge
(
bytes
nurseryOwned
true
)
;
}
else
if
(
IsSmallAllocSize
(
bytes
)
)
{
result
=
allocSmall
(
bytes
nurseryOwned
true
)
;
}
else
{
result
=
allocMedium
(
bytes
nurseryOwned
true
)
;
}
if
(
!
result
)
{
return
nullptr
;
}
if
(
nurseryOwned
)
{
markNurseryOwnedAlloc
(
result
false
)
;
}
return
result
;
}
#
ifdef
DEBUG
inline
Zone
*
LargeBuffer
:
:
zone
(
)
{
Zone
*
zone
=
zoneFromAnyThread
(
)
;
MOZ_ASSERT
(
CurrentThreadCanAccessZone
(
zone
)
)
;
return
zone
;
}
inline
Zone
*
LargeBuffer
:
:
zoneFromAnyThread
(
)
{
return
BufferChunk
:
:
from
(
this
)
-
>
zone
;
}
#
endif
#
ifdef
XP_DARWIN
static
inline
void
VirtualCopyPages
(
void
*
dst
const
void
*
src
size_t
bytes
)
{
MOZ_ASSERT
(
(
uintptr_t
(
dst
)
&
PageMask
)
=
=
0
)
;
MOZ_ASSERT
(
(
uintptr_t
(
src
)
&
PageMask
)
=
=
0
)
;
MOZ_ASSERT
(
bytes
>
=
ChunkSize
)
;
kern_return_t
r
=
vm_copy
(
mach_task_self
(
)
vm_address_t
(
src
)
vm_size_t
(
bytes
)
vm_address_t
(
dst
)
)
;
if
(
r
!
=
KERN_SUCCESS
)
{
MOZ_CRASH
(
"
vm_copy
(
)
failed
"
)
;
}
}
#
endif
void
*
BufferAllocator
:
:
realloc
(
void
*
alloc
size_t
bytes
bool
nurseryOwned
)
{
if
(
!
alloc
)
{
return
this
-
>
alloc
(
bytes
nurseryOwned
)
;
}
MOZ_ASSERT
(
isNurseryOwned
(
alloc
)
=
=
nurseryOwned
)
;
MOZ_ASSERT_IF
(
zone
-
>
isGCMarkingOrSweeping
(
)
majorState
=
=
State
:
:
Marking
)
;
bytes
=
GetGoodAllocSize
(
bytes
)
;
size_t
currentBytes
;
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
currentBytes
=
buffer
-
>
allocBytes
(
)
;
if
(
bytes
<
buffer
-
>
allocBytes
(
)
&
&
IsLargeAllocSize
(
bytes
)
)
{
if
(
shrinkLarge
(
buffer
bytes
)
)
{
return
alloc
;
}
}
}
else
if
(
IsMediumAlloc
(
alloc
)
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
)
;
currentBytes
=
chunk
-
>
allocBytes
(
alloc
)
;
if
(
bytes
<
currentBytes
&
&
!
IsSmallAllocSize
(
bytes
)
)
{
if
(
shrinkMedium
(
alloc
bytes
)
)
{
return
alloc
;
}
}
if
(
bytes
>
currentBytes
&
&
!
IsLargeAllocSize
(
bytes
)
)
{
if
(
growMedium
(
alloc
bytes
)
)
{
return
alloc
;
}
}
}
else
{
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
currentBytes
=
region
-
>
allocBytes
(
alloc
)
;
}
if
(
bytes
=
=
currentBytes
)
{
return
alloc
;
}
void
*
newAlloc
=
this
-
>
alloc
(
bytes
nurseryOwned
)
;
if
(
!
newAlloc
)
{
return
nullptr
;
}
auto
freeGuard
=
mozilla
:
:
MakeScopeExit
(
[
&
]
(
)
{
free
(
alloc
)
;
}
)
;
size_t
bytesToCopy
=
std
:
:
min
(
bytes
currentBytes
)
;
#
ifdef
XP_DARWIN
if
(
bytesToCopy
>
=
ChunkSize
)
{
MOZ_ASSERT
(
IsLargeAlloc
(
alloc
)
)
;
MOZ_ASSERT
(
IsLargeAlloc
(
newAlloc
)
)
;
VirtualCopyPages
(
newAlloc
alloc
bytesToCopy
)
;
return
newAlloc
;
}
#
endif
memcpy
(
newAlloc
alloc
bytesToCopy
)
;
return
newAlloc
;
}
void
BufferAllocator
:
:
free
(
void
*
alloc
)
{
MOZ_ASSERT
(
alloc
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
freeLarge
(
alloc
)
;
return
;
}
if
(
IsMediumAlloc
(
alloc
)
)
{
freeMedium
(
alloc
)
;
return
;
}
}
bool
BufferAllocator
:
:
IsBufferAlloc
(
void
*
alloc
)
{
if
(
IsLargeAlloc
(
alloc
)
)
{
return
true
;
}
ChunkBase
*
chunk
=
detail
:
:
GetGCAddressChunkBase
(
alloc
)
;
return
chunk
-
>
getKind
(
)
=
=
ChunkKind
:
:
Buffers
;
}
#
ifdef
DEBUG
bool
BufferAllocator
:
:
hasAlloc
(
void
*
alloc
)
{
MOZ_ASSERT
(
IsBufferAlloc
(
alloc
)
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
MaybeLock
lock
;
if
(
needLockToAccessBufferMap
(
)
)
{
lock
.
emplace
(
this
)
;
}
auto
ptr
=
largeAllocMap
.
ref
(
)
.
readonlyThreadsafeLookup
(
alloc
)
;
return
ptr
.
found
(
)
;
}
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
zone
=
=
zone
;
}
#
endif
size_t
BufferAllocator
:
:
getAllocSize
(
void
*
alloc
)
{
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
return
buffer
-
>
allocBytes
(
)
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
return
region
-
>
allocBytes
(
alloc
)
;
}
MOZ_ASSERT
(
IsMediumAlloc
(
alloc
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
allocBytes
(
alloc
)
;
}
bool
BufferAllocator
:
:
isNurseryOwned
(
void
*
alloc
)
{
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
return
buffer
-
>
isNurseryOwned
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
return
region
-
>
isNurseryOwned
(
alloc
)
;
}
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
isNurseryOwned
(
alloc
)
;
}
void
BufferAllocator
:
:
markNurseryOwnedAlloc
(
void
*
alloc
bool
ownerWasTenured
)
{
MOZ_ASSERT
(
alloc
)
;
MOZ_ASSERT
(
isNurseryOwned
(
alloc
)
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
Marking
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
MOZ_ASSERT
(
buffer
-
>
zone
(
)
=
=
zone
)
;
markLargeNurseryOwnedBuffer
(
buffer
ownerWasTenured
)
;
return
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
markSmallNurseryOwnedBuffer
(
alloc
ownerWasTenured
)
;
return
;
}
MOZ_ASSERT
(
IsMediumAlloc
(
alloc
)
)
;
markMediumNurseryOwnedBuffer
(
alloc
ownerWasTenured
)
;
}
void
BufferAllocator
:
:
markSmallNurseryOwnedBuffer
(
void
*
alloc
bool
ownerWasTenured
)
{
#
ifdef
DEBUG
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
)
;
#
endif
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
region
-
>
hasNurseryOwnedAllocs
(
)
)
;
MOZ_ASSERT
(
region
-
>
isNurseryOwned
(
alloc
)
)
;
if
(
ownerWasTenured
)
{
region
-
>
setNurseryOwned
(
alloc
false
)
;
return
;
}
region
-
>
setMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
markMediumNurseryOwnedBuffer
(
void
*
alloc
bool
ownerWasTenured
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
)
;
MOZ_ASSERT
(
chunk
-
>
isAllocated
(
alloc
)
)
;
MOZ_ASSERT
(
chunk
-
>
isNurseryOwned
(
alloc
)
)
;
if
(
ownerWasTenured
)
{
chunk
-
>
setNurseryOwned
(
alloc
false
)
;
size_t
size
=
chunk
-
>
allocBytes
(
alloc
)
;
increaseHeapSize
(
size
false
false
)
;
return
;
}
chunk
-
>
setMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
markLargeNurseryOwnedBuffer
(
LargeBuffer
*
buffer
bool
ownerWasTenured
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
)
;
auto
*
region
=
SmallBufferRegion
:
:
from
(
buffer
)
;
MOZ_ASSERT
(
region
-
>
isNurseryOwned
(
buffer
)
)
;
if
(
region
-
>
isMarked
(
buffer
)
)
{
MOZ_ASSERT
(
!
ownerWasTenured
)
;
return
;
}
markSmallNurseryOwnedBuffer
(
buffer
ownerWasTenured
)
;
largeNurseryAllocsToSweep
.
ref
(
)
.
remove
(
buffer
)
;
if
(
ownerWasTenured
)
{
buffer
-
>
isNurseryOwned
=
false
;
buffer
-
>
allocatedDuringCollection
=
majorState
!
=
State
:
:
NotCollecting
;
largeTenuredAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
size_t
usableSize
=
buffer
-
>
allocBytes
(
)
;
increaseHeapSize
(
usableSize
false
false
)
;
return
;
}
largeNurseryAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
}
bool
BufferAllocator
:
:
isMarkedBlack
(
void
*
alloc
)
{
if
(
IsLargeAlloc
(
alloc
)
)
{
alloc
=
lookupLargeBuffer
(
alloc
)
;
}
else
if
(
!
IsSmallAlloc
(
alloc
)
)
{
MOZ_ASSERT
(
IsMediumAlloc
(
alloc
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
isMarked
(
alloc
)
;
}
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
return
region
-
>
isMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
traceEdge
(
JSTracer
*
trc
Cell
*
owner
void
*
*
bufferp
const
char
*
name
)
{
MOZ_ASSERT
(
bufferp
)
;
void
*
buffer
=
*
bufferp
;
MOZ_ASSERT
(
buffer
)
;
if
(
trc
-
>
isMarkingTracer
(
)
&
&
!
zone
-
>
isGCMarking
(
)
)
{
return
;
}
MOZ_ASSERT_IF
(
trc
-
>
isTenuringTracer
(
)
minorState
.
refNoCheck
(
)
=
=
State
:
:
Marking
)
;
MOZ_ASSERT_IF
(
trc
-
>
isMarkingTracer
(
)
majorState
.
refNoCheck
(
)
=
=
State
:
:
Marking
)
;
if
(
!
IsLargeAlloc
(
buffer
)
&
&
js
:
:
gc
:
:
detail
:
:
GetGCAddressChunkBase
(
buffer
)
-
>
isNurseryChunk
(
)
)
{
return
;
}
MOZ_ASSERT
(
IsBufferAlloc
(
buffer
)
)
;
MOZ_ASSERT_IF
(
isNurseryOwned
(
buffer
)
owner
)
;
if
(
IsLargeAlloc
(
buffer
)
)
{
traceLargeAlloc
(
trc
owner
bufferp
name
)
;
return
;
}
if
(
IsSmallAlloc
(
buffer
)
)
{
traceSmallAlloc
(
trc
owner
bufferp
name
)
;
return
;
}
traceMediumAlloc
(
trc
owner
bufferp
name
)
;
}
void
BufferAllocator
:
:
traceSmallAlloc
(
JSTracer
*
trc
Cell
*
owner
void
*
*
allocp
const
char
*
name
)
{
void
*
alloc
=
*
allocp
;
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
if
(
trc
-
>
isTenuringTracer
(
)
)
{
if
(
region
-
>
isNurseryOwned
(
alloc
)
)
{
markSmallNurseryOwnedBuffer
(
alloc
owner
-
>
isTenured
(
)
)
;
}
return
;
}
if
(
trc
-
>
isMarkingTracer
(
)
)
{
if
(
!
region
-
>
isNurseryOwned
(
alloc
)
)
{
markSmallTenuredAlloc
(
alloc
)
;
}
return
;
}
}
void
BufferAllocator
:
:
traceMediumAlloc
(
JSTracer
*
trc
Cell
*
owner
void
*
*
allocp
const
char
*
name
)
{
void
*
alloc
=
*
allocp
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
if
(
trc
-
>
isTenuringTracer
(
)
)
{
if
(
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
markMediumNurseryOwnedBuffer
(
alloc
owner
-
>
isTenured
(
)
)
;
}
return
;
}
if
(
trc
-
>
isMarkingTracer
(
)
)
{
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
markMediumTenuredAlloc
(
alloc
)
;
}
return
;
}
}
void
BufferAllocator
:
:
traceLargeAlloc
(
JSTracer
*
trc
Cell
*
owner
void
*
*
allocp
const
char
*
name
)
{
void
*
alloc
=
*
allocp
;
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
if
(
trc
-
>
isTenuringTracer
(
)
)
{
if
(
buffer
-
>
isNurseryOwned
)
{
markLargeNurseryOwnedBuffer
(
buffer
owner
-
>
isTenured
(
)
)
;
}
return
;
}
if
(
trc
-
>
isMarkingTracer
(
)
)
{
if
(
!
buffer
-
>
isNurseryOwned
)
{
markLargeTenuredBuffer
(
buffer
)
;
}
return
;
}
}
bool
BufferAllocator
:
:
markTenuredAlloc
(
void
*
alloc
)
{
MOZ_ASSERT
(
alloc
)
;
MOZ_ASSERT
(
!
isNurseryOwned
(
alloc
)
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
return
markLargeTenuredBuffer
(
buffer
)
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
return
markSmallTenuredAlloc
(
alloc
)
;
}
return
markMediumTenuredAlloc
(
alloc
)
;
}
bool
BufferAllocator
:
:
markSmallTenuredAlloc
(
void
*
alloc
)
{
auto
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
if
(
chunk
-
>
allocatedDuringCollection
)
{
return
false
;
}
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
region
-
>
isAllocated
(
alloc
)
)
;
return
region
-
>
setMarked
(
alloc
)
;
}
bool
BufferAllocator
:
:
markMediumTenuredAlloc
(
void
*
alloc
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
isAllocated
(
alloc
)
)
;
if
(
chunk
-
>
allocatedDuringCollection
)
{
return
false
;
}
return
chunk
-
>
setMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
startMinorCollection
(
MaybeLock
&
lock
)
{
maybeMergeSweptData
(
lock
)
;
#
ifdef
DEBUG
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
if
(
majorState
=
=
State
:
:
NotCollecting
)
{
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromMainThread
(
)
-
>
gc
;
if
(
gc
-
>
hasZealMode
(
ZealMode
:
:
CheckHeapBeforeMinorGC
)
)
{
checkGCStateNotInUse
(
lock
)
;
}
}
#
endif
MOZ_ASSERT
(
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
std
:
:
swap
(
largeNurseryAllocs
.
ref
(
)
largeNurseryAllocsToSweep
.
ref
(
)
)
;
minorState
=
State
:
:
Marking
;
}
bool
BufferAllocator
:
:
startMinorSweeping
(
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
minorState
=
=
State
:
:
Marking
)
;
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT
(
!
minorSweepingFinished
)
;
MOZ_ASSERT
(
sweptMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
}
for
(
LargeBuffer
*
buffer
:
largeNurseryAllocs
.
ref
(
)
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
)
;
}
for
(
LargeBuffer
*
buffer
:
largeNurseryAllocsToSweep
.
ref
(
)
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
)
;
}
#
endif
if
(
mixedChunks
.
ref
(
)
.
isEmpty
(
)
&
&
availableMixedChunks
.
ref
(
)
.
isEmpty
(
)
&
&
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
minorState
=
State
:
:
NotCollecting
;
return
false
;
}
#
ifdef
DEBUG
for
(
BufferChunk
*
chunk
:
mixedChunks
.
ref
(
)
)
{
MOZ_ASSERT
(
!
chunk
-
>
ownsFreeLists
)
;
chunk
-
>
freeLists
.
ref
(
)
.
assertEmpty
(
)
;
}
#
endif
freeLists
.
ref
(
)
.
forEachRegion
(
[
]
(
FreeList
&
list
size_t
sizeClass
FreeRegion
*
region
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
region
)
;
if
(
!
chunk
-
>
hasNurseryOwnedAllocs
)
{
list
.
remove
(
region
)
;
chunk
-
>
freeLists
.
ref
(
)
.
pushBack
(
sizeClass
region
)
;
}
}
)
;
freeLists
.
ref
(
)
.
clear
(
)
;
for
(
BufferChunk
*
chunk
:
tenuredChunks
.
ref
(
)
)
{
MOZ_ASSERT
(
!
chunk
-
>
hasNurseryOwnedAllocs
)
;
chunk
-
>
ownsFreeLists
=
true
;
}
mixedChunksToSweep
.
ref
(
)
=
std
:
:
move
(
mixedChunks
.
ref
(
)
)
;
mixedChunksToSweep
.
ref
(
)
.
append
(
availableMixedChunks
.
ref
(
)
.
extractAllChunks
(
)
)
;
while
(
BufferChunk
*
chunk
=
tenuredChunks
.
ref
(
)
.
popFirst
(
)
)
{
availableTenuredChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
}
minorState
=
State
:
:
Sweeping
;
return
true
;
}
struct
LargeAllocToFree
{
size_t
bytes
;
LargeAllocToFree
*
next
=
nullptr
;
explicit
LargeAllocToFree
(
size_t
bytes
)
:
bytes
(
bytes
)
{
}
}
;
static
void
PushLargeAllocToFree
(
LargeAllocToFree
*
*
listHead
LargeBuffer
*
buffer
)
{
auto
*
alloc
=
new
(
buffer
-
>
data
(
)
)
LargeAllocToFree
(
buffer
-
>
bytes
)
;
alloc
-
>
next
=
*
listHead
;
*
listHead
=
alloc
;
}
static
void
FreeLargeAllocs
(
LargeAllocToFree
*
listHead
)
{
while
(
listHead
)
{
LargeAllocToFree
*
alloc
=
listHead
;
LargeAllocToFree
*
next
=
alloc
-
>
next
;
UnmapPages
(
alloc
alloc
-
>
bytes
)
;
listHead
=
next
;
}
}
void
BufferAllocator
:
:
sweepForMinorCollection
(
)
{
MOZ_ASSERT
(
minorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
)
;
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT
(
sweptMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
}
LargeAllocToFree
*
largeAllocsToFree
=
nullptr
;
while
(
!
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
LargeBuffer
*
buffer
=
largeNurseryAllocsToSweep
.
ref
(
)
.
popFirst
(
)
;
PushLargeAllocToFree
(
&
largeAllocsToFree
buffer
)
;
MaybeLock
lock
(
std
:
:
in_place
this
)
;
unregisterLarge
(
buffer
true
lock
)
;
}
while
(
!
mixedChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
BufferChunk
*
chunk
=
mixedChunksToSweep
.
ref
(
)
.
popFirst
(
)
;
if
(
sweepChunk
(
chunk
SweepKind
:
:
Nursery
false
)
)
{
{
AutoLock
lock
(
this
)
;
sweptMixedChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
}
hasMinorSweepDataToMerge
=
true
;
}
}
FreeLargeAllocs
(
largeAllocsToFree
)
;
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT
(
!
minorSweepingFinished
)
;
minorSweepingFinished
=
true
;
hasMinorSweepDataToMerge
=
true
;
}
}
void
BufferAllocator
:
:
startMajorCollection
(
MaybeLock
&
lock
)
{
maybeMergeSweptData
(
lock
)
;
#
ifdef
DEBUG
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
checkGCStateNotInUse
(
lock
)
;
MOZ_ASSERT
(
mixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
availableMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeNurseryAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
#
endif
#
ifdef
DEBUG
for
(
BufferChunk
*
chunk
:
tenuredChunks
.
ref
(
)
)
{
MOZ_ASSERT
(
!
chunk
-
>
ownsFreeLists
)
;
chunk
-
>
freeLists
.
ref
(
)
.
assertEmpty
(
)
;
}
#
endif
largeTenuredAllocsToSweep
.
ref
(
)
=
std
:
:
move
(
largeTenuredAllocs
.
ref
(
)
)
;
freeLists
.
ref
(
)
.
forEachRegion
(
[
]
(
FreeList
&
list
size_t
sizeClass
FreeRegion
*
region
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
region
)
;
MOZ_ASSERT
(
!
chunk
-
>
hasNurseryOwnedAllocs
)
;
list
.
remove
(
region
)
;
chunk
-
>
freeLists
.
ref
(
)
.
pushBack
(
sizeClass
region
)
;
}
)
;
for
(
BufferChunk
*
chunk
:
tenuredChunks
.
ref
(
)
)
{
MOZ_ASSERT
(
!
chunk
-
>
hasNurseryOwnedAllocs
)
;
chunk
-
>
ownsFreeLists
=
true
;
}
tenuredChunksToSweep
.
ref
(
)
=
std
:
:
move
(
tenuredChunks
.
ref
(
)
)
;
tenuredChunksToSweep
.
ref
(
)
.
append
(
availableTenuredChunks
.
ref
(
)
.
extractAllChunks
(
)
)
;
if
(
minorState
=
=
State
:
:
Sweeping
)
{
majorStartedWhileMinorSweeping
=
true
;
}
#
ifdef
DEBUG
MOZ_ASSERT
(
tenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
availableTenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
freeLists
.
ref
(
)
.
assertEmpty
(
)
;
MOZ_ASSERT
(
largeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
#
endif
majorState
=
State
:
:
Marking
;
}
void
BufferAllocator
:
:
startMajorSweeping
(
MaybeLock
&
lock
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
majorState
=
=
State
:
:
Marking
)
;
MOZ_ASSERT
(
zone
-
>
isGCFinished
(
)
)
;
MOZ_ASSERT
(
!
majorSweepingFinished
.
refNoCheck
(
)
)
;
#
endif
maybeMergeSweptData
(
lock
)
;
MOZ_ASSERT
(
!
majorStartedWhileMinorSweeping
)
;
majorState
=
State
:
:
Sweeping
;
}
void
BufferAllocator
:
:
sweepForMajorCollection
(
bool
shouldDecommit
)
{
MOZ_ASSERT
(
majorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
)
;
LargeAllocList
sweptLargeAllocs
;
LargeAllocToFree
*
largeAllocsToFree
=
nullptr
;
while
(
!
largeTenuredAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
LargeBuffer
*
buffer
=
largeTenuredAllocsToSweep
.
ref
(
)
.
popFirst
(
)
;
if
(
isLargeTenuredMarked
(
buffer
)
)
{
sweptLargeAllocs
.
pushBack
(
buffer
)
;
}
else
{
PushLargeAllocToFree
(
&
largeAllocsToFree
buffer
)
;
MaybeLock
lock
(
std
:
:
in_place
this
)
;
unregisterLarge
(
buffer
true
lock
)
;
}
}
while
(
!
tenuredChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
BufferChunk
*
chunk
=
tenuredChunksToSweep
.
ref
(
)
.
popFirst
(
)
;
if
(
sweepChunk
(
chunk
SweepKind
:
:
Tenured
shouldDecommit
)
)
{
{
AutoLock
lock
(
this
)
;
sweptTenuredChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
}
hasMinorSweepDataToMerge
=
true
;
}
}
FreeLargeAllocs
(
largeAllocsToFree
)
;
AutoLock
lock
(
this
)
;
sweptLargeTenuredAllocs
.
ref
(
)
=
std
:
:
move
(
sweptLargeAllocs
)
;
MOZ_ASSERT
(
!
majorSweepingFinished
)
;
majorSweepingFinished
=
true
;
}
void
BufferAllocator
:
:
finishMajorCollection
(
const
AutoLock
&
lock
)
{
MOZ_ASSERT_IF
(
majorState
=
=
State
:
:
Sweeping
majorSweepingFinished
)
;
if
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
{
mergeSweptData
(
lock
)
;
}
if
(
majorState
=
=
State
:
:
Marking
)
{
abortMajorSweeping
(
lock
)
;
}
#
ifdef
DEBUG
checkGCStateNotInUse
(
lock
)
;
#
endif
}
void
BufferAllocator
:
:
abortMajorSweeping
(
const
AutoLock
&
lock
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
majorState
=
=
State
:
:
Marking
)
;
MOZ_ASSERT
(
sweptTenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
for
(
auto
chunk
=
availableTenuredChunks
.
ref
(
)
.
chunkIter
(
)
;
!
chunk
.
done
(
)
;
chunk
.
next
(
)
)
{
MOZ_ASSERT
(
chunk
-
>
allocatedDuringCollection
)
;
}
#
endif
clearAllocatedDuringCollectionState
(
lock
)
;
for
(
BufferChunk
*
chunk
:
tenuredChunksToSweep
.
ref
(
)
)
{
MOZ_ASSERT
(
chunk
-
>
ownsFreeLists
)
;
clearChunkMarkBits
(
chunk
)
;
}
while
(
BufferChunk
*
chunk
=
tenuredChunksToSweep
.
ref
(
)
.
popFirst
(
)
)
{
availableTenuredChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
}
largeTenuredAllocs
.
ref
(
)
.
prepend
(
std
:
:
move
(
largeTenuredAllocsToSweep
.
ref
(
)
)
)
;
majorState
=
State
:
:
NotCollecting
;
}
void
BufferAllocator
:
:
clearAllocatedDuringCollectionState
(
const
AutoLock
&
lock
)
{
#
ifdef
DEBUG
for
(
LargeBuffer
*
buffer
:
largeNurseryAllocs
.
ref
(
)
)
{
MOZ_ASSERT
(
!
buffer
-
>
allocatedDuringCollection
)
;
}
#
endif
ClearAllocatedDuringCollection
(
mixedChunks
.
ref
(
)
)
;
ClearAllocatedDuringCollection
(
availableMixedChunks
.
ref
(
)
)
;
ClearAllocatedDuringCollection
(
tenuredChunks
.
ref
(
)
)
;
ClearAllocatedDuringCollection
(
availableTenuredChunks
.
ref
(
)
)
;
ClearAllocatedDuringCollection
(
largeTenuredAllocs
.
ref
(
)
)
;
}
void
BufferAllocator
:
:
ClearAllocatedDuringCollection
(
ChunkLists
&
chunks
)
{
for
(
auto
chunk
=
chunks
.
chunkIter
(
)
;
!
chunk
.
done
(
)
;
chunk
.
next
(
)
)
{
chunk
-
>
allocatedDuringCollection
=
false
;
}
}
void
BufferAllocator
:
:
ClearAllocatedDuringCollection
(
BufferChunkList
&
list
)
{
for
(
auto
*
chunk
:
list
)
{
chunk
-
>
allocatedDuringCollection
=
false
;
}
}
void
BufferAllocator
:
:
ClearAllocatedDuringCollection
(
LargeAllocList
&
list
)
{
for
(
auto
*
element
:
list
)
{
element
-
>
allocatedDuringCollection
=
false
;
}
}
void
BufferAllocator
:
:
maybeMergeSweptData
(
)
{
if
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
{
mergeSweptData
(
)
;
}
}
void
BufferAllocator
:
:
mergeSweptData
(
)
{
AutoLock
lock
(
this
)
;
mergeSweptData
(
lock
)
;
}
void
BufferAllocator
:
:
maybeMergeSweptData
(
MaybeLock
&
lock
)
{
if
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
{
if
(
lock
.
isNothing
(
)
)
{
lock
.
emplace
(
this
)
;
}
mergeSweptData
(
lock
.
ref
(
)
)
;
}
}
void
BufferAllocator
:
:
mergeSweptData
(
const
AutoLock
&
lock
)
{
MOZ_ASSERT
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
;
if
(
majorSweepingFinished
)
{
clearAllocatedDuringCollectionState
(
lock
)
;
if
(
minorState
=
=
State
:
:
Sweeping
)
{
majorFinishedWhileMinorSweeping
=
true
;
}
}
while
(
!
sweptMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
{
BufferChunk
*
chunk
=
sweptMixedChunks
.
ref
(
)
.
popLast
(
)
;
MOZ_ASSERT
(
chunk
-
>
ownsFreeLists
)
;
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
)
;
chunk
-
>
hasNurseryOwnedAllocs
=
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
;
MOZ_ASSERT_IF
(
majorState
=
=
State
:
:
NotCollecting
&
&
!
majorFinishedWhileMinorSweeping
!
chunk
-
>
allocatedDuringCollection
)
;
if
(
majorFinishedWhileMinorSweeping
)
{
chunk
-
>
allocatedDuringCollection
=
false
;
}
size_t
sizeClass
=
chunk
-
>
sizeClassForAvailableLists
(
)
;
if
(
chunk
-
>
hasNurseryOwnedAllocs
)
{
availableMixedChunks
.
ref
(
)
.
pushFront
(
sizeClass
chunk
)
;
}
else
if
(
majorStartedWhileMinorSweeping
)
{
tenuredChunksToSweep
.
ref
(
)
.
pushFront
(
chunk
)
;
}
else
{
availableTenuredChunks
.
ref
(
)
.
pushFront
(
sizeClass
chunk
)
;
}
}
#
ifdef
DEBUG
for
(
BufferChunk
*
chunk
:
sweptTenuredChunks
.
ref
(
)
)
{
MOZ_ASSERT
(
!
chunk
-
>
hasNurseryOwnedAllocs
)
;
MOZ_ASSERT
(
!
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
)
;
MOZ_ASSERT
(
!
chunk
-
>
allocatedDuringCollection
)
;
}
#
endif
while
(
BufferChunk
*
chunk
=
sweptTenuredChunks
.
ref
(
)
.
popFirst
(
)
)
{
size_t
sizeClass
=
chunk
-
>
sizeClassForAvailableLists
(
)
;
availableTenuredChunks
.
ref
(
)
.
pushFront
(
sizeClass
chunk
)
;
}
largeTenuredAllocs
.
ref
(
)
.
prepend
(
std
:
:
move
(
sweptLargeTenuredAllocs
.
ref
(
)
)
)
;
hasMinorSweepDataToMerge
=
false
;
if
(
minorSweepingFinished
)
{
MOZ_ASSERT
(
minorState
=
=
State
:
:
Sweeping
)
;
minorState
=
State
:
:
NotCollecting
;
minorSweepingFinished
=
false
;
majorStartedWhileMinorSweeping
=
false
;
majorFinishedWhileMinorSweeping
=
false
;
#
ifdef
DEBUG
for
(
BufferChunk
*
chunk
:
mixedChunks
.
ref
(
)
)
{
verifyChunk
(
chunk
true
)
;
}
for
(
BufferChunk
*
chunk
:
tenuredChunks
.
ref
(
)
)
{
verifyChunk
(
chunk
false
)
;
}
#
endif
}
if
(
majorSweepingFinished
)
{
MOZ_ASSERT
(
majorState
=
=
State
:
:
Sweeping
)
;
majorState
=
State
:
:
NotCollecting
;
majorSweepingFinished
=
false
;
MOZ_ASSERT
(
tenuredChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
}
}
void
BufferAllocator
:
:
clearMarkStateAfterBarrierVerification
(
)
{
MOZ_ASSERT
(
!
zone
-
>
wasGCStarted
(
)
)
;
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
for
(
auto
*
chunks
:
{
&
mixedChunks
.
ref
(
)
&
tenuredChunks
.
ref
(
)
}
)
{
for
(
auto
*
chunk
:
*
chunks
)
{
clearChunkMarkBits
(
chunk
)
;
}
}
for
(
auto
*
chunks
:
{
&
availableMixedChunks
.
ref
(
)
&
availableTenuredChunks
.
ref
(
)
}
)
{
for
(
auto
chunk
=
chunks
-
>
chunkIter
(
)
;
!
chunk
.
done
(
)
;
chunk
.
next
(
)
)
{
clearChunkMarkBits
(
chunk
)
;
}
}
#
ifdef
DEBUG
checkGCStateNotInUse
(
)
;
#
endif
}
void
BufferAllocator
:
:
clearChunkMarkBits
(
BufferChunk
*
chunk
)
{
chunk
-
>
markBits
.
ref
(
)
.
clear
(
)
;
for
(
auto
iter
=
chunk
-
>
smallRegionIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
SmallBufferRegion
*
region
=
iter
.
get
(
)
;
region
-
>
markBits
.
ref
(
)
.
clear
(
)
;
}
}
bool
BufferAllocator
:
:
isPointerWithinBuffer
(
void
*
ptr
)
{
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
mixedChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT_IF
(
majorState
!
=
State
:
:
Marking
tenuredChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
for
(
const
auto
*
chunks
:
{
&
mixedChunks
.
ref
(
)
&
tenuredChunks
.
ref
(
)
&
tenuredChunksToSweep
.
ref
(
)
}
)
{
for
(
auto
*
chunk
:
*
chunks
)
{
if
(
chunk
-
>
isPointerWithinAllocation
(
ptr
)
)
{
return
true
;
}
}
}
for
(
auto
*
chunks
:
{
&
availableMixedChunks
.
ref
(
)
&
availableTenuredChunks
.
ref
(
)
}
)
{
for
(
auto
chunk
=
chunks
-
>
chunkIter
(
)
;
!
chunk
.
done
(
)
;
chunk
.
next
(
)
)
{
if
(
chunk
-
>
isPointerWithinAllocation
(
ptr
)
)
{
return
true
;
}
}
}
for
(
const
auto
*
allocs
:
{
&
largeNurseryAllocs
.
ref
(
)
&
largeTenuredAllocs
.
ref
(
)
}
)
{
for
(
auto
*
alloc
:
*
allocs
)
{
if
(
alloc
-
>
isPointerWithinAllocation
(
ptr
)
)
{
return
true
;
}
}
}
return
false
;
}
bool
BufferChunk
:
:
isPointerWithinAllocation
(
void
*
ptr
)
const
{
uintptr_t
offset
=
uintptr_t
(
ptr
)
-
uintptr_t
(
this
)
;
if
(
offset
>
=
ChunkSize
|
|
offset
<
FirstMediumAllocOffset
)
{
return
false
;
}
if
(
smallRegionBitmap
.
ref
(
)
.
getBit
(
offset
/
SmallRegionSize
)
)
{
auto
*
region
=
SmallBufferRegion
:
:
from
(
ptr
)
;
return
region
-
>
isPointerWithinAllocation
(
ptr
)
;
}
uintptr_t
allocOffset
=
findPrevAllocated
(
RoundDown
(
offset
MinMediumAllocSize
)
)
;
MOZ_ASSERT
(
allocOffset
<
=
ChunkSize
)
;
if
(
allocOffset
=
=
ChunkSize
)
{
return
false
;
}
const
void
*
alloc
=
ptrFromOffset
(
allocOffset
)
;
size_t
size
=
allocBytes
(
alloc
)
;
return
offset
<
allocOffset
+
size
;
}
bool
SmallBufferRegion
:
:
isPointerWithinAllocation
(
void
*
ptr
)
const
{
uintptr_t
offset
=
uintptr_t
(
ptr
)
-
uintptr_t
(
this
)
;
MOZ_ASSERT
(
offset
<
SmallRegionSize
)
;
uintptr_t
allocOffset
=
findPrevAllocated
(
RoundDown
(
offset
SmallAllocGranularity
)
)
;
MOZ_ASSERT
(
allocOffset
<
=
SmallRegionSize
)
;
if
(
allocOffset
=
=
SmallRegionSize
)
{
return
false
;
}
const
void
*
alloc
=
ptrFromOffset
(
allocOffset
)
;
size_t
size
=
allocBytes
(
alloc
)
;
return
offset
<
allocOffset
+
size
;
}
bool
LargeBuffer
:
:
isPointerWithinAllocation
(
void
*
ptr
)
const
{
return
uintptr_t
(
ptr
)
-
uintptr_t
(
alloc
)
<
bytes
;
}
#
ifdef
DEBUG
void
BufferAllocator
:
:
checkGCStateNotInUse
(
)
{
maybeMergeSweptData
(
)
;
AutoLock
lock
(
this
)
;
checkGCStateNotInUse
(
lock
)
;
}
void
BufferAllocator
:
:
checkGCStateNotInUse
(
MaybeLock
&
maybeLock
)
{
if
(
maybeLock
.
isNothing
(
)
)
{
maybeLock
.
emplace
(
this
)
;
}
checkGCStateNotInUse
(
maybeLock
.
ref
(
)
)
;
}
void
BufferAllocator
:
:
checkGCStateNotInUse
(
const
AutoLock
&
lock
)
{
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
bool
isNurserySweeping
=
minorState
=
=
State
:
:
Sweeping
;
checkChunkListGCStateNotInUse
(
mixedChunks
.
ref
(
)
true
false
false
)
;
checkChunkListGCStateNotInUse
(
tenuredChunks
.
ref
(
)
false
false
false
)
;
checkChunkListsGCStateNotInUse
(
availableMixedChunks
.
ref
(
)
true
false
)
;
checkChunkListsGCStateNotInUse
(
availableTenuredChunks
.
ref
(
)
false
false
)
;
if
(
isNurserySweeping
)
{
checkChunkListGCStateNotInUse
(
sweptMixedChunks
.
ref
(
)
true
majorFinishedWhileMinorSweeping
true
)
;
checkChunkListGCStateNotInUse
(
sweptTenuredChunks
.
ref
(
)
false
false
true
)
;
}
else
{
MOZ_ASSERT
(
mixedChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
sweptMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
sweptTenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
!
majorStartedWhileMinorSweeping
)
;
MOZ_ASSERT
(
!
majorFinishedWhileMinorSweeping
)
;
MOZ_ASSERT
(
!
hasMinorSweepDataToMerge
)
;
MOZ_ASSERT
(
!
minorSweepingFinished
)
;
MOZ_ASSERT
(
!
majorSweepingFinished
)
;
}
MOZ_ASSERT
(
tenuredChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
checkAllocListGCStateNotInUse
(
largeNurseryAllocs
.
ref
(
)
true
)
;
checkAllocListGCStateNotInUse
(
largeTenuredAllocs
.
ref
(
)
false
)
;
MOZ_ASSERT
(
largeTenuredAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
sweptLargeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
}
void
BufferAllocator
:
:
checkChunkListsGCStateNotInUse
(
ChunkLists
&
chunkLists
bool
hasNurseryOwnedAllocs
bool
allowAllocatedDuringCollection
)
{
for
(
auto
chunk
=
chunkLists
.
chunkIter
(
)
;
!
chunk
.
done
(
)
;
chunk
.
next
(
)
)
{
checkChunkGCStateNotInUse
(
chunk
allowAllocatedDuringCollection
true
)
;
verifyChunk
(
chunk
hasNurseryOwnedAllocs
)
;
MOZ_ASSERT
(
chunk
-
>
ownsFreeLists
)
;
size_t
sizeClass
=
chunk
.
getSizeClass
(
)
;
MOZ_ASSERT
(
chunk
-
>
sizeClassForAvailableLists
(
)
=
=
sizeClass
)
;
MOZ_ASSERT_IF
(
sizeClass
!
=
FullChunkSizeClass
chunk
-
>
freeLists
.
ref
(
)
.
hasSizeClass
(
sizeClass
)
)
;
}
}
void
BufferAllocator
:
:
checkChunkListGCStateNotInUse
(
BufferChunkList
&
chunks
bool
hasNurseryOwnedAllocs
bool
allowAllocatedDuringCollection
bool
allowFreeLists
)
{
for
(
BufferChunk
*
chunk
:
chunks
)
{
checkChunkGCStateNotInUse
(
chunk
allowAllocatedDuringCollection
allowFreeLists
)
;
verifyChunk
(
chunk
hasNurseryOwnedAllocs
)
;
}
}
void
BufferAllocator
:
:
checkChunkGCStateNotInUse
(
BufferChunk
*
chunk
bool
allowAllocatedDuringCollection
bool
allowFreeLists
)
{
MOZ_ASSERT_IF
(
!
allowAllocatedDuringCollection
!
chunk
-
>
allocatedDuringCollection
)
;
MOZ_ASSERT
(
chunk
-
>
markBits
.
ref
(
)
.
isEmpty
(
)
)
;
for
(
auto
iter
=
chunk
-
>
smallRegionIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
SmallBufferRegion
*
region
=
iter
.
get
(
)
;
MOZ_ASSERT
(
region
-
>
markBits
.
ref
(
)
.
isEmpty
(
)
)
;
}
MOZ_ASSERT
(
allowFreeLists
=
=
chunk
-
>
ownsFreeLists
)
;
if
(
!
chunk
-
>
ownsFreeLists
)
{
chunk
-
>
freeLists
.
ref
(
)
.
assertEmpty
(
)
;
}
}
void
BufferAllocator
:
:
verifyChunk
(
BufferChunk
*
chunk
bool
hasNurseryOwnedAllocs
)
{
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
=
=
hasNurseryOwnedAllocs
)
;
static
constexpr
size_t
StepBytes
=
MediumAllocGranularity
;
size_t
freeOffset
=
FirstMediumAllocOffset
;
size_t
freeListsFreeRegionCount
=
0
;
if
(
chunk
-
>
ownsFreeLists
)
{
chunk
-
>
freeLists
.
ref
(
)
.
checkAvailable
(
)
;
for
(
auto
region
=
chunk
-
>
freeLists
.
ref
(
)
.
freeRegionIter
(
)
;
!
region
.
done
(
)
;
region
.
next
(
)
)
{
MOZ_ASSERT
(
BufferChunk
:
:
from
(
region
)
=
=
chunk
)
;
freeListsFreeRegionCount
+
+
;
}
}
else
{
MOZ_ASSERT
(
chunk
-
>
freeLists
.
ref
(
)
.
isEmpty
(
)
)
;
}
size_t
chunkFreeRegionCount
=
0
;
for
(
auto
iter
=
chunk
-
>
allocIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
size_t
offset
=
iter
.
getOffset
(
)
;
MOZ_ASSERT
(
offset
>
=
FirstMediumAllocOffset
)
;
if
(
offset
>
freeOffset
)
{
verifyFreeRegion
(
chunk
offset
offset
-
freeOffset
chunkFreeRegionCount
)
;
}
void
*
alloc
=
iter
.
get
(
)
;
MOZ_ASSERT_IF
(
chunk
-
>
isNurseryOwned
(
alloc
)
hasNurseryOwnedAllocs
)
;
size_t
bytes
=
chunk
-
>
allocBytes
(
alloc
)
;
uintptr_t
endOffset
=
offset
+
bytes
;
MOZ_ASSERT
(
endOffset
<
=
ChunkSize
)
;
for
(
size_t
i
=
offset
+
StepBytes
;
i
<
endOffset
;
i
+
=
StepBytes
)
{
MOZ_ASSERT
(
!
chunk
-
>
isAllocated
(
i
)
)
;
}
if
(
chunk
-
>
isSmallBufferRegion
(
alloc
)
)
{
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
MOZ_ASSERT_IF
(
region
-
>
hasNurseryOwnedAllocs
(
)
hasNurseryOwnedAllocs
)
;
verifySmallBufferRegion
(
region
chunkFreeRegionCount
)
;
}
freeOffset
=
endOffset
;
}
if
(
freeOffset
<
ChunkSize
)
{
verifyFreeRegion
(
chunk
ChunkSize
ChunkSize
-
freeOffset
chunkFreeRegionCount
)
;
}
MOZ_ASSERT_IF
(
chunk
-
>
ownsFreeLists
freeListsFreeRegionCount
=
=
chunkFreeRegionCount
)
;
}
void
BufferAllocator
:
:
verifyFreeRegion
(
BufferChunk
*
chunk
uintptr_t
endOffset
size_t
expectedSize
size_t
&
freeRegionCount
)
{
MOZ_ASSERT
(
expectedSize
>
=
MinFreeRegionSize
)
;
auto
*
freeRegion
=
FreeRegion
:
:
fromEndOffset
(
chunk
endOffset
)
;
MOZ_ASSERT
(
freeRegion
-
>
isInList
(
)
)
;
MOZ_ASSERT
(
freeRegion
-
>
size
(
)
=
=
expectedSize
)
;
freeRegionCount
+
+
;
}
void
BufferAllocator
:
:
verifySmallBufferRegion
(
SmallBufferRegion
*
region
size_t
&
freeRegionCount
)
{
bool
foundNurseryOwnedAllocs
=
false
;
static
constexpr
size_t
StepBytes
=
SmallAllocGranularity
;
size_t
freeOffset
=
FirstSmallAllocOffset
;
for
(
auto
iter
=
region
-
>
allocIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
size_t
offset
=
iter
.
getOffset
(
)
;
MOZ_ASSERT
(
offset
>
=
FirstSmallAllocOffset
)
;
if
(
offset
>
freeOffset
)
{
verifyFreeRegion
(
region
offset
offset
-
freeOffset
freeRegionCount
)
;
}
void
*
alloc
=
iter
.
get
(
)
;
MOZ_ASSERT_IF
(
region
-
>
isNurseryOwned
(
alloc
)
region
-
>
hasNurseryOwnedAllocs
(
)
)
;
size_t
bytes
=
region
-
>
allocBytes
(
alloc
)
;
uintptr_t
endOffset
=
offset
+
bytes
;
MOZ_ASSERT
(
endOffset
<
=
SmallRegionSize
)
;
for
(
size_t
i
=
offset
+
StepBytes
;
i
<
endOffset
;
i
+
=
StepBytes
)
{
MOZ_ASSERT
(
!
region
-
>
isAllocated
(
i
)
)
;
}
if
(
region
-
>
isNurseryOwned
(
alloc
)
)
{
foundNurseryOwnedAllocs
=
true
;
}
freeOffset
=
endOffset
;
}
MOZ_ASSERT
(
foundNurseryOwnedAllocs
=
=
region
-
>
hasNurseryOwnedAllocs
(
)
)
;
if
(
freeOffset
<
SmallRegionSize
)
{
verifyFreeRegion
(
region
SmallRegionSize
SmallRegionSize
-
freeOffset
freeRegionCount
)
;
}
}
void
BufferAllocator
:
:
verifyFreeRegion
(
SmallBufferRegion
*
region
uintptr_t
endOffset
size_t
expectedSize
size_t
&
freeRegionCount
)
{
if
(
expectedSize
<
MinFreeRegionSize
)
{
return
;
}
auto
*
freeRegion
=
FreeRegion
:
:
fromEndOffset
(
region
endOffset
)
;
MOZ_ASSERT
(
freeRegion
-
>
isInList
(
)
)
;
MOZ_ASSERT
(
freeRegion
-
>
size
(
)
=
=
expectedSize
)
;
freeRegionCount
+
+
;
}
void
BufferAllocator
:
:
checkAllocListGCStateNotInUse
(
LargeAllocList
&
list
bool
isNurseryOwned
)
{
for
(
LargeBuffer
*
buffer
:
list
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
=
=
isNurseryOwned
)
;
MOZ_ASSERT_IF
(
!
isNurseryOwned
!
buffer
-
>
allocatedDuringCollection
)
;
}
}
#
endif
void
*
BufferAllocator
:
:
allocSmall
(
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
MOZ_ASSERT
(
IsSmallAllocSize
(
bytes
)
)
;
bytes
=
RoundUp
(
std
:
:
max
(
bytes
MinSmallAllocSize
)
SmallAllocGranularity
)
;
MOZ_ASSERT
(
bytes
<
=
MaxSmallAllocSize
)
;
size_t
sizeClass
=
SizeClassForSmallAlloc
(
bytes
)
;
void
*
alloc
=
bumpAlloc
(
bytes
sizeClass
MaxSmallAllocClass
)
;
if
(
MOZ_UNLIKELY
(
!
alloc
)
)
{
alloc
=
retrySmallAlloc
(
bytes
sizeClass
inGC
)
;
if
(
!
alloc
)
{
return
nullptr
;
}
}
SmallBufferRegion
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
region
-
>
setAllocated
(
alloc
bytes
true
)
;
MOZ_ASSERT
(
region
-
>
allocBytes
(
alloc
)
=
=
bytes
)
;
MOZ_ASSERT
(
!
region
-
>
isNurseryOwned
(
alloc
)
)
;
region
-
>
setNurseryOwned
(
alloc
nurseryOwned
)
;
auto
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
if
(
nurseryOwned
&
&
!
region
-
>
hasNurseryOwnedAllocs
(
)
)
{
region
-
>
setHasNurseryOwnedAllocs
(
true
)
;
setChunkHasNurseryAllocs
(
chunk
)
;
}
MOZ_ASSERT
(
!
region
-
>
isMarked
(
alloc
)
)
;
MOZ_ASSERT
(
IsSmallAlloc
(
alloc
)
)
;
return
alloc
;
}
MOZ_NEVER_INLINE
void
*
BufferAllocator
:
:
retrySmallAlloc
(
size_t
bytes
size_t
sizeClass
bool
inGC
)
{
auto
alloc
=
[
&
]
(
)
{
return
bumpAlloc
(
bytes
sizeClass
MaxSmallAllocClass
)
;
}
;
auto
growHeap
=
[
&
]
(
)
{
return
allocNewSmallRegion
(
inGC
)
;
}
;
return
refillFreeListsAndRetryAlloc
(
sizeClass
MaxSmallAllocClass
alloc
growHeap
)
;
}
bool
BufferAllocator
:
:
allocNewSmallRegion
(
bool
inGC
)
{
void
*
ptr
=
allocMediumAligned
(
SmallRegionSize
inGC
)
;
if
(
!
ptr
)
{
return
false
;
}
auto
*
region
=
new
(
ptr
)
SmallBufferRegion
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
region
)
;
chunk
-
>
setSmallBufferRegion
(
region
true
)
;
uintptr_t
freeStart
=
uintptr_t
(
region
)
+
FirstSmallAllocOffset
;
uintptr_t
freeEnd
=
uintptr_t
(
region
)
+
SmallRegionSize
;
size_t
sizeClass
=
SizeClassForFreeRegion
(
freeEnd
-
freeStart
SizeKind
:
:
Small
)
;
ptr
=
reinterpret_cast
<
void
*
>
(
freeEnd
-
sizeof
(
FreeRegion
)
)
;
FreeRegion
*
freeRegion
=
new
(
ptr
)
FreeRegion
(
freeStart
)
;
MOZ_ASSERT
(
freeRegion
-
>
getEnd
(
)
=
=
freeEnd
)
;
freeLists
.
ref
(
)
.
pushFront
(
sizeClass
freeRegion
)
;
return
true
;
}
bool
BufferAllocator
:
:
IsSmallAlloc
(
void
*
alloc
)
{
MOZ_ASSERT
(
IsBufferAlloc
(
alloc
)
)
;
MOZ_ASSERT
(
!
IsLargeAlloc
(
alloc
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
isSmallBufferRegion
(
alloc
)
;
}
void
*
BufferAllocator
:
:
allocMedium
(
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
MOZ_ASSERT
(
!
IsSmallAllocSize
(
bytes
)
)
;
MOZ_ASSERT
(
!
IsLargeAllocSize
(
bytes
)
)
;
bytes
=
RoundUp
(
bytes
MediumAllocGranularity
)
;
MOZ_ASSERT
(
bytes
<
=
MaxMediumAllocSize
)
;
size_t
sizeClass
=
SizeClassForMediumAlloc
(
bytes
)
;
void
*
alloc
=
bumpAlloc
(
bytes
sizeClass
MaxMediumAllocClass
)
;
if
(
MOZ_UNLIKELY
(
!
alloc
)
)
{
alloc
=
retryMediumAlloc
(
bytes
sizeClass
inGC
)
;
if
(
!
alloc
)
{
return
nullptr
;
}
}
setAllocated
(
alloc
bytes
nurseryOwned
inGC
)
;
return
alloc
;
}
MOZ_NEVER_INLINE
void
*
BufferAllocator
:
:
retryMediumAlloc
(
size_t
bytes
size_t
sizeClass
bool
inGC
)
{
auto
alloc
=
[
&
]
(
)
{
return
bumpAlloc
(
bytes
sizeClass
MaxMediumAllocClass
)
;
}
;
auto
growHeap
=
[
&
]
(
)
{
return
allocNewChunk
(
inGC
)
;
}
;
return
refillFreeListsAndRetryAlloc
(
sizeClass
MaxMediumAllocClass
alloc
growHeap
)
;
}
template
<
typename
Alloc
typename
GrowHeap
>
void
*
BufferAllocator
:
:
refillFreeListsAndRetryAlloc
(
size_t
sizeClass
size_t
maxSizeClass
Alloc
&
&
alloc
GrowHeap
&
&
growHeap
)
{
for
(
;
;
)
{
RefillResult
r
=
refillFreeLists
(
sizeClass
maxSizeClass
growHeap
)
;
if
(
r
=
=
RefillResult
:
:
Fail
)
{
return
nullptr
;
}
if
(
r
=
=
RefillResult
:
:
Retry
)
{
continue
;
}
void
*
ptr
=
alloc
(
)
;
MOZ_ASSERT
(
ptr
)
;
return
ptr
;
}
}
template
<
typename
GrowHeap
>
BufferAllocator
:
:
RefillResult
BufferAllocator
:
:
refillFreeLists
(
size_t
sizeClass
size_t
maxSizeClass
GrowHeap
&
&
growHeap
)
{
MOZ_ASSERT
(
sizeClass
<
=
maxSizeClass
)
;
if
(
useAvailableChunk
(
sizeClass
maxSizeClass
)
)
{
return
RefillResult
:
:
Success
;
}
if
(
hasMinorSweepDataToMerge
)
{
mergeSweptData
(
)
;
return
RefillResult
:
:
Retry
;
}
if
(
growHeap
(
)
)
{
return
RefillResult
:
:
Success
;
}
return
RefillResult
:
:
Fail
;
}
bool
BufferAllocator
:
:
useAvailableChunk
(
size_t
sizeClass
size_t
maxSizeClass
)
{
return
useAvailableChunk
(
sizeClass
maxSizeClass
availableMixedChunks
.
ref
(
)
mixedChunks
.
ref
(
)
)
|
|
useAvailableChunk
(
sizeClass
maxSizeClass
availableTenuredChunks
.
ref
(
)
tenuredChunks
.
ref
(
)
)
;
}
bool
BufferAllocator
:
:
useAvailableChunk
(
size_t
sizeClass
size_t
maxSizeClass
ChunkLists
&
src
BufferChunkList
&
dst
)
{
MOZ_ASSERT
(
freeLists
.
ref
(
)
.
getFirstAvailableSizeClass
(
sizeClass
maxSizeClass
)
=
=
SIZE_MAX
)
;
SizeClassBitSet
sizeClasses
=
getChunkSizeClassesToMove
(
maxSizeClass
src
)
;
for
(
auto
i
=
BitSetIter
(
sizeClasses
)
;
!
i
.
done
(
)
;
i
.
next
(
)
)
{
MOZ_ASSERT
(
i
<
=
maxSizeClass
)
;
MOZ_ASSERT
(
!
freeLists
.
ref
(
)
.
hasSizeClass
(
i
)
)
;
BufferChunk
*
chunk
=
src
.
popFirstChunk
(
i
)
;
MOZ_ASSERT
(
chunk
-
>
ownsFreeLists
)
;
MOZ_ASSERT
(
chunk
-
>
freeLists
.
ref
(
)
.
hasSizeClass
(
i
)
)
;
dst
.
pushBack
(
chunk
)
;
freeLists
.
ref
(
)
.
append
(
std
:
:
move
(
chunk
-
>
freeLists
.
ref
(
)
)
)
;
chunk
-
>
ownsFreeLists
=
false
;
chunk
-
>
freeLists
.
ref
(
)
.
assertEmpty
(
)
;
if
(
i
>
=
sizeClass
)
{
MOZ_ASSERT
(
freeLists
.
ref
(
)
.
getFirstAvailableSizeClass
(
sizeClass
maxSizeClass
)
!
=
SIZE_MAX
)
;
return
true
;
}
}
MOZ_ASSERT
(
freeLists
.
ref
(
)
.
getFirstAvailableSizeClass
(
sizeClass
maxSizeClass
)
=
=
SIZE_MAX
)
;
return
false
;
}
BufferAllocator
:
:
SizeClassBitSet
BufferAllocator
:
:
getChunkSizeClassesToMove
(
size_t
maxSizeClass
ChunkLists
&
src
)
const
{
SizeClassBitSet
result
;
auto
&
sizeClasses
=
result
.
Storage
(
)
[
0
]
;
auto
&
srcAvailable
=
src
.
availableSizeClasses
(
)
.
Storage
(
)
[
0
]
;
auto
&
freeAvailable
=
freeLists
.
ref
(
)
.
availableSizeClasses
(
)
.
Storage
(
)
[
0
]
;
sizeClasses
=
srcAvailable
&
~
freeAvailable
&
BitMask
(
maxSizeClass
+
1
)
;
return
result
;
}
static
bool
IsMediumSizeClass
(
size_t
sizeClass
)
{
MOZ_ASSERT
(
sizeClass
<
BufferAllocator
:
:
AllocSizeClasses
)
;
return
sizeClass
>
=
MinMediumAllocClass
;
}
BufferAllocator
:
:
SizeKind
BufferAllocator
:
:
SizeClassKind
(
size_t
sizeClass
)
{
return
IsMediumSizeClass
(
sizeClass
)
?
SizeKind
:
:
Medium
:
SizeKind
:
:
Small
;
}
void
*
BufferAllocator
:
:
bumpAlloc
(
size_t
bytes
size_t
sizeClass
size_t
maxSizeClass
)
{
MOZ_ASSERT
(
SizeClassKind
(
sizeClass
)
=
=
SizeClassKind
(
maxSizeClass
)
)
;
freeLists
.
ref
(
)
.
checkAvailable
(
)
;
sizeClass
=
freeLists
.
ref
(
)
.
getFirstAvailableSizeClass
(
sizeClass
maxSizeClass
)
;
if
(
sizeClass
=
=
SIZE_MAX
)
{
return
nullptr
;
}
FreeRegion
*
region
=
freeLists
.
ref
(
)
.
getFirstRegion
(
sizeClass
)
;
MOZ_ASSERT
(
region
-
>
size
(
)
>
=
bytes
)
;
void
*
ptr
=
allocFromRegion
(
region
bytes
sizeClass
)
;
updateFreeListsAfterAlloc
(
&
freeLists
.
ref
(
)
region
sizeClass
)
;
return
ptr
;
}
#
ifdef
DEBUG
static
size_t
GranularityForSizeClass
(
size_t
sizeClass
)
{
return
IsMediumSizeClass
(
sizeClass
)
?
MediumAllocGranularity
:
SmallAllocGranularity
;
}
#
endif
void
*
BufferAllocator
:
:
allocFromRegion
(
FreeRegion
*
region
size_t
bytes
size_t
sizeClass
)
{
uintptr_t
start
=
region
-
>
startAddr
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
>
start
)
;
MOZ_ASSERT_IF
(
sizeClass
!
=
MaxMediumAllocClass
region
-
>
size
(
)
>
=
SizeClassBytes
(
sizeClass
)
)
;
MOZ_ASSERT_IF
(
sizeClass
=
=
MaxMediumAllocClass
region
-
>
size
(
)
>
=
MaxMediumAllocSize
)
;
MOZ_ASSERT
(
start
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
MOZ_ASSERT
(
region
-
>
size
(
)
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
if
(
region
-
>
hasDecommittedPages
)
{
recommitRegion
(
region
)
;
}
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
start
)
;
start
+
=
bytes
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
>
=
start
)
;
region
-
>
startAddr
=
start
;
return
ptr
;
}
void
*
BufferAllocator
:
:
allocMediumAligned
(
size_t
bytes
bool
inGC
)
{
MOZ_ASSERT
(
bytes
>
=
MinMediumAllocSize
)
;
MOZ_ASSERT
(
bytes
<
=
MaxAlignedAllocSize
)
;
MOZ_ASSERT
(
mozilla
:
:
IsPowerOfTwo
(
bytes
)
)
;
size_t
sizeClass
=
SizeClassForMediumAlloc
(
bytes
)
;
void
*
alloc
=
alignedAlloc
(
sizeClass
)
;
if
(
MOZ_UNLIKELY
(
!
alloc
)
)
{
alloc
=
retryAlignedAlloc
(
sizeClass
inGC
)
;
if
(
!
alloc
)
{
return
nullptr
;
}
}
setAllocated
(
alloc
bytes
false
inGC
)
;
return
alloc
;
}
MOZ_NEVER_INLINE
void
*
BufferAllocator
:
:
retryAlignedAlloc
(
size_t
sizeClass
bool
inGC
)
{
auto
alloc
=
[
&
]
(
)
{
return
alignedAlloc
(
sizeClass
)
;
}
;
auto
growHeap
=
[
&
]
(
)
{
return
allocNewChunk
(
inGC
)
;
}
;
return
refillFreeListsAndRetryAlloc
(
sizeClass
+
1
MaxMediumAllocClass
alloc
growHeap
)
;
}
void
*
BufferAllocator
:
:
alignedAlloc
(
size_t
sizeClass
)
{
freeLists
.
ref
(
)
.
checkAvailable
(
)
;
size_t
allocClass
=
freeLists
.
ref
(
)
.
getFirstAvailableSizeClass
(
sizeClass
MaxMediumAllocClass
)
;
MOZ_ASSERT
(
allocClass
>
=
sizeClass
)
;
if
(
allocClass
=
=
SIZE_MAX
)
{
return
nullptr
;
}
FreeRegion
*
region
=
freeLists
.
ref
(
)
.
getFirstRegion
(
allocClass
)
;
void
*
ptr
=
alignedAllocFromRegion
(
region
sizeClass
)
;
if
(
ptr
)
{
updateFreeListsAfterAlloc
(
&
freeLists
.
ref
(
)
region
allocClass
)
;
return
ptr
;
}
MOZ_ASSERT
(
allocClass
=
=
sizeClass
)
;
allocClass
=
freeLists
.
ref
(
)
.
getFirstAvailableSizeClass
(
sizeClass
+
1
MaxMediumAllocClass
)
;
if
(
allocClass
=
=
SIZE_MAX
)
{
return
nullptr
;
}
region
=
freeLists
.
ref
(
)
.
getFirstRegion
(
allocClass
)
;
ptr
=
alignedAllocFromRegion
(
region
sizeClass
)
;
MOZ_ASSERT
(
ptr
)
;
updateFreeListsAfterAlloc
(
&
freeLists
.
ref
(
)
region
allocClass
)
;
return
ptr
;
}
void
*
BufferAllocator
:
:
alignedAllocFromRegion
(
FreeRegion
*
region
size_t
sizeClass
)
{
uintptr_t
start
=
region
-
>
startAddr
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
>
start
)
;
MOZ_ASSERT
(
region
-
>
size
(
)
>
=
SizeClassBytes
(
sizeClass
)
)
;
MOZ_ASSERT
(
(
region
-
>
size
(
)
%
MinMediumAllocSize
)
=
=
0
)
;
size_t
bytes
=
SizeClassBytes
(
sizeClass
)
;
size_t
alignedStart
=
RoundUp
(
start
bytes
)
;
size_t
end
=
alignedStart
+
bytes
;
if
(
end
>
region
-
>
getEnd
(
)
)
{
return
nullptr
;
}
if
(
alignedStart
!
=
start
)
{
size_t
alignBytes
=
alignedStart
-
start
;
void
*
prefix
=
allocFromRegion
(
region
alignBytes
sizeClass
)
;
MOZ_ASSERT
(
uintptr_t
(
prefix
)
=
=
start
)
;
(
void
)
prefix
;
MOZ_ASSERT
(
!
region
-
>
hasDecommittedPages
)
;
addFreeRegion
(
&
freeLists
.
ref
(
)
start
alignBytes
SizeKind
:
:
Medium
false
ListPosition
:
:
Back
)
;
}
MOZ_ASSERT
(
region
-
>
startAddr
%
bytes
=
=
0
)
;
return
allocFromRegion
(
region
bytes
sizeClass
)
;
}
void
BufferAllocator
:
:
setAllocated
(
void
*
alloc
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
chunk
-
>
setAllocated
(
alloc
bytes
true
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
bytes
)
;
MOZ_ASSERT
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
;
chunk
-
>
setNurseryOwned
(
alloc
nurseryOwned
)
;
if
(
nurseryOwned
)
{
setChunkHasNurseryAllocs
(
chunk
)
;
}
MOZ_ASSERT
(
!
chunk
-
>
isMarked
(
alloc
)
)
;
if
(
!
nurseryOwned
)
{
bool
checkThresholds
=
!
inGC
;
increaseHeapSize
(
bytes
checkThresholds
false
)
;
}
MOZ_ASSERT
(
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
)
;
}
void
BufferAllocator
:
:
setChunkHasNurseryAllocs
(
BufferChunk
*
chunk
)
{
MOZ_ASSERT
(
!
chunk
-
>
ownsFreeLists
)
;
if
(
chunk
-
>
hasNurseryOwnedAllocs
)
{
return
;
}
tenuredChunks
.
ref
(
)
.
remove
(
chunk
)
;
mixedChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
chunk
-
>
hasNurseryOwnedAllocs
=
true
;
}
void
BufferAllocator
:
:
updateFreeListsAfterAlloc
(
FreeLists
*
freeLists
FreeRegion
*
region
size_t
sizeClass
)
{
freeLists
-
>
assertContains
(
sizeClass
region
)
;
size_t
classBytes
=
SizeClassBytes
(
sizeClass
)
;
size_t
newSize
=
region
-
>
size
(
)
;
MOZ_ASSERT
(
newSize
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
if
(
newSize
>
=
classBytes
)
{
return
;
}
freeLists
-
>
remove
(
sizeClass
region
)
;
if
(
newSize
=
=
0
)
{
return
;
}
if
(
newSize
<
MinFreeRegionSize
)
{
return
;
}
size_t
newSizeClass
=
SizeClassForFreeRegion
(
newSize
SizeClassKind
(
sizeClass
)
)
;
MOZ_ASSERT_IF
(
newSizeClass
!
=
MaxMediumAllocClass
newSize
>
=
SizeClassBytes
(
newSizeClass
)
)
;
MOZ_ASSERT
(
newSizeClass
<
=
sizeClass
)
;
MOZ_ASSERT_IF
(
newSizeClass
!
=
MaxMediumAllocClass
newSizeClass
<
sizeClass
)
;
MOZ_ASSERT
(
SizeClassKind
(
newSizeClass
)
=
=
SizeClassKind
(
sizeClass
)
)
;
freeLists
-
>
pushFront
(
newSizeClass
region
)
;
}
void
BufferAllocator
:
:
recommitRegion
(
FreeRegion
*
region
)
{
MOZ_ASSERT
(
region
-
>
hasDecommittedPages
)
;
MOZ_ASSERT
(
DecommitEnabled
(
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
region
)
;
uintptr_t
startAddr
=
RoundUp
(
region
-
>
startAddr
PageSize
)
;
uintptr_t
endAddr
=
RoundDown
(
uintptr_t
(
region
)
PageSize
)
;
size_t
startPage
=
(
startAddr
-
uintptr_t
(
chunk
)
)
/
PageSize
;
size_t
endPage
=
(
endAddr
-
uintptr_t
(
chunk
)
)
/
PageSize
;
MOZ_ASSERT_IF
(
(
region
-
>
startAddr
%
PageSize
)
!
=
0
!
chunk
-
>
decommittedPages
.
ref
(
)
[
startPage
-
1
]
)
;
MOZ_ASSERT
(
!
chunk
-
>
decommittedPages
.
ref
(
)
[
endPage
]
)
;
MarkPagesInUseSoft
(
reinterpret_cast
<
void
*
>
(
startAddr
)
endAddr
-
startAddr
)
;
for
(
size_t
i
=
startPage
;
i
!
=
endPage
;
i
+
+
)
{
chunk
-
>
decommittedPages
.
ref
(
)
[
i
]
=
false
;
}
region
-
>
hasDecommittedPages
=
false
;
}
static
inline
StallAndRetry
ShouldStallAndRetry
(
bool
inGC
)
{
return
inGC
?
StallAndRetry
:
:
Yes
:
StallAndRetry
:
:
No
;
}
bool
BufferAllocator
:
:
allocNewChunk
(
bool
inGC
)
{
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromMainThread
(
)
-
>
gc
;
AutoLockGCBgAlloc
lock
(
gc
)
;
ArenaChunk
*
baseChunk
=
gc
-
>
getOrAllocChunk
(
ShouldStallAndRetry
(
inGC
)
lock
)
;
if
(
!
baseChunk
)
{
return
false
;
}
CheckHighBitsOfPointer
(
baseChunk
)
;
if
(
!
baseChunk
-
>
decommittedPages
.
IsEmpty
(
)
)
{
MOZ_ASSERT
(
DecommitEnabled
(
)
)
;
MarkPagesInUseSoft
(
baseChunk
ChunkSize
)
;
}
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
uintptr_t
(
baseChunk
)
+
sizeof
(
ChunkBase
)
)
;
size_t
size
=
ChunkSize
-
sizeof
(
ChunkBase
)
;
SetMemCheckKind
(
ptr
size
MemCheckKind
:
:
MakeUndefined
)
;
BufferChunk
*
chunk
=
new
(
baseChunk
)
BufferChunk
(
zone
)
;
chunk
-
>
allocatedDuringCollection
=
majorState
!
=
State
:
:
NotCollecting
;
tenuredChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
uintptr_t
freeStart
=
uintptr_t
(
chunk
)
+
FirstMediumAllocOffset
;
uintptr_t
freeEnd
=
uintptr_t
(
chunk
)
+
ChunkSize
;
size_t
sizeClass
=
SizeClassForFreeRegion
(
freeEnd
-
freeStart
SizeKind
:
:
Medium
)
;
MOZ_ASSERT
(
sizeClass
>
MaxSmallAllocClass
)
;
MOZ_ASSERT
(
sizeClass
<
=
MaxMediumAllocClass
)
;
ptr
=
reinterpret_cast
<
void
*
>
(
freeEnd
-
sizeof
(
FreeRegion
)
)
;
FreeRegion
*
region
=
new
(
ptr
)
FreeRegion
(
freeStart
)
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
=
=
freeEnd
)
;
freeLists
.
ref
(
)
.
pushFront
(
sizeClass
region
)
;
return
true
;
}
static
void
SetDeallocated
(
BufferChunk
*
chunk
void
*
alloc
size_t
bytes
)
{
MOZ_ASSERT
(
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
bytes
)
;
chunk
-
>
setNurseryOwned
(
alloc
false
)
;
chunk
-
>
setAllocated
(
alloc
bytes
false
)
;
}
static
void
SetDeallocated
(
SmallBufferRegion
*
region
void
*
alloc
size_t
bytes
)
{
MOZ_ASSERT
(
region
-
>
allocBytes
(
alloc
)
=
=
bytes
)
;
region
-
>
setNurseryOwned
(
alloc
false
)
;
region
-
>
setAllocated
(
alloc
bytes
false
)
;
}
bool
BufferAllocator
:
:
sweepChunk
(
BufferChunk
*
chunk
SweepKind
sweepKind
bool
shouldDecommit
)
{
MOZ_ASSERT_IF
(
sweepKind
=
=
SweepKind
:
:
Tenured
!
chunk
-
>
allocatedDuringCollection
)
;
MOZ_ASSERT_IF
(
sweepKind
=
=
SweepKind
:
:
Tenured
chunk
-
>
ownsFreeLists
)
;
FreeLists
&
freeLists
=
chunk
-
>
freeLists
.
ref
(
)
;
freeLists
.
clear
(
)
;
chunk
-
>
ownsFreeLists
=
true
;
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromAnyThread
(
)
-
>
gc
;
bool
hasNurseryOwnedAllocs
=
false
;
size_t
freeStart
=
FirstMediumAllocOffset
;
bool
sweptAny
=
false
;
size_t
mallocHeapBytesFreed
=
0
;
for
(
auto
iter
=
chunk
-
>
smallRegionIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
SmallBufferRegion
*
region
=
iter
.
get
(
)
;
MOZ_ASSERT
(
!
chunk
-
>
isMarked
(
region
)
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
region
)
=
=
SmallRegionSize
)
;
if
(
!
sweepSmallBufferRegion
(
chunk
region
sweepKind
)
)
{
chunk
-
>
setSmallBufferRegion
(
region
false
)
;
SetDeallocated
(
chunk
region
SmallRegionSize
)
;
PoisonAlloc
(
region
JS_SWEPT_TENURED_PATTERN
sizeof
(
SmallBufferRegion
)
MemCheckKind
:
:
MakeUndefined
)
;
mallocHeapBytesFreed
+
=
SmallRegionSize
;
sweptAny
=
true
;
}
else
if
(
region
-
>
hasNurseryOwnedAllocs
(
)
)
{
hasNurseryOwnedAllocs
=
true
;
}
}
for
(
auto
iter
=
chunk
-
>
allocIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
void
*
alloc
=
iter
.
get
(
)
;
size_t
bytes
=
chunk
-
>
allocBytes
(
alloc
)
;
uintptr_t
allocEnd
=
iter
.
getOffset
(
)
+
bytes
;
bool
nurseryOwned
=
chunk
-
>
isNurseryOwned
(
alloc
)
;
bool
canSweep
=
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
&
&
CanSweepAlloc
(
nurseryOwned
sweepKind
)
;
bool
shouldSweep
=
canSweep
&
&
!
chunk
-
>
isMarked
(
alloc
)
;
if
(
shouldSweep
)
{
if
(
!
nurseryOwned
)
{
mallocHeapBytesFreed
+
=
bytes
;
}
SetDeallocated
(
chunk
alloc
bytes
)
;
PoisonAlloc
(
alloc
JS_SWEPT_TENURED_PATTERN
bytes
MemCheckKind
:
:
MakeUndefined
)
;
sweptAny
=
true
;
}
else
{
uintptr_t
allocStart
=
iter
.
getOffset
(
)
;
if
(
freeStart
!
=
allocStart
)
{
addSweptRegion
(
chunk
freeStart
allocStart
shouldDecommit
!
sweptAny
freeLists
)
;
}
freeStart
=
allocEnd
;
if
(
canSweep
)
{
chunk
-
>
setUnmarked
(
alloc
)
;
}
if
(
nurseryOwned
)
{
MOZ_ASSERT
(
sweepKind
=
=
SweepKind
:
:
Nursery
)
;
hasNurseryOwnedAllocs
=
true
;
}
}
}
if
(
mallocHeapBytesFreed
)
{
bool
inMajorGC
=
sweepKind
=
=
SweepKind
:
:
Tenured
;
decreaseHeapSize
(
mallocHeapBytesFreed
inMajorGC
)
;
}
if
(
freeStart
=
=
FirstMediumAllocOffset
)
{
bool
allMemoryCommitted
=
chunk
-
>
decommittedPages
.
ref
(
)
.
IsEmpty
(
)
;
chunk
-
>
~
BufferChunk
(
)
;
ArenaChunk
*
tenuredChunk
=
ArenaChunk
:
:
init
(
chunk
gc
allMemoryCommitted
)
;
AutoLockGC
lock
(
gc
)
;
gc
-
>
recycleChunk
(
tenuredChunk
lock
)
;
return
false
;
}
if
(
freeStart
!
=
ChunkSize
)
{
addSweptRegion
(
chunk
freeStart
ChunkSize
shouldDecommit
!
sweptAny
freeLists
)
;
}
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
=
hasNurseryOwnedAllocs
;
return
true
;
}
bool
BufferAllocator
:
:
CanSweepAlloc
(
bool
nurseryOwned
BufferAllocator
:
:
SweepKind
sweepKind
)
{
static_assert
(
SweepKind
:
:
Nursery
=
=
SweepKind
(
uint8_t
(
true
)
)
)
;
static_assert
(
SweepKind
:
:
Tenured
=
=
SweepKind
(
uint8_t
(
false
)
)
)
;
SweepKind
requiredKind
=
SweepKind
(
uint8_t
(
nurseryOwned
)
)
;
return
sweepKind
=
=
requiredKind
;
}
void
BufferAllocator
:
:
addSweptRegion
(
BufferChunk
*
chunk
uintptr_t
freeStart
uintptr_t
freeEnd
bool
shouldDecommit
bool
expectUnchanged
FreeLists
&
freeLists
)
{
MOZ_ASSERT
(
freeStart
>
=
FirstMediumAllocOffset
)
;
MOZ_ASSERT
(
freeStart
<
freeEnd
)
;
MOZ_ASSERT
(
freeEnd
<
=
ChunkSize
)
;
MOZ_ASSERT
(
(
freeStart
%
MediumAllocGranularity
)
=
=
0
)
;
MOZ_ASSERT
(
(
freeEnd
%
MediumAllocGranularity
)
=
=
0
)
;
MOZ_ASSERT_IF
(
shouldDecommit
DecommitEnabled
(
)
)
;
bool
anyDecommitted
=
false
;
uintptr_t
decommitStart
=
RoundUp
(
freeStart
PageSize
)
;
uintptr_t
decommitEnd
=
RoundDown
(
freeEnd
-
sizeof
(
FreeRegion
)
PageSize
)
;
size_t
endPage
=
decommitEnd
/
PageSize
;
if
(
shouldDecommit
&
&
decommitEnd
>
decommitStart
)
{
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
decommitStart
+
uintptr_t
(
chunk
)
)
;
MarkPagesUnusedSoft
(
ptr
decommitEnd
-
decommitStart
)
;
size_t
startPage
=
decommitStart
/
PageSize
;
for
(
size_t
i
=
startPage
;
i
!
=
endPage
;
i
+
+
)
{
chunk
-
>
decommittedPages
.
ref
(
)
[
i
]
=
true
;
}
anyDecommitted
=
true
;
}
else
{
uintptr_t
startPage
=
RoundDown
(
freeStart
PageSize
)
/
PageSize
;
for
(
size_t
i
=
startPage
;
i
!
=
endPage
;
i
+
+
)
{
if
(
chunk
-
>
decommittedPages
.
ref
(
)
[
i
]
)
{
anyDecommitted
=
true
;
}
}
}
MOZ_ASSERT
(
!
chunk
-
>
decommittedPages
.
ref
(
)
[
endPage
]
)
;
freeStart
+
=
uintptr_t
(
chunk
)
;
freeEnd
+
=
uintptr_t
(
chunk
)
;
size_t
bytes
=
freeEnd
-
freeStart
;
addFreeRegion
(
&
freeLists
freeStart
bytes
SizeKind
:
:
Medium
anyDecommitted
ListPosition
:
:
Back
expectUnchanged
)
;
}
bool
BufferAllocator
:
:
sweepSmallBufferRegion
(
BufferChunk
*
chunk
SmallBufferRegion
*
region
SweepKind
sweepKind
)
{
bool
hasNurseryOwnedAllocs
=
false
;
FreeLists
&
freeLists
=
chunk
-
>
freeLists
.
ref
(
)
;
size_t
freeStart
=
FirstSmallAllocOffset
;
bool
sweptAny
=
false
;
for
(
auto
iter
=
region
-
>
allocIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
void
*
alloc
=
iter
.
get
(
)
;
size_t
bytes
=
region
-
>
allocBytes
(
alloc
)
;
uintptr_t
allocEnd
=
iter
.
getOffset
(
)
+
bytes
;
bool
nurseryOwned
=
region
-
>
isNurseryOwned
(
alloc
)
;
bool
canSweep
=
CanSweepAlloc
(
nurseryOwned
sweepKind
)
;
bool
shouldSweep
=
canSweep
&
&
!
region
-
>
isMarked
(
alloc
)
;
if
(
shouldSweep
)
{
SetDeallocated
(
region
alloc
bytes
)
;
PoisonAlloc
(
alloc
JS_SWEPT_TENURED_PATTERN
bytes
MemCheckKind
:
:
MakeUndefined
)
;
sweptAny
=
true
;
}
else
{
uintptr_t
allocStart
=
iter
.
getOffset
(
)
;
if
(
freeStart
!
=
allocStart
)
{
addSweptRegion
(
region
freeStart
allocStart
!
sweptAny
freeLists
)
;
}
freeStart
=
allocEnd
;
if
(
canSweep
)
{
region
-
>
setUnmarked
(
alloc
)
;
}
if
(
nurseryOwned
)
{
MOZ_ASSERT
(
sweepKind
=
=
SweepKind
:
:
Nursery
)
;
hasNurseryOwnedAllocs
=
true
;
}
sweptAny
=
false
;
}
}
if
(
freeStart
=
=
FirstSmallAllocOffset
)
{
return
false
;
}
if
(
freeStart
!
=
SmallRegionSize
)
{
addSweptRegion
(
region
freeStart
SmallRegionSize
!
sweptAny
freeLists
)
;
}
region
-
>
setHasNurseryOwnedAllocs
(
hasNurseryOwnedAllocs
)
;
return
true
;
}
void
BufferAllocator
:
:
addSweptRegion
(
SmallBufferRegion
*
region
uintptr_t
freeStart
uintptr_t
freeEnd
bool
expectUnchanged
FreeLists
&
freeLists
)
{
MOZ_ASSERT
(
freeStart
>
=
FirstSmallAllocOffset
)
;
MOZ_ASSERT
(
freeStart
<
freeEnd
)
;
MOZ_ASSERT
(
freeEnd
<
=
SmallRegionSize
)
;
MOZ_ASSERT
(
freeStart
%
SmallAllocGranularity
=
=
0
)
;
MOZ_ASSERT
(
freeEnd
%
SmallAllocGranularity
=
=
0
)
;
freeStart
+
=
uintptr_t
(
region
)
;
freeEnd
+
=
uintptr_t
(
region
)
;
size_t
bytes
=
freeEnd
-
freeStart
;
addFreeRegion
(
&
freeLists
freeStart
bytes
SizeKind
:
:
Small
false
ListPosition
:
:
Back
expectUnchanged
)
;
}
void
BufferAllocator
:
:
freeMedium
(
void
*
alloc
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
size_t
bytes
=
chunk
-
>
allocBytes
(
alloc
)
;
PoisonAlloc
(
alloc
JS_FREED_BUFFER_PATTERN
bytes
MemCheckKind
:
:
MakeUndefined
)
;
if
(
isSweepingChunk
(
chunk
)
)
{
return
;
}
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bool
updateRetained
=
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
;
decreaseHeapSize
(
bytes
updateRetained
)
;
}
chunk
-
>
setUnmarked
(
alloc
)
;
SetDeallocated
(
chunk
alloc
bytes
)
;
FreeLists
*
freeLists
=
getChunkFreeLists
(
chunk
)
;
uintptr_t
startAddr
=
uintptr_t
(
alloc
)
;
uintptr_t
endAddr
=
startAddr
+
bytes
;
ChunkLists
*
availableChunks
=
getChunkAvailableLists
(
chunk
)
;
size_t
oldChunkSizeClass
=
SIZE_MAX
;
if
(
availableChunks
)
{
oldChunkSizeClass
=
chunk
-
>
sizeClassForAvailableLists
(
)
;
}
FreeRegion
*
region
;
uintptr_t
endOffset
=
endAddr
&
ChunkMask
;
if
(
endOffset
=
=
0
|
|
chunk
-
>
isAllocated
(
endOffset
)
)
{
region
=
addFreeRegion
(
freeLists
startAddr
bytes
SizeKind
:
:
Medium
false
ListPosition
:
:
Front
)
;
MOZ_ASSERT
(
region
)
;
}
else
{
region
=
chunk
-
>
findFollowingFreeRegion
(
endAddr
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
endAddr
)
;
updateFreeRegionStart
(
freeLists
region
startAddr
SizeKind
:
:
Medium
)
;
}
FreeRegion
*
precRegion
=
chunk
-
>
findPrecedingFreeRegion
(
startAddr
)
;
if
(
precRegion
)
{
if
(
freeLists
)
{
size_t
sizeClass
=
SizeClassForFreeRegion
(
precRegion
-
>
size
(
)
SizeKind
:
:
Medium
)
;
freeLists
-
>
remove
(
sizeClass
precRegion
)
;
}
updateFreeRegionStart
(
freeLists
region
precRegion
-
>
startAddr
SizeKind
:
:
Medium
)
;
if
(
precRegion
-
>
hasDecommittedPages
)
{
region
-
>
hasDecommittedPages
=
true
;
}
}
if
(
availableChunks
)
{
maybeUpdateAvailableLists
(
availableChunks
chunk
oldChunkSizeClass
)
;
}
}
void
BufferAllocator
:
:
maybeUpdateAvailableLists
(
ChunkLists
*
availableChunks
BufferChunk
*
chunk
size_t
oldChunkSizeClass
)
{
size_t
newChunkSizeClass
=
chunk
-
>
sizeClassForAvailableLists
(
)
;
if
(
newChunkSizeClass
!
=
oldChunkSizeClass
)
{
availableChunks
-
>
remove
(
oldChunkSizeClass
chunk
)
;
availableChunks
-
>
pushBack
(
newChunkSizeClass
chunk
)
;
}
}
bool
BufferAllocator
:
:
isSweepingChunk
(
BufferChunk
*
chunk
)
{
if
(
minorState
=
=
State
:
:
Sweeping
&
&
chunk
-
>
hasNurseryOwnedAllocs
)
{
if
(
!
hasMinorSweepDataToMerge
)
{
#
ifdef
DEBUG
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT_IF
(
!
hasMinorSweepDataToMerge
!
minorSweepingFinished
)
;
}
#
endif
return
true
;
}
mergeSweptData
(
)
;
if
(
minorState
=
=
State
:
:
Sweeping
&
&
chunk
-
>
hasNurseryOwnedAllocs
)
{
return
true
;
}
}
if
(
majorState
=
=
State
:
:
Sweeping
&
&
!
chunk
-
>
allocatedDuringCollection
)
{
return
true
;
}
return
false
;
}
BufferAllocator
:
:
FreeRegion
*
BufferAllocator
:
:
addFreeRegion
(
FreeLists
*
freeLists
uintptr_t
start
size_t
bytes
SizeKind
kind
bool
anyDecommitted
ListPosition
position
bool
expectUnchanged
)
{
static_assert
(
sizeof
(
FreeRegion
)
<
=
MinFreeRegionSize
)
;
if
(
bytes
<
MinFreeRegionSize
)
{
return
nullptr
;
}
size_t
sizeClass
=
SizeClassForFreeRegion
(
bytes
kind
)
;
MOZ_ASSERT_IF
(
sizeClass
!
=
MaxMediumAllocClass
bytes
>
=
SizeClassBytes
(
sizeClass
)
)
;
MOZ_ASSERT
(
start
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
MOZ_ASSERT
(
bytes
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
uintptr_t
end
=
start
+
bytes
;
#
ifdef
DEBUG
if
(
expectUnchanged
)
{
auto
*
region
=
FreeRegion
:
:
fromEndAddr
(
end
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
start
)
;
}
#
endif
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
end
-
sizeof
(
FreeRegion
)
)
;
FreeRegion
*
region
=
new
(
ptr
)
FreeRegion
(
start
anyDecommitted
)
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
=
=
end
)
;
if
(
freeLists
)
{
if
(
position
=
=
ListPosition
:
:
Front
)
{
freeLists
-
>
pushFront
(
sizeClass
region
)
;
}
else
{
freeLists
-
>
pushBack
(
sizeClass
region
)
;
}
}
return
region
;
}
void
BufferAllocator
:
:
updateFreeRegionStart
(
FreeLists
*
freeLists
FreeRegion
*
region
uintptr_t
newStart
SizeKind
kind
)
{
MOZ_ASSERT
(
(
newStart
&
~
ChunkMask
)
=
=
(
uintptr_t
(
region
)
&
~
ChunkMask
)
)
;
MOZ_ASSERT
(
region
-
>
startAddr
!
=
newStart
)
;
MOZ_ASSERT
(
kind
=
=
SizeKind
:
:
Medium
)
;
size_t
oldSize
=
region
-
>
size
(
)
;
region
-
>
startAddr
=
newStart
;
if
(
!
freeLists
)
{
return
;
}
size_t
currentSizeClass
=
SizeClassForFreeRegion
(
oldSize
kind
)
;
size_t
newSizeClass
=
SizeClassForFreeRegion
(
region
-
>
size
(
)
kind
)
;
MOZ_ASSERT
(
SizeClassKind
(
newSizeClass
)
=
=
SizeClassKind
(
currentSizeClass
)
)
;
if
(
currentSizeClass
!
=
newSizeClass
)
{
freeLists
-
>
remove
(
currentSizeClass
region
)
;
freeLists
-
>
pushFront
(
newSizeClass
region
)
;
}
}
bool
BufferAllocator
:
:
growMedium
(
void
*
alloc
size_t
newBytes
)
{
MOZ_ASSERT
(
!
IsSmallAllocSize
(
newBytes
)
)
;
MOZ_ASSERT
(
!
IsLargeAllocSize
(
newBytes
)
)
;
newBytes
=
std
:
:
max
(
newBytes
MinMediumAllocSize
)
;
MOZ_ASSERT
(
newBytes
=
=
GetGoodAllocSize
(
newBytes
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
if
(
isSweepingChunk
(
chunk
)
)
{
return
false
;
}
size_t
currentBytes
=
chunk
-
>
allocBytes
(
alloc
)
;
MOZ_ASSERT
(
newBytes
>
currentBytes
)
;
uintptr_t
endOffset
=
(
uintptr_t
(
alloc
)
&
ChunkMask
)
+
currentBytes
;
MOZ_ASSERT
(
endOffset
<
=
ChunkSize
)
;
if
(
endOffset
=
=
ChunkSize
)
{
return
false
;
}
size_t
endAddr
=
uintptr_t
(
chunk
)
+
endOffset
;
if
(
chunk
-
>
isAllocated
(
endOffset
)
)
{
return
false
;
}
FreeRegion
*
region
=
chunk
-
>
findFollowingFreeRegion
(
endAddr
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
endAddr
)
;
size_t
extraBytes
=
newBytes
-
currentBytes
;
if
(
region
-
>
size
(
)
<
extraBytes
)
{
return
false
;
}
size_t
sizeClass
=
SizeClassForFreeRegion
(
region
-
>
size
(
)
SizeKind
:
:
Medium
)
;
allocFromRegion
(
region
extraBytes
sizeClass
)
;
ChunkLists
*
availableChunks
=
getChunkAvailableLists
(
chunk
)
;
size_t
oldChunkSizeClass
=
SIZE_MAX
;
if
(
availableChunks
)
{
oldChunkSizeClass
=
chunk
-
>
sizeClassForAvailableLists
(
)
;
}
FreeLists
*
freeLists
=
getChunkFreeLists
(
chunk
)
;
updateFreeListsAfterAlloc
(
freeLists
region
sizeClass
)
;
if
(
availableChunks
)
{
maybeUpdateAvailableLists
(
availableChunks
chunk
oldChunkSizeClass
)
;
}
chunk
-
>
updateEndOffset
(
alloc
currentBytes
newBytes
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
newBytes
)
;
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bool
updateRetained
=
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
;
increaseHeapSize
(
extraBytes
true
updateRetained
)
;
}
return
true
;
}
bool
BufferAllocator
:
:
shrinkMedium
(
void
*
alloc
size_t
newBytes
)
{
MOZ_ASSERT
(
!
IsSmallAllocSize
(
newBytes
)
)
;
MOZ_ASSERT
(
!
IsLargeAllocSize
(
newBytes
)
)
;
newBytes
=
std
:
:
max
(
newBytes
MinMediumAllocSize
)
;
MOZ_ASSERT
(
newBytes
=
=
GetGoodAllocSize
(
newBytes
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
if
(
isSweepingChunk
(
chunk
)
)
{
return
false
;
}
size_t
currentBytes
=
chunk
-
>
allocBytes
(
alloc
)
;
if
(
newBytes
=
=
currentBytes
)
{
return
false
;
}
MOZ_ASSERT
(
newBytes
<
currentBytes
)
;
size_t
sizeChange
=
currentBytes
-
newBytes
;
chunk
-
>
updateEndOffset
(
alloc
currentBytes
newBytes
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
newBytes
)
;
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bool
updateRetained
=
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
;
decreaseHeapSize
(
sizeChange
updateRetained
)
;
}
uintptr_t
startOffset
=
uintptr_t
(
alloc
)
&
ChunkMask
;
uintptr_t
oldEndOffset
=
startOffset
+
currentBytes
;
uintptr_t
newEndOffset
=
startOffset
+
newBytes
;
MOZ_ASSERT
(
oldEndOffset
<
=
ChunkSize
)
;
uintptr_t
chunkAddr
=
uintptr_t
(
chunk
)
;
PoisonAlloc
(
reinterpret_cast
<
void
*
>
(
chunkAddr
+
newEndOffset
)
JS_SWEPT_TENURED_PATTERN
sizeChange
MemCheckKind
:
:
MakeUndefined
)
;
ChunkLists
*
availableChunks
=
getChunkAvailableLists
(
chunk
)
;
size_t
oldChunkSizeClass
=
SIZE_MAX
;
if
(
availableChunks
)
{
oldChunkSizeClass
=
chunk
-
>
sizeClassForAvailableLists
(
)
;
}
FreeLists
*
freeLists
=
getChunkFreeLists
(
chunk
)
;
if
(
oldEndOffset
=
=
ChunkSize
|
|
chunk
-
>
isAllocated
(
oldEndOffset
)
)
{
uintptr_t
freeStart
=
chunkAddr
+
newEndOffset
;
addFreeRegion
(
freeLists
freeStart
sizeChange
SizeKind
:
:
Medium
false
ListPosition
:
:
Front
)
;
}
else
{
FreeRegion
*
region
=
chunk
-
>
findFollowingFreeRegion
(
chunkAddr
+
oldEndOffset
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
chunkAddr
+
oldEndOffset
)
;
updateFreeRegionStart
(
freeLists
region
chunkAddr
+
newEndOffset
SizeKind
:
:
Medium
)
;
}
if
(
availableChunks
)
{
maybeUpdateAvailableLists
(
availableChunks
chunk
oldChunkSizeClass
)
;
}
return
true
;
}
BufferAllocator
:
:
FreeLists
*
BufferAllocator
:
:
getChunkFreeLists
(
BufferChunk
*
chunk
)
{
MOZ_ASSERT_IF
(
majorState
=
=
State
:
:
Sweeping
chunk
-
>
allocatedDuringCollection
)
;
MOZ_ASSERT_IF
(
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
chunk
-
>
ownsFreeLists
)
;
if
(
chunk
-
>
ownsFreeLists
)
{
return
&
chunk
-
>
freeLists
.
ref
(
)
;
}
return
&
freeLists
.
ref
(
)
;
}
BufferAllocator
:
:
ChunkLists
*
BufferAllocator
:
:
getChunkAvailableLists
(
BufferChunk
*
chunk
)
{
MOZ_ASSERT_IF
(
majorState
=
=
State
:
:
Sweeping
chunk
-
>
allocatedDuringCollection
)
;
if
(
!
chunk
-
>
ownsFreeLists
)
{
return
nullptr
;
}
if
(
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
)
{
return
nullptr
;
}
if
(
chunk
-
>
hasNurseryOwnedAllocs
)
{
return
&
availableMixedChunks
.
ref
(
)
;
}
return
&
availableTenuredChunks
.
ref
(
)
;
}
size_t
BufferAllocator
:
:
SizeClassForSmallAlloc
(
size_t
bytes
)
{
MOZ_ASSERT
(
bytes
>
=
MinSmallAllocSize
)
;
MOZ_ASSERT
(
bytes
<
=
MaxSmallAllocSize
)
;
size_t
log2Size
=
mozilla
:
:
CeilingLog2
(
bytes
)
;
MOZ_ASSERT
(
(
size_t
(
1
)
<
<
log2Size
)
>
=
bytes
)
;
MOZ_ASSERT
(
MinSizeClassShift
=
=
mozilla
:
:
CeilingLog2
(
MinFreeRegionSize
)
)
;
if
(
log2Size
<
MinSizeClassShift
)
{
return
0
;
}
size_t
sizeClass
=
log2Size
-
MinSizeClassShift
;
MOZ_ASSERT
(
sizeClass
<
=
MaxSmallAllocClass
)
;
return
sizeClass
;
}
size_t
BufferAllocator
:
:
SizeClassForMediumAlloc
(
size_t
bytes
)
{
MOZ_ASSERT
(
bytes
>
=
MinMediumAllocSize
)
;
MOZ_ASSERT
(
bytes
<
=
MaxMediumAllocSize
)
;
size_t
log2Size
=
mozilla
:
:
CeilingLog2
(
bytes
)
;
MOZ_ASSERT
(
(
size_t
(
1
)
<
<
log2Size
)
>
=
bytes
)
;
MOZ_ASSERT
(
log2Size
>
=
MinMediumAllocShift
)
;
size_t
sizeClass
=
log2Size
-
MinMediumAllocShift
+
MinMediumAllocClass
;
MOZ_ASSERT
(
sizeClass
>
=
MinMediumAllocClass
)
;
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
return
sizeClass
;
}
size_t
BufferAllocator
:
:
SizeClassForFreeRegion
(
size_t
bytes
SizeKind
kind
)
{
MOZ_ASSERT
(
bytes
>
=
MinFreeRegionSize
)
;
MOZ_ASSERT
(
bytes
<
ChunkSize
)
;
if
(
kind
=
=
SizeKind
:
:
Medium
&
&
bytes
>
=
MaxMediumAllocSize
)
{
return
MaxMediumAllocClass
;
}
size_t
log2Size
=
mozilla
:
:
FloorLog2
(
bytes
)
;
MOZ_ASSERT
(
(
size_t
(
1
)
<
<
log2Size
)
<
=
bytes
)
;
MOZ_ASSERT
(
log2Size
>
=
MinSizeClassShift
)
;
size_t
sizeClass
=
std
:
:
min
(
log2Size
-
MinSizeClassShift
AllocSizeClasses
-
1
)
;
if
(
kind
=
=
SizeKind
:
:
Small
)
{
return
std
:
:
min
(
sizeClass
MaxSmallAllocClass
)
;
}
sizeClass
+
+
;
MOZ_ASSERT
(
sizeClass
>
=
MinMediumAllocClass
)
;
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
return
sizeClass
;
}
inline
size_t
BufferAllocator
:
:
SizeClassBytes
(
size_t
sizeClass
)
{
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
if
(
sizeClass
>
=
MinMediumAllocClass
)
{
sizeClass
-
-
;
}
return
1
<
<
(
sizeClass
+
MinSizeClassShift
)
;
}
bool
BufferAllocator
:
:
IsMediumAlloc
(
void
*
alloc
)
{
MOZ_ASSERT
(
IsBufferAlloc
(
alloc
)
)
;
MOZ_ASSERT
(
!
IsLargeAlloc
(
alloc
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
;
}
bool
BufferAllocator
:
:
needLockToAccessBufferMap
(
)
const
{
MOZ_ASSERT
(
CurrentThreadCanAccessZone
(
zone
)
|
|
CurrentThreadIsPerformingGC
(
)
)
;
return
minorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
|
|
majorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
;
}
LargeBuffer
*
BufferAllocator
:
:
lookupLargeBuffer
(
void
*
alloc
)
{
MaybeLock
lock
;
return
lookupLargeBuffer
(
alloc
lock
)
;
}
LargeBuffer
*
BufferAllocator
:
:
lookupLargeBuffer
(
void
*
alloc
MaybeLock
&
lock
)
{
MOZ_ASSERT
(
lock
.
isNothing
(
)
)
;
if
(
needLockToAccessBufferMap
(
)
)
{
lock
.
emplace
(
this
)
;
}
auto
ptr
=
largeAllocMap
.
ref
(
)
.
readonlyThreadsafeLookup
(
alloc
)
;
MOZ_ASSERT
(
ptr
)
;
LargeBuffer
*
buffer
=
ptr
-
>
value
(
)
;
MOZ_ASSERT
(
buffer
-
>
data
(
)
=
=
alloc
)
;
MOZ_ASSERT
(
buffer
-
>
zoneFromAnyThread
(
)
=
=
zone
)
;
return
buffer
;
}
void
*
BufferAllocator
:
:
allocLarge
(
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
bytes
=
RoundUp
(
bytes
ChunkSize
)
;
MOZ_ASSERT
(
bytes
>
MaxMediumAllocSize
)
;
MOZ_ASSERT
(
bytes
>
=
bytes
)
;
static_assert
(
sizeof
(
LargeBuffer
)
<
=
MaxSmallAllocSize
)
;
void
*
bufferPtr
=
allocSmall
(
sizeof
(
LargeBuffer
)
nurseryOwned
inGC
)
;
if
(
!
bufferPtr
)
{
return
nullptr
;
}
void
*
alloc
=
MapAlignedPages
(
bytes
ChunkSize
ShouldStallAndRetry
(
inGC
)
)
;
if
(
!
alloc
)
{
return
nullptr
;
}
auto
freeGuard
=
mozilla
:
:
MakeScopeExit
(
[
&
]
(
)
{
UnmapPages
(
alloc
bytes
)
;
}
)
;
CheckHighBitsOfPointer
(
alloc
)
;
auto
*
buffer
=
new
(
bufferPtr
)
LargeBuffer
(
alloc
bytes
nurseryOwned
)
;
{
MaybeLock
lock
;
if
(
needLockToAccessBufferMap
(
)
)
{
lock
.
emplace
(
this
)
;
}
if
(
!
largeAllocMap
.
ref
(
)
.
putNew
(
alloc
buffer
)
)
{
return
nullptr
;
}
}
freeGuard
.
release
(
)
;
if
(
nurseryOwned
)
{
largeNurseryAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
}
else
{
buffer
-
>
allocatedDuringCollection
=
majorState
!
=
State
:
:
NotCollecting
;
largeTenuredAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
}
if
(
!
nurseryOwned
)
{
bool
checkThresholds
=
!
inGC
;
increaseHeapSize
(
bytes
checkThresholds
false
)
;
}
MOZ_ASSERT
(
IsLargeAlloc
(
alloc
)
)
;
return
alloc
;
}
void
BufferAllocator
:
:
increaseHeapSize
(
size_t
bytes
bool
checkThresholds
bool
updateRetainedSize
)
{
zone
-
>
mallocHeapSize
.
addBytes
(
bytes
updateRetainedSize
)
;
if
(
checkThresholds
)
{
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromAnyThread
(
)
-
>
gc
;
gc
-
>
maybeTriggerGCAfterMalloc
(
zone
)
;
}
}
void
BufferAllocator
:
:
decreaseHeapSize
(
size_t
bytes
bool
updateRetainedSize
)
{
zone
-
>
mallocHeapSize
.
removeBytes
(
bytes
updateRetainedSize
)
;
}
bool
BufferAllocator
:
:
IsLargeAlloc
(
void
*
alloc
)
{
return
(
uintptr_t
(
alloc
)
&
ChunkMask
)
=
=
0
;
}
bool
BufferAllocator
:
:
markLargeTenuredBuffer
(
LargeBuffer
*
buffer
)
{
MOZ_ASSERT
(
!
buffer
-
>
isNurseryOwned
)
;
if
(
buffer
-
>
allocatedDuringCollection
)
{
return
false
;
}
auto
*
region
=
SmallBufferRegion
:
:
from
(
buffer
)
;
return
region
-
>
setMarked
(
buffer
)
;
}
bool
BufferAllocator
:
:
isLargeTenuredMarked
(
LargeBuffer
*
buffer
)
{
MOZ_ASSERT
(
!
buffer
-
>
isNurseryOwned
)
;
MOZ_ASSERT
(
buffer
-
>
zoneFromAnyThread
(
)
=
=
zone
)
;
MOZ_ASSERT
(
!
buffer
-
>
isInList
(
)
)
;
auto
*
region
=
SmallBufferRegion
:
:
from
(
buffer
)
;
return
region
-
>
isMarked
(
buffer
)
;
}
void
BufferAllocator
:
:
freeLarge
(
void
*
alloc
)
{
MaybeLock
lock
;
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
lock
)
;
MOZ_ASSERT
(
buffer
-
>
zone
(
)
=
=
zone
)
;
DebugOnlyPoison
(
alloc
JS_FREED_BUFFER_PATTERN
buffer
-
>
allocBytes
(
)
MemCheckKind
:
:
MakeUndefined
)
;
if
(
!
buffer
-
>
isNurseryOwned
&
&
majorState
=
=
State
:
:
Sweeping
&
&
!
buffer
-
>
allocatedDuringCollection
)
{
return
;
}
MOZ_ASSERT
(
buffer
-
>
isInList
(
)
)
;
if
(
buffer
-
>
isNurseryOwned
)
{
largeNurseryAllocs
.
ref
(
)
.
remove
(
buffer
)
;
}
else
if
(
majorState
=
=
State
:
:
Marking
&
&
!
buffer
-
>
allocatedDuringCollection
)
{
largeTenuredAllocsToSweep
.
ref
(
)
.
remove
(
buffer
)
;
}
else
{
largeTenuredAllocs
.
ref
(
)
.
remove
(
buffer
)
;
}
unmapLarge
(
buffer
false
lock
)
;
}
bool
BufferAllocator
:
:
shrinkLarge
(
LargeBuffer
*
buffer
size_t
newBytes
)
{
MOZ_ASSERT
(
IsLargeAllocSize
(
newBytes
)
)
;
#
ifdef
XP_WIN
return
false
;
#
else
MOZ_ASSERT
(
buffer
-
>
zone
(
)
=
=
zone
)
;
if
(
!
buffer
-
>
isNurseryOwned
&
&
majorState
=
=
State
:
:
Sweeping
&
&
!
buffer
-
>
allocatedDuringCollection
)
{
return
false
;
}
MOZ_ASSERT
(
buffer
-
>
isInList
(
)
)
;
newBytes
=
RoundUp
(
newBytes
ChunkSize
)
;
size_t
oldBytes
=
buffer
-
>
bytes
;
MOZ_ASSERT
(
oldBytes
>
newBytes
)
;
size_t
shrinkBytes
=
oldBytes
-
newBytes
;
if
(
!
buffer
-
>
isNurseryOwned
)
{
decreaseHeapSize
(
shrinkBytes
false
)
;
}
buffer
-
>
bytes
=
newBytes
;
void
*
endPtr
=
reinterpret_cast
<
void
*
>
(
uintptr_t
(
buffer
-
>
data
(
)
)
+
newBytes
)
;
UnmapPages
(
endPtr
shrinkBytes
)
;
return
true
;
#
endif
}
void
BufferAllocator
:
:
unmapLarge
(
LargeBuffer
*
buffer
bool
isSweeping
MaybeLock
&
lock
)
{
unregisterLarge
(
buffer
isSweeping
lock
)
;
UnmapPages
(
buffer
-
>
data
(
)
buffer
-
>
bytes
)
;
}
void
BufferAllocator
:
:
unregisterLarge
(
LargeBuffer
*
buffer
bool
isSweeping
MaybeLock
&
lock
)
{
MOZ_ASSERT
(
buffer
-
>
zoneFromAnyThread
(
)
=
=
zone
)
;
MOZ_ASSERT
(
!
buffer
-
>
isInList
(
)
)
;
MOZ_ASSERT_IF
(
isSweeping
|
|
needLockToAccessBufferMap
(
)
lock
.
isSome
(
)
)
;
#
ifdef
DEBUG
auto
ptr
=
largeAllocMap
.
ref
(
)
.
lookup
(
buffer
-
>
data
(
)
)
;
MOZ_ASSERT
(
ptr
&
&
ptr
-
>
value
(
)
=
=
buffer
)
;
#
endif
largeAllocMap
.
ref
(
)
.
remove
(
buffer
-
>
data
(
)
)
;
lock
.
reset
(
)
;
if
(
!
buffer
-
>
isNurseryOwned
)
{
decreaseHeapSize
(
buffer
-
>
bytes
isSweeping
)
;
}
}
#
include
"
js
/
Printer
.
h
"
#
include
"
util
/
GetPidProvider
.
h
"
static
const
char
*
const
BufferAllocatorStatsPrefix
=
"
BufAllc
:
"
;
#
define
FOR_EACH_BUFFER_STATS_FIELD
(
_
)
\
_
(
"
PID
"
7
"
%
7zu
"
pid
)
\
_
(
"
Runtime
"
14
"
0x
%
12p
"
runtime
)
\
_
(
"
Timestamp
"
10
"
%
10
.
6f
"
timestamp
.
ToSeconds
(
)
)
\
_
(
"
Reason
"
20
"
%
-
20
.
20s
"
reason
)
\
_
(
"
"
2
"
%
2s
"
"
"
)
\
_
(
"
TotalKB
"
8
"
%
8zu
"
totalBytes
/
1024
)
\
_
(
"
UsedKB
"
8
"
%
8zu
"
stats
.
usedBytes
/
1024
)
\
_
(
"
FreeKB
"
8
"
%
8zu
"
stats
.
freeBytes
/
1024
)
\
_
(
"
Zs
"
3
"
%
3zu
"
zoneCount
)
\
_
(
"
"
7
"
%
7s
"
"
"
)
\
_
(
"
MixSRs
"
6
"
%
6zu
"
stats
.
mixedSmallRegions
)
\
_
(
"
TnrSRs
"
6
"
%
6zu
"
stats
.
tenuredSmallRegions
)
\
_
(
"
MixCs
"
6
"
%
6zu
"
stats
.
mixedChunks
)
\
_
(
"
TnrCs
"
6
"
%
6zu
"
stats
.
tenuredChunks
)
\
_
(
"
AMixCs
"
6
"
%
6zu
"
stats
.
availableMixedChunks
)
\
_
(
"
ATnrCs
"
6
"
%
6zu
"
stats
.
availableTenuredChunks
)
\
_
(
"
FreeRs
"
6
"
%
6zu
"
stats
.
freeRegions
)
\
_
(
"
LNurAs
"
6
"
%
6zu
"
stats
.
largeNurseryAllocs
)
\
_
(
"
LTnrAs
"
6
"
%
6zu
"
stats
.
largeTenuredAllocs
)
void
BufferAllocator
:
:
printStatsHeader
(
FILE
*
file
)
{
Sprinter
sprinter
;
if
(
!
sprinter
.
init
(
)
)
{
return
;
}
sprinter
.
put
(
BufferAllocatorStatsPrefix
)
;
#
define
PRINT_METADATA_NAME
(
name
width
_1
_2
)
\
sprinter
.
printf
(
"
%
-
*
s
"
width
name
)
;
FOR_EACH_BUFFER_STATS_FIELD
(
PRINT_METADATA_NAME
)
#
undef
PRINT_METADATA_NAME
sprinter
.
put
(
"
\
n
"
)
;
JS
:
:
UniqueChars
str
=
sprinter
.
release
(
)
;
if
(
!
str
)
{
return
;
}
fputs
(
str
.
get
(
)
file
)
;
}
void
BufferAllocator
:
:
printStats
(
GCRuntime
*
gc
mozilla
:
:
TimeStamp
creationTime
bool
isMajorGC
FILE
*
file
)
{
Sprinter
sprinter
;
if
(
!
sprinter
.
init
(
)
)
{
return
;
}
sprinter
.
put
(
BufferAllocatorStatsPrefix
)
;
size_t
pid
=
getpid
(
)
;
JSRuntime
*
runtime
=
gc
-
>
rt
;
mozilla
:
:
TimeDuration
timestamp
=
mozilla
:
:
TimeStamp
:
:
Now
(
)
-
creationTime
;
const
char
*
reason
=
isMajorGC
?
"
post
major
slice
"
:
"
pre
minor
GC
"
;
size_t
zoneCount
=
0
;
Stats
stats
;
for
(
AllZonesIter
zone
(
gc
)
;
!
zone
.
done
(
)
;
zone
.
next
(
)
)
{
zoneCount
+
+
;
zone
-
>
bufferAllocator
.
getStats
(
stats
)
;
}
size_t
totalBytes
=
stats
.
usedBytes
+
stats
.
freeBytes
+
stats
.
adminBytes
;
#
define
PRINT_FIELD_VALUE
(
_1
_2
format
value
)
\
sprinter
.
printf
(
"
"
format
value
)
;
FOR_EACH_BUFFER_STATS_FIELD
(
PRINT_FIELD_VALUE
)
#
undef
PRINT_FIELD_VALUE
sprinter
.
put
(
"
\
n
"
)
;
JS
:
:
UniqueChars
str
=
sprinter
.
release
(
)
;
if
(
!
str
)
{
return
;
}
fputs
(
str
.
get
(
)
file
)
;
}
size_t
BufferAllocator
:
:
getSizeOfNurseryBuffers
(
)
{
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
size_t
bytes
=
0
;
for
(
BufferChunk
*
chunk
:
mixedChunks
.
ref
(
)
)
{
for
(
auto
alloc
=
chunk
-
>
allocIter
(
)
;
!
alloc
.
done
(
)
;
alloc
.
next
(
)
)
{
if
(
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bytes
+
=
chunk
-
>
allocBytes
(
alloc
)
;
}
}
}
for
(
const
LargeBuffer
*
buffer
:
largeNurseryAllocs
.
ref
(
)
)
{
bytes
+
=
buffer
-
>
allocBytes
(
)
;
}
return
bytes
;
}
void
BufferAllocator
:
:
addSizeOfExcludingThis
(
size_t
*
usedBytesOut
size_t
*
freeBytesOut
size_t
*
adminBytesOut
)
{
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
Stats
stats
;
getStats
(
stats
)
;
*
usedBytesOut
+
=
stats
.
usedBytes
;
*
freeBytesOut
+
=
stats
.
freeBytes
;
*
adminBytesOut
+
=
stats
.
adminBytes
;
}
static
void
GetChunkStats
(
BufferChunk
*
chunk
BufferAllocator
:
:
Stats
&
stats
)
{
stats
.
usedBytes
+
=
ChunkSize
-
FirstMediumAllocOffset
;
stats
.
adminBytes
+
=
FirstMediumAllocOffset
;
for
(
auto
iter
=
chunk
-
>
smallRegionIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
SmallBufferRegion
*
region
=
iter
.
get
(
)
;
if
(
region
-
>
hasNurseryOwnedAllocs
(
)
)
{
stats
.
mixedSmallRegions
+
+
;
}
else
{
stats
.
tenuredSmallRegions
+
+
;
}
stats
.
adminBytes
+
=
FirstSmallAllocOffset
;
}
}
void
BufferAllocator
:
:
getStats
(
Stats
&
stats
)
{
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
for
(
BufferChunk
*
chunk
:
mixedChunks
.
ref
(
)
)
{
stats
.
mixedChunks
+
+
;
GetChunkStats
(
chunk
stats
)
;
}
for
(
auto
chunk
=
availableMixedChunks
.
ref
(
)
.
chunkIter
(
)
;
!
chunk
.
done
(
)
;
chunk
.
next
(
)
)
{
stats
.
availableMixedChunks
+
+
;
GetChunkStats
(
chunk
stats
)
;
}
for
(
BufferChunk
*
chunk
:
tenuredChunks
.
ref
(
)
)
{
stats
.
tenuredChunks
+
+
;
GetChunkStats
(
chunk
stats
)
;
}
for
(
auto
chunk
=
availableTenuredChunks
.
ref
(
)
.
chunkIter
(
)
;
!
chunk
.
done
(
)
;
chunk
.
next
(
)
)
{
stats
.
availableTenuredChunks
+
+
;
GetChunkStats
(
chunk
stats
)
;
}
for
(
const
LargeBuffer
*
buffer
:
largeNurseryAllocs
.
ref
(
)
)
{
stats
.
largeNurseryAllocs
+
+
;
stats
.
usedBytes
+
=
buffer
-
>
allocBytes
(
)
;
stats
.
adminBytes
+
=
sizeof
(
LargeBuffer
)
;
}
for
(
const
LargeBuffer
*
buffer
:
largeTenuredAllocs
.
ref
(
)
)
{
stats
.
largeTenuredAllocs
+
+
;
stats
.
usedBytes
+
=
buffer
-
>
allocBytes
(
)
;
stats
.
adminBytes
+
=
sizeof
(
LargeBuffer
)
;
}
for
(
auto
region
=
freeLists
.
ref
(
)
.
freeRegionIter
(
)
;
!
region
.
done
(
)
;
region
.
next
(
)
)
{
stats
.
freeRegions
+
+
;
size_t
size
=
region
-
>
size
(
)
;
MOZ_ASSERT
(
stats
.
usedBytes
>
=
size
)
;
stats
.
usedBytes
-
=
size
;
stats
.
freeBytes
+
=
size
;
}
}
