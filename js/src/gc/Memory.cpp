#
include
"
gc
/
Memory
.
h
"
#
include
"
mozilla
/
Atomics
.
h
"
#
include
"
mozilla
/
TaggedAnonymousMemory
.
h
"
#
include
"
js
/
HeapAPI
.
h
"
#
include
"
vm
/
Runtime
.
h
"
#
if
defined
(
XP_WIN
)
#
include
"
util
/
Windows
.
h
"
#
include
<
psapi
.
h
>
#
else
#
include
<
algorithm
>
#
include
<
errno
.
h
>
#
include
<
sys
/
mman
.
h
>
#
include
<
sys
/
resource
.
h
>
#
include
<
sys
/
stat
.
h
>
#
include
<
sys
/
types
.
h
>
#
include
<
unistd
.
h
>
#
endif
namespace
js
{
namespace
gc
{
static
size_t
pageSize
=
0
;
static
size_t
allocGranularity
=
0
;
#
if
defined
(
XP_DARWIN
)
|
|
\
(
!
defined
(
XP_WIN
)
&
&
(
defined
(
__ia64__
)
|
|
defined
(
__aarch64__
)
|
|
\
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
\
(
defined
(
__NetBSD__
)
|
|
defined
(
__linux__
)
)
)
)
)
static
mozilla
:
:
Atomic
<
int
mozilla
:
:
Relaxed
mozilla
:
:
recordreplay
:
:
Behavior
:
:
DontPreserve
>
growthDirection
(
1
)
;
#
elif
defined
(
XP_LINUX
)
static
mozilla
:
:
Atomic
<
int
mozilla
:
:
Relaxed
mozilla
:
:
recordreplay
:
:
Behavior
:
:
DontPreserve
>
growthDirection
(
0
)
;
#
endif
enum
class
Commit
:
bool
{
No
=
false
Yes
=
true
}
;
#
if
defined
(
XP_WIN
)
enum
class
PageAccess
:
DWORD
{
None
=
PAGE_NOACCESS
Read
=
PAGE_READONLY
ReadWrite
=
PAGE_READWRITE
Execute
=
PAGE_EXECUTE
ReadExecute
=
PAGE_EXECUTE_READ
ReadWriteExecute
=
PAGE_EXECUTE_READWRITE
}
;
#
else
enum
class
PageAccess
:
int
{
None
=
PROT_NONE
Read
=
PROT_READ
ReadWrite
=
PROT_READ
|
PROT_WRITE
Execute
=
PROT_EXEC
ReadExecute
=
PROT_READ
|
PROT_EXEC
ReadWriteExecute
=
PROT_READ
|
PROT_WRITE
|
PROT_EXEC
}
;
#
endif
static
const
int
MaxLastDitchAttempts
=
32
;
static
void
TryToAlignChunk
(
void
*
*
aRegion
void
*
*
aRetainedRegion
size_t
length
size_t
alignment
)
;
static
void
*
MapAlignedPagesSlow
(
size_t
length
size_t
alignment
)
;
static
void
*
MapAlignedPagesLastDitch
(
size_t
length
size_t
alignment
)
;
size_t
SystemPageSize
(
)
{
return
pageSize
;
}
static
inline
bool
DecommitEnabled
(
)
{
return
pageSize
=
=
ArenaSize
;
}
static
inline
size_t
OffsetFromAligned
(
void
*
region
size_t
alignment
)
{
return
uintptr_t
(
region
)
%
alignment
;
}
void
*
TestMapAlignedPagesLastDitch
(
size_t
length
size_t
alignment
)
{
return
MapAlignedPagesLastDitch
(
length
alignment
)
;
}
void
InitMemorySubsystem
(
)
{
if
(
pageSize
=
=
0
)
{
#
if
defined
(
XP_WIN
)
SYSTEM_INFO
sysinfo
;
GetSystemInfo
(
&
sysinfo
)
;
pageSize
=
sysinfo
.
dwPageSize
;
allocGranularity
=
sysinfo
.
dwAllocationGranularity
;
#
else
pageSize
=
size_t
(
sysconf
(
_SC_PAGESIZE
)
)
;
allocGranularity
=
pageSize
;
#
endif
}
}
template
<
Commit
commit
PageAccess
prot
>
static
inline
void
*
MapInternal
(
void
*
desired
size_t
length
)
{
#
if
defined
(
XP_WIN
)
DWORD
flags
=
(
commit
=
=
Commit
:
:
Yes
?
MEM_RESERVE
|
MEM_COMMIT
:
MEM_RESERVE
)
;
return
VirtualAlloc
(
desired
length
flags
DWORD
(
prot
)
)
;
#
else
int
flags
=
MAP_PRIVATE
|
MAP_ANON
;
void
*
region
=
MozTaggedAnonymousMmap
(
desired
length
int
(
prot
)
flags
-
1
0
"
js
-
gc
-
heap
"
)
;
if
(
region
=
=
MAP_FAILED
)
{
return
nullptr
;
}
return
region
;
#
endif
}
static
inline
void
UnmapInternal
(
void
*
region
size_t
length
)
{
MOZ_ASSERT
(
region
&
&
OffsetFromAligned
(
region
allocGranularity
)
=
=
0
)
;
MOZ_ASSERT
(
length
>
0
&
&
length
%
pageSize
=
=
0
)
;
#
if
defined
(
XP_WIN
)
MOZ_RELEASE_ASSERT
(
VirtualFree
(
region
0
MEM_RELEASE
)
!
=
0
)
;
#
else
if
(
munmap
(
region
length
)
)
{
MOZ_RELEASE_ASSERT
(
errno
=
=
ENOMEM
)
;
}
#
endif
}
static
inline
bool
IsInvalidRegion
(
void
*
region
size_t
length
)
{
const
uint64_t
invalidPointerMask
=
UINT64_C
(
0xffff800000000000
)
;
return
(
uintptr_t
(
region
)
+
length
-
1
)
&
invalidPointerMask
;
}
template
<
Commit
commit
=
Commit
:
:
Yes
PageAccess
prot
=
PageAccess
:
:
ReadWrite
>
static
inline
void
*
MapMemory
(
size_t
length
)
{
MOZ_ASSERT
(
length
>
0
)
;
#
if
defined
(
XP_WIN
)
return
MapInternal
<
commit
prot
>
(
nullptr
length
)
;
#
elif
defined
(
__ia64__
)
|
|
\
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__NetBSD__
)
)
const
uintptr_t
hint
=
UINT64_C
(
0x0000070000000000
)
;
void
*
region
=
MapInternal
<
commit
prot
>
(
reinterpret_cast
<
void
*
>
(
hint
)
length
)
;
if
(
IsInvalidRegion
(
region
length
)
)
{
UnmapInternal
(
region
length
)
;
return
nullptr
;
}
return
region
;
#
elif
defined
(
__aarch64__
)
|
|
\
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
defined
(
__linux__
)
)
const
uintptr_t
start
=
UINT64_C
(
0x0000070000000000
)
;
const
uintptr_t
end
=
UINT64_C
(
0x0000800000000000
)
;
const
uintptr_t
step
=
ChunkSize
;
uintptr_t
desired
;
void
*
region
=
nullptr
;
for
(
desired
=
start
;
!
region
&
&
desired
+
length
<
=
end
;
desired
+
=
step
)
{
region
=
MapInternal
<
commit
prot
>
(
reinterpret_cast
<
void
*
>
(
desired
)
length
)
;
if
(
region
)
{
MOZ_RELEASE_ASSERT
(
uintptr_t
(
region
)
=
=
desired
)
;
}
}
return
region
;
#
else
return
MapInternal
<
commit
prot
>
(
nullptr
length
)
;
#
endif
}
template
<
Commit
commit
=
Commit
:
:
Yes
PageAccess
prot
=
PageAccess
:
:
ReadWrite
>
static
inline
void
*
MapMemoryAt
(
void
*
desired
size_t
length
)
{
MOZ_ASSERT
(
desired
&
&
OffsetFromAligned
(
desired
allocGranularity
)
=
=
0
)
;
MOZ_ASSERT
(
length
>
0
)
;
#
if
defined
(
XP_WIN
)
return
MapInternal
<
commit
prot
>
(
desired
length
)
;
#
else
#
if
defined
(
__ia64__
)
|
|
defined
(
__aarch64__
)
|
|
\
(
defined
(
__sparc__
)
&
&
defined
(
__arch64__
)
&
&
\
(
defined
(
__NetBSD__
)
|
|
defined
(
__linux__
)
)
)
MOZ_RELEASE_ASSERT
(
!
IsInvalidRegion
(
desired
length
)
)
;
#
endif
void
*
region
=
MapInternal
<
commit
prot
>
(
desired
length
)
;
if
(
!
region
)
{
return
nullptr
;
}
if
(
region
!
=
desired
)
{
UnmapInternal
(
region
length
)
;
return
nullptr
;
}
return
region
;
#
endif
}
void
*
MapAlignedPages
(
size_t
length
size_t
alignment
)
{
MOZ_RELEASE_ASSERT
(
length
>
0
&
&
alignment
>
0
)
;
MOZ_RELEASE_ASSERT
(
length
%
pageSize
=
=
0
)
;
MOZ_RELEASE_ASSERT
(
std
:
:
max
(
alignment
allocGranularity
)
%
std
:
:
min
(
alignment
allocGranularity
)
=
=
0
)
;
void
*
region
=
MapMemory
(
length
)
;
if
(
OffsetFromAligned
(
region
alignment
)
=
=
0
)
{
return
region
;
}
void
*
retainedRegion
;
TryToAlignChunk
(
&
region
&
retainedRegion
length
alignment
)
;
if
(
retainedRegion
)
{
UnmapInternal
(
retainedRegion
length
)
;
}
if
(
region
)
{
if
(
OffsetFromAligned
(
region
alignment
)
=
=
0
)
{
return
region
;
}
UnmapInternal
(
region
length
)
;
}
region
=
MapAlignedPagesSlow
(
length
alignment
)
;
if
(
!
region
)
{
return
MapAlignedPagesLastDitch
(
length
alignment
)
;
}
MOZ_ASSERT
(
OffsetFromAligned
(
region
alignment
)
=
=
0
)
;
return
region
;
}
static
void
*
MapAlignedPagesSlow
(
size_t
length
size_t
alignment
)
{
void
*
alignedRegion
=
nullptr
;
do
{
size_t
reserveLength
=
length
+
alignment
-
pageSize
;
#
if
defined
(
XP_WIN
)
void
*
region
=
MapMemory
<
Commit
:
:
No
>
(
reserveLength
)
;
#
else
void
*
region
=
MapMemory
(
reserveLength
)
;
#
endif
if
(
!
region
)
{
return
nullptr
;
}
alignedRegion
=
reinterpret_cast
<
void
*
>
(
AlignBytes
(
uintptr_t
(
region
)
alignment
)
)
;
#
if
defined
(
XP_WIN
)
UnmapInternal
(
region
reserveLength
)
;
alignedRegion
=
MapMemoryAt
(
alignedRegion
length
)
;
#
else
if
(
alignedRegion
!
=
region
)
{
UnmapInternal
(
region
uintptr_t
(
alignedRegion
)
-
uintptr_t
(
region
)
)
;
}
void
*
regionEnd
=
reinterpret_cast
<
void
*
>
(
uintptr_t
(
region
)
+
reserveLength
)
;
void
*
alignedEnd
=
reinterpret_cast
<
void
*
>
(
uintptr_t
(
alignedRegion
)
+
length
)
;
if
(
alignedEnd
!
=
regionEnd
)
{
UnmapInternal
(
alignedEnd
uintptr_t
(
regionEnd
)
-
uintptr_t
(
alignedEnd
)
)
;
}
#
endif
}
while
(
!
alignedRegion
)
;
return
alignedRegion
;
}
static
void
*
MapAlignedPagesLastDitch
(
size_t
length
size_t
alignment
)
{
void
*
tempMaps
[
MaxLastDitchAttempts
]
;
int
attempt
=
0
;
void
*
region
=
MapMemory
(
length
)
;
if
(
OffsetFromAligned
(
region
alignment
)
=
=
0
)
{
return
region
;
}
for
(
;
attempt
<
MaxLastDitchAttempts
;
+
+
attempt
)
{
TryToAlignChunk
(
&
region
tempMaps
+
attempt
length
alignment
)
;
if
(
OffsetFromAligned
(
region
alignment
)
=
=
0
)
{
if
(
tempMaps
[
attempt
]
)
{
UnmapInternal
(
tempMaps
[
attempt
]
length
)
;
}
break
;
}
if
(
!
tempMaps
[
attempt
]
)
{
break
;
}
}
if
(
OffsetFromAligned
(
region
alignment
)
)
{
UnmapInternal
(
region
length
)
;
region
=
nullptr
;
}
while
(
-
-
attempt
>
=
0
)
{
UnmapInternal
(
tempMaps
[
attempt
]
length
)
;
}
return
region
;
}
#
if
defined
(
XP_WIN
)
static
void
TryToAlignChunk
(
void
*
*
aRegion
void
*
*
aRetainedRegion
size_t
length
size_t
alignment
)
{
void
*
region
=
*
aRegion
;
MOZ_ASSERT
(
region
&
&
OffsetFromAligned
(
region
alignment
)
!
=
0
)
;
void
*
retainedRegion
=
nullptr
;
do
{
size_t
offset
=
OffsetFromAligned
(
region
alignment
)
;
if
(
offset
=
=
0
)
{
break
;
}
UnmapInternal
(
region
length
)
;
size_t
retainedLength
=
alignment
-
offset
;
retainedRegion
=
MapMemoryAt
<
Commit
:
:
No
>
(
region
retainedLength
)
;
region
=
MapMemory
(
length
)
;
}
while
(
!
retainedRegion
)
;
*
aRegion
=
region
;
*
aRetainedRegion
=
retainedRegion
;
}
#
else
static
void
TryToAlignChunk
(
void
*
*
aRegion
void
*
*
aRetainedRegion
size_t
length
size_t
alignment
)
{
void
*
regionStart
=
*
aRegion
;
MOZ_ASSERT
(
regionStart
&
&
OffsetFromAligned
(
regionStart
alignment
)
!
=
0
)
;
bool
addressesGrowUpward
=
growthDirection
>
0
;
bool
directionUncertain
=
-
8
<
growthDirection
&
&
growthDirection
<
=
8
;
size_t
offsetLower
=
OffsetFromAligned
(
regionStart
alignment
)
;
size_t
offsetUpper
=
alignment
-
offsetLower
;
for
(
size_t
i
=
0
;
i
<
2
;
+
+
i
)
{
if
(
addressesGrowUpward
)
{
void
*
upperStart
=
reinterpret_cast
<
void
*
>
(
uintptr_t
(
regionStart
)
+
offsetUpper
)
;
void
*
regionEnd
=
reinterpret_cast
<
void
*
>
(
uintptr_t
(
regionStart
)
+
length
)
;
if
(
MapMemoryAt
(
regionEnd
offsetUpper
)
)
{
UnmapInternal
(
regionStart
offsetUpper
)
;
if
(
directionUncertain
)
{
+
+
growthDirection
;
}
regionStart
=
upperStart
;
break
;
}
}
else
{
void
*
lowerStart
=
reinterpret_cast
<
void
*
>
(
uintptr_t
(
regionStart
)
-
offsetLower
)
;
void
*
lowerEnd
=
reinterpret_cast
<
void
*
>
(
uintptr_t
(
lowerStart
)
+
length
)
;
if
(
MapMemoryAt
(
lowerStart
offsetLower
)
)
{
UnmapInternal
(
lowerEnd
offsetLower
)
;
if
(
directionUncertain
)
{
-
-
growthDirection
;
}
regionStart
=
lowerStart
;
break
;
}
}
if
(
!
directionUncertain
)
{
break
;
}
addressesGrowUpward
=
!
addressesGrowUpward
;
}
void
*
retainedRegion
=
nullptr
;
if
(
OffsetFromAligned
(
regionStart
alignment
)
!
=
0
)
{
retainedRegion
=
regionStart
;
regionStart
=
MapMemory
(
length
)
;
}
*
aRegion
=
regionStart
;
*
aRetainedRegion
=
retainedRegion
;
}
#
endif
void
UnmapPages
(
void
*
region
size_t
length
)
{
MOZ_RELEASE_ASSERT
(
region
&
&
OffsetFromAligned
(
region
allocGranularity
)
=
=
0
)
;
MOZ_RELEASE_ASSERT
(
length
>
0
&
&
length
%
pageSize
=
=
0
)
;
MOZ_MAKE_MEM_UNDEFINED
(
region
length
)
;
UnmapInternal
(
region
length
)
;
}
bool
MarkPagesUnused
(
void
*
region
size_t
length
)
{
MOZ_RELEASE_ASSERT
(
region
&
&
OffsetFromAligned
(
region
pageSize
)
=
=
0
)
;
MOZ_RELEASE_ASSERT
(
length
>
0
&
&
length
%
pageSize
=
=
0
)
;
MOZ_MAKE_MEM_NOACCESS
(
region
length
)
;
if
(
!
DecommitEnabled
(
)
)
{
return
true
;
}
#
if
defined
(
XP_WIN
)
return
VirtualAlloc
(
region
length
MEM_RESET
DWORD
(
PageAccess
:
:
ReadWrite
)
)
=
=
region
;
#
elif
defined
(
XP_DARWIN
)
return
madvise
(
region
length
MADV_FREE
)
=
=
0
;
#
else
return
madvise
(
region
length
MADV_DONTNEED
)
=
=
0
;
#
endif
}
void
MarkPagesInUse
(
void
*
region
size_t
length
)
{
MOZ_RELEASE_ASSERT
(
region
&
&
OffsetFromAligned
(
region
pageSize
)
=
=
0
)
;
MOZ_RELEASE_ASSERT
(
length
>
0
&
&
length
%
pageSize
=
=
0
)
;
MOZ_MAKE_MEM_UNDEFINED
(
region
length
)
;
}
size_t
GetPageFaultCount
(
)
{
if
(
mozilla
:
:
recordreplay
:
:
IsRecordingOrReplaying
(
)
)
{
return
0
;
}
#
if
defined
(
XP_WIN
)
PROCESS_MEMORY_COUNTERS
pmc
;
if
(
GetProcessMemoryInfo
(
GetCurrentProcess
(
)
&
pmc
sizeof
(
pmc
)
)
=
=
0
)
{
return
0
;
}
return
pmc
.
PageFaultCount
;
#
else
struct
rusage
usage
;
int
err
=
getrusage
(
RUSAGE_SELF
&
usage
)
;
if
(
err
)
{
return
0
;
}
return
usage
.
ru_majflt
;
#
endif
}
void
*
AllocateMappedContent
(
int
fd
size_t
offset
size_t
length
size_t
alignment
)
{
if
(
length
=
=
0
|
|
alignment
=
=
0
|
|
offset
%
alignment
!
=
0
|
|
std
:
:
max
(
alignment
allocGranularity
)
%
std
:
:
min
(
alignment
allocGranularity
)
!
=
0
)
{
return
nullptr
;
}
size_t
alignedOffset
=
offset
-
(
offset
%
allocGranularity
)
;
size_t
alignedLength
=
length
+
(
offset
%
allocGranularity
)
;
size_t
mappedLength
=
alignedLength
;
if
(
alignedLength
%
pageSize
!
=
0
)
{
mappedLength
+
=
pageSize
-
alignedLength
%
pageSize
;
}
#
if
defined
(
XP_WIN
)
HANDLE
hFile
=
reinterpret_cast
<
HANDLE
>
(
intptr_t
(
fd
)
)
;
HANDLE
hMap
=
CreateFileMapping
(
hFile
nullptr
PAGE_READONLY
0
0
nullptr
)
;
if
(
!
hMap
)
{
return
nullptr
;
}
DWORD
offsetH
=
uint32_t
(
uint64_t
(
alignedOffset
)
>
>
32
)
;
DWORD
offsetL
=
uint32_t
(
alignedOffset
)
;
uint8_t
*
map
=
nullptr
;
for
(
;
;
)
{
uintptr_t
region
=
uintptr_t
(
MapAlignedPages
(
mappedLength
alignment
)
)
;
if
(
region
=
=
0
)
{
break
;
}
UnmapInternal
(
reinterpret_cast
<
void
*
>
(
region
)
mappedLength
)
;
map
=
static_cast
<
uint8_t
*
>
(
MapViewOfFileEx
(
hMap
FILE_MAP_COPY
offsetH
offsetL
alignedLength
reinterpret_cast
<
void
*
>
(
region
)
)
)
;
if
(
map
|
|
GetLastError
(
)
!
=
ERROR_INVALID_ADDRESS
)
{
break
;
}
}
CloseHandle
(
hMap
)
;
if
(
!
map
)
{
return
nullptr
;
}
#
else
struct
stat
st
;
if
(
fstat
(
fd
&
st
)
|
|
offset
>
=
uint64_t
(
st
.
st_size
)
|
|
length
>
uint64_t
(
st
.
st_size
)
-
offset
)
{
return
nullptr
;
}
void
*
region
=
MapAlignedPages
(
mappedLength
alignment
)
;
if
(
!
region
)
{
return
nullptr
;
}
uint8_t
*
map
=
static_cast
<
uint8_t
*
>
(
mmap
(
region
alignedLength
PROT_READ
|
PROT_WRITE
MAP_PRIVATE
|
MAP_FIXED
fd
alignedOffset
)
)
;
MOZ_RELEASE_ASSERT
(
map
!
=
MAP_FAILED
)
;
#
endif
#
ifdef
DEBUG
if
(
offset
!
=
alignedOffset
)
{
memset
(
map
0
offset
-
alignedOffset
)
;
}
if
(
alignedLength
%
pageSize
)
{
memset
(
map
+
alignedLength
0
pageSize
-
(
alignedLength
%
pageSize
)
)
;
}
#
endif
return
map
+
(
offset
-
alignedOffset
)
;
}
void
DeallocateMappedContent
(
void
*
region
size_t
length
)
{
if
(
!
region
)
{
return
;
}
uintptr_t
map
=
uintptr_t
(
region
)
-
(
uintptr_t
(
region
)
%
allocGranularity
)
;
#
if
defined
(
XP_WIN
)
MOZ_RELEASE_ASSERT
(
UnmapViewOfFile
(
reinterpret_cast
<
void
*
>
(
map
)
)
!
=
0
)
;
#
else
size_t
alignedLength
=
length
+
(
uintptr_t
(
region
)
%
allocGranularity
)
;
if
(
munmap
(
reinterpret_cast
<
void
*
>
(
map
)
alignedLength
)
)
{
MOZ_RELEASE_ASSERT
(
errno
=
=
ENOMEM
)
;
}
#
endif
}
static
inline
void
ProtectMemory
(
void
*
region
size_t
length
PageAccess
prot
)
{
MOZ_RELEASE_ASSERT
(
region
&
&
OffsetFromAligned
(
region
pageSize
)
=
=
0
)
;
MOZ_RELEASE_ASSERT
(
length
>
0
&
&
length
%
pageSize
=
=
0
)
;
#
if
defined
(
XP_WIN
)
DWORD
oldProtect
;
MOZ_RELEASE_ASSERT
(
VirtualProtect
(
region
length
DWORD
(
prot
)
&
oldProtect
)
!
=
0
)
;
#
else
MOZ_RELEASE_ASSERT
(
mprotect
(
region
length
int
(
prot
)
)
=
=
0
)
;
#
endif
}
void
ProtectPages
(
void
*
region
size_t
length
)
{
ProtectMemory
(
region
length
PageAccess
:
:
None
)
;
}
void
MakePagesReadOnly
(
void
*
region
size_t
length
)
{
ProtectMemory
(
region
length
PageAccess
:
:
Read
)
;
}
void
UnprotectPages
(
void
*
region
size_t
length
)
{
ProtectMemory
(
region
length
PageAccess
:
:
ReadWrite
)
;
}
}
}
