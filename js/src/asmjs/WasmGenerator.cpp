#
include
"
asmjs
/
WasmGenerator
.
h
"
#
include
"
asmjs
/
AsmJS
.
h
"
#
include
"
asmjs
/
WasmStubs
.
h
"
#
include
"
jit
/
MacroAssembler
-
inl
.
h
"
using
namespace
js
;
using
namespace
js
:
:
jit
;
using
namespace
js
:
:
wasm
;
static
const
unsigned
GENERATOR_LIFO_DEFAULT_CHUNK_SIZE
=
4
*
1024
;
static
const
unsigned
COMPILATION_LIFO_DEFAULT_CHUNK_SIZE
=
64
*
1024
;
ModuleGenerator
:
:
ModuleGenerator
(
ExclusiveContext
*
cx
)
:
cx_
(
cx
)
slowFuncs_
(
cx
)
lifo_
(
GENERATOR_LIFO_DEFAULT_CHUNK_SIZE
)
jcx_
(
CompileRuntime
:
:
get
(
cx
-
>
compartment
(
)
-
>
runtimeFromAnyThread
(
)
)
)
alloc_
(
&
lifo_
)
masm_
(
MacroAssembler
:
:
AsmJSToken
(
)
alloc_
)
sigs_
(
cx
)
funcEntryOffsets_
(
cx
)
exportFuncIndices_
(
cx
)
parallel_
(
false
)
outstanding_
(
0
)
tasks_
(
cx
)
freeTasks_
(
cx
)
activeFunc_
(
nullptr
)
finishedFuncs_
(
false
)
{
MOZ_ASSERT
(
IsCompilingAsmJS
(
)
)
;
}
ModuleGenerator
:
:
~
ModuleGenerator
(
)
{
if
(
parallel_
)
{
if
(
outstanding_
)
{
AutoLockHelperThreadState
lock
;
while
(
true
)
{
IonCompileTaskVector
&
worklist
=
HelperThreadState
(
)
.
wasmWorklist
(
)
;
MOZ_ASSERT
(
outstanding_
>
=
worklist
.
length
(
)
)
;
outstanding_
-
=
worklist
.
length
(
)
;
worklist
.
clear
(
)
;
IonCompileTaskVector
&
finished
=
HelperThreadState
(
)
.
wasmFinishedList
(
)
;
MOZ_ASSERT
(
outstanding_
>
=
finished
.
length
(
)
)
;
outstanding_
-
=
finished
.
length
(
)
;
finished
.
clear
(
)
;
uint32_t
numFailed
=
HelperThreadState
(
)
.
harvestFailedWasmJobs
(
)
;
MOZ_ASSERT
(
outstanding_
>
=
numFailed
)
;
outstanding_
-
=
numFailed
;
if
(
!
outstanding_
)
break
;
HelperThreadState
(
)
.
wait
(
GlobalHelperThreadState
:
:
CONSUMER
)
;
}
}
MOZ_ASSERT
(
HelperThreadState
(
)
.
wasmCompilationInProgress
)
;
HelperThreadState
(
)
.
wasmCompilationInProgress
=
false
;
}
else
{
MOZ_ASSERT
(
!
outstanding_
)
;
}
}
static
bool
ParallelCompilationEnabled
(
ExclusiveContext
*
cx
)
{
if
(
HelperThreadState
(
)
.
threadCount
<
=
1
|
|
!
CanUseExtraThreads
(
)
)
return
false
;
return
!
cx
-
>
isJSContext
(
)
|
|
cx
-
>
asJSContext
(
)
-
>
runtime
(
)
-
>
canUseOffthreadIonCompilation
(
)
;
}
bool
ModuleGenerator
:
:
init
(
)
{
module_
=
cx_
-
>
make_unique
<
ModuleData
>
(
)
;
if
(
!
module_
)
return
false
;
module_
-
>
globalBytes
=
InitialGlobalDataBytes
;
module_
-
>
compileArgs
=
CompileArgs
(
cx_
)
;
link_
=
cx_
-
>
make_unique
<
StaticLinkData
>
(
)
;
if
(
!
link_
)
return
false
;
if
(
!
sigs_
.
init
(
)
)
return
false
;
uint32_t
numTasks
;
if
(
ParallelCompilationEnabled
(
cx_
)
&
&
HelperThreadState
(
)
.
wasmCompilationInProgress
.
compareExchange
(
false
true
)
)
{
#
ifdef
DEBUG
{
AutoLockHelperThreadState
lock
;
MOZ_ASSERT
(
!
HelperThreadState
(
)
.
wasmFailed
(
)
)
;
MOZ_ASSERT
(
HelperThreadState
(
)
.
wasmWorklist
(
)
.
empty
(
)
)
;
MOZ_ASSERT
(
HelperThreadState
(
)
.
wasmFinishedList
(
)
.
empty
(
)
)
;
}
#
endif
parallel_
=
true
;
numTasks
=
HelperThreadState
(
)
.
maxWasmCompilationThreads
(
)
;
}
else
{
numTasks
=
1
;
}
if
(
!
tasks_
.
initCapacity
(
numTasks
)
)
return
false
;
JSRuntime
*
runtime
=
cx_
-
>
compartment
(
)
-
>
runtimeFromAnyThread
(
)
;
for
(
size_t
i
=
0
;
i
<
numTasks
;
i
+
+
)
tasks_
.
infallibleEmplaceBack
(
runtime
args
(
)
COMPILATION_LIFO_DEFAULT_CHUNK_SIZE
)
;
if
(
!
freeTasks_
.
reserve
(
numTasks
)
)
return
false
;
for
(
size_t
i
=
0
;
i
<
numTasks
;
i
+
+
)
freeTasks_
.
infallibleAppend
(
&
tasks_
[
i
]
)
;
return
true
;
}
bool
ModuleGenerator
:
:
allocateGlobalBytes
(
uint32_t
bytes
uint32_t
align
uint32_t
*
globalDataOffset
)
{
uint32_t
globalBytes
=
module_
-
>
globalBytes
;
uint32_t
pad
=
ComputeByteAlignment
(
globalBytes
align
)
;
if
(
UINT32_MAX
-
globalBytes
<
pad
+
bytes
)
return
false
;
globalBytes
+
=
pad
;
*
globalDataOffset
=
globalBytes
;
globalBytes
+
=
bytes
;
module_
-
>
globalBytes
=
globalBytes
;
return
true
;
}
bool
ModuleGenerator
:
:
finishOutstandingTask
(
)
{
MOZ_ASSERT
(
parallel_
)
;
IonCompileTask
*
task
=
nullptr
;
{
AutoLockHelperThreadState
lock
;
while
(
true
)
{
MOZ_ASSERT
(
outstanding_
>
0
)
;
if
(
HelperThreadState
(
)
.
wasmFailed
(
)
)
return
false
;
if
(
!
HelperThreadState
(
)
.
wasmFinishedList
(
)
.
empty
(
)
)
{
outstanding_
-
-
;
task
=
HelperThreadState
(
)
.
wasmFinishedList
(
)
.
popCopy
(
)
;
break
;
}
HelperThreadState
(
)
.
wait
(
GlobalHelperThreadState
:
:
CONSUMER
)
;
}
}
return
finishTask
(
task
)
;
}
bool
ModuleGenerator
:
:
finishTask
(
IonCompileTask
*
task
)
{
const
FuncBytecode
&
func
=
task
-
>
func
(
)
;
FuncCompileResults
&
results
=
task
-
>
results
(
)
;
uint32_t
offsetInWhole
=
masm_
.
size
(
)
;
results
.
offsets
(
)
.
offsetBy
(
offsetInWhole
)
;
if
(
func
.
index
(
)
>
=
funcEntryOffsets_
.
length
(
)
)
{
if
(
!
funcEntryOffsets_
.
resize
(
func
.
index
(
)
+
1
)
)
return
false
;
}
funcEntryOffsets_
[
func
.
index
(
)
]
=
results
.
offsets
(
)
.
nonProfilingEntry
;
DebugOnly
<
size_t
>
sizeBefore
=
masm_
.
size
(
)
;
if
(
!
masm_
.
asmMergeWith
(
results
.
masm
(
)
)
)
return
false
;
MOZ_ASSERT
(
masm_
.
size
(
)
=
=
offsetInWhole
+
results
.
masm
(
)
.
size
(
)
)
;
CacheableChars
funcName
=
StringToNewUTF8CharsZ
(
cx_
*
func
.
name
(
)
)
;
if
(
!
funcName
)
return
false
;
uint32_t
nameIndex
=
module_
-
>
funcNames
.
length
(
)
;
if
(
!
module_
-
>
funcNames
.
emplaceBack
(
Move
(
funcName
)
)
)
return
false
;
if
(
!
module_
-
>
codeRanges
.
emplaceBack
(
nameIndex
func
.
line
(
)
results
.
offsets
(
)
)
)
return
false
;
unsigned
totalTime
=
func
.
generateTime
(
)
+
results
.
compileTime
(
)
;
if
(
totalTime
>
=
SlowFunction
:
:
msThreshold
)
{
if
(
!
slowFuncs_
.
emplaceBack
(
func
.
name
(
)
totalTime
func
.
line
(
)
func
.
column
(
)
)
)
return
false
;
}
freeTasks_
.
infallibleAppend
(
task
)
;
return
true
;
}
const
LifoSig
*
ModuleGenerator
:
:
newLifoSig
(
const
MallocSig
&
sig
)
{
SigSet
:
:
AddPtr
p
=
sigs_
.
lookupForAdd
(
sig
)
;
if
(
p
)
return
*
p
;
LifoSig
*
lifoSig
=
LifoSig
:
:
new_
(
lifo_
sig
)
;
if
(
!
lifoSig
|
|
!
sigs_
.
add
(
p
lifoSig
)
)
return
nullptr
;
return
lifoSig
;
}
bool
ModuleGenerator
:
:
allocateGlobalVar
(
ValType
type
uint32_t
*
globalDataOffset
)
{
unsigned
width
=
0
;
switch
(
type
)
{
case
wasm
:
:
ValType
:
:
I32
:
case
wasm
:
:
ValType
:
:
F32
:
width
=
4
;
break
;
case
wasm
:
:
ValType
:
:
I64
:
case
wasm
:
:
ValType
:
:
F64
:
width
=
8
;
break
;
case
wasm
:
:
ValType
:
:
I32x4
:
case
wasm
:
:
ValType
:
:
F32x4
:
case
wasm
:
:
ValType
:
:
B32x4
:
width
=
16
;
break
;
}
return
allocateGlobalBytes
(
width
width
globalDataOffset
)
;
}
bool
ModuleGenerator
:
:
declareImport
(
MallocSig
&
&
sig
unsigned
*
index
)
{
static_assert
(
Module
:
:
SizeOfImportExit
%
sizeof
(
void
*
)
=
=
0
"
word
aligned
"
)
;
uint32_t
globalDataOffset
;
if
(
!
allocateGlobalBytes
(
Module
:
:
SizeOfImportExit
sizeof
(
void
*
)
&
globalDataOffset
)
)
return
false
;
*
index
=
unsigned
(
module_
-
>
imports
.
length
(
)
)
;
return
module_
-
>
imports
.
emplaceBack
(
Move
(
sig
)
globalDataOffset
)
;
}
uint32_t
ModuleGenerator
:
:
numDeclaredImports
(
)
const
{
return
module_
-
>
imports
.
length
(
)
;
}
uint32_t
ModuleGenerator
:
:
importExitGlobalDataOffset
(
uint32_t
index
)
const
{
return
module_
-
>
imports
[
index
]
.
exitGlobalDataOffset
(
)
;
}
const
MallocSig
&
ModuleGenerator
:
:
importSig
(
uint32_t
index
)
const
{
return
module_
-
>
imports
[
index
]
.
sig
(
)
;
}
bool
ModuleGenerator
:
:
defineImport
(
uint32_t
index
ProfilingOffsets
interpExit
ProfilingOffsets
jitExit
)
{
Import
&
import
=
module_
-
>
imports
[
index
]
;
import
.
initInterpExitOffset
(
interpExit
.
begin
)
;
import
.
initJitExitOffset
(
jitExit
.
begin
)
;
return
module_
-
>
codeRanges
.
emplaceBack
(
CodeRange
:
:
ImportInterpExit
interpExit
)
&
&
module_
-
>
codeRanges
.
emplaceBack
(
CodeRange
:
:
ImportJitExit
jitExit
)
;
}
bool
ModuleGenerator
:
:
declareExport
(
MallocSig
&
&
sig
uint32_t
funcIndex
)
{
return
module_
-
>
exports
.
emplaceBack
(
Move
(
sig
)
)
&
&
exportFuncIndices_
.
append
(
funcIndex
)
;
}
uint32_t
ModuleGenerator
:
:
exportFuncIndex
(
uint32_t
index
)
const
{
return
exportFuncIndices_
[
index
]
;
}
const
MallocSig
&
ModuleGenerator
:
:
exportSig
(
uint32_t
index
)
const
{
return
module_
-
>
exports
[
index
]
.
sig
(
)
;
}
uint32_t
ModuleGenerator
:
:
numDeclaredExports
(
)
const
{
return
module_
-
>
exports
.
length
(
)
;
}
bool
ModuleGenerator
:
:
defineExport
(
uint32_t
index
Offsets
offsets
)
{
module_
-
>
exports
[
index
]
.
initStubOffset
(
offsets
.
begin
)
;
return
module_
-
>
codeRanges
.
emplaceBack
(
CodeRange
:
:
Entry
offsets
)
;
}
bool
ModuleGenerator
:
:
startFunc
(
PropertyName
*
name
unsigned
line
unsigned
column
UniqueBytecode
*
recycled
FunctionGenerator
*
fg
)
{
MOZ_ASSERT
(
!
activeFunc_
)
;
MOZ_ASSERT
(
!
finishedFuncs_
)
;
if
(
freeTasks_
.
empty
(
)
&
&
!
finishOutstandingTask
(
)
)
return
false
;
IonCompileTask
*
task
=
freeTasks_
.
popCopy
(
)
;
task
-
>
reset
(
recycled
)
;
fg
-
>
name_
=
name
;
fg
-
>
line_
=
line
;
fg
-
>
column_
=
column
;
fg
-
>
m_
=
this
;
fg
-
>
task_
=
task
;
activeFunc_
=
fg
;
return
true
;
}
bool
ModuleGenerator
:
:
finishFunc
(
uint32_t
funcIndex
const
LifoSig
&
sig
UniqueBytecode
bytecode
unsigned
generateTime
FunctionGenerator
*
fg
)
{
MOZ_ASSERT
(
activeFunc_
=
=
fg
)
;
UniqueFuncBytecode
func
=
cx_
-
>
make_unique
<
FuncBytecode
>
(
fg
-
>
name_
fg
-
>
line_
fg
-
>
column_
Move
(
fg
-
>
callSourceCoords_
)
funcIndex
sig
Move
(
bytecode
)
Move
(
fg
-
>
localVars_
)
generateTime
)
;
if
(
!
func
)
return
false
;
fg
-
>
task_
-
>
init
(
Move
(
func
)
)
;
if
(
parallel_
)
{
if
(
!
StartOffThreadWasmCompile
(
cx_
fg
-
>
task_
)
)
return
false
;
outstanding_
+
+
;
}
else
{
if
(
!
IonCompileFunction
(
fg
-
>
task_
)
)
return
false
;
if
(
!
finishTask
(
fg
-
>
task_
)
)
return
false
;
}
fg
-
>
m_
=
nullptr
;
fg
-
>
task_
=
nullptr
;
activeFunc_
=
nullptr
;
return
true
;
}
bool
ModuleGenerator
:
:
finishFuncs
(
)
{
MOZ_ASSERT
(
!
activeFunc_
)
;
MOZ_ASSERT
(
!
finishedFuncs_
)
;
while
(
outstanding_
>
0
)
{
if
(
!
finishOutstandingTask
(
)
)
return
false
;
}
for
(
CallSiteAndTarget
&
cs
:
masm_
.
callSites
(
)
)
{
if
(
!
cs
.
isInternal
(
)
)
continue
;
MOZ_ASSERT
(
cs
.
kind
(
)
=
=
CallSiteDesc
:
:
Relative
)
;
uint32_t
callerOffset
=
cs
.
returnAddressOffset
(
)
;
uint32_t
calleeOffset
=
funcEntryOffsets_
[
cs
.
targetIndex
(
)
]
;
masm_
.
patchCall
(
callerOffset
calleeOffset
)
;
}
module_
-
>
functionBytes
=
masm_
.
size
(
)
;
finishedFuncs_
=
true
;
return
true
;
}
bool
ModuleGenerator
:
:
declareFuncPtrTable
(
uint32_t
numElems
uint32_t
*
index
)
{
if
(
numElems
>
1024
*
1024
)
return
false
;
uint32_t
globalDataOffset
;
if
(
!
allocateGlobalBytes
(
numElems
*
sizeof
(
void
*
)
sizeof
(
void
*
)
&
globalDataOffset
)
)
return
false
;
StaticLinkData
:
:
FuncPtrTableVector
&
tables
=
link_
-
>
funcPtrTables
;
*
index
=
tables
.
length
(
)
;
if
(
!
tables
.
emplaceBack
(
globalDataOffset
)
)
return
false
;
if
(
!
tables
.
back
(
)
.
elemOffsets
.
resize
(
numElems
)
)
return
false
;
return
true
;
}
uint32_t
ModuleGenerator
:
:
funcPtrTableGlobalDataOffset
(
uint32_t
index
)
const
{
return
link_
-
>
funcPtrTables
[
index
]
.
globalDataOffset
;
}
void
ModuleGenerator
:
:
defineFuncPtrTable
(
uint32_t
index
const
Vector
<
uint32_t
>
&
elemFuncIndices
)
{
MOZ_ASSERT
(
finishedFuncs_
)
;
StaticLinkData
:
:
FuncPtrTable
&
table
=
link_
-
>
funcPtrTables
[
index
]
;
MOZ_ASSERT
(
table
.
elemOffsets
.
length
(
)
=
=
elemFuncIndices
.
length
(
)
)
;
for
(
size_t
i
=
0
;
i
<
elemFuncIndices
.
length
(
)
;
i
+
+
)
table
.
elemOffsets
[
i
]
=
funcEntryOffsets_
[
elemFuncIndices
[
i
]
]
;
}
bool
ModuleGenerator
:
:
defineInlineStub
(
Offsets
offsets
)
{
MOZ_ASSERT
(
finishedFuncs_
)
;
return
module_
-
>
codeRanges
.
emplaceBack
(
CodeRange
:
:
Inline
offsets
)
;
}
bool
ModuleGenerator
:
:
defineSyncInterruptStub
(
ProfilingOffsets
offsets
)
{
MOZ_ASSERT
(
finishedFuncs_
)
;
return
module_
-
>
codeRanges
.
emplaceBack
(
CodeRange
:
:
Interrupt
offsets
)
;
}
bool
ModuleGenerator
:
:
defineAsyncInterruptStub
(
Offsets
offsets
)
{
MOZ_ASSERT
(
finishedFuncs_
)
;
link_
-
>
pod
.
interruptOffset
=
offsets
.
begin
;
return
module_
-
>
codeRanges
.
emplaceBack
(
CodeRange
:
:
Inline
offsets
)
;
}
bool
ModuleGenerator
:
:
defineOutOfBoundsStub
(
Offsets
offsets
)
{
MOZ_ASSERT
(
finishedFuncs_
)
;
link_
-
>
pod
.
outOfBoundsOffset
=
offsets
.
begin
;
return
module_
-
>
codeRanges
.
emplaceBack
(
CodeRange
:
:
Inline
offsets
)
;
}
bool
ModuleGenerator
:
:
finish
(
HeapUsage
heapUsage
MutedErrorsBool
mutedErrors
CacheableChars
filename
CacheableTwoByteChars
displayURL
UniqueModuleData
*
module
UniqueStaticLinkData
*
linkData
SlowFunctionVector
*
slowFuncs
)
{
MOZ_ASSERT
(
!
activeFunc_
)
;
MOZ_ASSERT
(
finishedFuncs_
)
;
module_
-
>
heapUsage
=
heapUsage
;
module_
-
>
mutedErrors
=
mutedErrors
;
module_
-
>
filename
=
Move
(
filename
)
;
if
(
!
GenerateStubs
(
*
this
UsesHeap
(
heapUsage
)
)
)
return
false
;
masm_
.
finish
(
)
;
if
(
masm_
.
oom
(
)
)
return
false
;
module_
-
>
codeBytes
=
AlignBytes
(
masm_
.
bytesNeeded
(
)
AsmJSPageSize
)
;
module_
-
>
globalBytes
=
AlignBytes
(
module_
-
>
globalBytes
AsmJSPageSize
)
;
module_
-
>
code
=
AllocateCode
(
cx_
module_
-
>
totalBytes
(
)
)
;
if
(
!
module_
-
>
code
)
return
false
;
AutoFlushICache
afc
(
"
ModuleGenerator
:
:
finish
"
true
)
;
uint8_t
*
code
=
module_
-
>
code
.
get
(
)
;
masm_
.
executableCopy
(
code
)
;
MOZ_ASSERT
(
masm_
.
jumpRelocationTableBytes
(
)
=
=
0
)
;
MOZ_ASSERT
(
masm_
.
dataRelocationTableBytes
(
)
=
=
0
)
;
MOZ_ASSERT
(
masm_
.
preBarrierTableBytes
(
)
=
=
0
)
;
MOZ_ASSERT
(
!
masm_
.
hasSelfReference
(
)
)
;
if
(
!
module_
-
>
callSites
.
appendAll
(
masm_
.
callSites
(
)
)
)
return
false
;
module_
-
>
heapAccesses
=
masm_
.
extractHeapAccesses
(
)
;
StaticLinkData
:
:
SymbolicLinkArray
&
symbolicLinks
=
link_
-
>
symbolicLinks
;
for
(
size_t
i
=
0
;
i
<
masm_
.
numAsmJSAbsoluteAddresses
(
)
;
i
+
+
)
{
AsmJSAbsoluteAddress
src
=
masm_
.
asmJSAbsoluteAddress
(
i
)
;
if
(
!
symbolicLinks
[
src
.
target
]
.
append
(
src
.
patchAt
.
offset
(
)
)
)
return
false
;
}
for
(
size_t
i
=
0
;
i
<
masm_
.
numCodeLabels
(
)
;
i
+
+
)
{
CodeLabel
cl
=
masm_
.
codeLabel
(
i
)
;
StaticLinkData
:
:
InternalLink
link
(
StaticLinkData
:
:
InternalLink
:
:
CodeLabel
)
;
link
.
patchAtOffset
=
masm_
.
labelToPatchOffset
(
*
cl
.
patchAt
(
)
)
;
link
.
targetOffset
=
cl
.
target
(
)
-
>
offset
(
)
;
if
(
!
link_
-
>
internalLinks
.
append
(
link
)
)
return
false
;
}
#
if
defined
(
JS_CODEGEN_X86
)
for
(
size_t
i
=
0
;
i
<
masm_
.
numAsmJSGlobalAccesses
(
)
;
i
+
+
)
{
AsmJSGlobalAccess
a
=
masm_
.
asmJSGlobalAccess
(
i
)
;
StaticLinkData
:
:
InternalLink
link
(
StaticLinkData
:
:
InternalLink
:
:
RawPointer
)
;
link
.
patchAtOffset
=
masm_
.
labelToPatchOffset
(
a
.
patchAt
)
;
link
.
targetOffset
=
module_
-
>
codeBytes
+
a
.
globalDataOffset
;
if
(
!
link_
-
>
internalLinks
.
append
(
link
)
)
return
false
;
}
#
endif
#
if
defined
(
JS_CODEGEN_MIPS32
)
|
|
defined
(
JS_CODEGEN_MIPS64
)
for
(
size_t
i
=
0
;
i
<
masm_
.
numLongJumps
(
)
;
i
+
+
)
{
size_t
off
=
masm_
.
longJump
(
i
)
;
StaticLinkData
:
:
InternalLink
link
(
StaticLinkData
:
:
InternalLink
:
:
InstructionImmediate
)
;
link
.
patchAtOffset
=
off
;
link
.
targetOffset
=
Assembler
:
:
ExtractInstructionImmediate
(
code
+
off
)
-
uintptr_t
(
code
)
;
if
(
!
link_
-
>
internalLinks
.
append
(
link
)
)
return
false
;
}
#
endif
#
if
defined
(
JS_CODEGEN_X64
)
uint8_t
*
globalData
=
code
+
module_
-
>
codeBytes
;
for
(
size_t
i
=
0
;
i
<
masm_
.
numAsmJSGlobalAccesses
(
)
;
i
+
+
)
{
AsmJSGlobalAccess
a
=
masm_
.
asmJSGlobalAccess
(
i
)
;
masm_
.
patchAsmJSGlobalAccess
(
a
.
patchAt
code
globalData
a
.
globalDataOffset
)
;
}
#
endif
*
module
=
Move
(
module_
)
;
*
linkData
=
Move
(
link_
)
;
*
slowFuncs
=
Move
(
slowFuncs_
)
;
return
true
;
}
